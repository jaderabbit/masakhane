{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Masakhane - MT for African Languages",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Igc5itf-xMGj"
      },
      "source": [
        "# Masakhane - Machine Translation for African Languages (Using JoeyNMT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x4fXCKCf36IK"
      },
      "source": [
        "## Note before beginning:\n",
        "### - The idea is that you should be able to make minimal changes to this in order to get SOME result for your own translation corpus. \n",
        "\n",
        "### - The tl;dr: Go to the **\"TODO\"** comments which will tell you what to update to get up and running\n",
        "\n",
        "### - If you actually want to have a clue what you're doing, read the text and peek at the links\n",
        "\n",
        "### - With 100 epochs, it should take around 7 hours to run in Google Colab\n",
        "\n",
        "### - Once you've gotten a result for your language, please attach and email your notebook that generated it to masakhanetranslation@gmail.com\n",
        "\n",
        "### - If you care enough and get a chance, doing a brief background on your language would be amazing. See examples in  [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l929HimrxS0a"
      },
      "source": [
        "## Retrieve your data & make a parallel corpus\n",
        "\n",
        "So the easiest way to get your data is to use some unix tools such as `wget` to fetch it from a url, and then `gunzip` to extract it if it's zipped. \n",
        "\n",
        "Parallel corpuses come in many formats. The ideal corpus comes with 2 files: `file.source` and `file.target` where `\"source\"` is your source language, such as `en` and `\"target\"` is your target language, such as `xh` (Xhosa)\n",
        "\n",
        "Sometimes they come in a **.tmx** file a.k.a a **translation memory file**. This is an xml structure which will include the sentences in your target language and your source language in a single file. Thankfully, Jade wrote a silly `tmx2dataframe` package which converts your tmx file to a pandas dataframe. #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izh0h7wU5Q4-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0852a726-69fb-4c2c-e7c1-3d79154037aa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cn3tgQLzUxwn",
        "colab": {}
      },
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\n",
        "import os\n",
        "source_language = \"en\"\n",
        "target_language = \"xh\"\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = tag\n",
        "\n",
        "# This will save it to a folder in our gdrive instead!\n",
        "!mkdir -p \"/content/drive/My Drive/masakhane/$src-$tgt-$tag\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/masakhane/%s-%s\" % (source_language, target_language)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBSgJHEw7Nvx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "330551b6-94c6-440d-a312-f5e8d9b50c78"
      },
      "source": [
        "!echo $gdrive_path"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masakhane/en-xh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3CNdwLBCfSIl",
        "outputId": "2be091a9-b659-498b-8480-f24b10682bbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "# Downloading and unzipping our xhosa corpus\n",
        "# TODO: You'll need to download & extract your own corpus here! \n",
        "! wget \"http://opus.nlpl.eu/download.php?f=XhosaNavy/v1/tmx/en-xh.tmx.gz\" -O en-xh.tmx.gz\n",
        "! gunzip -k  en-xh.tmx.gz\n",
        "! ls -lh\n",
        "\n",
        "# This is useful if you end up having to use a tmx file,\n",
        "! pip install tmx2dataframe"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-05 12:25:50--  http://opus.nlpl.eu/download.php?f=XhosaNavy/v1/tmx/en-xh.tmx.gz\n",
            "Resolving opus.nlpl.eu (opus.nlpl.eu)... 193.166.25.9\n",
            "Connecting to opus.nlpl.eu (opus.nlpl.eu)|193.166.25.9|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://object.pouta.csc.fi/OPUS-XhosaNavy/v1/tmx/en-xh.tmx.gz [following]\n",
            "--2019-10-05 12:25:51--  https://object.pouta.csc.fi/OPUS-XhosaNavy/v1/tmx/en-xh.tmx.gz\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3526058 (3.4M) [application/gzip]\n",
            "Saving to: ‘en-xh.tmx.gz’\n",
            "\n",
            "en-xh.tmx.gz        100%[===================>]   3.36M  2.83MB/s    in 1.2s    \n",
            "\n",
            "2019-10-05 12:25:53 (2.83 MB/s) - ‘en-xh.tmx.gz’ saved [3526058/3526058]\n",
            "\n",
            "gzip: en-xh.tmx already exists; do you wish to overwrite (y or n)? y\n",
            "total 46M\n",
            "-rw-r--r--  1 root root  892 Oct  5 11:57 bpe.codes.4000\n",
            "-rw-r--r--  1 root root 226K Oct  5 11:57 dev.bpe.en\n",
            "-rw-r--r--  1 root root 255K Oct  5 11:57 dev.bpe.xh\n",
            "-rw-r--r--  1 root root 110K Oct  5 11:51 dev.en\n",
            "-rw-r--r--  1 root root 115K Oct  5 11:51 dev.xh\n",
            "-rw-r--r--  1 root root  14M Nov 17  2018 en-xh.tmx\n",
            "-rw-r--r--  1 root root 3.4M Nov 17  2018 en-xh.tmx.gz\n",
            "drwx------  2 root root 4.0K Oct  5 11:49 gdrive\n",
            "drwxr-xr-x 10 root root 4.0K Oct  5 11:53 joeynmt\n",
            "drwxr-xr-x  1 root root 4.0K Aug 27 16:17 sample_data\n",
            "-rw-r--r--  1 root root    0 Oct  5 11:53 test.bpe.en\n",
            "-rw-r--r--  1 root root 248K Oct  5 11:57 test.bp.en\n",
            "-rw-r--r--  1 root root    0 Oct  5 11:53 test.bpe.xh\n",
            "-rw-r--r--  1 root root 279K Oct  5 11:57 test.bp.xh\n",
            "-rw-r--r--  1 root root 122K Oct  5 11:51 test.en\n",
            "-rw-r--r--  1 root root 128K Oct  5 11:51 test.xh\n",
            "-rw-r--r--  1 root root 8.3M Oct  5 11:57 train.bpe.en\n",
            "-rw-r--r--  1 root root  10M Oct  5 11:57 train.bpe.xh\n",
            "-rw-r--r--  1 root root 4.0M Oct  5 11:51 train.en\n",
            "-rw-r--r--  1 root root 4.6M Oct  5 11:51 train.xh\n",
            "-rw-r--r--  1 root root    0 Oct  5 11:56 vocab.en\n",
            "-rw-r--r--  1 root root    0 Oct  5 11:56 vocab.xh\n",
            "Requirement already satisfied: tmx2dataframe in /usr/local/lib/python3.6/dist-packages (0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6Zwi3M-RwtFd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "3dcd67ce-a9d3-49d5-a78a-55dbbd1c666a"
      },
      "source": [
        "import pandas as pd\n",
        "from tmx2dataframe import tmx2dataframe\n",
        "\n",
        "# TODO:\n",
        "# If your source is a translation memory file (tmx file), then the one file contains both your target and source language. If so, set tmx_file = \"your file here\"\n",
        "# Comment if you have 2 files, instead of the tmx file\n",
        "tmx_file = \"en-xh.tmx\"\n",
        "source_file = None\n",
        "target_file = None \n",
        "\n",
        "# Uncomment if you have 2 files and set source_file and target_file to the path of your parallel corpus files\n",
        "# tmx_file = None\n",
        "# source_file = 'file.src'\n",
        "# target_file = 'file.tgt'\n",
        "\n",
        "# Read in the files so we have an appropriate python dataframe\n",
        "if tmx_file is not None:\n",
        "    # tmx files\n",
        "    metadata, df = tmx2dataframe.read(tmx_file)\n",
        "else:\n",
        "    # For 2 parallel files\n",
        "    df_src = pd.read(\"file.src\", header=None, names=[\"source_sentence\"])\n",
        "    df_tgt = pd.read(\"file.tgt\", header=None, names=[\"target_sentence\"])\n",
        "    df = pd.concat([df_src, df_tgt], axis=1)\n",
        "    df[\"source_language\"] = source_language\n",
        "    df[\"target_language\"] = target_language\n",
        "\n",
        "# Have a peak at the data\n",
        "df.head(3)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_language</th>\n",
              "      <th>source_sentence</th>\n",
              "      <th>target_language</th>\n",
              "      <th>target_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>en</td>\n",
              "      <td>Rope and its Usage</td>\n",
              "      <td>xh</td>\n",
              "      <td>Intambo nomsebenzi ewenzayo.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>en</td>\n",
              "      <td>In this chapter are described the various type...</td>\n",
              "      <td>xh</td>\n",
              "      <td>Kwesi sahluko sixelelwa ngendindi zeentambo at...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>en</td>\n",
              "      <td>The chapter has been divided into seven sectio...</td>\n",
              "      <td>xh</td>\n",
              "      <td>Esi sahluko sahlulwa - hlulwe kasixhenxe, zica...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  source_language  ...                                    target_sentence\n",
              "0              en  ...                       Intambo nomsebenzi ewenzayo.\n",
              "1              en  ...  Kwesi sahluko sixelelwa ngendindi zeentambo at...\n",
              "2              en  ...  Esi sahluko sahlulwa - hlulwe kasixhenxe, zica...\n",
              "\n",
              "[3 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hxxBOCA-xXhy",
        "colab": {}
      },
      "source": [
        "# This section does the split between train/test/dev for the parallel corpora then saves them as separate files\n",
        "# We use 1000 dev test and 1000 test set. In practice, it's useful to use an external test set\n",
        "\n",
        "\n",
        "# Do the split between dev/test/train and create parallel corpora\n",
        "num_dev_patterns = 1000\n",
        "num_test_patterns = 1000\n",
        "\n",
        "# Lower case the corpora\n",
        "df[\"source_sentence\"] = df[\"source_sentence\"].str.lower()\n",
        "df[\"target_sentence\"] = df[\"target_sentence\"].str.lower()\n",
        "\n",
        "\n",
        "devtest = df.tail(num_dev_patterns + num_test_patterns)\n",
        "test = devtest.tail(num_test_patterns) # Herman\n",
        "dev = devtest.head(num_dev_patterns)  # Herman: Error in original\n",
        "stripped = df.drop(df.tail(num_dev_patterns + num_test_patterns).index)\n",
        "\n",
        "stripped[[\"source_sentence\"]].to_csv(\"train.en\", index=False)\n",
        "stripped[[\"target_sentence\"]].to_csv(\"train.xh\", index=False)\n",
        "\n",
        "dev[[\"source_sentence\"]].to_csv(\"dev.en\", index=False)\n",
        "dev[[\"target_sentence\"]].to_csv(\"dev.xh\", index=False)\n",
        "\n",
        "test[[\"source_sentence\"]].to_csv(\"test.en\", index=False)\n",
        "test[[\"target_sentence\"]].to_csv(\"test.xh\", index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epeCydmCyS8X"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Installation of JoeyNMT\n",
        "\n",
        "JoeyNMT is a simple, minimalist NMT package which is useful for learning and teaching. Check out the documentation for JoeyNMT [here](https://joeynmt.readthedocs.io)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iBRMm4kMxZ8L",
        "colab": {}
      },
      "source": [
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AaE77Tcppex9"
      },
      "source": [
        "# Preprocessing the Data into Subword BPE Tokens\n",
        "\n",
        "- One of the most powerful improvements for agglutinative languages (a feature of most Bantu languages) is using BPE tokenization [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n",
        "\n",
        "- It was also shown that by optimizing the umber of BPE codes we significantly improve results for low-resourced languages [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021) [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)\n",
        "\n",
        "- Below we have the scripts for doing BPE tokenization of our data. We use 4000 tokens as recommended by [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021). You do not need to change anything. Simply running the below will be suitable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H-TyjtmXB1mL",
        "outputId": "6979e2a7-22c4-4789-c70c-8433bce224b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
        "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
        "\n",
        "# Do subword NMT\n",
        "from os import path\n",
        "\n",
        "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
        "!subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
        "\n",
        "# Create directory, move everyone we care about to the correct location\n",
        "! mkdir -p $data_path\n",
        "! cp train.* $data_path\n",
        "! cp test.* $data_path\n",
        "! cp dev.* $data_path\n",
        "! cp bpe.codes.4000 $data_path\n",
        "! ls $data_path\n",
        "\n",
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\"\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# Some output\n",
        "! echo \"BPE Xhosa Sentences\"\n",
        "! tail -n 5 test.bpe.$tgt\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! tail -n 10 joeynmt/data/enxh/vocab.txt"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.en\t     test.bp.en   test.en\ttrain.bpe.xh  vocab.txt\n",
            "dev.bpe.en\tdev.xh\t     test.bpe.xh  test.xh\ttrain.en\n",
            "dev.bpe.xh\ttest.bpe.en  test.bp.xh   train.bpe.en\ttrain.xh\n",
            "cp: target 'Drive/masakhane/en-xh' is not a directory\n",
            "cp: target 'Drive/masakhane/en-xh' is not a directory\n",
            "cp: target 'Drive/masakhane/en-xh' is not a directory\n",
            "cp: target 'Drive/masakhane/en-xh' is not a directory\n",
            "ls: cannot access '/content/drive/My': No such file or directory\n",
            "ls: cannot access 'Drive/masakhane/en-xh': No such file or directory\n",
            "BPE Xhosa Sentences\n",
            "is@@ enzo soku@@ jika inqanawa kwelinye icala kwindawo yayo yokum@@ isa (@@ uku@@ jika kwisiphelo ukuya kwes@@ inye isiph@@ el@@ o@@ ).\n",
            "\"um@@ linganiselo w@@ enyawo ezint@@ andathu uling@@ ana ne ft ezint@@ and@@ athu. umlinganiselo w@@ enyawo ezint@@ andathu ing@@ um@@ v@@ o jikelele wom@@ linganiselo w@@ entamb@@ o, intambo y@@ entsimb@@ i, ubun@@ zulu b@@ amanzi kunye nes@@ and@@ i.\"\n",
            "\"xa ints@@ on@@ el@@ e@@ o yokubophelela inqanawa iling@@ ana nem@@ ay@@ ile ezil@@ ishumi (@@ im@@ ay@@ ile zas@@ elw@@ and@@ le@@ ) kwaye ilinganis@@ elwe ne@@ 6@@ 8@@ 0@@ f@@ t, okanye nje kwi@@ 20@@ 0 y@@ d. ngum@@ v@@ o woku@@ linganis@@ elela im@@ igama om@@ f@@ utsh@@ ane. (@@ ubude be ankile y@@ ents@@ ontela yokubophelela inqanawa kuqala bab@@ ukh@@ e bay@@ i@@ 10@@ 1 y@@ enyawo ezint@@ andathu (@@ 60@@ 6 f@@ t@@ ), ubude be@@ ankile y@@ ents@@ ontela yoku@@ b@@ ph@@ elela inqanawa yez@@ inqanawa zikh@@ ona iy@@ ash@@ iy@@ an ngok@@ obung@@ akan@@ an@@ a@@ i benqanawa kwaye ayin@@ akun@@ yam@@ ez@@ ela un@@ xulum@@ ano kum@@ linganiselo w@@ ents@@ ont@@ elo.\"\n",
            "\"im@@ ay@@ ile enye y@@ olwandle iling@@ ana kunye ne 6@@ ,@@ 0@@ 8@@ 0 f@@ t, okanye nje 2@@ ,@@ 00@@ 00 y@@ d. ing@@ umlinganiselo wom@@ gama om@@ de@@ .\"\n",
            "\"is@@ at@@ ya senqanawa ngum@@ v@@ o ol@@ ing@@ ana m@@ em@@ ay@@ ile yas@@ elwandle ng@@ ey@@ ure. umzekelo inqanawa ing@@ abal@@ eka ngesantya es@@ ik@@ u 15@@ . (@@ xa isantya esib@@ onisa ng@@ ey@@ ure s@@ iph@@ os@@ isa kufuneka sing@@ aph@@ inde sis@@ ety@@ en@@ n@@ z@@ iswe uku@@ ch@@ aza is@@ anty@@ a@@ .@@ ) el@@ igama ling@@ undoqo lis@@ us@@ el@@ akw@@ indlela es@@ etyenzisw@@ ayo yoku@@ linganisa isantya ng@@ ents@@ uku soku@@ hamba ng@@ enqanawa, xa iqh@@ ek@@ eza lo m@@ thi li@@ the l@@ ach@@ amath@@ ela kumgca l@@ iph@@ os@@ wa emanzin@@ i, un@@ inzi lwes@@ antya senqanawa olun@@ eth@@ uba ol@@ uling@@ anayo kumgca o@@ gq@@ itha kum@@ va wenqanawa kwixesha elim@@ isiweyo in@@ ika isantya n@@ qanaw@@ a.\"\n",
            "Combined BPE Vocab\n",
            "uld\n",
            "\n",
            "ü@@\n",
            "…@@\n",
            ">\n",
            "strum@@\n",
            "…\n",
            "}\n",
            "`\n",
            "@@\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlMitUHR8Qy-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d28061dd-73cc-45c7-9f59-528db3b8e985"
      },
      "source": [
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\""
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.en\t     test.bp.en   test.en\ttrain.bpe.xh\n",
            "dev.bpe.en\tdev.xh\t     test.bpe.xh  test.xh\ttrain.en\n",
            "dev.bpe.xh\ttest.bpe.en  test.bp.xh   train.bpe.en\ttrain.xh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ixmzi60WsUZ8"
      },
      "source": [
        "# Creating the JoeyNMT Config\n",
        "\n",
        "JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n",
        "\n",
        "- We used Transformer architecture \n",
        "- We set our dropout to reasonably high: 0.3 (recommended in  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
        "\n",
        "Things worth playing with:\n",
        "- The batch size (also recommended to change for low-resourced languages)\n",
        "- The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n",
        "- The decoder options (beam_size, alpha)\n",
        "- Evaluation metrics (BLEU versus Crhf4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PIs1lY2hxMsl",
        "colab": {}
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/train.bpe\"\n",
        "    dev:   \"data/{name}/dev.bpe\"\n",
        "    test:  \"data/{name}/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"noam\"            # Try switching from plateau to Noam scheduling\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    patience: 8\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0002\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30 # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 4000 # Decrease this for testing\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: True\n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 8\n",
        "        embeddings:\n",
        "            embedding_dim: 512\n",
        "            scale: True\n",
        "            dropout: 0.\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 512\n",
        "        ff_size: 2048\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 8\n",
        "        embeddings:\n",
        "            embedding_dim: 512\n",
        "            scale: True\n",
        "            dropout: 0.\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 512\n",
        "        ff_size: 2048\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pIifxE3Qzuvs"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "This single line of joeynmt runs the training using the config we made above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6ZBPFwT94WpI",
        "outputId": "883fcfbd-cb48-4da9-c626-9606c922eac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the model\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-05 14:10:33,381 Hello! This is Joey-NMT.\n",
            "2019-10-05 14:10:33,387 Total params: 46273024\n",
            "2019-10-05 14:10:33,388 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']\n",
            "2019-10-05 14:10:35,995 Loading model from /content/drive/My Drive/masakhane/en-xh/models/enxh_transformer/1.ckpt\n",
            "2019-10-05 14:10:36,393 cfg.name                           : enxh_transformer\n",
            "2019-10-05 14:10:36,393 cfg.data.src                       : en\n",
            "2019-10-05 14:10:36,393 cfg.data.trg                       : xh\n",
            "2019-10-05 14:10:36,393 cfg.data.train                     : data/enxh/train.bpe\n",
            "2019-10-05 14:10:36,394 cfg.data.dev                       : data/enxh/dev.bpe\n",
            "2019-10-05 14:10:36,394 cfg.data.test                      : data/enxh/test.bpe\n",
            "2019-10-05 14:10:36,394 cfg.data.level                     : bpe\n",
            "2019-10-05 14:10:36,394 cfg.data.lowercase                 : False\n",
            "2019-10-05 14:10:36,394 cfg.data.max_sent_length           : 100\n",
            "2019-10-05 14:10:36,394 cfg.data.src_vocab                 : data/enxh/vocab.txt\n",
            "2019-10-05 14:10:36,394 cfg.data.trg_vocab                 : data/enxh/vocab.txt\n",
            "2019-10-05 14:10:36,394 cfg.testing.beam_size              : 5\n",
            "2019-10-05 14:10:36,394 cfg.testing.alpha                  : 1.0\n",
            "2019-10-05 14:10:36,394 cfg.training.load_model            : /content/drive/My Drive/masakhane/en-xh/models/enxh_transformer/1.ckpt\n",
            "2019-10-05 14:10:36,395 cfg.training.random_seed           : 42\n",
            "2019-10-05 14:10:36,395 cfg.training.optimizer             : adam\n",
            "2019-10-05 14:10:36,395 cfg.training.normalization         : tokens\n",
            "2019-10-05 14:10:36,395 cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2019-10-05 14:10:36,395 cfg.training.scheduling            : noam\n",
            "2019-10-05 14:10:36,395 cfg.training.learning_rate_factor  : 0.5\n",
            "2019-10-05 14:10:36,395 cfg.training.learning_rate_warmup  : 1000\n",
            "2019-10-05 14:10:36,395 cfg.training.patience              : 8\n",
            "2019-10-05 14:10:36,396 cfg.training.decrease_factor       : 0.7\n",
            "2019-10-05 14:10:36,396 cfg.training.loss                  : crossentropy\n",
            "2019-10-05 14:10:36,396 cfg.training.learning_rate         : 0.0002\n",
            "2019-10-05 14:10:36,396 cfg.training.learning_rate_min     : 1e-08\n",
            "2019-10-05 14:10:36,396 cfg.training.weight_decay          : 0.0\n",
            "2019-10-05 14:10:36,396 cfg.training.label_smoothing       : 0.1\n",
            "2019-10-05 14:10:36,396 cfg.training.batch_size            : 4096\n",
            "2019-10-05 14:10:36,396 cfg.training.batch_type            : token\n",
            "2019-10-05 14:10:36,396 cfg.training.eval_batch_size       : 3600\n",
            "2019-10-05 14:10:36,396 cfg.training.eval_batch_type       : token\n",
            "2019-10-05 14:10:36,397 cfg.training.batch_multiplier      : 1\n",
            "2019-10-05 14:10:36,397 cfg.training.early_stopping_metric : ppl\n",
            "2019-10-05 14:10:36,397 cfg.training.epochs                : 1\n",
            "2019-10-05 14:10:36,397 cfg.training.validation_freq       : 1\n",
            "2019-10-05 14:10:36,397 cfg.training.logging_freq          : 100\n",
            "2019-10-05 14:10:36,397 cfg.training.eval_metric           : bleu\n",
            "2019-10-05 14:10:36,397 cfg.training.model_dir             : models/enxh_transformer\n",
            "2019-10-05 14:10:36,397 cfg.training.overwrite             : True\n",
            "2019-10-05 14:10:36,397 cfg.training.shuffle               : True\n",
            "2019-10-05 14:10:36,397 cfg.training.use_cuda              : True\n",
            "2019-10-05 14:10:36,398 cfg.training.max_output_length     : 100\n",
            "2019-10-05 14:10:36,398 cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2019-10-05 14:10:36,398 cfg.training.keep_last_ckpts       : 3\n",
            "2019-10-05 14:10:36,398 cfg.model.initializer              : xavier\n",
            "2019-10-05 14:10:36,398 cfg.model.bias_initializer         : zeros\n",
            "2019-10-05 14:10:36,398 cfg.model.init_gain                : 1.0\n",
            "2019-10-05 14:10:36,398 cfg.model.embed_initializer        : xavier\n",
            "2019-10-05 14:10:36,398 cfg.model.embed_init_gain          : 1.0\n",
            "2019-10-05 14:10:36,398 cfg.model.tied_embeddings          : True\n",
            "2019-10-05 14:10:36,398 cfg.model.tied_softmax             : True\n",
            "2019-10-05 14:10:36,399 cfg.model.encoder.type             : transformer\n",
            "2019-10-05 14:10:36,399 cfg.model.encoder.num_layers       : 6\n",
            "2019-10-05 14:10:36,399 cfg.model.encoder.num_heads        : 8\n",
            "2019-10-05 14:10:36,399 cfg.model.encoder.embeddings.embedding_dim : 512\n",
            "2019-10-05 14:10:36,399 cfg.model.encoder.embeddings.scale : True\n",
            "2019-10-05 14:10:36,399 cfg.model.encoder.embeddings.dropout : 0.0\n",
            "2019-10-05 14:10:36,399 cfg.model.encoder.hidden_size      : 512\n",
            "2019-10-05 14:10:36,399 cfg.model.encoder.ff_size          : 2048\n",
            "2019-10-05 14:10:36,400 cfg.model.encoder.dropout          : 0.3\n",
            "2019-10-05 14:10:36,400 cfg.model.decoder.type             : transformer\n",
            "2019-10-05 14:10:36,400 cfg.model.decoder.num_layers       : 6\n",
            "2019-10-05 14:10:36,400 cfg.model.decoder.num_heads        : 8\n",
            "2019-10-05 14:10:36,400 cfg.model.decoder.embeddings.embedding_dim : 512\n",
            "2019-10-05 14:10:36,400 cfg.model.decoder.embeddings.scale : True\n",
            "2019-10-05 14:10:36,400 cfg.model.decoder.embeddings.dropout : 0.0\n",
            "2019-10-05 14:10:36,400 cfg.model.decoder.hidden_size      : 512\n",
            "2019-10-05 14:10:36,400 cfg.model.decoder.ff_size          : 2048\n",
            "2019-10-05 14:10:36,401 cfg.model.decoder.dropout          : 0.3\n",
            "2019-10-05 14:10:36,401 Data set sizes: \n",
            "\ttrain 47489,\n",
            "\tvalid 1001,\n",
            "\ttest 1001\n",
            "2019-10-05 14:10:36,401 First training example:\n",
            "\t[SRC] so@@ ur@@ c@@ e@@ _@@ s@@ ent@@ ence\n",
            "\t[TRG] t@@ arg@@ et@@ _@@ s@@ ent@@ ence\n",
            "2019-10-05 14:10:36,401 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) of (6) a (7) to (8) and (9) \"@@\n",
            "2019-10-05 14:10:36,401 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) of (6) a (7) to (8) and (9) \"@@\n",
            "2019-10-05 14:10:36,401 Number of Src words (types): 4165\n",
            "2019-10-05 14:10:36,401 Number of Trg words (types): 4165\n",
            "2019-10-05 14:10:36,401 Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=8),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=8),\n",
            "\tsrc_embed=Embeddings(embedding_dim=512, vocab_size=4165),\n",
            "\ttrg_embed=Embeddings(embedding_dim=512, vocab_size=4165))\n",
            "2019-10-05 14:10:36,405 EPOCH 1\n",
            "2019-10-05 14:15:12,564 Hooray! New best validation result [ppl]!\n",
            "2019-10-05 14:15:12,565 Saving new checkpoint.\n",
            "2019-10-05 14:15:14,936 Example #0\n",
            "2019-10-05 14:15:14,936 \tSource:     source_sentence\n",
            "2019-10-05 14:15:14,936 \tReference:  target_sentence\n",
            "2019-10-05 14:15:14,936 \tHypothesis: \n",
            "2019-10-05 14:15:14,936 Example #1\n",
            "2019-10-05 14:15:14,936 \tSource:     \"at such close ranges the power of penetration of roundshot was impressive, at 30 yards an 18 pound shot would penetrate four oak planks 32.5 inches thick (just under 1 meter thick), hurling a shower of splinters up to thirty yards.\"\n",
            "2019-10-05 14:15:14,937 \tReference:  \"ngumganyana omfutshen ngolo hlobo amandla okungena kwembumbulu kwakunika umdla, kwiyadi ezingama-30 imbumbulu enobunzima obuli-18 yayinokugqobhoza amaplanga om-oki azi-intsi ezingama-32.5 ububanzi obukhupha ngaphantsi kwemitha e-1 ububanzi), ukuphosa ngaphandle iimvula yamaceba kubude obuziyadi ezingamashumi amathathu.\"\n",
            "2019-10-05 14:15:14,937 \tHypothesis: \n",
            "2019-10-05 14:15:14,937 Example #2\n",
            "2019-10-05 14:15:14,937 \tSource:     at 300 yards range a 32 pounder firing grapeshot could penetrate 5 inches of firm planking and 4 of oak.\n",
            "2019-10-05 14:15:14,937 \tReference:  kumgama ongama-300 eyadi umpu ohlohla iimbumbulu ezingama-32 odubula uhlwayelo lweebhombhu zentsimbi ezinokungena ngaphakathi kumaplanga azi intsi ezi-5 namane om-oki.\n",
            "2019-10-05 14:15:14,937 \tHypothesis: \n",
            "2019-10-05 14:15:14,937 Example #3\n",
            "2019-10-05 14:15:14,937 \tSource:     small arms\n",
            "2019-10-05 14:15:14,937 \tReference:  izixhobo ezincinci\n",
            "2019-10-05 14:15:14,937 \tHypothesis: \n",
            "2019-10-05 14:15:14,937 Validation result at epoch   1, step        2: bleu:   0.00, loss: 220568.7188, ppl: 1101.9646, duration: 277.6179s\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 41, in <module>\n",
            "    main()\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 29, in main\n",
            "    train(cfg_file=args.config_path)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 559, in train\n",
            "    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 303, in train_and_validate\n",
            "    batch_type=self.eval_batch_type\n",
            "  File \"/content/joeynmt/joeynmt/prediction.py\", line 98, in validate_on_data\n",
            "    max_output_length=max_output_length)\n",
            "  File \"/content/joeynmt/joeynmt/model.py\", line 170, in run_batch\n",
            "    max_output_length=max_output_length)\n",
            "  File \"/content/joeynmt/joeynmt/search.py\", line 43, in greedy\n",
            "    decoder, encoder_output, encoder_hidden)\n",
            "  File \"/content/joeynmt/joeynmt/search.py\", line 139, in transformer_greedy\n",
            "    trg_mask=trg_mask\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/joeynmt/joeynmt/decoders.py\", line 514, in forward\n",
            "    trg_embed.size(1)).type_as(trg_mask)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBoDS09JM807",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "!cp -r joeynmt/models/${src}${tgt}_transformer/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n94wlrCjVc17",
        "outputId": "1c9a7514-2b4e-4a9d-87c3-11b73103385f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 2\tLoss: 220568.71875\tPPL: 1101.96460\tbleu: 0.00000\tLR: 0.00000070\t*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "66WhRE9lIhoD",
        "outputId": "2d34e7cb-5462-468d-c57b-54bcdcc69b82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Test our model\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-05 14:18:07,761 -  dev bleu:   0.00 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2019-10-05 14:18:14,912 - test bleu:   0.00 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}