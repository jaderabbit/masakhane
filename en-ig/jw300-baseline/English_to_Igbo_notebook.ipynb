{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "starter_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Igc5itf-xMGj"
      },
      "source": [
        "# Masakhane - Machine Translation for African Languages (Using JoeyNMT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x4fXCKCf36IK"
      },
      "source": [
        "## Note before beginning:\n",
        "### - The idea is that you should be able to make minimal changes to this in order to get SOME result for your own translation corpus. \n",
        "\n",
        "### - The tl;dr: Go to the **\"TODO\"** comments which will tell you what to update to get up and running\n",
        "\n",
        "### - If you actually want to have a clue what you're doing, read the text and peek at the links\n",
        "\n",
        "### - With 100 epochs, it should take around 7 hours to run in Google Colab\n",
        "\n",
        "### - Once you've gotten a result for your language, please attach and email your notebook that generated it to masakhanetranslation@gmail.com\n",
        "\n",
        "### - If you care enough and get a chance, doing a brief background on your language would be amazing. See examples in  [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l929HimrxS0a"
      },
      "source": [
        "## Retrieve your data & make a parallel corpus\n",
        "\n",
        "If you are wanting to use the JW300 data referenced on the Masakhane website or in our GitHub repo, you can use `opus-tools` to convert the data into a convenient format. `opus_read` from that package provides a convenient tool for reading the native aligned XML files and to convert them to TMX format. The tool can also be used to fetch relevant files from OPUS on the fly and to filter the data as necessary. [Read the documentation](https://pypi.org/project/opustools-pkg/) for more details.\n",
        "\n",
        "Once you have your corpus files in TMX format (an xml structure which will include the sentences in your target language and your source language in a single file), we recommend reading them into a pandas dataframe. Thankfully, Jade wrote a silly `tmx2dataframe` package which converts your tmx file to a pandas dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oGRmDELn7Az0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "6d25743e-0429-412d-8d24-44373efcb177"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cn3tgQLzUxwn",
        "colab": {}
      },
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\n",
        "import os\n",
        "source_language = \"en\"\n",
        "target_language = \"ig\" \n",
        "lc = False  # If True, lowercase the data.\n",
        "seed = 42  # Random seed for shuffling.\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = tag\n",
        "\n",
        "# This will save it to a folder in our gdrive instead!\n",
        "!mkdir -p \"/content/drive/My Drive/masakhane/$src-$tgt-$tag\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/masakhane/%s-%s-%s\" % (source_language, target_language, tag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kBSgJHEw7Nvx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c3e83d3-e58c-4763-e418-647f69fb1d93"
      },
      "source": [
        "!echo $gdrive_path"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masakhane/en-ig-baseline\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gA75Fs9ys8Y9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "373d5a27-514d-4051-a527-572e8610dabf"
      },
      "source": [
        "# Install opus-tools\n",
        "! pip install opustools-pkg"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opustools-pkg\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/9f/e829a0cceccc603450cd18e1ff80807b6237a88d9a8df2c0bb320796e900/opustools_pkg-0.0.52-py3-none-any.whl (80kB)\n",
            "\r\u001b[K     |████                            | 10kB 28.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 20kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 4.3MB/s \n",
            "\u001b[?25hInstalling collected packages: opustools-pkg\n",
            "Successfully installed opustools-pkg-0.0.52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xq-tDZVks7ZD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "13dd867b-35a3-4d90-ccca-f887805b5072"
      },
      "source": [
        "# Downloading our corpus\n",
        "! opus_read -d JW300 -s $src -t $tgt -wm moses -w jw300.$src jw300.$tgt -q\n",
        "\n",
        "# extract the corpus file\n",
        "! gunzip JW300_latest_xml_$src-$tgt.xml.gz"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Alignment file /proj/nlpl/data/OPUS/JW300/latest/xml/en-ig.xml.gz not found. The following files are available for downloading:\n",
            "\n",
            "   4 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/en-ig.xml.gz\n",
            " 263 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/en.zip\n",
            "  54 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/ig.zip\n",
            "\n",
            " 321 MB Total size\n",
            "./JW300_latest_xml_en-ig.xml.gz ... 100% of 4 MB\n",
            "./JW300_latest_xml_en.zip ... 100% of 263 MB\n",
            "./JW300_latest_xml_ig.zip ... 100% of 54 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n48GDRnP8y2G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "20e4d330-3a53-4441-abb0-47441f6ea03c"
      },
      "source": [
        "# Download the global test set.\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
        "  \n",
        "# And the specific test set for this language pair.\n",
        "os.environ[\"trg\"] = target_language \n",
        "os.environ[\"src\"] = source_language \n",
        "\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.en \n",
        "! mv test.en-$trg.en test.en\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.$trg \n",
        "! mv test.en-$trg.$trg test.$trg"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-12 07:57:39--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 277791 (271K) [text/plain]\n",
            "Saving to: ‘test.en-any.en’\n",
            "\n",
            "\rtest.en-any.en        0%[                    ]       0  --.-KB/s               \rtest.en-any.en      100%[===================>] 271.28K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-01-12 07:57:39 (9.27 MB/s) - ‘test.en-any.en’ saved [277791/277791]\n",
            "\n",
            "--2020-01-12 07:57:40--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-ig.en\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 205323 (201K) [text/plain]\n",
            "Saving to: ‘test.en-ig.en’\n",
            "\n",
            "test.en-ig.en       100%[===================>] 200.51K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-01-12 07:57:40 (9.04 MB/s) - ‘test.en-ig.en’ saved [205323/205323]\n",
            "\n",
            "--2020-01-12 07:57:41--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-ig.ig\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 259911 (254K) [text/plain]\n",
            "Saving to: ‘test.en-ig.ig’\n",
            "\n",
            "test.en-ig.ig       100%[===================>] 253.82K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-01-12 07:57:42 (8.37 MB/s) - ‘test.en-ig.ig’ saved [259911/259911]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqDG-CI28y2L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d5cab60-02e9-4ffd-a28a-235c4ae3533c"
      },
      "source": [
        "# Read the test data to filter from train and dev splits.\n",
        "# Store english portion in set for quick filtering checks.\n",
        "en_test_sents = set()\n",
        "filter_test_sents = \"test.en-any.en\"\n",
        "j = 0\n",
        "with open(filter_test_sents) as f:\n",
        "  for line in f:\n",
        "    en_test_sents.add(line.strip())\n",
        "    j += 1\n",
        "print('Loaded {} global test sentences to filter from the training/dev data.'.format(j))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 3571 global test sentences to filter from the training/dev data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3CNdwLBCfSIl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "e0645d93-e1d2-4436-ecdb-c2f4f61b8f51"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# TMX file to dataframe\n",
        "source_file = 'jw300.' + source_language\n",
        "target_file = 'jw300.' + target_language\n",
        "\n",
        "source = []\n",
        "target = []\n",
        "skip_lines = []  # Collect the line numbers of the source portion to skip the same lines for the target portion.\n",
        "with open(source_file) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        # Skip sentences that are contained in the test set.\n",
        "        if line.strip() not in en_test_sents:\n",
        "            source.append(line.strip())\n",
        "        else:\n",
        "            skip_lines.append(i)             \n",
        "with open(target_file) as f:\n",
        "    for j, line in enumerate(f):\n",
        "        # Only add to corpus if corresponding source was not skipped.\n",
        "        if j not in skip_lines:\n",
        "            target.append(line.strip())\n",
        "    \n",
        "print('Loaded data and skipped {}/{} lines since contained in test set.'.format(len(skip_lines), i))\n",
        "    \n",
        "df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n",
        "# if you get TypeError: data argument can't be an iterator is because of your zip version run this below\n",
        "#df = pd.DataFrame(list(zip(source, target)), columns=['source_sentence', 'target_sentence'])\n",
        "df.head(3)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded data and skipped 6326/475205 lines since contained in test set.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_sentence</th>\n",
              "      <th>target_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Using Ladders — Do You Make These Safety Checks ?</td>\n",
              "      <td>Iji Ubube Eme Ihe — Ị̀ Na - eme Nnyocha Ndị A ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>By Awake !</td>\n",
              "      <td>Site n’aka onye nta akụkọ Teta !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>correspondent in Ireland</td>\n",
              "      <td>na Ireland</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     source_sentence                                    target_sentence\n",
              "0  Using Ladders — Do You Make These Safety Checks ?  Iji Ubube Eme Ihe — Ị̀ Na - eme Nnyocha Ndị A ...\n",
              "1                                         By Awake !                   Site n’aka onye nta akụkọ Teta !\n",
              "2                           correspondent in Ireland                                         na Ireland"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YkuK3B4p2AkN"
      },
      "source": [
        "## Pre-processing and export\n",
        "\n",
        "It is generally a good idea to remove duplicate translations and conflicting translations from the corpus. In practice, these public corpora include some number of these that need to be cleaned.\n",
        "\n",
        "In addition we will split our data into dev/test/train and export to the filesystem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M_2ouEOH1_1q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "9f234528-b661-4c88-b3f8-105bba7ce6ec"
      },
      "source": [
        "# drop duplicate translations\n",
        "df_pp = df.drop_duplicates()\n",
        "\n",
        "# drop conflicting translations\n",
        "# (this is optional and something that you might want to comment out \n",
        "# depending on the size of your corpus)\n",
        "df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n",
        "df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n",
        "\n",
        "# Shuffle the data to remove bias in dev set selection.\n",
        "df_pp = df_pp.sample(frac=1, random_state=seed).reset_index(drop=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_1BwAApEtMk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "791b9ff9-fc3c-4ec6-f3ad-a6738edb5217"
      },
      "source": [
        "# Install fuzzy wuzzy to remove \"almost duplicate\" sentences in the\n",
        "# test and training sets.\n",
        "! pip install fuzzywuzzy\n",
        "! pip install python-Levenshtein\n",
        "import time\n",
        "from fuzzywuzzy import process\n",
        "import numpy as np\n",
        "\n",
        "# reset the index of the training set after previous filtering\n",
        "df_pp.reset_index(drop=False, inplace=True)\n",
        "\n",
        "# Remove samples from the training data set if they \"almost overlap\" with the\n",
        "# samples in the test set.\n",
        "\n",
        "# Filtering function. Adjust pad to narrow down the candidate matches to\n",
        "# within a certain length of characters of the given sample.\n",
        "def fuzzfilter(sample, candidates, pad):\n",
        "  candidates = [x for x in candidates if len(x) <= len(sample)+pad and len(x) >= len(sample)-pad] \n",
        "  if len(candidates) > 0:\n",
        "    return process.extractOne(sample, candidates)[1]\n",
        "  else:\n",
        "    return np.nan\n",
        "\n",
        "# NOTE - This might run slow depending on the size of your training set. We are\n",
        "# printing some information to help you track how long it would take. \n",
        "scores = []\n",
        "start_time = time.time()\n",
        "for idx, row in df_pp.iterrows():\n",
        "  scores.append(fuzzfilter(row['source_sentence'], list(en_test_sents), 5))\n",
        "  if idx % 1000 == 0:\n",
        "    hours, rem = divmod(time.time() - start_time, 3600)\n",
        "    minutes, seconds = divmod(rem, 60)\n",
        "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds), \"%0.2f percent complete\" % (100.0*float(idx)/float(len(df_pp))))\n",
        "\n",
        "# Filter out \"almost overlapping samples\"\n",
        "df_pp['scores'] = scores\n",
        "df_pp = df_pp[df_pp['scores'] < 95]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading https://files.pythonhosted.org/packages/d8/f1/5a267addb30ab7eaa1beab2b9323073815da4551076554ecc890a3595ec9/fuzzywuzzy-0.17.0-py2.py3-none-any.whl\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.17.0\n",
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (42.0.2)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144679 sha256=4863c3de1bedaa5001dd58d9106a4d97cde13ce9c1f9f51c8ed649178c814bde\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.0\n",
            "00:00:00.14 0.00 percent complete\n",
            "00:00:19.80 0.24 percent complete\n",
            "00:00:38.56 0.48 percent complete\n",
            "00:00:57.39 0.72 percent complete\n",
            "00:01:16.48 0.96 percent complete\n",
            "00:01:34.80 1.20 percent complete\n",
            "00:01:53.09 1.44 percent complete\n",
            "00:02:11.90 1.68 percent complete\n",
            "00:02:30.37 1.92 percent complete\n",
            "00:02:49.36 2.16 percent complete\n",
            "00:03:08.57 2.40 percent complete\n",
            "00:03:26.94 2.63 percent complete\n",
            "00:03:46.23 2.87 percent complete\n",
            "00:04:04.79 3.11 percent complete\n",
            "00:04:23.68 3.35 percent complete\n",
            "00:04:41.95 3.59 percent complete\n",
            "00:05:01.28 3.83 percent complete\n",
            "00:05:20.04 4.07 percent complete\n",
            "00:05:39.56 4.31 percent complete\n",
            "00:05:58.38 4.55 percent complete\n",
            "00:06:16.55 4.79 percent complete\n",
            "00:06:34.61 5.03 percent complete\n",
            "00:06:53.53 5.27 percent complete\n",
            "00:07:12.51 5.51 percent complete\n",
            "00:07:32.03 5.75 percent complete\n",
            "00:07:50.81 5.99 percent complete\n",
            "00:08:09.96 6.23 percent complete\n",
            "00:08:28.38 6.47 percent complete\n",
            "00:08:46.95 6.71 percent complete\n",
            "00:09:07.16 6.95 percent complete\n",
            "00:09:27.12 7.19 percent complete\n",
            "00:09:46.63 7.43 percent complete\n",
            "00:10:06.41 7.66 percent complete\n",
            "00:10:25.54 7.90 percent complete\n",
            "00:10:46.10 8.14 percent complete\n",
            "00:11:05.52 8.38 percent complete\n",
            "00:11:24.69 8.62 percent complete\n",
            "00:11:44.32 8.86 percent complete\n",
            "00:12:03.75 9.10 percent complete\n",
            "00:12:23.57 9.34 percent complete\n",
            "00:12:44.08 9.58 percent complete\n",
            "00:13:03.46 9.82 percent complete\n",
            "00:13:23.42 10.06 percent complete\n",
            "00:13:42.70 10.30 percent complete\n",
            "00:14:01.19 10.54 percent complete\n",
            "00:14:20.43 10.78 percent complete\n",
            "00:14:39.47 11.02 percent complete\n",
            "00:14:59.24 11.26 percent complete\n",
            "00:15:17.81 11.50 percent complete\n",
            "00:15:37.19 11.74 percent complete\n",
            "00:15:57.64 11.98 percent complete\n",
            "00:16:17.08 12.22 percent complete\n",
            "00:16:36.61 12.46 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:16:55.79 12.69 percent complete\n",
            "00:17:15.13 12.93 percent complete\n",
            "00:17:35.19 13.17 percent complete\n",
            "00:17:54.04 13.41 percent complete\n",
            "00:18:13.27 13.65 percent complete\n",
            "00:18:33.75 13.89 percent complete\n",
            "00:18:53.24 14.13 percent complete\n",
            "00:19:12.60 14.37 percent complete\n",
            "00:19:32.36 14.61 percent complete\n",
            "00:19:51.97 14.85 percent complete\n",
            "00:20:11.70 15.09 percent complete\n",
            "00:20:32.01 15.33 percent complete\n",
            "00:20:51.44 15.57 percent complete\n",
            "00:21:12.28 15.81 percent complete\n",
            "00:21:32.13 16.05 percent complete\n",
            "00:21:52.41 16.29 percent complete\n",
            "00:22:11.89 16.53 percent complete\n",
            "00:22:31.66 16.77 percent complete\n",
            "00:22:51.52 17.01 percent complete\n",
            "00:23:11.14 17.25 percent complete\n",
            "00:23:30.33 17.49 percent complete\n",
            "00:23:51.44 17.72 percent complete\n",
            "00:24:11.74 17.96 percent complete\n",
            "00:24:31.87 18.20 percent complete\n",
            "00:24:52.43 18.44 percent complete\n",
            "00:25:11.94 18.68 percent complete\n",
            "00:25:31.81 18.92 percent complete\n",
            "00:25:51.83 19.16 percent complete\n",
            "00:26:11.69 19.40 percent complete\n",
            "00:26:32.22 19.64 percent complete\n",
            "00:26:52.87 19.88 percent complete\n",
            "00:27:12.97 20.12 percent complete\n",
            "00:27:32.92 20.36 percent complete\n",
            "00:27:52.46 20.60 percent complete\n",
            "00:28:12.15 20.84 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '— ― ― ― ― ― ― ―']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:28:31.73 21.08 percent complete\n",
            "00:28:51.20 21.32 percent complete\n",
            "00:29:12.49 21.56 percent complete\n",
            "00:29:32.67 21.80 percent complete\n",
            "00:29:52.59 22.04 percent complete\n",
            "00:30:12.49 22.28 percent complete\n",
            "00:30:32.56 22.52 percent complete\n",
            "00:30:52.20 22.75 percent complete\n",
            "00:31:12.62 22.99 percent complete\n",
            "00:31:32.41 23.23 percent complete\n",
            "00:31:53.43 23.47 percent complete\n",
            "00:32:13.44 23.71 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '↓ ↓']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:32:33.70 23.95 percent complete\n",
            "00:32:54.36 24.19 percent complete\n",
            "00:33:13.93 24.43 percent complete\n",
            "00:33:32.54 24.67 percent complete\n",
            "00:33:51.89 24.91 percent complete\n",
            "00:34:11.30 25.15 percent complete\n",
            "00:34:32.24 25.39 percent complete\n",
            "00:34:52.03 25.63 percent complete\n",
            "00:35:11.04 25.87 percent complete\n",
            "00:35:30.22 26.11 percent complete\n",
            "00:35:49.74 26.35 percent complete\n",
            "00:36:09.10 26.59 percent complete\n",
            "00:36:29.03 26.83 percent complete\n",
            "00:36:48.44 27.07 percent complete\n",
            "00:37:08.19 27.31 percent complete\n",
            "00:37:27.47 27.55 percent complete\n",
            "00:37:46.58 27.78 percent complete\n",
            "00:38:05.66 28.02 percent complete\n",
            "00:38:24.86 28.26 percent complete\n",
            "00:38:43.80 28.50 percent complete\n",
            "00:39:03.46 28.74 percent complete\n",
            "00:39:22.21 28.98 percent complete\n",
            "00:39:42.99 29.22 percent complete\n",
            "00:40:02.72 29.46 percent complete\n",
            "00:40:22.64 29.70 percent complete\n",
            "00:40:42.11 29.94 percent complete\n",
            "00:41:01.16 30.18 percent complete\n",
            "00:41:20.42 30.42 percent complete\n",
            "00:41:39.90 30.66 percent complete\n",
            "00:41:58.99 30.90 percent complete\n",
            "00:42:18.74 31.14 percent complete\n",
            "00:42:37.92 31.38 percent complete\n",
            "00:42:57.10 31.62 percent complete\n",
            "00:43:16.82 31.86 percent complete\n",
            "00:43:35.71 32.10 percent complete\n",
            "00:43:54.50 32.34 percent complete\n",
            "00:44:14.45 32.58 percent complete\n",
            "00:44:33.63 32.81 percent complete\n",
            "00:44:53.10 33.05 percent complete\n",
            "00:45:11.81 33.29 percent complete\n",
            "00:45:30.61 33.53 percent complete\n",
            "00:45:49.80 33.77 percent complete\n",
            "00:46:08.43 34.01 percent complete\n",
            "00:46:27.35 34.25 percent complete\n",
            "00:46:46.55 34.49 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '․ ․ ․ ․ ․ ․ ․ ․ ․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:47:06.10 34.73 percent complete\n",
            "00:47:25.26 34.97 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '⇧']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:47:44.45 35.21 percent complete\n",
            "00:48:03.63 35.45 percent complete\n",
            "00:48:22.95 35.69 percent complete\n",
            "00:48:41.82 35.93 percent complete\n",
            "00:49:01.17 36.17 percent complete\n",
            "00:49:19.66 36.41 percent complete\n",
            "00:49:38.41 36.65 percent complete\n",
            "00:49:57.64 36.89 percent complete\n",
            "00:50:18.10 37.13 percent complete\n",
            "00:50:37.61 37.37 percent complete\n",
            "00:50:56.85 37.60 percent complete\n",
            "00:51:16.51 37.84 percent complete\n",
            "00:51:35.04 38.08 percent complete\n",
            "00:51:53.93 38.32 percent complete\n",
            "00:52:13.06 38.56 percent complete\n",
            "00:52:31.54 38.80 percent complete\n",
            "00:52:51.43 39.04 percent complete\n",
            "00:53:10.38 39.28 percent complete\n",
            "00:53:29.93 39.52 percent complete\n",
            "00:53:48.81 39.76 percent complete\n",
            "00:54:07.26 40.00 percent complete\n",
            "00:54:25.98 40.24 percent complete\n",
            "00:54:45.14 40.48 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '” *']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:55:04.58 40.72 percent complete\n",
            "00:55:25.29 40.96 percent complete\n",
            "00:55:44.85 41.20 percent complete\n",
            "00:56:04.07 41.44 percent complete\n",
            "00:56:23.04 41.68 percent complete\n",
            "00:56:41.71 41.92 percent complete\n",
            "00:57:00.39 42.16 percent complete\n",
            "00:57:19.45 42.40 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '․ ․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:57:38.68 42.63 percent complete\n",
            "00:57:59.36 42.87 percent complete\n",
            "00:58:18.11 43.11 percent complete\n",
            "00:58:37.26 43.35 percent complete\n",
            "00:58:56.11 43.59 percent complete\n",
            "00:59:14.59 43.83 percent complete\n",
            "00:59:33.38 44.07 percent complete\n",
            "00:59:52.08 44.31 percent complete\n",
            "01:00:10.80 44.55 percent complete\n",
            "01:00:30.07 44.79 percent complete\n",
            "01:00:48.46 45.03 percent complete\n",
            "01:01:07.40 45.27 percent complete\n",
            "01:01:26.43 45.51 percent complete\n",
            "01:01:45.89 45.75 percent complete\n",
            "01:02:04.95 45.99 percent complete\n",
            "01:02:23.39 46.23 percent complete\n",
            "01:02:42.16 46.47 percent complete\n",
            "01:03:02.49 46.71 percent complete\n",
            "01:03:20.98 46.95 percent complete\n",
            "01:03:40.04 47.19 percent complete\n",
            "01:03:58.78 47.43 percent complete\n",
            "01:04:17.32 47.66 percent complete\n",
            "01:04:36.12 47.90 percent complete\n",
            "01:04:55.29 48.14 percent complete\n",
            "01:05:14.33 48.38 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:05:34.40 48.62 percent complete\n",
            "01:05:54.03 48.86 percent complete\n",
            "01:06:13.37 49.10 percent complete\n",
            "01:06:32.37 49.34 percent complete\n",
            "01:06:51.58 49.58 percent complete\n",
            "01:07:10.84 49.82 percent complete\n",
            "01:07:29.60 50.06 percent complete\n",
            "01:07:48.45 50.30 percent complete\n",
            "01:08:08.55 50.54 percent complete\n",
            "01:08:27.04 50.78 percent complete\n",
            "01:08:45.81 51.02 percent complete\n",
            "01:09:05.49 51.26 percent complete\n",
            "01:09:24.29 51.50 percent complete\n",
            "01:09:42.92 51.74 percent complete\n",
            "01:10:02.63 51.98 percent complete\n",
            "01:10:21.14 52.22 percent complete\n",
            "01:10:41.09 52.46 percent complete\n",
            "01:11:00.58 52.69 percent complete\n",
            "01:11:19.68 52.93 percent complete\n",
            "01:11:39.00 53.17 percent complete\n",
            "01:11:58.38 53.41 percent complete\n",
            "01:12:17.27 53.65 percent complete\n",
            "01:12:36.38 53.89 percent complete\n",
            "01:12:55.11 54.13 percent complete\n",
            "01:13:15.35 54.37 percent complete\n",
            "01:13:34.80 54.61 percent complete\n",
            "01:13:53.70 54.85 percent complete\n",
            "01:14:13.63 55.09 percent complete\n",
            "01:14:32.97 55.33 percent complete\n",
            "01:14:51.97 55.57 percent complete\n",
            "01:15:11.39 55.81 percent complete\n",
            "01:15:29.87 56.05 percent complete\n",
            "01:15:49.07 56.29 percent complete\n",
            "01:16:09.68 56.53 percent complete\n",
            "01:16:29.05 56.77 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '●']\n",
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '․ ․ ․ ․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:16:48.34 57.01 percent complete\n",
            "01:17:07.32 57.25 percent complete\n",
            "01:17:25.63 57.49 percent complete\n",
            "01:17:44.34 57.72 percent complete\n",
            "01:18:03.40 57.96 percent complete\n",
            "01:18:22.29 58.20 percent complete\n",
            "01:18:41.97 58.44 percent complete\n",
            "01:19:01.16 58.68 percent complete\n",
            "01:19:20.34 58.92 percent complete\n",
            "01:19:39.60 59.16 percent complete\n",
            "01:19:58.34 59.40 percent complete\n",
            "01:20:17.67 59.64 percent complete\n",
            "01:20:36.73 59.88 percent complete\n",
            "01:20:55.99 60.12 percent complete\n",
            "01:21:16.64 60.36 percent complete\n",
            "01:21:35.86 60.60 percent complete\n",
            "01:21:54.99 60.84 percent complete\n",
            "01:22:14.43 61.08 percent complete\n",
            "01:22:33.62 61.32 percent complete\n",
            "01:22:52.35 61.56 percent complete\n",
            "01:23:10.78 61.80 percent complete\n",
            "01:23:30.56 62.04 percent complete\n",
            "01:23:51.13 62.28 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '\\']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:24:10.17 62.52 percent complete\n",
            "01:24:29.11 62.75 percent complete\n",
            "01:24:48.15 62.99 percent complete\n",
            "01:25:06.76 63.23 percent complete\n",
            "01:25:25.51 63.47 percent complete\n",
            "01:25:44.58 63.71 percent complete\n",
            "01:26:03.27 63.95 percent complete\n",
            "01:26:23.73 64.19 percent complete\n",
            "01:26:42.75 64.43 percent complete\n",
            "01:27:02.08 64.67 percent complete\n",
            "01:27:21.14 64.91 percent complete\n",
            "01:27:39.64 65.15 percent complete\n",
            "01:27:59.19 65.39 percent complete\n",
            "01:28:17.89 65.63 percent complete\n",
            "01:28:37.12 65.87 percent complete\n",
            "01:28:57.86 66.11 percent complete\n",
            "01:29:16.61 66.35 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '□ ․ ․ ․ ․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:29:35.52 66.59 percent complete\n",
            "01:29:54.49 66.83 percent complete\n",
            "01:30:13.45 67.07 percent complete\n",
            "01:30:32.51 67.31 percent complete\n",
            "01:30:52.07 67.55 percent complete\n",
            "01:31:11.10 67.78 percent complete\n",
            "01:31:31.33 68.02 percent complete\n",
            "01:31:50.77 68.26 percent complete\n",
            "01:32:10.19 68.50 percent complete\n",
            "01:32:29.09 68.74 percent complete\n",
            "01:32:47.90 68.98 percent complete\n",
            "01:33:06.82 69.22 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '*']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:33:25.52 69.46 percent complete\n",
            "01:33:43.82 69.70 percent complete\n",
            "01:34:03.28 69.94 percent complete\n",
            "01:34:22.70 70.18 percent complete\n",
            "01:34:41.51 70.42 percent complete\n",
            "01:34:59.91 70.66 percent complete\n",
            "01:35:18.57 70.90 percent complete\n",
            "01:35:37.40 71.14 percent complete\n",
            "01:35:56.31 71.38 percent complete\n",
            "01:36:15.95 71.62 percent complete\n",
            "01:36:36.38 71.86 percent complete\n",
            "01:36:55.64 72.10 percent complete\n",
            "01:37:14.12 72.34 percent complete\n",
            "01:37:33.07 72.58 percent complete\n",
            "01:37:51.54 72.81 percent complete\n",
            "01:38:10.87 73.05 percent complete\n",
            "01:38:29.71 73.29 percent complete\n",
            "01:38:48.13 73.53 percent complete\n",
            "01:39:07.77 73.77 percent complete\n",
            "01:39:27.19 74.01 percent complete\n",
            "01:39:45.80 74.25 percent complete\n",
            "01:40:04.64 74.49 percent complete\n",
            "01:40:23.15 74.73 percent complete\n",
            "01:40:42.33 74.97 percent complete\n",
            "01:41:01.79 75.21 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '→ →']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:41:21.05 75.45 percent complete\n",
            "01:41:40.17 75.69 percent complete\n",
            "01:42:00.23 75.93 percent complete\n",
            "01:42:19.04 76.17 percent complete\n",
            "01:42:37.97 76.41 percent complete\n",
            "01:42:56.58 76.65 percent complete\n",
            "01:43:15.17 76.89 percent complete\n",
            "01:43:34.31 77.13 percent complete\n",
            "01:43:52.81 77.37 percent complete\n",
            "01:44:12.11 77.61 percent complete\n",
            "01:44:32.91 77.84 percent complete\n",
            "01:44:52.22 78.08 percent complete\n",
            "01:45:10.86 78.32 percent complete\n",
            "01:45:30.09 78.56 percent complete\n",
            "01:45:49.03 78.80 percent complete\n",
            "01:46:08.09 79.04 percent complete\n",
            "01:46:27.50 79.28 percent complete\n",
            "01:46:46.15 79.52 percent complete\n",
            "01:47:06.15 79.76 percent complete\n",
            "01:47:25.74 80.00 percent complete\n",
            "01:47:44.91 80.24 percent complete\n",
            "01:48:03.97 80.48 percent complete\n",
            "01:48:21.95 80.72 percent complete\n",
            "01:48:41.51 80.96 percent complete\n",
            "01:49:00.14 81.20 percent complete\n",
            "01:49:18.60 81.44 percent complete\n",
            "01:49:38.36 81.68 percent complete\n",
            "01:49:57.60 81.92 percent complete\n",
            "01:50:16.57 82.16 percent complete\n",
            "01:50:35.61 82.40 percent complete\n",
            "01:50:53.93 82.64 percent complete\n",
            "01:51:12.99 82.87 percent complete\n",
            "01:51:31.83 83.11 percent complete\n",
            "01:51:50.63 83.35 percent complete\n",
            "01:52:10.97 83.59 percent complete\n",
            "01:52:30.11 83.83 percent complete\n",
            "01:52:49.59 84.07 percent complete\n",
            "01:53:08.21 84.31 percent complete\n",
            "01:53:27.33 84.55 percent complete\n",
            "01:53:46.12 84.79 percent complete\n",
            "01:54:05.15 85.03 percent complete\n",
            "01:54:24.17 85.27 percent complete\n",
            "01:54:44.24 85.51 percent complete\n",
            "01:55:03.50 85.75 percent complete\n",
            "01:55:22.98 85.99 percent complete\n",
            "01:55:43.60 86.23 percent complete\n",
            "01:56:02.79 86.47 percent complete\n",
            "01:56:22.69 86.71 percent complete\n",
            "01:56:41.59 86.95 percent complete\n",
            "01:57:00.96 87.19 percent complete\n",
            "01:57:21.16 87.43 percent complete\n",
            "01:57:40.86 87.67 percent complete\n",
            "01:58:00.47 87.90 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '$ $ $']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:58:19.63 88.14 percent complete\n",
            "01:58:38.07 88.38 percent complete\n",
            "01:58:57.10 88.62 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '↓ ↓ ↓ ↓']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:59:16.90 88.86 percent complete\n",
            "01:59:37.19 89.10 percent complete\n",
            "01:59:57.69 89.34 percent complete\n",
            "02:00:16.97 89.58 percent complete\n",
            "02:00:35.82 89.82 percent complete\n",
            "02:00:54.60 90.06 percent complete\n",
            "02:01:14.32 90.30 percent complete\n",
            "02:01:34.50 90.54 percent complete\n",
            "02:01:54.07 90.78 percent complete\n",
            "02:02:13.59 91.02 percent complete\n",
            "02:02:33.10 91.26 percent complete\n",
            "02:02:51.51 91.50 percent complete\n",
            "02:03:11.79 91.74 percent complete\n",
            "02:03:30.93 91.98 percent complete\n",
            "02:03:49.60 92.22 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '. .']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "02:04:08.83 92.46 percent complete\n",
            "02:04:27.47 92.70 percent complete\n",
            "02:04:46.64 92.93 percent complete\n",
            "02:05:06.11 93.17 percent complete\n",
            "02:05:25.67 93.41 percent complete\n",
            "02:05:43.84 93.65 percent complete\n",
            "02:06:02.80 93.89 percent complete\n",
            "02:06:21.93 94.13 percent complete\n",
            "02:06:40.96 94.37 percent complete\n",
            "02:06:59.60 94.61 percent complete\n",
            "02:07:18.53 94.85 percent complete\n",
            "02:07:38.08 95.09 percent complete\n",
            "02:07:57.28 95.33 percent complete\n",
            "02:08:16.07 95.57 percent complete\n",
            "02:08:34.73 95.81 percent complete\n",
            "02:08:53.66 96.05 percent complete\n",
            "02:09:12.34 96.29 percent complete\n",
            "02:09:31.67 96.53 percent complete\n",
            "02:09:50.88 96.77 percent complete\n",
            "02:10:10.40 97.01 percent complete\n",
            "02:10:30.20 97.25 percent complete\n",
            "02:10:48.18 97.49 percent complete\n",
            "02:11:06.85 97.73 percent complete\n",
            "02:11:25.74 97.96 percent complete\n",
            "02:11:45.60 98.20 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '”']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "02:12:04.37 98.44 percent complete\n",
            "02:12:23.27 98.68 percent complete\n",
            "02:12:42.66 98.92 percent complete\n",
            "02:13:02.28 99.16 percent complete\n",
            "02:13:21.16 99.40 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '⇩']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "02:13:40.29 99.64 percent complete\n",
            "02:13:59.55 99.88 percent complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hxxBOCA-xXhy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "509966df-78fb-4eab-e944-eaf90ef72704"
      },
      "source": [
        "# This section does the split between train/dev for the parallel corpora then saves them as separate files\n",
        "# We use 1000 dev test and the given test set.\n",
        "import csv\n",
        "\n",
        "# Do the split between dev/train and create parallel corpora\n",
        "num_dev_patterns = 1000\n",
        "\n",
        "# Optional: lower case the corpora - this will make it easier to generalize, but without proper casing.\n",
        "if lc:  # Julia: making lowercasing optional\n",
        "    df_pp[\"source_sentence\"] = df_pp[\"source_sentence\"].str.lower()\n",
        "    df_pp[\"target_sentence\"] = df_pp[\"target_sentence\"].str.lower()\n",
        "\n",
        "# Julia: test sets are already generated\n",
        "dev = df_pp.tail(num_dev_patterns) # Herman: Error in original\n",
        "stripped = df_pp.drop(df_pp.tail(num_dev_patterns).index)\n",
        "\n",
        "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in stripped.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "    \n",
        "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in dev.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "\n",
        "#stripped[[\"source_sentence\"]].to_csv(\"train.\"+source_language, header=False, index=False)  # Herman: Added `header=False` everywhere\n",
        "#stripped[[\"target_sentence\"]].to_csv(\"train.\"+target_language, header=False, index=False)  # Julia: Problematic handling of quotation marks.\n",
        "\n",
        "#dev[[\"source_sentence\"]].to_csv(\"dev.\"+source_language, header=False, index=False)\n",
        "#dev[[\"target_sentence\"]].to_csv(\"dev.\"+target_language, header=False, index=False)\n",
        "\n",
        "# Doublecheck the format below. There should be no extra quotation marks or weird characters.\n",
        "! head train.*\n",
        "! head dev.*"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> train.en <==\n",
            "“ Whatever we ask we receive from him , ” wrote the apostle John , “ because we are observing his commandments and are doing the things that are pleasing in his eyes . ”\n",
            "Did Leah conspire to deceive Jacob ?\n",
            "While the one under demon attack should pray fervently for relief , other true Christians may also pray in behalf of the individual who deeply desires to serve Jehovah and who is earnestly trying to resist wicked forces .\n",
            "The articles on pages 5 to 9 of this issue discuss what that Kingdom is and when it will come .\n",
            "It answers the following questions : What factors presently prevent diplomacy from producing peace ?\n",
            "Are these mammoth sea monsters slow and clumsy in the water ?\n",
            "A good counselor shows empathy , respect , and kindness\n",
            "Jehovah raises up the horns of his people , causing them to be exalted , whereas he ‘ cuts down the horns of the wicked ones . ’\n",
            "And it came about at the closing of the gate by dark that the men went out . ”\n",
            "For instance , an angel was asked by Manoah , the father of Samson : “ What is your name , that when your word comes true we shall certainly do you honor ? ”\n",
            "\n",
            "==> train.ig <==\n",
            "“ Anyị na - anatakwa ihe ọ bụla anyị rịọrọ n’aka ya , ” ka Jọn onyeozi dere , “ n’ihi na anyị na - edebe ihe ndị o nyere n’iwu , na - emekwa ihe ndị dị mma n’anya ya . ”\n",
            "Ọ̀ bụ Lia kpara nkata a ka a ghọgbuo Jekọb ?\n",
            "Ọ bụ ezie na onye ahụ ndị mmụọ ọjọọ na - enye nsogbu kwesịrị ikpesi ekpere ike iji nwere onwe ya , ezi Ndị Kraịst ndị ọzọ pụkwara ikpere onye ahụ ji obi ya nile chọọ ijere Jehova ozi ma na - agbalịsi ike iguzogide ndị mmụọ ọjọọ ekpere .\n",
            "Isiokwu ndị dị na peeji nke ise ruo na nke itoolu na magazin a i ji n’aka kwuru ihe Alaeze ahụ bụ na mgbe ọ ga - abịa .\n",
            "Ọ na - aza ajụjụ ndị na - esonụ : N’agbanyeghị mgbalị ndị a na - eme n’oge a ịhụ na mba na mba dị ná mma , olee ihe ndị mere na udo adịbeghị ?\n",
            "Nnukwu anụ mmiri ndị a hà na - aga nwayọọ nwayọọ n’ime mmiri ?\n",
            "Ezi onye ndụmọdụ na - enwe ọmịiko n’ahụ́ onye ọ na - adụ ọdụ , na - akwanyere ya ùgwù ma jiri obiọma na - emeso ya ihe\n",
            "Jehova na - ebuli mpi nile nke ndị ya elu , na - eme ka ha dị elu , ebe ọ ‘ na - ebipụ mpi nile nke ndị na - emebi iwu . ’\n",
            "O wee ruo na mgbe chi jiwere , mgbe a chọrọ imechi ọnụ ụzọ ámá , ndị ikom ahụ pụrụ . ”\n",
            "A kpọghị aha ndị ọzọ . Dị ka ihe atụ , mgbe nna Samsịn aha ya bụ Manoa jụrụ otu mmụọ ozi , sị : “ Gịnị bụ aha gị , ka ọ ga - abụ mgbe okwu gị mezuru , anyị ga - asọpụrụ gị ? ”\n",
            "==> dev.en <==\n",
            "Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "That is the second step toward gaining God’s forgiveness .\n",
            "The teacher was amazed that someone actually had something to say on the subject , since most of the students were having a difficult time understanding it .\n",
            "The Bible tells us : “ God opposes the haughty ones , but he gives undeserved kindness to the humble ones . ”\n",
            "Indeed , mildness and self - control were listed together by the apostle Paul when he outlined “ the fruitage of the spirit . ”\n",
            "God will bring those young men back to life , and they will have the opportunity to learn the truth about him .\n",
            "Still , applying the Bible’s advice , we worked out our differences and enjoyed success as pioneer partners .\n",
            "Use the time well to reach others with the good news of the Kingdom , ” urged Brother Pierce .\n",
            "\n",
            "==> dev.ig <==\n",
            "Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "O juru onye nkụzi ahụ anya na e nwere n’ezie onye nwere ihe ọ ga - ekwu n’isiokwu ahụ , ebe ọ bụ na o siiri ọnụ ọgụgụ ka ukwuu nke ụmụ akwụkwọ ahụ ike ịghọta ya .\n",
            "Bible na - agwa anyị , sị : “ Chineke na - emegide ndị mpako , ma ọ na - enye ndị dị umeala n’obi obiọma na - erughịrị mmadụ . ”\n",
            "N’ezie , Pọl onyeozi denyere ịdị nwayọọ na njide onwe onye mgbe o depụtara “ mkpụrụ nke mmụọ nsọ . ”\n",
            "Chineke ga - akpọlite ụmụ okorobịa ndị ahụ n’ọnwụ , a kụziere ha eziokwu banyere ya .\n",
            "Ma , ime ihe ndị anyị na - amụta na Baịbụl mere ka anyị dozie ihe niile , meekwa ka anyị jiri obi ụtọ na - eje ozi anyị .\n",
            "Jiri oge ahụ mee ihe nke ọma site n’iwegara ndị ọzọ ozi ọma Alaeze ahụ , ” ka Nwanna Pierce gbara ume .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epeCydmCyS8X"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Installation of JoeyNMT\n",
        "\n",
        "JoeyNMT is a simple, minimalist NMT package which is useful for learning and teaching. Check out the documentation for JoeyNMT [here](https://joeynmt.readthedocs.io)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iBRMm4kMxZ8L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "06d6f21a-b219-4cc8-a3a1-f0557049f1f2"
      },
      "source": [
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'joeynmt'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 2199 (delta 4), reused 5 (delta 3), pack-reused 2184\u001b[K\n",
            "Receiving objects: 100% (2199/2199), 2.60 MiB | 1.45 MiB/s, done.\n",
            "Resolving deltas: 100% (1525/1525), done.\n",
            "Processing /content/joeynmt\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (6.2.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.17.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (42.0.2)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.3.1)\n",
            "Requirement already satisfied: tensorflow>=1.14 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.3.1)\n",
            "Collecting sacrebleu>=1.3.6\n",
            "  Downloading https://files.pythonhosted.org/packages/45/31/1a135b964c169984b27fb2f7a50280fa7f8e6d9d404d8a9e596180487fd1/sacrebleu-1.4.3-py3-none-any.whl\n",
            "Collecting subword-nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (3.1.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.9.0)\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/d9/ea9816aea31beeadccd03f1f8b625ecf8f645bd66744484d162d84803ce5/PyYAML-5.3.tar.gz (268kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 11.5MB/s \n",
            "\u001b[?25hCollecting pylint\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/59/43fc36c5ee316bb9aeb7cf5329cdbdca89e5749c34d5602753827c0aa2dc/pylint-2.4.4-py3-none-any.whl (302kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six==1.12 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.11.2)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.33.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.1.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (2.21.0)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.3.6->joeynmt==0.0.1) (3.6.6)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/91/db/7bc703c0760df726839e0699b7f78a4d8217fdc9c7fcb1b51b39c5a22a4e/portalocker-1.5.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.4.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.6.1)\n",
            "Requirement already satisfied: pandas>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt==0.0.1) (0.25.3)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt==0.0.1) (1.4.1)\n",
            "Collecting mccabe<0.7,>=0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Collecting isort<5,>=4.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/b0/c121fd1fa3419ea9bfd55c7f9c4fedfec5143208d8c7ad3ce3db6c623c21/isort-4.3.21-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.5MB/s \n",
            "\u001b[?25hCollecting astroid<2.4,>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/ae/86734823047962e7b8c8529186a1ac4a7ca19aaf1aa0c7713c022ef593fd/astroid-2.3.3-py3-none-any.whl (205kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 58.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14->joeynmt==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14->joeynmt==0.0.1) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow>=1.14->joeynmt==0.0.1) (2.8.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2019.11.28)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn->joeynmt==0.0.1) (2018.9)\n",
            "Collecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/d3/9d1802c161626d0278bafb1ffb32f76b9d01e123881bbf9d91e8ccf28e18/typed_ast-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (736kB)\n",
            "\u001b[K     |████████████████████████████████| 737kB 68.5MB/s \n",
            "\u001b[?25hCollecting lazy-object-proxy==1.4.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/dd/b1e3407e9e6913cf178e506cd0dee818e58694d9a5cd1984e3f6a8b9a10f/lazy_object_proxy-1.4.3-cp36-cp36m-manylinux1_x86_64.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: joeynmt, pyyaml\n",
            "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for joeynmt: filename=joeynmt-0.0.1-cp36-none-any.whl size=72136 sha256=18f9b987041cdc5803f19a9fcf70eac72f38fc6db0d7104d306349a977414a5a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-id0kdhdp/wheels/db/01/db/751cc9f3e7f6faec127c43644ba250a3ea7ad200594aeda70a\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3-cp36-cp36m-linux_x86_64.whl size=44229 sha256=e21f6b2da4a597dfaa02b2a4b192841eeea40bf69044fe72657e3897ae4013e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/76/4d/a95b8dd7b452b69e8ed4f68b69e1b55e12c9c9624dd962b191\n",
            "Successfully built joeynmt pyyaml\n",
            "Installing collected packages: portalocker, sacrebleu, subword-nmt, pyyaml, mccabe, isort, typed-ast, lazy-object-proxy, astroid, pylint, joeynmt\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed astroid-2.3.3 isort-4.3.21 joeynmt-0.0.1 lazy-object-proxy-1.4.3 mccabe-0.6.1 portalocker-1.5.2 pylint-2.4.4 pyyaml-5.3 sacrebleu-1.4.3 subword-nmt-0.3.7 typed-ast-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AaE77Tcppex9"
      },
      "source": [
        "# Preprocessing the Data into Subword BPE Tokens\n",
        "\n",
        "- One of the most powerful improvements for agglutinative languages (a feature of most Bantu languages) is using BPE tokenization [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n",
        "\n",
        "- It was also shown that by optimizing the umber of BPE codes we significantly improve results for low-resourced languages [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021) [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)\n",
        "\n",
        "- Below we have the scripts for doing BPE tokenization of our data. We use 4000 tokens as recommended by [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021). You do not need to change anything. Simply running the below will be suitable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H-TyjtmXB1mL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "9cd40ef0-100e-4d1c-b2c5-739035ca4205"
      },
      "source": [
        "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
        "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
        "\n",
        "# Do subword NMT\n",
        "from os import path\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "\n",
        "# Learn BPEs on the training data.\n",
        "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "# Apply BPE splits to the development and test data.\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
        "\n",
        "# Create directory, move everyone we care about to the correct location\n",
        "! mkdir -p $data_path\n",
        "! cp train.* $data_path\n",
        "! cp test.* $data_path\n",
        "! cp dev.* $data_path\n",
        "! cp bpe.codes.4000 $data_path\n",
        "! ls $data_path\n",
        "\n",
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\"\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# Some output\n",
        "! echo \"BPE Igbo Sentences\"\n",
        "! tail -n 5 test.bpe.$tgt\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! tail -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman\n",
        "! cp joeynmt/data/$src$tgt/vocab.txt \"$gdrive_path\""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.en\t     test.bpe.ig     test.ig\t   train.en\n",
            "dev.bpe.en\tdev.ig\t     test.en\t     train.bpe.en  train.ig\n",
            "dev.bpe.ig\ttest.bpe.en  test.en-any.en  train.bpe.ig\n",
            "bpe.codes.4000\tdev.en\t     test.bpe.ig     test.ig\t   train.en\n",
            "dev.bpe.en\tdev.ig\t     test.en\t     train.bpe.en  train.ig\n",
            "dev.bpe.ig\ttest.bpe.en  test.en-any.en  train.bpe.ig\n",
            "BPE Igbo Sentences\n",
            "Gịnị mere H@@ us@@ ha@@ ị ji kwesị inwe obi ike ka o nwee ike ịkw@@ ado Chineke ?\n",
            "Gịnị mere anyị ji kwesị inwe obi ike ka anyị nwee ike ịna - akwado Jehova ?\n",
            "M kpere ekpere ka Jehova nye m obi ike ime ihe m kpebiri .\n",
            "I@@ we ha ad@@ aj@@ ụ@@ ọla ugbu a . M na - ag@@ azi ahụ ha mgbe niile . ” — Gụọ Ilu 29 : 25 .\n",
            "[ 1 ] ( par@@ ag@@ ra@@ f nke 7 ) A@@ ha a kpọrọ ụfọdụ ndị n’isiokwu a abụghị ezigbo aha ha .\n",
            "Combined BPE Vocab\n",
            "ạ@@\n",
            "ô\n",
            "ọgar@@\n",
            "̇\n",
            "ş\n",
            "ů@@\n",
            "◀\n",
            "eazụ\n",
            "ʺ\n",
            "×\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IlMitUHR8Qy-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "951dce57-1198-4d00-ed95-3a9fcde69a44"
      },
      "source": [
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.en\t     test.bpe.ig     test.ig\t   train.en\n",
            "dev.bpe.en\tdev.ig\t     test.en\t     train.bpe.en  train.ig\n",
            "dev.bpe.ig\ttest.bpe.en  test.en-any.en  train.bpe.ig  vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ixmzi60WsUZ8"
      },
      "source": [
        "# Creating the JoeyNMT Config\n",
        "\n",
        "JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n",
        "\n",
        "- We used Transformer architecture \n",
        "- We set our dropout to reasonably high: 0.3 (recommended in  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
        "\n",
        "Things worth playing with:\n",
        "- The batch size (also recommended to change for low-resourced languages)\n",
        "- The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n",
        "- The decoder options (beam_size, alpha)\n",
        "- Evaluation metrics (BLEU versus Crhf4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PIs1lY2hxMsl",
        "colab": {}
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"{gdrive_path}/train.bpe\"\n",
        "    dev:   \"{gdrive_path}/dev.bpe\"\n",
        "    test:  \"{gdrive_path}/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"{gdrive_path}/vocab.txt\"\n",
        "    trg_vocab: \"{gdrive_path}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"{gdrive_path}/models/{name}_transformer\"\n",
        "    overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)\n",
        "\n",
        "! cp joeynmt/configs/transformer_$src$tgt.yaml \"$gdrive_path\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pIifxE3Qzuvs"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "This single line of joeynmt runs the training using the config we made above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6ZBPFwT94WpI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "707674ec-e1e7-4ddd-98bb-2b5b2b73263b"
      },
      "source": [
        "# Train the model\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "# !cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml\n",
        "!python3 -m joeynmt train \"$gdrive_path/transformer_$src$tgt.yaml\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-12 19:01:36,159 Hello! This is Joey-NMT.\n",
            "2020-01-12 19:01:37,233 Total params: 12188928\n",
            "2020-01-12 19:01:37,234 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']\n",
            "2020-01-12 19:01:41,242 cfg.name                           : enig_transformer\n",
            "2020-01-12 19:01:41,243 cfg.data.src                       : en\n",
            "2020-01-12 19:01:41,243 cfg.data.trg                       : ig\n",
            "2020-01-12 19:01:41,243 cfg.data.train                     : /content/drive/My Drive/masakhane/en-ig-baseline/train.bpe\n",
            "2020-01-12 19:01:41,243 cfg.data.dev                       : /content/drive/My Drive/masakhane/en-ig-baseline/dev.bpe\n",
            "2020-01-12 19:01:41,243 cfg.data.test                      : /content/drive/My Drive/masakhane/en-ig-baseline/test.bpe\n",
            "2020-01-12 19:01:41,243 cfg.data.level                     : bpe\n",
            "2020-01-12 19:01:41,244 cfg.data.lowercase                 : False\n",
            "2020-01-12 19:01:41,244 cfg.data.max_sent_length           : 100\n",
            "2020-01-12 19:01:41,244 cfg.data.src_vocab                 : /content/drive/My Drive/masakhane/en-ig-baseline/vocab.txt\n",
            "2020-01-12 19:01:41,244 cfg.data.trg_vocab                 : /content/drive/My Drive/masakhane/en-ig-baseline/vocab.txt\n",
            "2020-01-12 19:01:41,244 cfg.testing.beam_size              : 5\n",
            "2020-01-12 19:01:41,244 cfg.testing.alpha                  : 1.0\n",
            "2020-01-12 19:01:41,244 cfg.training.random_seed           : 42\n",
            "2020-01-12 19:01:41,245 cfg.training.optimizer             : adam\n",
            "2020-01-12 19:01:41,245 cfg.training.normalization         : tokens\n",
            "2020-01-12 19:01:41,245 cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2020-01-12 19:01:41,245 cfg.training.scheduling            : plateau\n",
            "2020-01-12 19:01:41,245 cfg.training.patience              : 5\n",
            "2020-01-12 19:01:41,245 cfg.training.learning_rate_factor  : 0.5\n",
            "2020-01-12 19:01:41,245 cfg.training.learning_rate_warmup  : 1000\n",
            "2020-01-12 19:01:41,246 cfg.training.decrease_factor       : 0.7\n",
            "2020-01-12 19:01:41,246 cfg.training.loss                  : crossentropy\n",
            "2020-01-12 19:01:41,246 cfg.training.learning_rate         : 0.0003\n",
            "2020-01-12 19:01:41,246 cfg.training.learning_rate_min     : 1e-08\n",
            "2020-01-12 19:01:41,246 cfg.training.weight_decay          : 0.0\n",
            "2020-01-12 19:01:41,246 cfg.training.label_smoothing       : 0.1\n",
            "2020-01-12 19:01:41,246 cfg.training.batch_size            : 4096\n",
            "2020-01-12 19:01:41,246 cfg.training.batch_type            : token\n",
            "2020-01-12 19:01:41,247 cfg.training.eval_batch_size       : 3600\n",
            "2020-01-12 19:01:41,247 cfg.training.eval_batch_type       : token\n",
            "2020-01-12 19:01:41,247 cfg.training.batch_multiplier      : 1\n",
            "2020-01-12 19:01:41,247 cfg.training.early_stopping_metric : ppl\n",
            "2020-01-12 19:01:41,247 cfg.training.epochs                : 30\n",
            "2020-01-12 19:01:41,247 cfg.training.validation_freq       : 1000\n",
            "2020-01-12 19:01:41,247 cfg.training.logging_freq          : 100\n",
            "2020-01-12 19:01:41,248 cfg.training.eval_metric           : bleu\n",
            "2020-01-12 19:01:41,248 cfg.training.model_dir             : /content/drive/My Drive/masakhane/en-ig-baseline/models/enig_transformer\n",
            "2020-01-12 19:01:41,248 cfg.training.overwrite             : True\n",
            "2020-01-12 19:01:41,248 cfg.training.shuffle               : True\n",
            "2020-01-12 19:01:41,248 cfg.training.use_cuda              : True\n",
            "2020-01-12 19:01:41,248 cfg.training.max_output_length     : 100\n",
            "2020-01-12 19:01:41,249 cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2020-01-12 19:01:41,249 cfg.training.keep_last_ckpts       : 3\n",
            "2020-01-12 19:01:41,249 cfg.model.initializer              : xavier\n",
            "2020-01-12 19:01:41,249 cfg.model.bias_initializer         : zeros\n",
            "2020-01-12 19:01:41,249 cfg.model.init_gain                : 1.0\n",
            "2020-01-12 19:01:41,249 cfg.model.embed_initializer        : xavier\n",
            "2020-01-12 19:01:41,249 cfg.model.embed_init_gain          : 1.0\n",
            "2020-01-12 19:01:41,250 cfg.model.tied_embeddings          : True\n",
            "2020-01-12 19:01:41,250 cfg.model.tied_softmax             : True\n",
            "2020-01-12 19:01:41,250 cfg.model.encoder.type             : transformer\n",
            "2020-01-12 19:01:41,250 cfg.model.encoder.num_layers       : 6\n",
            "2020-01-12 19:01:41,250 cfg.model.encoder.num_heads        : 4\n",
            "2020-01-12 19:01:41,250 cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2020-01-12 19:01:41,251 cfg.model.encoder.embeddings.scale : True\n",
            "2020-01-12 19:01:41,251 cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2020-01-12 19:01:41,251 cfg.model.encoder.hidden_size      : 256\n",
            "2020-01-12 19:01:41,251 cfg.model.encoder.ff_size          : 1024\n",
            "2020-01-12 19:01:41,251 cfg.model.encoder.dropout          : 0.3\n",
            "2020-01-12 19:01:41,251 cfg.model.decoder.type             : transformer\n",
            "2020-01-12 19:01:41,251 cfg.model.decoder.num_layers       : 6\n",
            "2020-01-12 19:01:41,252 cfg.model.decoder.num_heads        : 4\n",
            "2020-01-12 19:01:41,252 cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2020-01-12 19:01:41,252 cfg.model.decoder.embeddings.scale : True\n",
            "2020-01-12 19:01:41,252 cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2020-01-12 19:01:41,252 cfg.model.decoder.hidden_size      : 256\n",
            "2020-01-12 19:01:41,252 cfg.model.decoder.ff_size          : 1024\n",
            "2020-01-12 19:01:41,252 cfg.model.decoder.dropout          : 0.3\n",
            "2020-01-12 19:01:41,253 Data set sizes: \n",
            "\ttrain 414467,\n",
            "\tvalid 1000,\n",
            "\ttest 2700\n",
            "2020-01-12 19:01:41,253 First training example:\n",
            "\t[SRC] “ Wh@@ atever we ask we receive from him , ” wrote the apostle John , “ because we are ob@@ serving his comman@@ d@@ ments and are doing the things that are pleas@@ ing in his eyes . ”\n",
            "\t[TRG] “ Anyị na - an@@ at@@ akwa ihe ọ bụla anyị rị@@ ọrọ n’aka ya , ” ka Jọn onyeozi dere , “ n’ihi na anyị na - edebe ihe ndị o nyere n’iwu , na - emekwa ihe ndị dị mma n’anya ya . ”\n",
            "2020-01-12 19:01:41,253 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) na (7) - (8) the (9) a\n",
            "2020-01-12 19:01:41,254 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) na (7) - (8) the (9) a\n",
            "2020-01-12 19:01:41,254 Number of Src words (types): 4409\n",
            "2020-01-12 19:01:41,254 Number of Trg words (types): 4409\n",
            "2020-01-12 19:01:41,254 Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4409),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4409))\n",
            "2020-01-12 19:01:41,263 EPOCH 1\n",
            "2020-01-12 19:01:56,468 Epoch   1 Step:      100 Batch Loss:     5.102782 Tokens per Sec:    15136, Lr: 0.000300\n",
            "2020-01-12 19:02:11,076 Epoch   1 Step:      200 Batch Loss:     4.873945 Tokens per Sec:    16009, Lr: 0.000300\n",
            "2020-01-12 19:02:25,698 Epoch   1 Step:      300 Batch Loss:     4.682858 Tokens per Sec:    15464, Lr: 0.000300\n",
            "2020-01-12 19:02:40,382 Epoch   1 Step:      400 Batch Loss:     4.478440 Tokens per Sec:    15535, Lr: 0.000300\n",
            "2020-01-12 19:02:55,022 Epoch   1 Step:      500 Batch Loss:     4.081635 Tokens per Sec:    15586, Lr: 0.000300\n",
            "2020-01-12 19:03:09,953 Epoch   1 Step:      600 Batch Loss:     4.257251 Tokens per Sec:    15427, Lr: 0.000300\n",
            "2020-01-12 19:03:24,819 Epoch   1 Step:      700 Batch Loss:     4.279203 Tokens per Sec:    15178, Lr: 0.000300\n",
            "2020-01-12 19:03:39,868 Epoch   1 Step:      800 Batch Loss:     4.113895 Tokens per Sec:    15441, Lr: 0.000300\n",
            "2020-01-12 19:03:54,849 Epoch   1 Step:      900 Batch Loss:     3.948514 Tokens per Sec:    15119, Lr: 0.000300\n",
            "2020-01-12 19:04:09,979 Epoch   1 Step:     1000 Batch Loss:     3.819502 Tokens per Sec:    15508, Lr: 0.000300\n",
            "2020-01-12 19:04:56,567 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 19:04:56,568 Saving new checkpoint.\n",
            "2020-01-12 19:04:57,667 Example #0\n",
            "2020-01-12 19:04:57,667 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 19:04:57,667 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 19:04:57,668 \tHypothesis: Ọ bụ ezie na ọ bụ na - eme ka ọ bụ na - eme ka ọ bụ na - eme ka ọ bụ na - eme ka ọ bụ na - eme ka ọ bụ na - eme ka ọ na - eme ka ọ bụ na - eme ka ọ na - eme ka ọ na - eme ka ọ bụ na - eme ka ọ bụ na - eme ka ọ bụ na - eme ka ọ na - eme ka ọ bụ na - eme ka ọ bụ na - eme ka ọ na - eme\n",
            "2020-01-12 19:04:57,668 Example #1\n",
            "2020-01-12 19:04:57,668 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 19:04:57,668 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 19:04:57,668 \tHypothesis: Ọ bụrụ na ọ bụ na ọ bụ na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na -\n",
            "2020-01-12 19:04:57,668 Example #2\n",
            "2020-01-12 19:04:57,668 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 19:04:57,668 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 19:04:57,668 \tHypothesis: Ọ bụ ezie na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na - eme ka anyị na -\n",
            "2020-01-12 19:04:57,669 Example #3\n",
            "2020-01-12 19:04:57,669 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 19:04:57,669 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 19:04:57,669 \tHypothesis: Ọ bụ ezie na - eme ka ọ bụ na - eme ka ọ bụ na - eme ka ọ bụ na - eme ka ọ bụ na - eme ka ọ bụ na - eme ka ọ na - eme ka ọ bụ na - eme ka ọ na - eme ka ọ bụ na - eme ka ọ bụ na - eme ka ọ bụ na - eme ka ọ bụ na - eme ka ọ na - eme .\n",
            "2020-01-12 19:04:57,669 Validation result (greedy) at epoch   1, step     1000: bleu:   0.63, loss: 110350.1797, ppl:  44.3149, duration: 47.6892s\n",
            "2020-01-12 19:05:13,099 Epoch   1 Step:     1100 Batch Loss:     3.951693 Tokens per Sec:    14754, Lr: 0.000300\n",
            "2020-01-12 19:05:28,283 Epoch   1 Step:     1200 Batch Loss:     3.630899 Tokens per Sec:    15088, Lr: 0.000300\n",
            "2020-01-12 19:05:43,427 Epoch   1 Step:     1300 Batch Loss:     4.112037 Tokens per Sec:    15168, Lr: 0.000300\n",
            "2020-01-12 19:05:58,682 Epoch   1 Step:     1400 Batch Loss:     3.341799 Tokens per Sec:    14727, Lr: 0.000300\n",
            "2020-01-12 19:06:13,944 Epoch   1 Step:     1500 Batch Loss:     3.554186 Tokens per Sec:    14836, Lr: 0.000300\n",
            "2020-01-12 19:06:29,255 Epoch   1 Step:     1600 Batch Loss:     3.609030 Tokens per Sec:    14927, Lr: 0.000300\n",
            "2020-01-12 19:06:44,619 Epoch   1 Step:     1700 Batch Loss:     3.718790 Tokens per Sec:    15047, Lr: 0.000300\n",
            "2020-01-12 19:07:00,083 Epoch   1 Step:     1800 Batch Loss:     3.243469 Tokens per Sec:    14899, Lr: 0.000300\n",
            "2020-01-12 19:07:15,395 Epoch   1 Step:     1900 Batch Loss:     3.473003 Tokens per Sec:    14862, Lr: 0.000300\n",
            "2020-01-12 19:07:30,617 Epoch   1 Step:     2000 Batch Loss:     2.964391 Tokens per Sec:    14946, Lr: 0.000300\n",
            "2020-01-12 19:08:18,213 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 19:08:18,213 Saving new checkpoint.\n",
            "2020-01-12 19:08:19,191 Example #0\n",
            "2020-01-12 19:08:19,192 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 19:08:19,192 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 19:08:19,192 \tHypothesis: Ọ bụ ezie na anyị na - eme ihe ndị a na - eme ka anyị na - eme ka anyị na - eme ihe ndị anyị na - eme .\n",
            "2020-01-12 19:08:19,192 Example #1\n",
            "2020-01-12 19:08:19,193 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 19:08:19,193 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 19:08:19,193 \tHypothesis: Gịnị ka ihe ndị na - eme ka ha na - eme ka ha na - eme ihe ndị na - eme ?\n",
            "2020-01-12 19:08:19,193 Example #2\n",
            "2020-01-12 19:08:19,193 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 19:08:19,193 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 19:08:19,193 \tHypothesis: Ha na - eme ka ndị na - eso ụzọ Jizọs , bụ́ ndị na - eso ụzọ ya na - eso ụzọ ya , “ ndị na - eso ụzọ . ”\n",
            "2020-01-12 19:08:19,193 Example #3\n",
            "2020-01-12 19:08:19,193 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 19:08:19,194 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 19:08:19,194 \tHypothesis: Nke a bụ na ọ bụ na Chineke na - eme ka o nwee ike ịna - eme ihe .\n",
            "2020-01-12 19:08:19,194 Validation result (greedy) at epoch   1, step     2000: bleu:   4.21, loss: 93340.1172, ppl:  24.7025, duration: 48.5760s\n",
            "2020-01-12 19:08:34,542 Epoch   1 Step:     2100 Batch Loss:     3.258224 Tokens per Sec:    14744, Lr: 0.000300\n",
            "2020-01-12 19:08:50,130 Epoch   1 Step:     2200 Batch Loss:     3.070368 Tokens per Sec:    15005, Lr: 0.000300\n",
            "2020-01-12 19:09:05,393 Epoch   1 Step:     2300 Batch Loss:     3.172004 Tokens per Sec:    14560, Lr: 0.000300\n",
            "2020-01-12 19:09:20,836 Epoch   1 Step:     2400 Batch Loss:     3.360785 Tokens per Sec:    15054, Lr: 0.000300\n",
            "2020-01-12 19:09:36,185 Epoch   1 Step:     2500 Batch Loss:     3.350886 Tokens per Sec:    15030, Lr: 0.000300\n",
            "2020-01-12 19:09:51,747 Epoch   1 Step:     2600 Batch Loss:     3.055159 Tokens per Sec:    15212, Lr: 0.000300\n",
            "2020-01-12 19:10:07,240 Epoch   1 Step:     2700 Batch Loss:     3.014586 Tokens per Sec:    14785, Lr: 0.000300\n",
            "2020-01-12 19:10:22,862 Epoch   1 Step:     2800 Batch Loss:     3.070951 Tokens per Sec:    14819, Lr: 0.000300\n",
            "2020-01-12 19:10:38,359 Epoch   1 Step:     2900 Batch Loss:     3.360399 Tokens per Sec:    15134, Lr: 0.000300\n",
            "2020-01-12 19:10:53,684 Epoch   1 Step:     3000 Batch Loss:     3.298276 Tokens per Sec:    14717, Lr: 0.000300\n",
            "2020-01-12 19:11:41,497 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 19:11:41,497 Saving new checkpoint.\n",
            "2020-01-12 19:11:42,550 Example #0\n",
            "2020-01-12 19:11:42,551 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 19:11:42,551 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 19:11:42,551 \tHypothesis: Anyị na - eme ka anyị na - eme ka onye amụma ahụ na - eme ka anyị na - eme ihe ndị a na - eme ka anyị nwee ike ime ka anyị nwee ike ịna - eme ihe .\n",
            "2020-01-12 19:11:42,551 Example #1\n",
            "2020-01-12 19:11:42,551 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 19:11:42,552 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 19:11:42,552 \tHypothesis: Gịnị mere o ji dị mkpa ka onye na - eto eto eto eto eto na - eme ka ọ bụrụ na ọ bụ onye na - eso ụzọ Jizọs ?\n",
            "2020-01-12 19:11:42,552 Example #2\n",
            "2020-01-12 19:11:42,552 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 19:11:42,552 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 19:11:42,553 \tHypothesis: Ha na - eme ka ndị na - eso ụzọ Kraịst na - eso ụzọ Jizọs , bụ́ ndị na - eso ụzọ ya “ ndị mmụọ nsọ ” na - ekwusa ozi ọma .\n",
            "2020-01-12 19:11:42,553 Example #3\n",
            "2020-01-12 19:11:42,553 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 19:11:42,553 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 19:11:42,554 \tHypothesis: Nke a bụ ihe karịrị afọ iri na ise na ise .\n",
            "2020-01-12 19:11:42,554 Validation result (greedy) at epoch   1, step     3000: bleu:   6.20, loss: 83622.4219, ppl:  17.6905, duration: 48.8696s\n",
            "2020-01-12 19:11:58,130 Epoch   1 Step:     3100 Batch Loss:     3.091814 Tokens per Sec:    14940, Lr: 0.000300\n",
            "2020-01-12 19:12:13,515 Epoch   1 Step:     3200 Batch Loss:     2.823326 Tokens per Sec:    14846, Lr: 0.000300\n",
            "2020-01-12 19:12:28,876 Epoch   1 Step:     3300 Batch Loss:     2.790272 Tokens per Sec:    14567, Lr: 0.000300\n",
            "2020-01-12 19:12:44,401 Epoch   1 Step:     3400 Batch Loss:     3.120058 Tokens per Sec:    15072, Lr: 0.000300\n",
            "2020-01-12 19:12:59,858 Epoch   1 Step:     3500 Batch Loss:     2.711159 Tokens per Sec:    14786, Lr: 0.000300\n",
            "2020-01-12 19:13:15,354 Epoch   1 Step:     3600 Batch Loss:     3.260988 Tokens per Sec:    14859, Lr: 0.000300\n",
            "2020-01-12 19:13:30,774 Epoch   1 Step:     3700 Batch Loss:     3.191509 Tokens per Sec:    14806, Lr: 0.000300\n",
            "2020-01-12 19:13:46,234 Epoch   1 Step:     3800 Batch Loss:     2.769399 Tokens per Sec:    15106, Lr: 0.000300\n",
            "2020-01-12 19:14:01,599 Epoch   1 Step:     3900 Batch Loss:     2.544449 Tokens per Sec:    14738, Lr: 0.000300\n",
            "2020-01-12 19:14:17,173 Epoch   1 Step:     4000 Batch Loss:     2.964627 Tokens per Sec:    14813, Lr: 0.000300\n",
            "2020-01-12 19:15:04,972 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 19:15:04,972 Saving new checkpoint.\n",
            "2020-01-12 19:15:06,076 Example #0\n",
            "2020-01-12 19:15:06,077 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 19:15:06,077 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 19:15:06,078 \tHypothesis: Anyị na - eche na anyị na - eme ka Nna anyị na - ahụ n’anya na - enwe obi ụtọ .\n",
            "2020-01-12 19:15:06,078 Example #1\n",
            "2020-01-12 19:15:06,078 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 19:15:06,079 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 19:15:06,079 \tHypothesis: Gịnị ka anyị kwesịrị ime iji na - eme ihe ndị na - eme ka alụmdi na nwunye na nwunye na - enwe obi ụtọ ?\n",
            "2020-01-12 19:15:06,079 Example #2\n",
            "2020-01-12 19:15:06,080 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 19:15:06,080 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 19:15:06,080 \tHypothesis: Ha na - eme ka ndị enyi Kraịst na - eso ụzọ Kraịst na - eje ozi , ha na - enwe ike ịna - ekwusa ozi ọma Alaeze ahụ .\n",
            "2020-01-12 19:15:06,080 Example #3\n",
            "2020-01-12 19:15:06,081 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 19:15:06,081 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 19:15:06,081 \tHypothesis: Nke a bụ ihe abụọ a na - eme ka obi sie ike na Chineke .\n",
            "2020-01-12 19:15:06,081 Validation result (greedy) at epoch   1, step     4000: bleu:   9.01, loss: 77625.0312, ppl:  14.3964, duration: 48.9076s\n",
            "2020-01-12 19:15:21,624 Epoch   1 Step:     4100 Batch Loss:     2.923499 Tokens per Sec:    14926, Lr: 0.000300\n",
            "2020-01-12 19:15:37,072 Epoch   1 Step:     4200 Batch Loss:     2.807016 Tokens per Sec:    14881, Lr: 0.000300\n",
            "2020-01-12 19:15:52,642 Epoch   1 Step:     4300 Batch Loss:     2.658695 Tokens per Sec:    14685, Lr: 0.000300\n",
            "2020-01-12 19:16:07,992 Epoch   1 Step:     4400 Batch Loss:     2.610003 Tokens per Sec:    14721, Lr: 0.000300\n",
            "2020-01-12 19:16:23,513 Epoch   1 Step:     4500 Batch Loss:     2.858058 Tokens per Sec:    14690, Lr: 0.000300\n",
            "2020-01-12 19:16:38,803 Epoch   1 Step:     4600 Batch Loss:     2.275445 Tokens per Sec:    14910, Lr: 0.000300\n",
            "2020-01-12 19:16:54,134 Epoch   1 Step:     4700 Batch Loss:     2.861472 Tokens per Sec:    14728, Lr: 0.000300\n",
            "2020-01-12 19:17:09,650 Epoch   1 Step:     4800 Batch Loss:     2.684176 Tokens per Sec:    14884, Lr: 0.000300\n",
            "2020-01-12 19:17:25,016 Epoch   1 Step:     4900 Batch Loss:     2.610985 Tokens per Sec:    14873, Lr: 0.000300\n",
            "2020-01-12 19:17:40,658 Epoch   1 Step:     5000 Batch Loss:     2.987013 Tokens per Sec:    15068, Lr: 0.000300\n",
            "2020-01-12 19:18:28,607 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 19:18:28,608 Saving new checkpoint.\n",
            "2020-01-12 19:18:29,646 Example #0\n",
            "2020-01-12 19:18:29,647 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 19:18:29,647 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 19:18:29,647 \tHypothesis: Anyị na - eme ka Nna anyị nke eluigwe na - achị anyị na ndị na - eto eto eto na - enwe mmetụta dị otú ahụ .\n",
            "2020-01-12 19:18:29,647 Example #1\n",
            "2020-01-12 19:18:29,647 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 19:18:29,647 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 19:18:29,647 \tHypothesis: Gịnị ka anyị kwesịrị ime iji na - eme ihe ndị a na - eme ka alụmdi na nwunye na - eme ihe Setan kwuru ?\n",
            "2020-01-12 19:18:29,648 Example #2\n",
            "2020-01-12 19:18:29,648 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 19:18:29,648 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 19:18:29,648 \tHypothesis: Ha na - eme ka ndị enyi Kraịst na - efe Chineke , bụ́ ndị na - eme ihe , na - eme ka ha nwee ike ịna - ekwusa ozi ọma Alaeze ahụ .\n",
            "2020-01-12 19:18:29,648 Example #3\n",
            "2020-01-12 19:18:29,648 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 19:18:29,648 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 19:18:29,648 \tHypothesis: Nke abụọ bụ ihe abụọ a na - eme ka mmadụ na - eme ihe Chineke kwuru .\n",
            "2020-01-12 19:18:29,648 Validation result (greedy) at epoch   1, step     5000: bleu:  10.67, loss: 73021.6641, ppl:  12.2904, duration: 48.9903s\n",
            "2020-01-12 19:18:45,122 Epoch   1: total training loss 17076.52\n",
            "2020-01-12 19:18:45,122 EPOCH 2\n",
            "2020-01-12 19:18:45,750 Epoch   2 Step:     5100 Batch Loss:     2.231782 Tokens per Sec:     2714, Lr: 0.000300\n",
            "2020-01-12 19:19:01,136 Epoch   2 Step:     5200 Batch Loss:     2.595849 Tokens per Sec:    14887, Lr: 0.000300\n",
            "2020-01-12 19:19:16,525 Epoch   2 Step:     5300 Batch Loss:     3.035987 Tokens per Sec:    14780, Lr: 0.000300\n",
            "2020-01-12 19:19:32,039 Epoch   2 Step:     5400 Batch Loss:     2.325001 Tokens per Sec:    14914, Lr: 0.000300\n",
            "2020-01-12 19:19:47,498 Epoch   2 Step:     5500 Batch Loss:     2.619349 Tokens per Sec:    14911, Lr: 0.000300\n",
            "2020-01-12 19:20:03,007 Epoch   2 Step:     5600 Batch Loss:     2.536304 Tokens per Sec:    14889, Lr: 0.000300\n",
            "2020-01-12 19:20:18,468 Epoch   2 Step:     5700 Batch Loss:     2.300004 Tokens per Sec:    14976, Lr: 0.000300\n",
            "2020-01-12 19:20:33,841 Epoch   2 Step:     5800 Batch Loss:     2.598516 Tokens per Sec:    14901, Lr: 0.000300\n",
            "2020-01-12 19:20:49,221 Epoch   2 Step:     5900 Batch Loss:     2.733305 Tokens per Sec:    14872, Lr: 0.000300\n",
            "2020-01-12 19:21:04,561 Epoch   2 Step:     6000 Batch Loss:     2.511631 Tokens per Sec:    14593, Lr: 0.000300\n",
            "2020-01-12 19:21:52,388 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 19:21:52,388 Saving new checkpoint.\n",
            "2020-01-12 19:21:53,429 Example #0\n",
            "2020-01-12 19:21:53,429 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 19:21:53,429 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 19:21:53,430 \tHypothesis: Ịmata na anyị nwere Onye kere eluigwe na onye hụrụ n’anya , bụ́ onye hụrụ anyị n’anya , na - echebara anyị echiche .\n",
            "2020-01-12 19:21:53,430 Example #1\n",
            "2020-01-12 19:21:53,430 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 19:21:53,430 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 19:21:53,430 \tHypothesis: Gịnị ka anyị kwesịrị ime iji oge na - aga , na - eme ka alụmdi na nwunye ghara inwe ohere ime ihe Setan kwuru ?\n",
            "2020-01-12 19:21:53,430 Example #2\n",
            "2020-01-12 19:21:53,430 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 19:21:53,430 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 19:21:53,430 \tHypothesis: Ha na - efe ndị enyi Kraịst , bụ́ ndị na - efe Chineke , na - eme ka ha nwee ike iji ozi ọma a na - ekwusa ozi ọma Alaeze ahụ .\n",
            "2020-01-12 19:21:53,431 Example #3\n",
            "2020-01-12 19:21:53,431 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 19:21:53,431 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 19:21:53,431 \tHypothesis: Nke a bụ ihe mgbaru ọsọ nke abụọ Chineke na - eme ka anyị ghara ime .\n",
            "2020-01-12 19:21:53,431 Validation result (greedy) at epoch   2, step     6000: bleu:  12.05, loss: 69382.8438, ppl:  10.8460, duration: 48.8696s\n",
            "2020-01-12 19:22:08,853 Epoch   2 Step:     6100 Batch Loss:     2.705502 Tokens per Sec:    14868, Lr: 0.000300\n",
            "2020-01-12 19:22:24,493 Epoch   2 Step:     6200 Batch Loss:     2.570399 Tokens per Sec:    14947, Lr: 0.000300\n",
            "2020-01-12 19:22:39,692 Epoch   2 Step:     6300 Batch Loss:     2.920894 Tokens per Sec:    14517, Lr: 0.000300\n",
            "2020-01-12 19:22:55,172 Epoch   2 Step:     6400 Batch Loss:     2.328222 Tokens per Sec:    14796, Lr: 0.000300\n",
            "2020-01-12 19:23:10,731 Epoch   2 Step:     6500 Batch Loss:     2.593675 Tokens per Sec:    14791, Lr: 0.000300\n",
            "2020-01-12 19:23:26,174 Epoch   2 Step:     6600 Batch Loss:     2.684051 Tokens per Sec:    14963, Lr: 0.000300\n",
            "2020-01-12 19:23:41,576 Epoch   2 Step:     6700 Batch Loss:     2.198647 Tokens per Sec:    14835, Lr: 0.000300\n",
            "2020-01-12 19:23:57,340 Epoch   2 Step:     6800 Batch Loss:     2.399312 Tokens per Sec:    15020, Lr: 0.000300\n",
            "2020-01-12 19:24:12,727 Epoch   2 Step:     6900 Batch Loss:     2.588180 Tokens per Sec:    14788, Lr: 0.000300\n",
            "2020-01-12 19:24:28,213 Epoch   2 Step:     7000 Batch Loss:     2.460248 Tokens per Sec:    14793, Lr: 0.000300\n",
            "2020-01-12 19:25:16,091 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 19:25:16,092 Saving new checkpoint.\n",
            "2020-01-12 19:25:17,147 Example #0\n",
            "2020-01-12 19:25:17,148 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 19:25:17,148 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 19:25:17,148 \tHypothesis: Ịmata na anyị nwere Nna nke eluigwe na ndị hụrụ anyị n’anya na - enwe mmetụta dị ukwuu nke ịdị mkpa nke ịdị na - echegbu onwe anyị .\n",
            "2020-01-12 19:25:17,148 Example #1\n",
            "2020-01-12 19:25:17,149 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 19:25:17,149 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 19:25:17,149 \tHypothesis: Gịnị ka anyị kwesịrị ime iji na - eme ihe ịga nke ọma n’alụmdi na nwunye Setan ?\n",
            "2020-01-12 19:25:17,149 Example #2\n",
            "2020-01-12 19:25:17,150 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 19:25:17,150 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 19:25:17,150 \tHypothesis: Ha na - eme ka ndị enyi Kraịst na - eguzosi ike n’ihe , na - enye aka na - enye ụmụnna anyị aka ‘ ozi ọma ’ nke a na - ekwusa ozi ọma .\n",
            "2020-01-12 19:25:17,150 Example #3\n",
            "2020-01-12 19:25:17,151 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 19:25:17,151 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 19:25:17,151 \tHypothesis: Nke a bụ ihe abụọ nke abụọ nke Chineke na - eme ka o mee ka a ghara ịhapụ ya .\n",
            "2020-01-12 19:25:17,151 Validation result (greedy) at epoch   2, step     7000: bleu:  13.62, loss: 66273.4375, ppl:   9.7471, duration: 48.9377s\n",
            "2020-01-12 19:25:32,792 Epoch   2 Step:     7100 Batch Loss:     2.343935 Tokens per Sec:    14793, Lr: 0.000300\n",
            "2020-01-12 19:25:48,230 Epoch   2 Step:     7200 Batch Loss:     2.396335 Tokens per Sec:    14630, Lr: 0.000300\n",
            "2020-01-12 19:26:03,795 Epoch   2 Step:     7300 Batch Loss:     2.477901 Tokens per Sec:    14823, Lr: 0.000300\n",
            "2020-01-12 19:26:19,194 Epoch   2 Step:     7400 Batch Loss:     2.464233 Tokens per Sec:    14560, Lr: 0.000300\n",
            "2020-01-12 19:26:34,598 Epoch   2 Step:     7500 Batch Loss:     2.516159 Tokens per Sec:    14773, Lr: 0.000300\n",
            "2020-01-12 19:26:49,976 Epoch   2 Step:     7600 Batch Loss:     2.492342 Tokens per Sec:    14635, Lr: 0.000300\n",
            "2020-01-12 19:27:05,410 Epoch   2 Step:     7700 Batch Loss:     2.735834 Tokens per Sec:    14866, Lr: 0.000300\n",
            "2020-01-12 19:27:20,825 Epoch   2 Step:     7800 Batch Loss:     2.416639 Tokens per Sec:    15052, Lr: 0.000300\n",
            "2020-01-12 19:27:36,117 Epoch   2 Step:     7900 Batch Loss:     2.469671 Tokens per Sec:    14618, Lr: 0.000300\n",
            "2020-01-12 19:27:51,798 Epoch   2 Step:     8000 Batch Loss:     2.192414 Tokens per Sec:    15010, Lr: 0.000300\n",
            "2020-01-12 19:28:39,592 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 19:28:39,592 Saving new checkpoint.\n",
            "2020-01-12 19:28:40,695 Example #0\n",
            "2020-01-12 19:28:40,695 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 19:28:40,695 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 19:28:40,695 \tHypothesis: Ịghọta na anyị nwere Nna eluigwe nke eluigwe na - ahụ n’anya , na - echebara anyị echiche dị oké mkpa .\n",
            "2020-01-12 19:28:40,696 Example #1\n",
            "2020-01-12 19:28:40,696 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 19:28:40,696 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 19:28:40,696 \tHypothesis: Gịnị ka anyị kwesịrị ime iji mee ka alụmdi na nwunye ghara ịna - enwe ọganihu nke ime mgbalị Setan na - eme ?\n",
            "2020-01-12 19:28:40,697 Example #2\n",
            "2020-01-12 19:28:40,697 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 19:28:40,697 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 19:28:40,697 \tHypothesis: Ha na - anọgide na - eguzosi ike n’ezi ndị na - eguzosi ike n’ezi ihe Kraịst , na - enye aka n’ọrụ nkwusa a “ alaeze nke alaeze a ” maka mba nile .\n",
            "2020-01-12 19:28:40,697 Example #3\n",
            "2020-01-12 19:28:40,698 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 19:28:40,698 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 19:28:40,698 \tHypothesis: Nke ahụ bụ ihe abụọ Chineke mere ka o doo anya na ọ bụ ihe Chineke gwara ya .\n",
            "2020-01-12 19:28:40,698 Validation result (greedy) at epoch   2, step     8000: bleu:  15.56, loss: 63518.2070, ppl:   8.8667, duration: 48.8999s\n",
            "2020-01-12 19:28:56,419 Epoch   2 Step:     8100 Batch Loss:     2.987033 Tokens per Sec:    14947, Lr: 0.000300\n",
            "2020-01-12 19:29:11,782 Epoch   2 Step:     8200 Batch Loss:     2.294496 Tokens per Sec:    14583, Lr: 0.000300\n",
            "2020-01-12 19:29:27,277 Epoch   2 Step:     8300 Batch Loss:     2.506599 Tokens per Sec:    14842, Lr: 0.000300\n",
            "2020-01-12 19:29:42,835 Epoch   2 Step:     8400 Batch Loss:     2.432357 Tokens per Sec:    14777, Lr: 0.000300\n",
            "2020-01-12 19:29:58,241 Epoch   2 Step:     8500 Batch Loss:     2.354065 Tokens per Sec:    14637, Lr: 0.000300\n",
            "2020-01-12 19:30:13,802 Epoch   2 Step:     8600 Batch Loss:     2.218861 Tokens per Sec:    14744, Lr: 0.000300\n",
            "2020-01-12 19:30:29,256 Epoch   2 Step:     8700 Batch Loss:     2.357839 Tokens per Sec:    14680, Lr: 0.000300\n",
            "2020-01-12 19:30:44,664 Epoch   2 Step:     8800 Batch Loss:     2.120715 Tokens per Sec:    14975, Lr: 0.000300\n",
            "2020-01-12 19:31:00,005 Epoch   2 Step:     8900 Batch Loss:     2.338389 Tokens per Sec:    14320, Lr: 0.000300\n",
            "2020-01-12 19:31:15,505 Epoch   2 Step:     9000 Batch Loss:     2.529732 Tokens per Sec:    14759, Lr: 0.000300\n",
            "2020-01-12 19:32:03,461 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 19:32:03,461 Saving new checkpoint.\n",
            "2020-01-12 19:32:04,892 Example #0\n",
            "2020-01-12 19:32:04,892 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 19:32:04,892 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 19:32:04,892 \tHypothesis: Ịmara na anyị bụ Nna eluigwe na ndị hụrụ anyị n’anya , na - echebara anyị echiche .\n",
            "2020-01-12 19:32:04,892 Example #1\n",
            "2020-01-12 19:32:04,893 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 19:32:04,893 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 19:32:04,893 \tHypothesis: Gịnị ka anyị kwesịrị ime iji nwee ihe ịga nke alụmdi na nwunye Setan na - eme ka mgbalị Setan na - eme ?\n",
            "2020-01-12 19:32:04,893 Example #2\n",
            "2020-01-12 19:32:04,893 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 19:32:04,893 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 19:32:04,893 \tHypothesis: Ha na - agbalịsi ike ka ụmụnna Kraịst na - erubere ndị ọzọ isi , na - enye aka n’ọrụ nkwusa a “ alaeze ” nke mba nile .\n",
            "2020-01-12 19:32:04,893 Example #3\n",
            "2020-01-12 19:32:04,894 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 19:32:04,894 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 19:32:04,894 \tHypothesis: Nke abụọ bụ na ihe Chineke na - eme ka anyị ghara ime .\n",
            "2020-01-12 19:32:04,894 Validation result (greedy) at epoch   2, step     9000: bleu:  16.82, loss: 61569.1602, ppl:   8.2924, duration: 49.3889s\n",
            "2020-01-12 19:32:20,426 Epoch   2 Step:     9100 Batch Loss:     2.462310 Tokens per Sec:    14870, Lr: 0.000300\n",
            "2020-01-12 19:32:35,968 Epoch   2 Step:     9200 Batch Loss:     2.324891 Tokens per Sec:    14714, Lr: 0.000300\n",
            "2020-01-12 19:32:51,505 Epoch   2 Step:     9300 Batch Loss:     2.199579 Tokens per Sec:    14955, Lr: 0.000300\n",
            "2020-01-12 19:33:06,990 Epoch   2 Step:     9400 Batch Loss:     2.137403 Tokens per Sec:    14628, Lr: 0.000300\n",
            "2020-01-12 19:33:22,881 Epoch   2 Step:     9500 Batch Loss:     2.271494 Tokens per Sec:    14916, Lr: 0.000300\n",
            "2020-01-12 19:33:38,260 Epoch   2 Step:     9600 Batch Loss:     2.228008 Tokens per Sec:    14422, Lr: 0.000300\n",
            "2020-01-12 19:33:53,755 Epoch   2 Step:     9700 Batch Loss:     1.961855 Tokens per Sec:    14860, Lr: 0.000300\n",
            "2020-01-12 19:34:09,165 Epoch   2 Step:     9800 Batch Loss:     2.399347 Tokens per Sec:    14800, Lr: 0.000300\n",
            "2020-01-12 19:34:24,705 Epoch   2 Step:     9900 Batch Loss:     2.245614 Tokens per Sec:    14968, Lr: 0.000300\n",
            "2020-01-12 19:34:40,251 Epoch   2 Step:    10000 Batch Loss:     1.935881 Tokens per Sec:    14802, Lr: 0.000300\n",
            "2020-01-12 19:35:28,192 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 19:35:28,192 Saving new checkpoint.\n",
            "2020-01-12 19:35:29,272 Example #0\n",
            "2020-01-12 19:35:29,272 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 19:35:29,272 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 19:35:29,272 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe na - ahụ n’anya na - elekọta anyị dị oké mkpa iji chegbu onwe anyị .\n",
            "2020-01-12 19:35:29,273 Example #1\n",
            "2020-01-12 19:35:29,273 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 19:35:29,273 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 19:35:29,273 \tHypothesis: Gịnị ka anyị kwesịrị ime iji nwee ihe ịga nke ọma n’alụmdi na nwunye nke Setan na - eme ka mgbalị Setan na - eme ka e nwee nzube ?\n",
            "2020-01-12 19:35:29,273 Example #2\n",
            "2020-01-12 19:35:29,274 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 19:35:29,274 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 19:35:29,274 \tHypothesis: Ha na - aghọwo ndị enyi Kraịst , na - enye ihe ùgwù nke iji nyere ha aka ‘ ozi ọma ’ nke ikwusa ozi ọma a maka mba nile .\n",
            "2020-01-12 19:35:29,274 Example #3\n",
            "2020-01-12 19:35:29,274 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 19:35:29,274 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 19:35:29,274 \tHypothesis: Nke abụọ bụ ihe abụọ Chineke mere ka anyị ghara ime ka anyị na - agbaghara anyị .\n",
            "2020-01-12 19:35:29,274 Validation result (greedy) at epoch   2, step    10000: bleu:  17.47, loss: 60147.5312, ppl:   7.8971, duration: 49.0228s\n",
            "2020-01-12 19:35:44,785 Epoch   2 Step:    10100 Batch Loss:     2.177105 Tokens per Sec:    14870, Lr: 0.000300\n",
            "2020-01-12 19:36:00,307 Epoch   2 Step:    10200 Batch Loss:     1.980740 Tokens per Sec:    14550, Lr: 0.000300\n",
            "2020-01-12 19:36:01,728 Epoch   2: total training loss 12413.49\n",
            "2020-01-12 19:36:01,728 EPOCH 3\n",
            "2020-01-12 19:36:16,434 Epoch   3 Step:    10300 Batch Loss:     2.711673 Tokens per Sec:    14217, Lr: 0.000300\n",
            "2020-01-12 19:36:31,856 Epoch   3 Step:    10400 Batch Loss:     2.160725 Tokens per Sec:    14860, Lr: 0.000300\n",
            "2020-01-12 19:36:47,452 Epoch   3 Step:    10500 Batch Loss:     2.534353 Tokens per Sec:    14819, Lr: 0.000300\n",
            "2020-01-12 19:37:03,019 Epoch   3 Step:    10600 Batch Loss:     2.057720 Tokens per Sec:    14828, Lr: 0.000300\n",
            "2020-01-12 19:37:18,637 Epoch   3 Step:    10700 Batch Loss:     2.250278 Tokens per Sec:    14894, Lr: 0.000300\n",
            "2020-01-12 19:37:34,233 Epoch   3 Step:    10800 Batch Loss:     2.346390 Tokens per Sec:    14951, Lr: 0.000300\n",
            "2020-01-12 19:37:49,719 Epoch   3 Step:    10900 Batch Loss:     2.240621 Tokens per Sec:    14566, Lr: 0.000300\n",
            "2020-01-12 19:38:05,029 Epoch   3 Step:    11000 Batch Loss:     2.282218 Tokens per Sec:    14628, Lr: 0.000300\n",
            "2020-01-12 19:38:52,946 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 19:38:52,946 Saving new checkpoint.\n",
            "2020-01-12 19:38:54,019 Example #0\n",
            "2020-01-12 19:38:54,020 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 19:38:54,020 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 19:38:54,020 \tHypothesis: Ịmara na anyị nwere Nna eluigwe nke eluigwe na - ahụ n’anya ma na - elekọta anyị dị oké mkpa iji chegbu onwe anyị .\n",
            "2020-01-12 19:38:54,021 Example #1\n",
            "2020-01-12 19:38:54,021 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 19:38:54,021 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 19:38:54,021 \tHypothesis: Gịnị ka anyị kwesịrị ime iji nwee ike ime ka alụmdi na nwunye ghara ịna - eme ihe Setan kwuru ?\n",
            "2020-01-12 19:38:54,022 Example #2\n",
            "2020-01-12 19:38:54,022 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 19:38:54,022 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 19:38:54,022 \tHypothesis: Ha na - aghọ ndị enyi Kraịst na - eguzosi ike n’ihe , na - enye aka n’ọrụ nkwusa a “ ozi ọma nke alaeze ” maka ndị mba nile .\n",
            "2020-01-12 19:38:54,022 Example #3\n",
            "2020-01-12 19:38:54,023 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 19:38:54,023 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 19:38:54,023 \tHypothesis: Nke ahụ bụ ihe abụọ na - eme ka Chineke ghara ime ka anyị na - agbaghara ndị mmadụ .\n",
            "2020-01-12 19:38:54,023 Validation result (greedy) at epoch   3, step    11000: bleu:  18.36, loss: 57937.8242, ppl:   7.3198, duration: 48.9944s\n",
            "2020-01-12 19:39:09,626 Epoch   3 Step:    11100 Batch Loss:     2.274273 Tokens per Sec:    14731, Lr: 0.000300\n",
            "2020-01-12 19:39:25,103 Epoch   3 Step:    11200 Batch Loss:     1.980234 Tokens per Sec:    15001, Lr: 0.000300\n",
            "2020-01-12 19:39:40,666 Epoch   3 Step:    11300 Batch Loss:     2.138157 Tokens per Sec:    14874, Lr: 0.000300\n",
            "2020-01-12 19:39:56,142 Epoch   3 Step:    11400 Batch Loss:     2.142994 Tokens per Sec:    14641, Lr: 0.000300\n",
            "2020-01-12 19:40:11,683 Epoch   3 Step:    11500 Batch Loss:     2.263927 Tokens per Sec:    14941, Lr: 0.000300\n",
            "2020-01-12 19:40:27,081 Epoch   3 Step:    11600 Batch Loss:     2.287887 Tokens per Sec:    14828, Lr: 0.000300\n",
            "2020-01-12 19:40:42,659 Epoch   3 Step:    11700 Batch Loss:     2.353704 Tokens per Sec:    15028, Lr: 0.000300\n",
            "2020-01-12 19:40:58,052 Epoch   3 Step:    11800 Batch Loss:     2.078084 Tokens per Sec:    14581, Lr: 0.000300\n",
            "2020-01-12 19:41:13,481 Epoch   3 Step:    11900 Batch Loss:     2.274297 Tokens per Sec:    14682, Lr: 0.000300\n",
            "2020-01-12 19:41:29,121 Epoch   3 Step:    12000 Batch Loss:     2.249229 Tokens per Sec:    15168, Lr: 0.000300\n",
            "2020-01-12 19:42:16,936 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 19:42:16,937 Saving new checkpoint.\n",
            "2020-01-12 19:42:18,075 Example #0\n",
            "2020-01-12 19:42:18,076 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 19:42:18,076 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 19:42:18,076 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe bụ́ onye hụrụ anyị n’anya ma na - elekọta anyị dị oké mkpa iji chegbu onwe anyị .\n",
            "2020-01-12 19:42:18,076 Example #1\n",
            "2020-01-12 19:42:18,077 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 19:42:18,077 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 19:42:18,077 \tHypothesis: Gịnị ka anyị kwesịrị ime iji nwee ihe ịga nke ọma n’alụmdi na nwunye Setan iji mee mgbalị Setan na - eme iji mee ndokwa ahụ ?\n",
            "2020-01-12 19:42:18,077 Example #2\n",
            "2020-01-12 19:42:18,077 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 19:42:18,077 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 19:42:18,077 \tHypothesis: Ha aghọwo ndị enyi nke ụmụnna Kraịst , na - enye aka n’ọrụ nkwusa a “ ozi ọma nke alaeze ” nke mba nile .\n",
            "2020-01-12 19:42:18,077 Example #3\n",
            "2020-01-12 19:42:18,077 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 19:42:18,078 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 19:42:18,078 \tHypothesis: Nke ahụ bụ ihe abụọ na - enye Chineke otuto .\n",
            "2020-01-12 19:42:18,078 Validation result (greedy) at epoch   3, step    12000: bleu:  19.74, loss: 56542.4023, ppl:   6.9771, duration: 48.9557s\n",
            "2020-01-12 19:42:33,443 Epoch   3 Step:    12100 Batch Loss:     2.358702 Tokens per Sec:    14326, Lr: 0.000300\n",
            "2020-01-12 19:42:48,857 Epoch   3 Step:    12200 Batch Loss:     2.235313 Tokens per Sec:    14729, Lr: 0.000300\n",
            "2020-01-12 19:43:04,354 Epoch   3 Step:    12300 Batch Loss:     2.098613 Tokens per Sec:    14822, Lr: 0.000300\n",
            "2020-01-12 19:43:19,943 Epoch   3 Step:    12400 Batch Loss:     2.119691 Tokens per Sec:    14904, Lr: 0.000300\n",
            "2020-01-12 19:43:35,452 Epoch   3 Step:    12500 Batch Loss:     2.133314 Tokens per Sec:    14651, Lr: 0.000300\n",
            "2020-01-12 19:43:50,887 Epoch   3 Step:    12600 Batch Loss:     1.970241 Tokens per Sec:    14550, Lr: 0.000300\n",
            "2020-01-12 19:44:06,255 Epoch   3 Step:    12700 Batch Loss:     1.887319 Tokens per Sec:    14530, Lr: 0.000300\n",
            "2020-01-12 19:44:21,793 Epoch   3 Step:    12800 Batch Loss:     2.312356 Tokens per Sec:    14999, Lr: 0.000300\n",
            "2020-01-12 19:44:37,232 Epoch   3 Step:    12900 Batch Loss:     1.939623 Tokens per Sec:    14765, Lr: 0.000300\n",
            "2020-01-12 19:44:52,611 Epoch   3 Step:    13000 Batch Loss:     2.277380 Tokens per Sec:    14720, Lr: 0.000300\n",
            "2020-01-12 19:45:40,567 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 19:45:40,567 Saving new checkpoint.\n",
            "2020-01-12 19:45:41,841 Example #0\n",
            "2020-01-12 19:45:41,842 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 19:45:41,842 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 19:45:41,843 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe na - ahụ n’anya , na - echekwa anyị ihe dị oké mkpa iji chebara echiche echiche .\n",
            "2020-01-12 19:45:41,843 Example #1\n",
            "2020-01-12 19:45:41,843 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 19:45:41,843 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 19:45:41,843 \tHypothesis: Gịnị ka ọ dị mkpa ime iji nwee ihe ịga nke ọma n’alụmdi na nwunye nke mgbalị Setan iji ghọta na e mere ndokwa ahụ ?\n",
            "2020-01-12 19:45:41,844 Example #2\n",
            "2020-01-12 19:45:41,844 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 19:45:41,844 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 19:45:41,844 \tHypothesis: Ha aghọwo ndị enyi Kraịst , na - enye aka n’ọrụ nkwusa a “ ozi ọma nke alaeze ” maka mba nile .\n",
            "2020-01-12 19:45:41,845 Example #3\n",
            "2020-01-12 19:45:41,845 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 19:45:41,845 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 19:45:41,845 \tHypothesis: Nke ahụ bụ ihe abụọ Chineke na - eme ka anyị na - agbaghara mmehie .\n",
            "2020-01-12 19:45:41,845 Validation result (greedy) at epoch   3, step    13000: bleu:  19.77, loss: 55412.5625, ppl:   6.7115, duration: 49.2343s\n",
            "2020-01-12 19:45:57,284 Epoch   3 Step:    13100 Batch Loss:     2.059752 Tokens per Sec:    14901, Lr: 0.000300\n",
            "2020-01-12 19:46:12,750 Epoch   3 Step:    13200 Batch Loss:     2.081054 Tokens per Sec:    14802, Lr: 0.000300\n",
            "2020-01-12 19:46:28,404 Epoch   3 Step:    13300 Batch Loss:     2.045239 Tokens per Sec:    14938, Lr: 0.000300\n",
            "2020-01-12 19:46:43,821 Epoch   3 Step:    13400 Batch Loss:     2.125617 Tokens per Sec:    14702, Lr: 0.000300\n",
            "2020-01-12 19:46:59,264 Epoch   3 Step:    13500 Batch Loss:     2.023389 Tokens per Sec:    14656, Lr: 0.000300\n",
            "2020-01-12 19:47:14,940 Epoch   3 Step:    13600 Batch Loss:     2.123055 Tokens per Sec:    15095, Lr: 0.000300\n",
            "2020-01-12 19:47:30,536 Epoch   3 Step:    13700 Batch Loss:     1.876970 Tokens per Sec:    14932, Lr: 0.000300\n",
            "2020-01-12 19:47:45,920 Epoch   3 Step:    13800 Batch Loss:     2.287321 Tokens per Sec:    14757, Lr: 0.000300\n",
            "2020-01-12 19:48:01,268 Epoch   3 Step:    13900 Batch Loss:     2.030738 Tokens per Sec:    14606, Lr: 0.000300\n",
            "2020-01-12 19:48:16,638 Epoch   3 Step:    14000 Batch Loss:     2.265191 Tokens per Sec:    14608, Lr: 0.000300\n",
            "2020-01-12 19:49:04,500 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 19:49:04,501 Saving new checkpoint.\n",
            "2020-01-12 19:49:06,022 Example #0\n",
            "2020-01-12 19:49:06,023 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 19:49:06,023 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 19:49:06,024 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe nke hụrụ anyị n’anya ma na - elekọta anyị dị oké mkpa ka anyị na - enwe mmetụta nke ichegbu onwe anyị .\n",
            "2020-01-12 19:49:06,024 Example #1\n",
            "2020-01-12 19:49:06,024 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 19:49:06,024 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 19:49:06,024 \tHypothesis: Gịnị ka anyị kwesịrị ime iji nwee ihe ịga nke ọma n’alụmdi na nwunye nke ime mgbalị Setan na - eme iji ghọta na e nwere ndokwa ahụ ?\n",
            "2020-01-12 19:49:06,025 Example #2\n",
            "2020-01-12 19:49:06,025 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 19:49:06,026 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 19:49:06,026 \tHypothesis: Ha aghọwo ndị enyi nke ụmụnna Kraịst na - ekwesị ntụkwasị obi , na - enye aka n’ọrụ nkwusa a “ ozi ọma nke alaeze ” maka ndị mba nile .\n",
            "2020-01-12 19:49:06,026 Example #3\n",
            "2020-01-12 19:49:06,027 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 19:49:06,027 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 19:49:06,027 \tHypothesis: Nke ahụ bụ ụzọ nke abụọ si eme ka Chineke ghara ịdị ná mma .\n",
            "2020-01-12 19:49:06,027 Validation result (greedy) at epoch   3, step    14000: bleu:  19.87, loss: 54678.0898, ppl:   6.5442, duration: 49.3886s\n",
            "2020-01-12 19:49:21,627 Epoch   3 Step:    14100 Batch Loss:     2.058698 Tokens per Sec:    14512, Lr: 0.000300\n",
            "2020-01-12 19:49:36,891 Epoch   3 Step:    14200 Batch Loss:     2.101703 Tokens per Sec:    14800, Lr: 0.000300\n",
            "2020-01-12 19:49:52,549 Epoch   3 Step:    14300 Batch Loss:     2.206704 Tokens per Sec:    14797, Lr: 0.000300\n",
            "2020-01-12 19:50:07,842 Epoch   3 Step:    14400 Batch Loss:     2.184158 Tokens per Sec:    14797, Lr: 0.000300\n",
            "2020-01-12 19:50:23,405 Epoch   3 Step:    14500 Batch Loss:     2.299253 Tokens per Sec:    14879, Lr: 0.000300\n",
            "2020-01-12 19:50:38,742 Epoch   3 Step:    14600 Batch Loss:     2.111452 Tokens per Sec:    15014, Lr: 0.000300\n",
            "2020-01-12 19:50:54,218 Epoch   3 Step:    14700 Batch Loss:     2.283750 Tokens per Sec:    14539, Lr: 0.000300\n",
            "2020-01-12 19:51:09,805 Epoch   3 Step:    14800 Batch Loss:     2.313886 Tokens per Sec:    14936, Lr: 0.000300\n",
            "2020-01-12 19:51:25,117 Epoch   3 Step:    14900 Batch Loss:     2.026765 Tokens per Sec:    14843, Lr: 0.000300\n",
            "2020-01-12 19:51:40,501 Epoch   3 Step:    15000 Batch Loss:     1.965167 Tokens per Sec:    14721, Lr: 0.000300\n",
            "2020-01-12 19:52:28,335 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 19:52:28,335 Saving new checkpoint.\n",
            "2020-01-12 19:52:29,390 Example #0\n",
            "2020-01-12 19:52:29,391 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 19:52:29,391 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 19:52:29,391 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe na - ahụ n’anya , na - elekọta anyị dị oké mkpa n’ịtụle nchegbu nke nchegbu .\n",
            "2020-01-12 19:52:29,391 Example #1\n",
            "2020-01-12 19:52:29,391 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 19:52:29,392 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 19:52:29,392 \tHypothesis: Gịnị ka anyị kwesịrị ime iji nwee ihe ịga nke ọma n’alụmdi na nwunye Setan iji mee mgbalị ndị Setan na - eme iji ghọta nke a ?\n",
            "2020-01-12 19:52:29,392 Example #2\n",
            "2020-01-12 19:52:29,392 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 19:52:29,392 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 19:52:29,392 \tHypothesis: Ha aghọwo ndị enyi Kraịst , na - enye aka n’ọrụ nkwusa a “ ozi ọma nke alaeze ” maka mba nile .\n",
            "2020-01-12 19:52:29,392 Example #3\n",
            "2020-01-12 19:52:29,393 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 19:52:29,393 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 19:52:29,393 \tHypothesis: Nke ahụ bụ ihe abụọ na - eme ka anyị na - eme ihe Chineke chọrọ .\n",
            "2020-01-12 19:52:29,393 Validation result (greedy) at epoch   3, step    15000: bleu:  20.34, loss: 53244.4961, ppl:   6.2297, duration: 48.8912s\n",
            "2020-01-12 19:52:44,859 Epoch   3 Step:    15100 Batch Loss:     2.546642 Tokens per Sec:    14963, Lr: 0.000300\n",
            "2020-01-12 19:53:00,373 Epoch   3 Step:    15200 Batch Loss:     2.188915 Tokens per Sec:    14979, Lr: 0.000300\n",
            "2020-01-12 19:53:15,893 Epoch   3 Step:    15300 Batch Loss:     2.829539 Tokens per Sec:    15059, Lr: 0.000300\n",
            "2020-01-12 19:53:18,323 Epoch   3: total training loss 10855.74\n",
            "2020-01-12 19:53:18,323 EPOCH 4\n",
            "2020-01-12 19:53:31,931 Epoch   4 Step:    15400 Batch Loss:     2.075468 Tokens per Sec:    14178, Lr: 0.000300\n",
            "2020-01-12 19:53:47,297 Epoch   4 Step:    15500 Batch Loss:     2.535760 Tokens per Sec:    14953, Lr: 0.000300\n",
            "2020-01-12 19:54:02,632 Epoch   4 Step:    15600 Batch Loss:     1.847664 Tokens per Sec:    14529, Lr: 0.000300\n",
            "2020-01-12 19:54:18,242 Epoch   4 Step:    15700 Batch Loss:     2.297014 Tokens per Sec:    15187, Lr: 0.000300\n",
            "2020-01-12 19:54:33,443 Epoch   4 Step:    15800 Batch Loss:     2.318980 Tokens per Sec:    14619, Lr: 0.000300\n",
            "2020-01-12 19:54:48,995 Epoch   4 Step:    15900 Batch Loss:     2.030545 Tokens per Sec:    14887, Lr: 0.000300\n",
            "2020-01-12 19:55:04,521 Epoch   4 Step:    16000 Batch Loss:     1.989331 Tokens per Sec:    14990, Lr: 0.000300\n",
            "2020-01-12 19:55:52,313 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 19:55:52,313 Saving new checkpoint.\n",
            "2020-01-12 19:55:53,527 Example #0\n",
            "2020-01-12 19:55:53,528 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 19:55:53,528 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 19:55:53,529 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe nke hụrụ anyị n’anya ma na - elekọta anyị dị oké mkpa n’echegbu onwe anyị .\n",
            "2020-01-12 19:55:53,529 Example #1\n",
            "2020-01-12 19:55:53,529 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 19:55:53,529 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 19:55:53,529 \tHypothesis: Gịnị ka anyị kwesịrị ime iji nwee ihe ịga nke ọma n’alụmdi na nwunye nke Setan iji mee ndokwa ahụ ?\n",
            "2020-01-12 19:55:53,530 Example #2\n",
            "2020-01-12 19:55:53,530 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 19:55:53,530 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 19:55:53,530 \tHypothesis: Ha aghọwo ndị enyi nke Kraịst , na - enye aka n’ọrụ ime nkwusa “ ozi ọma nke alaeze ” maka mba nile .\n",
            "2020-01-12 19:55:53,530 Example #3\n",
            "2020-01-12 19:55:53,531 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 19:55:53,531 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 19:55:53,531 \tHypothesis: Nke ahụ bụ ihe abụọ ga - eme ka anyị na - agbaghara Chineke .\n",
            "2020-01-12 19:55:53,531 Validation result (greedy) at epoch   4, step    16000: bleu:  21.92, loss: 52183.0664, ppl:   6.0066, duration: 49.0102s\n",
            "2020-01-12 19:56:09,036 Epoch   4 Step:    16100 Batch Loss:     2.112695 Tokens per Sec:    14802, Lr: 0.000300\n",
            "2020-01-12 19:56:24,486 Epoch   4 Step:    16200 Batch Loss:     1.867597 Tokens per Sec:    14682, Lr: 0.000300\n",
            "2020-01-12 19:56:39,958 Epoch   4 Step:    16300 Batch Loss:     2.049259 Tokens per Sec:    15089, Lr: 0.000300\n",
            "2020-01-12 19:56:55,504 Epoch   4 Step:    16400 Batch Loss:     2.028902 Tokens per Sec:    14989, Lr: 0.000300\n",
            "2020-01-12 19:57:10,884 Epoch   4 Step:    16500 Batch Loss:     2.032426 Tokens per Sec:    14746, Lr: 0.000300\n",
            "2020-01-12 19:57:26,384 Epoch   4 Step:    16600 Batch Loss:     1.951506 Tokens per Sec:    14758, Lr: 0.000300\n",
            "2020-01-12 19:57:41,768 Epoch   4 Step:    16700 Batch Loss:     2.062690 Tokens per Sec:    14636, Lr: 0.000300\n",
            "2020-01-12 19:57:57,176 Epoch   4 Step:    16800 Batch Loss:     1.913994 Tokens per Sec:    14985, Lr: 0.000300\n",
            "2020-01-12 19:58:12,601 Epoch   4 Step:    16900 Batch Loss:     2.120028 Tokens per Sec:    14918, Lr: 0.000300\n",
            "2020-01-12 19:58:28,081 Epoch   4 Step:    17000 Batch Loss:     1.881297 Tokens per Sec:    14953, Lr: 0.000300\n",
            "2020-01-12 19:59:15,952 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 19:59:15,953 Saving new checkpoint.\n",
            "2020-01-12 19:59:17,115 Example #0\n",
            "2020-01-12 19:59:17,116 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 19:59:17,116 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 19:59:17,116 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe bụ́ onye hụrụ anyị n’anya ma na - elekọta anyị dị oké mkpa iji chebara nchegbu anya .\n",
            "2020-01-12 19:59:17,116 Example #1\n",
            "2020-01-12 19:59:17,117 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 19:59:17,117 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 19:59:17,117 \tHypothesis: Gịnị ka anyị kwesịrị ime iji nwee ihe ịga nke ọma n’alụmdi na nwunye Setan iji ghọta ndokwa ahụ ?\n",
            "2020-01-12 19:59:17,117 Example #2\n",
            "2020-01-12 19:59:17,117 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 19:59:17,117 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 19:59:17,117 \tHypothesis: Ha aghọwo ndị na - eso ụzọ Kraịst , na - enye aka n’ọrụ nkwusa “ ozi ọma nke alaeze a ” maka mba nile .\n",
            "2020-01-12 19:59:17,117 Example #3\n",
            "2020-01-12 19:59:17,118 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 19:59:17,118 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 19:59:17,118 \tHypothesis: Nke ahụ bụ ihe abụọ Chineke mere iji gbaghara anyị .\n",
            "2020-01-12 19:59:17,118 Validation result (greedy) at epoch   4, step    17000: bleu:  22.21, loss: 51514.5586, ppl:   5.8702, duration: 49.0367s\n",
            "2020-01-12 19:59:32,443 Epoch   4 Step:    17100 Batch Loss:     1.745181 Tokens per Sec:    14555, Lr: 0.000300\n",
            "2020-01-12 19:59:47,800 Epoch   4 Step:    17200 Batch Loss:     1.986036 Tokens per Sec:    14877, Lr: 0.000300\n",
            "2020-01-12 20:00:03,291 Epoch   4 Step:    17300 Batch Loss:     1.736710 Tokens per Sec:    14916, Lr: 0.000300\n",
            "2020-01-12 20:00:18,591 Epoch   4 Step:    17400 Batch Loss:     2.043699 Tokens per Sec:    14766, Lr: 0.000300\n",
            "2020-01-12 20:00:34,097 Epoch   4 Step:    17500 Batch Loss:     2.032184 Tokens per Sec:    14820, Lr: 0.000300\n",
            "2020-01-12 20:00:49,430 Epoch   4 Step:    17600 Batch Loss:     2.314284 Tokens per Sec:    14692, Lr: 0.000300\n",
            "2020-01-12 20:01:04,906 Epoch   4 Step:    17700 Batch Loss:     1.741087 Tokens per Sec:    14854, Lr: 0.000300\n",
            "2020-01-12 20:01:20,569 Epoch   4 Step:    17800 Batch Loss:     2.217250 Tokens per Sec:    14898, Lr: 0.000300\n",
            "2020-01-12 20:01:36,022 Epoch   4 Step:    17900 Batch Loss:     1.802930 Tokens per Sec:    14854, Lr: 0.000300\n",
            "2020-01-12 20:01:51,387 Epoch   4 Step:    18000 Batch Loss:     2.294038 Tokens per Sec:    14785, Lr: 0.000300\n",
            "2020-01-12 20:02:39,285 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 20:02:39,285 Saving new checkpoint.\n",
            "2020-01-12 20:02:40,342 Example #0\n",
            "2020-01-12 20:02:40,343 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 20:02:40,343 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 20:02:40,344 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe hụrụ anyị n’anya ma na - elekọta anyị dị oké mkpa n’echegbu onwe anyị .\n",
            "2020-01-12 20:02:40,344 Example #1\n",
            "2020-01-12 20:02:40,344 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 20:02:40,345 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 20:02:40,345 \tHypothesis: Gịnị ka anyị kwesịrị ime iji nwee ihe ịga nke ọma n’alụmdi na nwunye Setan iji ghọta ihe mere e ji mee ndokwa ahụ ?\n",
            "2020-01-12 20:02:40,345 Example #2\n",
            "2020-01-12 20:02:40,345 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 20:02:40,345 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 20:02:40,346 \tHypothesis: Ha aghọwo ndị enyi Kraịst na - eguzosi ike n’ihe , na - enye aka n’ọrụ ime nkwusa “ ozi ọma nke alaeze a ” maka ndị mba nile .\n",
            "2020-01-12 20:02:40,346 Example #3\n",
            "2020-01-12 20:02:40,348 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 20:02:40,349 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 20:02:40,349 \tHypothesis: Nke ahụ bụ ihe abụọ na - eme ka anyị na - agbaghara Chineke .\n",
            "2020-01-12 20:02:40,349 Validation result (greedy) at epoch   4, step    18000: bleu:  22.27, loss: 50726.2617, ppl:   5.7134, duration: 48.9622s\n",
            "2020-01-12 20:02:55,852 Epoch   4 Step:    18100 Batch Loss:     2.045071 Tokens per Sec:    14500, Lr: 0.000300\n",
            "2020-01-12 20:03:11,549 Epoch   4 Step:    18200 Batch Loss:     2.153994 Tokens per Sec:    15308, Lr: 0.000300\n",
            "2020-01-12 20:03:27,004 Epoch   4 Step:    18300 Batch Loss:     1.926756 Tokens per Sec:    14945, Lr: 0.000300\n",
            "2020-01-12 20:03:42,435 Epoch   4 Step:    18400 Batch Loss:     1.882547 Tokens per Sec:    14646, Lr: 0.000300\n",
            "2020-01-12 20:03:57,960 Epoch   4 Step:    18500 Batch Loss:     1.969740 Tokens per Sec:    15027, Lr: 0.000300\n",
            "2020-01-12 20:04:13,419 Epoch   4 Step:    18600 Batch Loss:     1.443806 Tokens per Sec:    14766, Lr: 0.000300\n",
            "2020-01-12 20:04:28,876 Epoch   4 Step:    18700 Batch Loss:     1.541483 Tokens per Sec:    14873, Lr: 0.000300\n",
            "2020-01-12 20:04:44,193 Epoch   4 Step:    18800 Batch Loss:     2.021899 Tokens per Sec:    14817, Lr: 0.000300\n",
            "2020-01-12 20:04:59,431 Epoch   4 Step:    18900 Batch Loss:     1.605643 Tokens per Sec:    14668, Lr: 0.000300\n",
            "2020-01-12 20:05:14,821 Epoch   4 Step:    19000 Batch Loss:     1.763457 Tokens per Sec:    14913, Lr: 0.000300\n",
            "2020-01-12 20:06:02,547 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 20:06:02,548 Saving new checkpoint.\n",
            "2020-01-12 20:06:04,119 Example #0\n",
            "2020-01-12 20:06:04,120 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 20:06:04,120 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 20:06:04,120 \tHypothesis: Ịmara na anyị nwere Nna eluigwe nke hụrụ anyị n’anya , na - echekwa anyị ihe dị oké mkpa iji chegbu onwe anyị .\n",
            "2020-01-12 20:06:04,120 Example #1\n",
            "2020-01-12 20:06:04,121 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 20:06:04,121 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 20:06:04,121 \tHypothesis: Gịnị ka ọ dị mkpa ime ka alụmdi na nwunye nwee ihe ịga nke ọma ná mgbalị Setan iji ghọta ndokwa ahụ ?\n",
            "2020-01-12 20:06:04,121 Example #2\n",
            "2020-01-12 20:06:04,122 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 20:06:04,122 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 20:06:04,122 \tHypothesis: Ha aghọwo ndị enyi nke ụmụnna Kraịst , na - enye aka n’ọrụ nkwusa “ ozi ọma nke alaeze a ” maka àmà nye mba nile .\n",
            "2020-01-12 20:06:04,122 Example #3\n",
            "2020-01-12 20:06:04,122 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 20:06:04,122 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 20:06:04,123 \tHypothesis: Nke ahụ bụ ihe nke abụọ ga - eme ka anyị na - agbaghara mmehie Chineke .\n",
            "2020-01-12 20:06:04,123 Validation result (greedy) at epoch   4, step    19000: bleu:  21.71, loss: 50213.1797, ppl:   5.6135, duration: 49.3016s\n",
            "2020-01-12 20:06:19,820 Epoch   4 Step:    19100 Batch Loss:     1.993259 Tokens per Sec:    14763, Lr: 0.000300\n",
            "2020-01-12 20:06:35,192 Epoch   4 Step:    19200 Batch Loss:     1.800808 Tokens per Sec:    14583, Lr: 0.000300\n",
            "2020-01-12 20:06:50,879 Epoch   4 Step:    19300 Batch Loss:     2.089101 Tokens per Sec:    14919, Lr: 0.000300\n",
            "2020-01-12 20:07:06,175 Epoch   4 Step:    19400 Batch Loss:     1.806424 Tokens per Sec:    14875, Lr: 0.000300\n",
            "2020-01-12 20:07:21,870 Epoch   4 Step:    19500 Batch Loss:     1.903788 Tokens per Sec:    15141, Lr: 0.000300\n",
            "2020-01-12 20:07:37,330 Epoch   4 Step:    19600 Batch Loss:     1.945468 Tokens per Sec:    14933, Lr: 0.000300\n",
            "2020-01-12 20:07:52,810 Epoch   4 Step:    19700 Batch Loss:     1.669935 Tokens per Sec:    14878, Lr: 0.000300\n",
            "2020-01-12 20:08:08,094 Epoch   4 Step:    19800 Batch Loss:     1.826680 Tokens per Sec:    14629, Lr: 0.000300\n",
            "2020-01-12 20:08:23,506 Epoch   4 Step:    19900 Batch Loss:     2.091230 Tokens per Sec:    14742, Lr: 0.000300\n",
            "2020-01-12 20:08:38,931 Epoch   4 Step:    20000 Batch Loss:     1.793100 Tokens per Sec:    14879, Lr: 0.000300\n",
            "2020-01-12 20:09:26,773 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 20:09:26,773 Saving new checkpoint.\n",
            "2020-01-12 20:09:27,826 Example #0\n",
            "2020-01-12 20:09:27,826 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 20:09:27,826 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 20:09:27,827 \tHypothesis: Ịmata na anyị nwere Nna nke eluigwe nke hụrụ anyị n’anya ma na - elekọta anyị dị oké mkpa iji chegbuo nchegbu .\n",
            "2020-01-12 20:09:27,827 Example #1\n",
            "2020-01-12 20:09:27,827 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 20:09:27,827 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 20:09:27,827 \tHypothesis: Gịnị ka anyị kwesịrị ime iji nwee ihe ịga nke ọma n’alụmdi na nwunye nke mgbalị Setan iji ghọta ndokwa ahụ ?\n",
            "2020-01-12 20:09:27,827 Example #2\n",
            "2020-01-12 20:09:27,827 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 20:09:27,827 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 20:09:27,828 \tHypothesis: Ha aghọwo ndị enyi nke ụmụnna Kraịst , na - enye aka n’ọrụ nkwusa dị ukwuu “ ozi ọma nke alaeze ” maka izi ndị mba nile ozi ọma .\n",
            "2020-01-12 20:09:27,828 Example #3\n",
            "2020-01-12 20:09:27,828 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 20:09:27,828 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 20:09:27,828 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ n’ebe Chineke nọ .\n",
            "2020-01-12 20:09:27,828 Validation result (greedy) at epoch   4, step    20000: bleu:  22.68, loss: 49596.1797, ppl:   5.4958, duration: 48.8966s\n",
            "2020-01-12 20:09:43,300 Epoch   4 Step:    20100 Batch Loss:     1.781547 Tokens per Sec:    14871, Lr: 0.000300\n",
            "2020-01-12 20:09:58,757 Epoch   4 Step:    20200 Batch Loss:     1.761801 Tokens per Sec:    14949, Lr: 0.000300\n",
            "2020-01-12 20:10:14,339 Epoch   4 Step:    20300 Batch Loss:     1.738310 Tokens per Sec:    15068, Lr: 0.000300\n",
            "2020-01-12 20:10:29,763 Epoch   4 Step:    20400 Batch Loss:     1.955249 Tokens per Sec:    14652, Lr: 0.000300\n",
            "2020-01-12 20:10:32,335 Epoch   4: total training loss 10009.05\n",
            "2020-01-12 20:10:32,335 EPOCH 5\n",
            "2020-01-12 20:10:45,566 Epoch   5 Step:    20500 Batch Loss:     1.768850 Tokens per Sec:    14259, Lr: 0.000300\n",
            "2020-01-12 20:11:01,044 Epoch   5 Step:    20600 Batch Loss:     1.717727 Tokens per Sec:    15125, Lr: 0.000300\n",
            "2020-01-12 20:11:16,439 Epoch   5 Step:    20700 Batch Loss:     1.746971 Tokens per Sec:    14838, Lr: 0.000300\n",
            "2020-01-12 20:11:31,912 Epoch   5 Step:    20800 Batch Loss:     1.896045 Tokens per Sec:    14950, Lr: 0.000300\n",
            "2020-01-12 20:11:47,228 Epoch   5 Step:    20900 Batch Loss:     2.018464 Tokens per Sec:    14804, Lr: 0.000300\n",
            "2020-01-12 20:12:02,772 Epoch   5 Step:    21000 Batch Loss:     1.946200 Tokens per Sec:    15029, Lr: 0.000300\n",
            "2020-01-12 20:12:50,494 Example #0\n",
            "2020-01-12 20:12:50,494 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 20:12:50,495 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 20:12:50,495 \tHypothesis: Ịmara na anyị nwere Nna eluigwe nke hụrụ anyị n’anya ma na - elekọta anyị dị oké mkpa n’echegbu onwe anyị .\n",
            "2020-01-12 20:12:50,495 Example #1\n",
            "2020-01-12 20:12:50,496 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 20:12:50,496 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 20:12:50,496 \tHypothesis: Gịnị ka anyị kwesịrị ime iji nwee ihe ịga nke ọma n’alụmdi na nwunye Setan iji chọpụta na ndokwa ahụ ?\n",
            "2020-01-12 20:12:50,496 Example #2\n",
            "2020-01-12 20:12:50,497 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 20:12:50,497 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 20:12:50,497 \tHypothesis: Ha aghọwo ndị enyi nke ụmụnna Kraịst , na - enye aka n’ọrụ nkwusa a “ ozi ọma nke alaeze ” a maka ndị mba nile .\n",
            "2020-01-12 20:12:50,497 Example #3\n",
            "2020-01-12 20:12:50,498 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 20:12:50,498 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 20:12:50,498 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ nke ịhapụ mgbaghara Chineke .\n",
            "2020-01-12 20:12:50,498 Validation result (greedy) at epoch   5, step    21000: bleu:  22.67, loss: 49650.1172, ppl:   5.5060, duration: 47.7262s\n",
            "2020-01-12 20:13:06,078 Epoch   5 Step:    21100 Batch Loss:     1.621326 Tokens per Sec:    14860, Lr: 0.000300\n",
            "2020-01-12 20:13:21,471 Epoch   5 Step:    21200 Batch Loss:     2.004471 Tokens per Sec:    14558, Lr: 0.000300\n",
            "2020-01-12 20:13:36,902 Epoch   5 Step:    21300 Batch Loss:     1.853573 Tokens per Sec:    14795, Lr: 0.000300\n",
            "2020-01-12 20:13:52,421 Epoch   5 Step:    21400 Batch Loss:     1.838913 Tokens per Sec:    14769, Lr: 0.000300\n",
            "2020-01-12 20:14:07,784 Epoch   5 Step:    21500 Batch Loss:     1.653535 Tokens per Sec:    14554, Lr: 0.000300\n",
            "2020-01-12 20:14:23,218 Epoch   5 Step:    21600 Batch Loss:     2.659740 Tokens per Sec:    14698, Lr: 0.000300\n",
            "2020-01-12 20:14:38,601 Epoch   5 Step:    21700 Batch Loss:     2.120613 Tokens per Sec:    14830, Lr: 0.000300\n",
            "2020-01-12 20:14:54,256 Epoch   5 Step:    21800 Batch Loss:     2.044950 Tokens per Sec:    14953, Lr: 0.000300\n",
            "2020-01-12 20:15:09,459 Epoch   5 Step:    21900 Batch Loss:     1.773553 Tokens per Sec:    14604, Lr: 0.000300\n",
            "2020-01-12 20:15:25,023 Epoch   5 Step:    22000 Batch Loss:     1.982513 Tokens per Sec:    14822, Lr: 0.000300\n",
            "2020-01-12 20:16:12,813 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 20:16:12,813 Saving new checkpoint.\n",
            "2020-01-12 20:16:13,979 Example #0\n",
            "2020-01-12 20:16:13,980 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 20:16:13,980 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 20:16:13,981 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe na - ahụ n’anya ma na - elekọta anyị bụ nzọụkwụ dị oké mkpa n’ichegbu onwe anyị .\n",
            "2020-01-12 20:16:13,981 Example #1\n",
            "2020-01-12 20:16:13,981 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 20:16:13,981 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 20:16:13,982 \tHypothesis: Gịnị ka anyị kwesịrị ime iji nwee ihe ịga nke ọma n’alụmdi na nwunye Setan iji gosi na ndokwa ahụ bụ ?\n",
            "2020-01-12 20:16:13,982 Example #2\n",
            "2020-01-12 20:16:13,983 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 20:16:13,983 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 20:16:13,983 \tHypothesis: Ha aghọọla ndị òtù ụmụnna Kraịst na - eguzosi ike n’ihe , na - enye enyemaka dị oké ọnụ ahịa n’ikwusa “ ozi ọma nke alaeze a ” maka ịgba àmà nye mba nile .\n",
            "2020-01-12 20:16:13,983 Example #3\n",
            "2020-01-12 20:16:13,983 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 20:16:13,984 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 20:16:13,984 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ n’ebe ịchụso mgbaghara Chineke .\n",
            "2020-01-12 20:16:13,984 Validation result (greedy) at epoch   5, step    22000: bleu:  23.35, loss: 48651.5625, ppl:   5.3203, duration: 48.9601s\n",
            "2020-01-12 20:16:29,499 Epoch   5 Step:    22100 Batch Loss:     1.820616 Tokens per Sec:    14835, Lr: 0.000300\n",
            "2020-01-12 20:16:44,891 Epoch   5 Step:    22200 Batch Loss:     2.087495 Tokens per Sec:    14674, Lr: 0.000300\n",
            "2020-01-12 20:17:00,374 Epoch   5 Step:    22300 Batch Loss:     1.595281 Tokens per Sec:    14754, Lr: 0.000300\n",
            "2020-01-12 20:17:15,644 Epoch   5 Step:    22400 Batch Loss:     1.883745 Tokens per Sec:    14498, Lr: 0.000300\n",
            "2020-01-12 20:17:31,057 Epoch   5 Step:    22500 Batch Loss:     1.949706 Tokens per Sec:    14872, Lr: 0.000300\n",
            "2020-01-12 20:17:46,496 Epoch   5 Step:    22600 Batch Loss:     1.722104 Tokens per Sec:    14947, Lr: 0.000300\n",
            "2020-01-12 20:18:02,022 Epoch   5 Step:    22700 Batch Loss:     2.094099 Tokens per Sec:    14811, Lr: 0.000300\n",
            "2020-01-12 20:18:17,270 Epoch   5 Step:    22800 Batch Loss:     1.663779 Tokens per Sec:    14894, Lr: 0.000300\n",
            "2020-01-12 20:18:32,506 Epoch   5 Step:    22900 Batch Loss:     2.000713 Tokens per Sec:    14787, Lr: 0.000300\n",
            "2020-01-12 20:18:47,931 Epoch   5 Step:    23000 Batch Loss:     1.943195 Tokens per Sec:    14839, Lr: 0.000300\n",
            "2020-01-12 20:19:35,753 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 20:19:35,753 Saving new checkpoint.\n",
            "2020-01-12 20:19:36,848 Example #0\n",
            "2020-01-12 20:19:36,849 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 20:19:36,849 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 20:19:36,849 \tHypothesis: Ịmata na anyị nwere Nna nke eluigwe nke hụrụ anyị n’anya ma na - elekọta anyị bụ ihe dị oké mkpa ná mmetụta nke ichegbu onwe onye .\n",
            "2020-01-12 20:19:36,849 Example #1\n",
            "2020-01-12 20:19:36,849 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 20:19:36,850 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 20:19:36,850 \tHypothesis: Gịnị ka ọ dị mkpa ka e nwee ihe ịga nke ọma n’alụmdi na nwunye Setan iji ghọta na ndokwa ahụ ?\n",
            "2020-01-12 20:19:36,850 Example #2\n",
            "2020-01-12 20:19:36,850 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 20:19:36,850 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 20:19:36,850 \tHypothesis: Ha aghọwo ndị enyi nke ụmụnna Kraịst , na - enye aka n’ọrụ nkwusa “ ozi ọma nke alaeze a ” maka mba nile .\n",
            "2020-01-12 20:19:36,850 Example #3\n",
            "2020-01-12 20:19:36,851 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 20:19:36,851 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 20:19:36,851 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ nke ịnakwere mgbaghara Chineke .\n",
            "2020-01-12 20:19:36,851 Validation result (greedy) at epoch   5, step    23000: bleu:  23.43, loss: 48246.4648, ppl:   5.2468, duration: 48.9194s\n",
            "2020-01-12 20:19:52,325 Epoch   5 Step:    23100 Batch Loss:     1.893462 Tokens per Sec:    14862, Lr: 0.000300\n",
            "2020-01-12 20:20:07,730 Epoch   5 Step:    23200 Batch Loss:     2.013434 Tokens per Sec:    14771, Lr: 0.000300\n",
            "2020-01-12 20:20:23,241 Epoch   5 Step:    23300 Batch Loss:     1.921765 Tokens per Sec:    15032, Lr: 0.000300\n",
            "2020-01-12 20:20:38,698 Epoch   5 Step:    23400 Batch Loss:     1.954149 Tokens per Sec:    14919, Lr: 0.000300\n",
            "2020-01-12 20:20:54,298 Epoch   5 Step:    23500 Batch Loss:     1.871895 Tokens per Sec:    14799, Lr: 0.000300\n",
            "2020-01-12 20:21:09,649 Epoch   5 Step:    23600 Batch Loss:     1.861973 Tokens per Sec:    14745, Lr: 0.000300\n",
            "2020-01-12 20:21:25,126 Epoch   5 Step:    23700 Batch Loss:     2.005286 Tokens per Sec:    14985, Lr: 0.000300\n",
            "2020-01-12 20:21:40,749 Epoch   5 Step:    23800 Batch Loss:     1.741544 Tokens per Sec:    15007, Lr: 0.000300\n",
            "2020-01-12 20:21:56,119 Epoch   5 Step:    23900 Batch Loss:     1.923695 Tokens per Sec:    14594, Lr: 0.000300\n",
            "2020-01-12 20:22:11,752 Epoch   5 Step:    24000 Batch Loss:     1.898084 Tokens per Sec:    15079, Lr: 0.000300\n",
            "2020-01-12 20:22:59,431 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 20:22:59,432 Saving new checkpoint.\n",
            "2020-01-12 20:23:01,227 Example #0\n",
            "2020-01-12 20:23:01,228 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 20:23:01,228 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 20:23:01,228 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe nke hụrụ anyị n’anya , na - echekwa anyị ụra dị oké mkpa n’ịtachi obi .\n",
            "2020-01-12 20:23:01,228 Example #1\n",
            "2020-01-12 20:23:01,228 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 20:23:01,228 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 20:23:01,228 \tHypothesis: Gịnị ka ọ dị mkpa ime ka alụmdi na nwunye nwee ihe ịga nke ọma n’itinye mgbalị Setan n’ọrụ ime ndokwa ahụ ?\n",
            "2020-01-12 20:23:01,229 Example #2\n",
            "2020-01-12 20:23:01,229 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 20:23:01,229 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 20:23:01,229 \tHypothesis: Ha aghọwo ndị enyi nke ụmụnna Kraịst , na - enye aka n’ụzọ dị ukwuu n’ikwusa “ ozi ọma nke alaeze a ” maka àmà nye mba nile .\n",
            "2020-01-12 20:23:01,229 Example #3\n",
            "2020-01-12 20:23:01,229 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 20:23:01,230 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 20:23:01,230 \tHypothesis: Nke ahụ bụ ihe nke abụọ a na - eme iji gbaghara Chineke .\n",
            "2020-01-12 20:23:01,230 Validation result (greedy) at epoch   5, step    24000: bleu:  23.93, loss: 47527.7734, ppl:   5.1188, duration: 49.4766s\n",
            "2020-01-12 20:23:16,637 Epoch   5 Step:    24100 Batch Loss:     1.822410 Tokens per Sec:    14919, Lr: 0.000300\n",
            "2020-01-12 20:23:32,063 Epoch   5 Step:    24200 Batch Loss:     1.725930 Tokens per Sec:    15052, Lr: 0.000300\n",
            "2020-01-12 20:23:47,524 Epoch   5 Step:    24300 Batch Loss:     2.087996 Tokens per Sec:    14943, Lr: 0.000300\n",
            "2020-01-12 20:24:03,028 Epoch   5 Step:    24400 Batch Loss:     1.686516 Tokens per Sec:    15006, Lr: 0.000300\n",
            "2020-01-12 20:24:18,436 Epoch   5 Step:    24500 Batch Loss:     1.760337 Tokens per Sec:    14535, Lr: 0.000300\n",
            "2020-01-12 20:24:33,771 Epoch   5 Step:    24600 Batch Loss:     1.888720 Tokens per Sec:    14923, Lr: 0.000300\n",
            "2020-01-12 20:24:49,238 Epoch   5 Step:    24700 Batch Loss:     2.218540 Tokens per Sec:    14971, Lr: 0.000300\n",
            "2020-01-12 20:25:04,767 Epoch   5 Step:    24800 Batch Loss:     1.782394 Tokens per Sec:    14888, Lr: 0.000300\n",
            "2020-01-12 20:25:20,103 Epoch   5 Step:    24900 Batch Loss:     1.820529 Tokens per Sec:    14831, Lr: 0.000300\n",
            "2020-01-12 20:25:35,618 Epoch   5 Step:    25000 Batch Loss:     1.791429 Tokens per Sec:    15138, Lr: 0.000300\n",
            "2020-01-12 20:26:23,472 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 20:26:23,472 Saving new checkpoint.\n",
            "2020-01-12 20:26:24,627 Example #0\n",
            "2020-01-12 20:26:24,628 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 20:26:24,628 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 20:26:24,628 \tHypothesis: Ịmara na anyị nwere Nna eluigwe nke hụrụ anyị n’anya na ilekọta anyị dị oké mkpa n’ilekọta onwe anyị .\n",
            "2020-01-12 20:26:24,628 Example #1\n",
            "2020-01-12 20:26:24,628 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 20:26:24,629 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 20:26:24,629 \tHypothesis: Gịnị dị mkpa iji nwee ihe ịga nke ọma n’alụmdi na nwunye Setan iji ghọta ihe mere o ji dị mkpa ka e mee ndokwa ahụ ?\n",
            "2020-01-12 20:26:24,629 Example #2\n",
            "2020-01-12 20:26:24,629 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 20:26:24,630 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 20:26:24,630 \tHypothesis: Ha aghọwo ndị enyi nke ụmụnna Kraịst , na - enye aka n’ụzọ dị ukwuu n’ikwusa “ ozi ọma nke alaeze a ” n’ihi àmà nye mba nile .\n",
            "2020-01-12 20:26:24,630 Example #3\n",
            "2020-01-12 20:26:24,630 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 20:26:24,630 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 20:26:24,631 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ nke ịchụso mgbaghara Chineke .\n",
            "2020-01-12 20:26:24,631 Validation result (greedy) at epoch   5, step    25000: bleu:  24.31, loss: 47287.5234, ppl:   5.0767, duration: 49.0127s\n",
            "2020-01-12 20:26:40,121 Epoch   5 Step:    25100 Batch Loss:     1.766291 Tokens per Sec:    14734, Lr: 0.000300\n",
            "2020-01-12 20:26:55,628 Epoch   5 Step:    25200 Batch Loss:     1.882705 Tokens per Sec:    14623, Lr: 0.000300\n",
            "2020-01-12 20:27:11,149 Epoch   5 Step:    25300 Batch Loss:     1.459983 Tokens per Sec:    14861, Lr: 0.000300\n",
            "2020-01-12 20:27:26,587 Epoch   5 Step:    25400 Batch Loss:     1.783412 Tokens per Sec:    15125, Lr: 0.000300\n",
            "2020-01-12 20:27:41,825 Epoch   5 Step:    25500 Batch Loss:     1.755316 Tokens per Sec:    14753, Lr: 0.000300\n",
            "2020-01-12 20:27:45,322 Epoch   5: total training loss 9463.44\n",
            "2020-01-12 20:27:45,322 EPOCH 6\n",
            "2020-01-12 20:27:57,738 Epoch   6 Step:    25600 Batch Loss:     1.840005 Tokens per Sec:    14501, Lr: 0.000300\n",
            "2020-01-12 20:28:13,180 Epoch   6 Step:    25700 Batch Loss:     1.882670 Tokens per Sec:    14845, Lr: 0.000300\n",
            "2020-01-12 20:28:28,602 Epoch   6 Step:    25800 Batch Loss:     1.907817 Tokens per Sec:    14967, Lr: 0.000300\n",
            "2020-01-12 20:28:43,937 Epoch   6 Step:    25900 Batch Loss:     1.902242 Tokens per Sec:    14916, Lr: 0.000300\n",
            "2020-01-12 20:28:59,417 Epoch   6 Step:    26000 Batch Loss:     1.587400 Tokens per Sec:    14797, Lr: 0.000300\n",
            "2020-01-12 20:29:47,319 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 20:29:47,320 Saving new checkpoint.\n",
            "2020-01-12 20:29:48,467 Example #0\n",
            "2020-01-12 20:29:48,468 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 20:29:48,468 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 20:29:48,468 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe na - ahụ n’anya ma na - elekọta anyị bụ ihe dị oké mkpa n’ichegbu onwe anyị .\n",
            "2020-01-12 20:29:48,468 Example #1\n",
            "2020-01-12 20:29:48,469 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 20:29:48,469 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 20:29:48,469 \tHypothesis: Gịnị ka anyị kwesịrị ime iji nwee ihe ịga nke ọma n’alụmdi na nwunye nke Setan iji gosi na ọ bụ ndokwa ahụ ?\n",
            "2020-01-12 20:29:48,469 Example #2\n",
            "2020-01-12 20:29:48,470 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 20:29:48,470 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 20:29:48,470 \tHypothesis: Ha aghọwo ndị enyi nke ụmụnna Kraịst , na - enye aka n’ụzọ dị ukwuu n’ikwusa “ ozi ọma nke alaeze a ” maka àmà nye mba nile .\n",
            "2020-01-12 20:29:48,470 Example #3\n",
            "2020-01-12 20:29:48,470 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 20:29:48,471 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 20:29:48,471 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ iji chebe mgbaghara Chineke .\n",
            "2020-01-12 20:29:48,471 Validation result (greedy) at epoch   6, step    26000: bleu:  24.37, loss: 46842.1055, ppl:   4.9996, duration: 49.0531s\n",
            "2020-01-12 20:30:04,005 Epoch   6 Step:    26100 Batch Loss:     1.890887 Tokens per Sec:    14810, Lr: 0.000300\n",
            "2020-01-12 20:30:19,286 Epoch   6 Step:    26200 Batch Loss:     1.695370 Tokens per Sec:    14924, Lr: 0.000300\n",
            "2020-01-12 20:30:34,840 Epoch   6 Step:    26300 Batch Loss:     1.497647 Tokens per Sec:    14644, Lr: 0.000300\n",
            "2020-01-12 20:30:50,292 Epoch   6 Step:    26400 Batch Loss:     1.941909 Tokens per Sec:    14898, Lr: 0.000300\n",
            "2020-01-12 20:31:05,703 Epoch   6 Step:    26500 Batch Loss:     1.946962 Tokens per Sec:    14963, Lr: 0.000300\n",
            "2020-01-12 20:31:21,106 Epoch   6 Step:    26600 Batch Loss:     1.829854 Tokens per Sec:    14888, Lr: 0.000300\n",
            "2020-01-12 20:31:36,665 Epoch   6 Step:    26700 Batch Loss:     1.747787 Tokens per Sec:    14986, Lr: 0.000300\n",
            "2020-01-12 20:31:52,204 Epoch   6 Step:    26800 Batch Loss:     1.681036 Tokens per Sec:    14973, Lr: 0.000300\n",
            "2020-01-12 20:32:07,832 Epoch   6 Step:    26900 Batch Loss:     1.602152 Tokens per Sec:    15093, Lr: 0.000300\n",
            "2020-01-12 20:32:23,329 Epoch   6 Step:    27000 Batch Loss:     1.784320 Tokens per Sec:    14799, Lr: 0.000300\n",
            "2020-01-12 20:33:11,041 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 20:33:11,042 Saving new checkpoint.\n",
            "2020-01-12 20:33:12,175 Example #0\n",
            "2020-01-12 20:33:12,176 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 20:33:12,176 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 20:33:12,176 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe nke hụrụ anyị n’anya ma na - eche banyere anyị dị oké mkpa n’inwe nchegbu .\n",
            "2020-01-12 20:33:12,176 Example #1\n",
            "2020-01-12 20:33:12,177 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 20:33:12,177 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 20:33:12,177 \tHypothesis: Gịnị dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’iso mgbalị Setan na - eme iji mee ndokwa ahụ ?\n",
            "2020-01-12 20:33:12,177 Example #2\n",
            "2020-01-12 20:33:12,177 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 20:33:12,177 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 20:33:12,177 \tHypothesis: Ha aghọwo ndị enyi nke ụmụnna Kraịst , na - enye aka n’ụzọ dị irè n’ikwusa “ ozi ọma nke alaeze a ” maka ịgba àmà nye mba nile .\n",
            "2020-01-12 20:33:12,177 Example #3\n",
            "2020-01-12 20:33:12,178 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 20:33:12,178 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 20:33:12,178 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ n’ebe ịchụso mgbaghara Chineke .\n",
            "2020-01-12 20:33:12,178 Validation result (greedy) at epoch   6, step    27000: bleu:  25.08, loss: 46441.1016, ppl:   4.9312, duration: 48.8481s\n",
            "2020-01-12 20:33:27,587 Epoch   6 Step:    27100 Batch Loss:     1.607757 Tokens per Sec:    14583, Lr: 0.000300\n",
            "2020-01-12 20:33:42,786 Epoch   6 Step:    27200 Batch Loss:     1.913949 Tokens per Sec:    14663, Lr: 0.000300\n",
            "2020-01-12 20:33:58,321 Epoch   6 Step:    27300 Batch Loss:     1.606479 Tokens per Sec:    15106, Lr: 0.000300\n",
            "2020-01-12 20:34:13,678 Epoch   6 Step:    27400 Batch Loss:     1.747176 Tokens per Sec:    14843, Lr: 0.000300\n",
            "2020-01-12 20:34:29,060 Epoch   6 Step:    27500 Batch Loss:     1.625142 Tokens per Sec:    14879, Lr: 0.000300\n",
            "2020-01-12 20:34:44,322 Epoch   6 Step:    27600 Batch Loss:     1.741222 Tokens per Sec:    14984, Lr: 0.000300\n",
            "2020-01-12 20:34:59,906 Epoch   6 Step:    27700 Batch Loss:     1.995491 Tokens per Sec:    14865, Lr: 0.000300\n",
            "2020-01-12 20:35:15,537 Epoch   6 Step:    27800 Batch Loss:     1.775587 Tokens per Sec:    15193, Lr: 0.000300\n",
            "2020-01-12 20:35:30,992 Epoch   6 Step:    27900 Batch Loss:     1.750137 Tokens per Sec:    14711, Lr: 0.000300\n",
            "2020-01-12 20:35:46,554 Epoch   6 Step:    28000 Batch Loss:     1.839852 Tokens per Sec:    15167, Lr: 0.000300\n",
            "2020-01-12 20:36:34,544 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 20:36:34,544 Saving new checkpoint.\n",
            "2020-01-12 20:36:35,619 Example #0\n",
            "2020-01-12 20:36:35,619 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 20:36:35,619 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 20:36:35,619 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe nke hụrụ anyị n’anya na - eche banyere anyị bụ ihe dị oké mkpa n’inwe nchegbu nke nchegbu .\n",
            "2020-01-12 20:36:35,619 Example #1\n",
            "2020-01-12 20:36:35,620 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 20:36:35,620 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 20:36:35,620 \tHypothesis: Gịnị ka ọ dị mkpa ime ka alụmdi na nwunye bụrụ nke ga - eme ka mgbalị Setan na - eme iji ghọta ndokwa ahụ ?\n",
            "2020-01-12 20:36:35,620 Example #2\n",
            "2020-01-12 20:36:35,620 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 20:36:35,620 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 20:36:35,620 \tHypothesis: Ha aghọọla ndị enyi nke ụmụnna Kraịst , na - enye aka ime ka “ ozi ọma nke alaeze a ” bụrụ àmà nye mba nile .\n",
            "2020-01-12 20:36:35,620 Example #3\n",
            "2020-01-12 20:36:35,620 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 20:36:35,621 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 20:36:35,621 \tHypothesis: Nke ahụ bụ ihe nke abụọ anyị ga - eme iji gbaghara Chineke .\n",
            "2020-01-12 20:36:35,621 Validation result (greedy) at epoch   6, step    28000: bleu:  24.95, loss: 46262.4414, ppl:   4.9010, duration: 49.0663s\n",
            "2020-01-12 20:36:51,042 Epoch   6 Step:    28100 Batch Loss:     1.867888 Tokens per Sec:    14594, Lr: 0.000300\n",
            "2020-01-12 20:37:06,411 Epoch   6 Step:    28200 Batch Loss:     1.802740 Tokens per Sec:    14871, Lr: 0.000300\n",
            "2020-01-12 20:37:21,750 Epoch   6 Step:    28300 Batch Loss:     1.812864 Tokens per Sec:    14792, Lr: 0.000300\n",
            "2020-01-12 20:37:37,170 Epoch   6 Step:    28400 Batch Loss:     1.986362 Tokens per Sec:    14951, Lr: 0.000300\n",
            "2020-01-12 20:37:52,435 Epoch   6 Step:    28500 Batch Loss:     1.691198 Tokens per Sec:    14885, Lr: 0.000300\n",
            "2020-01-12 20:38:07,845 Epoch   6 Step:    28600 Batch Loss:     1.494598 Tokens per Sec:    15057, Lr: 0.000300\n",
            "2020-01-12 20:38:23,282 Epoch   6 Step:    28700 Batch Loss:     1.905141 Tokens per Sec:    15013, Lr: 0.000300\n",
            "2020-01-12 20:38:38,685 Epoch   6 Step:    28800 Batch Loss:     1.791110 Tokens per Sec:    14672, Lr: 0.000300\n",
            "2020-01-12 20:38:54,180 Epoch   6 Step:    28900 Batch Loss:     1.927312 Tokens per Sec:    15001, Lr: 0.000300\n",
            "2020-01-12 20:39:09,567 Epoch   6 Step:    29000 Batch Loss:     1.954334 Tokens per Sec:    14807, Lr: 0.000300\n",
            "2020-01-12 20:39:57,477 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 20:39:57,478 Saving new checkpoint.\n",
            "2020-01-12 20:39:58,583 Example #0\n",
            "2020-01-12 20:39:58,583 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 20:39:58,584 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 20:39:58,584 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe nke hụrụ anyị n’anya ma na - elekọta anyị bụ nzọụkwụ dị oké mkpa n’ileghara nchegbu anya .\n",
            "2020-01-12 20:39:58,584 Example #1\n",
            "2020-01-12 20:39:58,584 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 20:39:58,584 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 20:39:58,585 \tHypothesis: Gịnị ka ọ dị mkpa ime iji nwee ihe ịga nke ọma n’iso mgbalị Setan iji ghọta ndokwa ahụ ?\n",
            "2020-01-12 20:39:58,585 Example #2\n",
            "2020-01-12 20:39:58,585 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 20:39:58,585 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 20:39:58,585 \tHypothesis: Ha aghọwo ndị enyi nke ụmụnna Kraịst , na - enye aka n’ụzọ dị oké ọnụ ahịa n’ikwusa “ ozi ọma nke alaeze a ” maka àmà nye mba nile .\n",
            "2020-01-12 20:39:58,585 Example #3\n",
            "2020-01-12 20:39:58,586 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 20:39:58,586 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 20:39:58,586 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ nke ịhapụ mgbaghara Chineke .\n",
            "2020-01-12 20:39:58,586 Validation result (greedy) at epoch   6, step    29000: bleu:  24.82, loss: 45884.4805, ppl:   4.8378, duration: 49.0187s\n",
            "2020-01-12 20:40:14,177 Epoch   6 Step:    29100 Batch Loss:     2.135021 Tokens per Sec:    15077, Lr: 0.000300\n",
            "2020-01-12 20:40:29,579 Epoch   6 Step:    29200 Batch Loss:     1.646612 Tokens per Sec:    14816, Lr: 0.000300\n",
            "2020-01-12 20:40:44,990 Epoch   6 Step:    29300 Batch Loss:     1.623582 Tokens per Sec:    14909, Lr: 0.000300\n",
            "2020-01-12 20:41:00,275 Epoch   6 Step:    29400 Batch Loss:     1.761126 Tokens per Sec:    14818, Lr: 0.000300\n",
            "2020-01-12 20:41:15,717 Epoch   6 Step:    29500 Batch Loss:     1.670034 Tokens per Sec:    14904, Lr: 0.000300\n",
            "2020-01-12 20:41:31,190 Epoch   6 Step:    29600 Batch Loss:     1.823590 Tokens per Sec:    14727, Lr: 0.000300\n",
            "2020-01-12 20:41:46,620 Epoch   6 Step:    29700 Batch Loss:     1.834014 Tokens per Sec:    14695, Lr: 0.000300\n",
            "2020-01-12 20:42:02,021 Epoch   6 Step:    29800 Batch Loss:     1.655501 Tokens per Sec:    14922, Lr: 0.000300\n",
            "2020-01-12 20:42:17,200 Epoch   6 Step:    29900 Batch Loss:     1.605438 Tokens per Sec:    14596, Lr: 0.000300\n",
            "2020-01-12 20:42:32,594 Epoch   6 Step:    30000 Batch Loss:     1.807694 Tokens per Sec:    14787, Lr: 0.000300\n",
            "2020-01-12 20:43:20,367 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 20:43:20,367 Saving new checkpoint.\n",
            "2020-01-12 20:43:21,527 Example #0\n",
            "2020-01-12 20:43:21,528 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 20:43:21,528 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 20:43:21,528 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe nke hụrụ anyị n’anya ma na - elekọta anyị bụ nzọụkwụ dị oké mkpa n’ilekọta nchegbu .\n",
            "2020-01-12 20:43:21,528 Example #1\n",
            "2020-01-12 20:43:21,528 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 20:43:21,529 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 20:43:21,529 \tHypothesis: Gịnị ka anyị kwesịrị ime iji nwee ihe ịga nke ọma n’iso mgbalị Setan na - eme iji chọpụta ndokwa ahụ ?\n",
            "2020-01-12 20:43:21,529 Example #2\n",
            "2020-01-12 20:43:21,529 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 20:43:21,529 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 20:43:21,529 \tHypothesis: Ha aghọwo ndị na - eguzosi ike n’ihe nke ụmụnna Kraịst , na - enye aka n’ọrụ nkwusa “ ozi ọma nke alaeze a ” maka ịgba àmà nye mba nile .\n",
            "2020-01-12 20:43:21,529 Example #3\n",
            "2020-01-12 20:43:21,529 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 20:43:21,530 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 20:43:21,530 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ nke ịhapụ mgbaghara Chineke .\n",
            "2020-01-12 20:43:21,530 Validation result (greedy) at epoch   6, step    30000: bleu:  25.31, loss: 45450.1211, ppl:   4.7661, duration: 48.9356s\n",
            "2020-01-12 20:43:36,945 Epoch   6 Step:    30100 Batch Loss:     2.143972 Tokens per Sec:    14951, Lr: 0.000300\n",
            "2020-01-12 20:43:52,425 Epoch   6 Step:    30200 Batch Loss:     1.664897 Tokens per Sec:    15167, Lr: 0.000300\n",
            "2020-01-12 20:44:07,764 Epoch   6 Step:    30300 Batch Loss:     1.798384 Tokens per Sec:    15043, Lr: 0.000300\n",
            "2020-01-12 20:44:23,136 Epoch   6 Step:    30400 Batch Loss:     1.426055 Tokens per Sec:    14659, Lr: 0.000300\n",
            "2020-01-12 20:44:38,706 Epoch   6 Step:    30500 Batch Loss:     1.554048 Tokens per Sec:    15235, Lr: 0.000300\n",
            "2020-01-12 20:44:54,119 Epoch   6 Step:    30600 Batch Loss:     1.716260 Tokens per Sec:    14873, Lr: 0.000300\n",
            "2020-01-12 20:44:56,497 Epoch   6: total training loss 9049.22\n",
            "2020-01-12 20:44:56,497 EPOCH 7\n",
            "2020-01-12 20:45:09,918 Epoch   7 Step:    30700 Batch Loss:     1.722278 Tokens per Sec:    14192, Lr: 0.000300\n",
            "2020-01-12 20:45:25,291 Epoch   7 Step:    30800 Batch Loss:     1.571839 Tokens per Sec:    14937, Lr: 0.000300\n",
            "2020-01-12 20:45:40,943 Epoch   7 Step:    30900 Batch Loss:     1.697905 Tokens per Sec:    15093, Lr: 0.000300\n",
            "2020-01-12 20:45:56,365 Epoch   7 Step:    31000 Batch Loss:     1.730338 Tokens per Sec:    14896, Lr: 0.000300\n",
            "2020-01-12 20:46:44,058 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 20:46:44,059 Saving new checkpoint.\n",
            "2020-01-12 20:46:45,675 Example #0\n",
            "2020-01-12 20:46:45,685 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 20:46:45,685 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 20:46:45,685 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe nke hụrụ anyị n’anya ma na - eche banyere anyị bụ nzọụkwụ dị oké mkpa n’ileghara nchegbu .\n",
            "2020-01-12 20:46:45,685 Example #1\n",
            "2020-01-12 20:46:45,685 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 20:46:45,685 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 20:46:45,685 \tHypothesis: Gịnị ka ọ dị mkpa ime ka alụmdi na nwunye bụrụ ihe ịga nke ọma n’iso mgbalị Setan na - eme iji ghọta ndokwa ahụ ?\n",
            "2020-01-12 20:46:45,685 Example #2\n",
            "2020-01-12 20:46:45,686 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 20:46:45,686 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 20:46:45,686 \tHypothesis: Ha aghọwo ndị so ụmụnna Kraịst na - eguzosi ike n’ihe , na - enye aka n’ọrụ nkwusa dị oké ọnụ ahịa n’ikwusa “ ozi ọma nke alaeze ” a maka àmà nye mba nile .\n",
            "2020-01-12 20:46:45,686 Example #3\n",
            "2020-01-12 20:46:45,686 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 20:46:45,686 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 20:46:45,686 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ n’ebe Chineke nọ .\n",
            "2020-01-12 20:46:45,686 Validation result (greedy) at epoch   7, step    31000: bleu:  25.56, loss: 45049.0586, ppl:   4.7009, duration: 49.3208s\n",
            "2020-01-12 20:47:01,469 Epoch   7 Step:    31100 Batch Loss:     1.938125 Tokens per Sec:    14878, Lr: 0.000300\n",
            "2020-01-12 20:47:17,041 Epoch   7 Step:    31200 Batch Loss:     1.850609 Tokens per Sec:    15004, Lr: 0.000300\n",
            "2020-01-12 20:47:32,481 Epoch   7 Step:    31300 Batch Loss:     1.615668 Tokens per Sec:    14763, Lr: 0.000300\n",
            "2020-01-12 20:47:47,962 Epoch   7 Step:    31400 Batch Loss:     1.775559 Tokens per Sec:    14870, Lr: 0.000300\n",
            "2020-01-12 20:48:03,430 Epoch   7 Step:    31500 Batch Loss:     1.548018 Tokens per Sec:    14774, Lr: 0.000300\n",
            "2020-01-12 20:48:19,058 Epoch   7 Step:    31600 Batch Loss:     2.114221 Tokens per Sec:    14913, Lr: 0.000300\n",
            "2020-01-12 20:48:34,457 Epoch   7 Step:    31700 Batch Loss:     1.684968 Tokens per Sec:    14939, Lr: 0.000300\n",
            "2020-01-12 20:48:49,933 Epoch   7 Step:    31800 Batch Loss:     1.762909 Tokens per Sec:    14993, Lr: 0.000300\n",
            "2020-01-12 20:49:05,225 Epoch   7 Step:    31900 Batch Loss:     1.810884 Tokens per Sec:    14783, Lr: 0.000300\n",
            "2020-01-12 20:49:20,585 Epoch   7 Step:    32000 Batch Loss:     1.792334 Tokens per Sec:    14676, Lr: 0.000300\n",
            "2020-01-12 20:50:08,376 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 20:50:08,376 Saving new checkpoint.\n",
            "2020-01-12 20:50:09,624 Example #0\n",
            "2020-01-12 20:50:09,624 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 20:50:09,625 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 20:50:09,625 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe nke hụrụ anyị n’anya ma na - elekọta anyị bụ nzọụkwụ dị oké mkpa n’ileghara nchegbu .\n",
            "2020-01-12 20:50:09,625 Example #1\n",
            "2020-01-12 20:50:09,625 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 20:50:09,625 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 20:50:09,626 \tHypothesis: Gịnị ka anyị kwesịrị ime iji nwee ihe ịga nke ọma n’alụmdi na nwunye nke mgbalị Setan iji mee ka ndokwa ahụ pụta ìhè ?\n",
            "2020-01-12 20:50:09,626 Example #2\n",
            "2020-01-12 20:50:09,626 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 20:50:09,626 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 20:50:09,626 \tHypothesis: Ha aghọwo ndị enyi nke ụmụnna Kraịst , na - enye aka ime nkwusa dị oké ọnụ ahịa n’ikwusa “ ozi ọma nke alaeze a ” maka ịgba àmà nye mba nile .\n",
            "2020-01-12 20:50:09,627 Example #3\n",
            "2020-01-12 20:50:09,627 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 20:50:09,627 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 20:50:09,627 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ n’ebe ịchụso mgbaghara Chineke .\n",
            "2020-01-12 20:50:09,627 Validation result (greedy) at epoch   7, step    32000: bleu:  25.99, loss: 45007.8516, ppl:   4.6943, duration: 49.0416s\n",
            "2020-01-12 20:50:25,012 Epoch   7 Step:    32100 Batch Loss:     1.600971 Tokens per Sec:    14811, Lr: 0.000300\n",
            "2020-01-12 20:50:40,495 Epoch   7 Step:    32200 Batch Loss:     1.644560 Tokens per Sec:    14797, Lr: 0.000300\n",
            "2020-01-12 20:50:55,881 Epoch   7 Step:    32300 Batch Loss:     1.722667 Tokens per Sec:    14603, Lr: 0.000300\n",
            "2020-01-12 20:51:11,001 Epoch   7 Step:    32400 Batch Loss:     1.669684 Tokens per Sec:    14894, Lr: 0.000300\n",
            "2020-01-12 20:51:26,455 Epoch   7 Step:    32500 Batch Loss:     1.797009 Tokens per Sec:    14917, Lr: 0.000300\n",
            "2020-01-12 20:51:41,910 Epoch   7 Step:    32600 Batch Loss:     1.648935 Tokens per Sec:    14981, Lr: 0.000300\n",
            "2020-01-12 20:51:57,446 Epoch   7 Step:    32700 Batch Loss:     1.926457 Tokens per Sec:    14988, Lr: 0.000300\n",
            "2020-01-12 20:52:12,874 Epoch   7 Step:    32800 Batch Loss:     1.675307 Tokens per Sec:    14984, Lr: 0.000300\n",
            "2020-01-12 20:52:28,183 Epoch   7 Step:    32900 Batch Loss:     1.770228 Tokens per Sec:    14529, Lr: 0.000300\n",
            "2020-01-12 20:52:43,635 Epoch   7 Step:    33000 Batch Loss:     1.503718 Tokens per Sec:    14806, Lr: 0.000300\n",
            "2020-01-12 20:53:31,387 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 20:53:31,388 Saving new checkpoint.\n",
            "2020-01-12 20:53:32,524 Example #0\n",
            "2020-01-12 20:53:32,525 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 20:53:32,525 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 20:53:32,526 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe bụ́ onye hụrụ anyị n’anya ma na - eche banyere anyị bụ nzọụkwụ dị oké mkpa n’inwe nchegbu .\n",
            "2020-01-12 20:53:32,526 Example #1\n",
            "2020-01-12 20:53:32,526 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 20:53:32,526 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 20:53:32,526 \tHypothesis: Gịnị ka anyị kwesịrị ime iji nwee ihe ịga nke ọma n’alụmdi na nwunye nke Setan iji mee ka ndokwa ahụ mee ?\n",
            "2020-01-12 20:53:32,526 Example #2\n",
            "2020-01-12 20:53:32,527 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 20:53:32,527 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 20:53:32,527 \tHypothesis: Ha aghọwo ndị enyi nke ụmụnna Kraịst , na - enye aka n’ụzọ bara uru n’ikwusa “ ozi ọma nke alaeze ” maka mba nile .\n",
            "2020-01-12 20:53:32,527 Example #3\n",
            "2020-01-12 20:53:32,528 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 20:53:32,528 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 20:53:32,528 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ nke ịchụso mgbaghara Chineke .\n",
            "2020-01-12 20:53:32,528 Validation result (greedy) at epoch   7, step    33000: bleu:  26.07, loss: 44696.9766, ppl:   4.6444, duration: 48.8921s\n",
            "2020-01-12 20:53:47,833 Epoch   7 Step:    33100 Batch Loss:     1.803834 Tokens per Sec:    14819, Lr: 0.000300\n",
            "2020-01-12 20:54:03,136 Epoch   7 Step:    33200 Batch Loss:     1.845018 Tokens per Sec:    14851, Lr: 0.000300\n",
            "2020-01-12 20:54:18,579 Epoch   7 Step:    33300 Batch Loss:     1.916604 Tokens per Sec:    14791, Lr: 0.000300\n",
            "2020-01-12 20:54:34,023 Epoch   7 Step:    33400 Batch Loss:     1.687369 Tokens per Sec:    15130, Lr: 0.000300\n",
            "2020-01-12 20:54:49,310 Epoch   7 Step:    33500 Batch Loss:     2.030026 Tokens per Sec:    14687, Lr: 0.000300\n",
            "2020-01-12 20:55:04,757 Epoch   7 Step:    33600 Batch Loss:     1.438121 Tokens per Sec:    14800, Lr: 0.000300\n",
            "2020-01-12 20:55:20,125 Epoch   7 Step:    33700 Batch Loss:     1.866433 Tokens per Sec:    14792, Lr: 0.000300\n",
            "2020-01-12 20:55:35,512 Epoch   7 Step:    33800 Batch Loss:     1.673443 Tokens per Sec:    14925, Lr: 0.000300\n",
            "2020-01-12 20:55:50,801 Epoch   7 Step:    33900 Batch Loss:     1.770368 Tokens per Sec:    14436, Lr: 0.000300\n",
            "2020-01-12 20:56:06,169 Epoch   7 Step:    34000 Batch Loss:     1.641713 Tokens per Sec:    15031, Lr: 0.000300\n",
            "2020-01-12 20:56:54,065 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 20:56:54,065 Saving new checkpoint.\n",
            "2020-01-12 20:56:55,213 Example #0\n",
            "2020-01-12 20:56:55,214 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 20:56:55,214 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 20:56:55,214 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe nke hụrụ anyị n’anya ma na - eche banyere anyị bụ nzọụkwụ dị oké mkpa n’inwe nchegbu .\n",
            "2020-01-12 20:56:55,214 Example #1\n",
            "2020-01-12 20:56:55,214 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 20:56:55,215 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 20:56:55,215 \tHypothesis: Gịnị ka ọ dị mkpa ime ka alụmdi na nwunye nwee ihe ịga nke ọma n’iso mgbalị Setan na - eme iji ghọta ndokwa ahụ ?\n",
            "2020-01-12 20:56:55,215 Example #2\n",
            "2020-01-12 20:56:55,215 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 20:56:55,215 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 20:56:55,215 \tHypothesis: Ha aghọwo ndị na - eguzosi ike n’ihe nke ụmụnna Kraịst , na - enye aka n’ụzọ bara uru n’ikwusa “ ozi ọma nke a nke alaeze ” maka ịgba àmà nye mba nile .\n",
            "2020-01-12 20:56:55,215 Example #3\n",
            "2020-01-12 20:56:55,215 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 20:56:55,216 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 20:56:55,216 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ nke ịgụta mgbaghara Chineke .\n",
            "2020-01-12 20:56:55,216 Validation result (greedy) at epoch   7, step    34000: bleu:  26.62, loss: 44376.6406, ppl:   4.5936, duration: 49.0466s\n",
            "2020-01-12 20:57:10,684 Epoch   7 Step:    34100 Batch Loss:     1.628509 Tokens per Sec:    14775, Lr: 0.000300\n",
            "2020-01-12 20:57:26,348 Epoch   7 Step:    34200 Batch Loss:     1.812291 Tokens per Sec:    15175, Lr: 0.000300\n",
            "2020-01-12 20:57:41,741 Epoch   7 Step:    34300 Batch Loss:     1.802730 Tokens per Sec:    14709, Lr: 0.000300\n",
            "2020-01-12 20:57:57,215 Epoch   7 Step:    34400 Batch Loss:     1.968211 Tokens per Sec:    14967, Lr: 0.000300\n",
            "2020-01-12 20:58:12,608 Epoch   7 Step:    34500 Batch Loss:     2.013632 Tokens per Sec:    14650, Lr: 0.000300\n",
            "2020-01-12 20:58:27,989 Epoch   7 Step:    34600 Batch Loss:     1.782141 Tokens per Sec:    14754, Lr: 0.000300\n",
            "2020-01-12 20:58:43,395 Epoch   7 Step:    34700 Batch Loss:     1.635022 Tokens per Sec:    14666, Lr: 0.000300\n",
            "2020-01-12 20:58:58,759 Epoch   7 Step:    34800 Batch Loss:     1.606699 Tokens per Sec:    14908, Lr: 0.000300\n",
            "2020-01-12 20:59:14,226 Epoch   7 Step:    34900 Batch Loss:     1.534078 Tokens per Sec:    14830, Lr: 0.000300\n",
            "2020-01-12 20:59:29,523 Epoch   7 Step:    35000 Batch Loss:     1.974217 Tokens per Sec:    14816, Lr: 0.000300\n",
            "2020-01-12 21:00:17,340 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 21:00:17,340 Saving new checkpoint.\n",
            "2020-01-12 21:00:18,472 Example #0\n",
            "2020-01-12 21:00:18,473 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 21:00:18,473 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 21:00:18,473 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe nke hụrụ anyị n’anya ma na - elekọta anyị bụ nzọụkwụ dị oké mkpa n’ileghara nchegbu anya .\n",
            "2020-01-12 21:00:18,473 Example #1\n",
            "2020-01-12 21:00:18,473 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 21:00:18,473 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 21:00:18,473 \tHypothesis: Gịnị ka ọ dị mkpa ime ka alụmdi na nwunye bụrụ ihe ịga nke ọma n’iso mgbalị Setan na - eme iji mee ka ndokwa ahụ pụta ìhè ?\n",
            "2020-01-12 21:00:18,473 Example #2\n",
            "2020-01-12 21:00:18,474 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 21:00:18,474 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 21:00:18,474 \tHypothesis: Ha aghọwo ndị òtù ụmụnna Kraịst na - eguzosi ike n’ihe , na - enye aka n’ụzọ dị irè n’ikwusa “ ozi ọma a ” maka ịgba àmà nye mba nile .\n",
            "2020-01-12 21:00:18,474 Example #3\n",
            "2020-01-12 21:00:18,474 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 21:00:18,474 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 21:00:18,474 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ iji nweta mgbaghara Chineke .\n",
            "2020-01-12 21:00:18,475 Validation result (greedy) at epoch   7, step    35000: bleu:  26.51, loss: 44150.9727, ppl:   4.5581, duration: 48.9509s\n",
            "2020-01-12 21:00:33,910 Epoch   7 Step:    35100 Batch Loss:     1.888368 Tokens per Sec:    15114, Lr: 0.000300\n",
            "2020-01-12 21:00:49,272 Epoch   7 Step:    35200 Batch Loss:     1.766853 Tokens per Sec:    14656, Lr: 0.000300\n",
            "2020-01-12 21:01:04,727 Epoch   7 Step:    35300 Batch Loss:     1.676388 Tokens per Sec:    14694, Lr: 0.000300\n",
            "2020-01-12 21:01:20,227 Epoch   7 Step:    35400 Batch Loss:     1.629535 Tokens per Sec:    15188, Lr: 0.000300\n",
            "2020-01-12 21:01:35,512 Epoch   7 Step:    35500 Batch Loss:     1.882165 Tokens per Sec:    14641, Lr: 0.000300\n",
            "2020-01-12 21:01:50,869 Epoch   7 Step:    35600 Batch Loss:     1.761919 Tokens per Sec:    15009, Lr: 0.000300\n",
            "2020-01-12 21:02:06,253 Epoch   7 Step:    35700 Batch Loss:     1.726080 Tokens per Sec:    15067, Lr: 0.000300\n",
            "2020-01-12 21:02:09,909 Epoch   7: total training loss 8782.38\n",
            "2020-01-12 21:02:09,910 EPOCH 8\n",
            "2020-01-12 21:02:22,290 Epoch   8 Step:    35800 Batch Loss:     1.750560 Tokens per Sec:    14324, Lr: 0.000300\n",
            "2020-01-12 21:02:37,583 Epoch   8 Step:    35900 Batch Loss:     1.729045 Tokens per Sec:    15011, Lr: 0.000300\n",
            "2020-01-12 21:02:53,040 Epoch   8 Step:    36000 Batch Loss:     1.765973 Tokens per Sec:    14993, Lr: 0.000300\n",
            "2020-01-12 21:03:40,972 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 21:03:40,972 Saving new checkpoint.\n",
            "2020-01-12 21:03:42,507 Example #0\n",
            "2020-01-12 21:03:42,507 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 21:03:42,508 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 21:03:42,508 \tHypothesis: Ịmata na anyị nwere Nna nke eluigwe nke hụrụ anyị n’anya ma na - eche banyere anyị bụ ihe dị oké mkpa n’inwe nchegbu .\n",
            "2020-01-12 21:03:42,508 Example #1\n",
            "2020-01-12 21:03:42,508 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 21:03:42,508 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 21:03:42,508 \tHypothesis: Gịnị ka anyị kwesịrị ime iji nwee ihe ịga nke ọma n’alụmdi na nwunye anyị n’esepụghị aka ná mgbalị Setan iji mee ka ndokwa ahụ pụta ìhè ?\n",
            "2020-01-12 21:03:42,509 Example #2\n",
            "2020-01-12 21:03:42,509 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 21:03:42,509 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 21:03:42,509 \tHypothesis: Ha aghọwo ndị enyi Kraịst na - eguzosi ike n’ihe , na - enye aka n’ụzọ bara uru n’ikwusa “ ozi ọma nke alaeze a ” maka ịgba àmà nye mba nile .\n",
            "2020-01-12 21:03:42,509 Example #3\n",
            "2020-01-12 21:03:42,510 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 21:03:42,510 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 21:03:42,510 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ iji nweta mgbaghara Chineke .\n",
            "2020-01-12 21:03:42,510 Validation result (greedy) at epoch   8, step    36000: bleu:  26.44, loss: 43972.8125, ppl:   4.5303, duration: 49.4697s\n",
            "2020-01-12 21:03:58,028 Epoch   8 Step:    36100 Batch Loss:     1.641339 Tokens per Sec:    14748, Lr: 0.000300\n",
            "2020-01-12 21:04:13,383 Epoch   8 Step:    36200 Batch Loss:     1.781644 Tokens per Sec:    15055, Lr: 0.000300\n",
            "2020-01-12 21:04:28,733 Epoch   8 Step:    36300 Batch Loss:     1.473602 Tokens per Sec:    14784, Lr: 0.000300\n",
            "2020-01-12 21:04:43,839 Epoch   8 Step:    36400 Batch Loss:     1.629753 Tokens per Sec:    15000, Lr: 0.000300\n",
            "2020-01-12 21:04:59,253 Epoch   8 Step:    36500 Batch Loss:     1.594085 Tokens per Sec:    15001, Lr: 0.000300\n",
            "2020-01-12 21:05:14,640 Epoch   8 Step:    36600 Batch Loss:     1.540841 Tokens per Sec:    14829, Lr: 0.000300\n",
            "2020-01-12 21:05:30,082 Epoch   8 Step:    36700 Batch Loss:     1.495332 Tokens per Sec:    15118, Lr: 0.000300\n",
            "2020-01-12 21:05:45,408 Epoch   8 Step:    36800 Batch Loss:     1.833434 Tokens per Sec:    14762, Lr: 0.000300\n",
            "2020-01-12 21:06:00,945 Epoch   8 Step:    36900 Batch Loss:     1.560205 Tokens per Sec:    15196, Lr: 0.000300\n",
            "2020-01-12 21:06:16,353 Epoch   8 Step:    37000 Batch Loss:     1.957934 Tokens per Sec:    15204, Lr: 0.000300\n",
            "2020-01-12 21:07:04,145 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 21:07:04,146 Saving new checkpoint.\n",
            "2020-01-12 21:07:05,317 Example #0\n",
            "2020-01-12 21:07:05,318 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 21:07:05,318 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 21:07:05,318 \tHypothesis: Ịmata na anyị nwere Nna nke eluigwe nke hụrụ anyị n’anya ma na - eche banyere anyị bụ nzọụkwụ dị oké mkpa n’imeri nchegbu .\n",
            "2020-01-12 21:07:05,318 Example #1\n",
            "2020-01-12 21:07:05,319 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 21:07:05,319 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 21:07:05,319 \tHypothesis: Gịnị ka anyị kwesịrị ime iji nwee ihe ịga nke ọma n’alụmdi na nwunye Setan iji mee ka ndokwa ahụ mee ?\n",
            "2020-01-12 21:07:05,319 Example #2\n",
            "2020-01-12 21:07:05,320 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 21:07:05,320 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 21:07:05,320 \tHypothesis: Ha aghọwo ndị enyi Kraịst na - eguzosi ike n’ihe , na - enye aka n’ụzọ bara uru n’ikwusa “ ozi ọma a nke alaeze ” maka ịgba àmà nye mba nile .\n",
            "2020-01-12 21:07:05,320 Example #3\n",
            "2020-01-12 21:07:05,320 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 21:07:05,321 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 21:07:05,322 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ iji nweta mgbaghara Chineke .\n",
            "2020-01-12 21:07:05,322 Validation result (greedy) at epoch   8, step    37000: bleu:  26.42, loss: 43499.7656, ppl:   4.4572, duration: 48.9682s\n",
            "2020-01-12 21:07:20,919 Epoch   8 Step:    37100 Batch Loss:     1.644894 Tokens per Sec:    14793, Lr: 0.000300\n",
            "2020-01-12 21:07:36,204 Epoch   8 Step:    37200 Batch Loss:     1.694431 Tokens per Sec:    14684, Lr: 0.000300\n",
            "2020-01-12 21:07:51,479 Epoch   8 Step:    37300 Batch Loss:     1.822424 Tokens per Sec:    14692, Lr: 0.000300\n",
            "2020-01-12 21:08:06,873 Epoch   8 Step:    37400 Batch Loss:     1.538625 Tokens per Sec:    14570, Lr: 0.000300\n",
            "2020-01-12 21:08:22,321 Epoch   8 Step:    37500 Batch Loss:     2.017817 Tokens per Sec:    14850, Lr: 0.000300\n",
            "2020-01-12 21:08:37,640 Epoch   8 Step:    37600 Batch Loss:     1.649877 Tokens per Sec:    14658, Lr: 0.000300\n",
            "2020-01-12 21:08:52,875 Epoch   8 Step:    37700 Batch Loss:     1.639162 Tokens per Sec:    14811, Lr: 0.000300\n",
            "2020-01-12 21:09:08,296 Epoch   8 Step:    37800 Batch Loss:     1.471858 Tokens per Sec:    15031, Lr: 0.000300\n",
            "2020-01-12 21:09:23,619 Epoch   8 Step:    37900 Batch Loss:     1.679891 Tokens per Sec:    14786, Lr: 0.000300\n",
            "2020-01-12 21:09:39,180 Epoch   8 Step:    38000 Batch Loss:     1.774557 Tokens per Sec:    14719, Lr: 0.000300\n",
            "2020-01-12 21:10:27,133 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 21:10:27,133 Saving new checkpoint.\n",
            "2020-01-12 21:10:28,237 Example #0\n",
            "2020-01-12 21:10:28,237 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 21:10:28,237 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 21:10:28,237 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe nke hụrụ anyị n’anya ma na - eche banyere anyị bụ nzọụkwụ dị oké mkpa n’ileghara nchegbu anya .\n",
            "2020-01-12 21:10:28,237 Example #1\n",
            "2020-01-12 21:10:28,238 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 21:10:28,238 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 21:10:28,238 \tHypothesis: Gịnị ka ọ dị mkpa ka alụmdi na nwunye bụrụ ihe ịga nke ọma n’esemokwu Setan iji mee ndokwa ahụ ?\n",
            "2020-01-12 21:10:28,238 Example #2\n",
            "2020-01-12 21:10:28,239 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 21:10:28,239 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 21:10:28,239 \tHypothesis: Ha aghọwo ndị enyi nke ụmụnna Kraịst na - eguzosi ike n’ihe , na - enye aka n’ụzọ bara uru n’ikwusa “ ozi ọma nke alaeze ahụ ” maka ịgba àmà nye mba nile .\n",
            "2020-01-12 21:10:28,239 Example #3\n",
            "2020-01-12 21:10:28,240 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 21:10:28,240 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 21:10:28,240 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ iji chebe mgbaghara Chineke .\n",
            "2020-01-12 21:10:28,240 Validation result (greedy) at epoch   8, step    38000: bleu:  26.73, loss: 43479.1094, ppl:   4.4541, duration: 49.0593s\n",
            "2020-01-12 21:10:43,610 Epoch   8 Step:    38100 Batch Loss:     1.599038 Tokens per Sec:    14876, Lr: 0.000300\n",
            "2020-01-12 21:10:59,211 Epoch   8 Step:    38200 Batch Loss:     1.702956 Tokens per Sec:    14865, Lr: 0.000300\n",
            "2020-01-12 21:11:14,708 Epoch   8 Step:    38300 Batch Loss:     1.505344 Tokens per Sec:    14819, Lr: 0.000300\n",
            "2020-01-12 21:11:30,108 Epoch   8 Step:    38400 Batch Loss:     1.662680 Tokens per Sec:    14898, Lr: 0.000300\n",
            "2020-01-12 21:11:45,679 Epoch   8 Step:    38500 Batch Loss:     1.729976 Tokens per Sec:    14865, Lr: 0.000300\n",
            "2020-01-12 21:12:01,053 Epoch   8 Step:    38600 Batch Loss:     1.734115 Tokens per Sec:    14586, Lr: 0.000300\n",
            "2020-01-12 21:12:16,502 Epoch   8 Step:    38700 Batch Loss:     1.423306 Tokens per Sec:    14779, Lr: 0.000300\n",
            "2020-01-12 21:12:31,978 Epoch   8 Step:    38800 Batch Loss:     1.666478 Tokens per Sec:    15098, Lr: 0.000300\n",
            "2020-01-12 21:12:47,418 Epoch   8 Step:    38900 Batch Loss:     1.939111 Tokens per Sec:    14761, Lr: 0.000300\n",
            "2020-01-12 21:13:02,642 Epoch   8 Step:    39000 Batch Loss:     1.795410 Tokens per Sec:    14766, Lr: 0.000300\n",
            "2020-01-12 21:13:50,518 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 21:13:50,519 Saving new checkpoint.\n",
            "2020-01-12 21:13:51,694 Example #0\n",
            "2020-01-12 21:13:51,695 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 21:13:51,695 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 21:13:51,695 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe nke hụrụ anyị n’anya , ọ na - emekwa ka anyị nwee ezigbo nsogbu .\n",
            "2020-01-12 21:13:51,695 Example #1\n",
            "2020-01-12 21:13:51,695 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 21:13:51,695 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 21:13:51,695 \tHypothesis: Gịnị ka anyị kwesịrị ime iji nwee ihe ịga nke ọma n’alụmdi na nwunye anyị n’agbanyeghị mgbalị Setan mere iji ghọta na ọ bụ ya mere o ji dị otú ahụ ?\n",
            "2020-01-12 21:13:51,696 Example #2\n",
            "2020-01-12 21:13:51,696 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 21:13:51,696 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 21:13:51,696 \tHypothesis: Ha aghọwo ndị enyi nke ụmụnna Kraịst na - eguzosi ike n’ihe , na - enye aka mee ka ha nwee ike ikwusa “ ozi ọma nke alaeze a ” maka ịgba àmà nye mba nile .\n",
            "2020-01-12 21:13:51,696 Example #3\n",
            "2020-01-12 21:13:51,696 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 21:13:51,696 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 21:13:51,696 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ nke ịhapụ mgbaghara Chineke .\n",
            "2020-01-12 21:13:51,697 Validation result (greedy) at epoch   8, step    39000: bleu:  27.01, loss: 43067.3906, ppl:   4.3915, duration: 49.0538s\n",
            "2020-01-12 21:14:07,115 Epoch   8 Step:    39100 Batch Loss:     1.662139 Tokens per Sec:    14985, Lr: 0.000300\n",
            "2020-01-12 21:14:22,419 Epoch   8 Step:    39200 Batch Loss:     1.789282 Tokens per Sec:    14634, Lr: 0.000300\n",
            "2020-01-12 21:14:38,004 Epoch   8 Step:    39300 Batch Loss:     1.837257 Tokens per Sec:    15009, Lr: 0.000300\n",
            "2020-01-12 21:14:53,354 Epoch   8 Step:    39400 Batch Loss:     1.717028 Tokens per Sec:    14870, Lr: 0.000300\n",
            "2020-01-12 21:15:08,638 Epoch   8 Step:    39500 Batch Loss:     1.685619 Tokens per Sec:    14741, Lr: 0.000300\n",
            "2020-01-12 21:15:24,021 Epoch   8 Step:    39600 Batch Loss:     1.907214 Tokens per Sec:    14857, Lr: 0.000300\n",
            "2020-01-12 21:15:39,365 Epoch   8 Step:    39700 Batch Loss:     1.538033 Tokens per Sec:    14805, Lr: 0.000300\n",
            "2020-01-12 21:15:54,837 Epoch   8 Step:    39800 Batch Loss:     1.742601 Tokens per Sec:    14988, Lr: 0.000300\n",
            "2020-01-12 21:16:10,255 Epoch   8 Step:    39900 Batch Loss:     1.619566 Tokens per Sec:    14586, Lr: 0.000300\n",
            "2020-01-12 21:16:25,853 Epoch   8 Step:    40000 Batch Loss:     1.430568 Tokens per Sec:    14831, Lr: 0.000300\n",
            "2020-01-12 21:17:13,802 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 21:17:13,803 Saving new checkpoint.\n",
            "2020-01-12 21:17:14,963 Example #0\n",
            "2020-01-12 21:17:14,964 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 21:17:14,964 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 21:17:14,964 \tHypothesis: Ịmara na anyị nwere Nna eluigwe nke hụrụ anyị n’anya ma na - eche banyere anyị bụ nzọụkwụ dị oké mkpa n’ileghara nchegbu anya .\n",
            "2020-01-12 21:17:14,964 Example #1\n",
            "2020-01-12 21:17:14,964 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 21:17:14,964 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 21:17:14,965 \tHypothesis: Gịnị ka ọ dị mkpa ime ka alụmdi na nwunye bụrụ ihe ịga nke ọma n’agbanyeghị mgbalị Setan mere iji chọpụta ndokwa ahụ ?\n",
            "2020-01-12 21:17:14,965 Example #2\n",
            "2020-01-12 21:17:14,965 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 21:17:14,965 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 21:17:14,965 \tHypothesis: Ha aghọwo ndị na - eguzosi ike n’ihe nke ụmụnna Kraịst , na - enye aka n’ọrụ nkwusa dị oké ọnụ ahịa n’ikwusa “ ozi ọma nke alaeze ” maka ịgba àmà nye mba nile .\n",
            "2020-01-12 21:17:14,965 Example #3\n",
            "2020-01-12 21:17:14,966 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 21:17:14,966 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 21:17:14,966 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ iji nweta mgbaghara Chineke .\n",
            "2020-01-12 21:17:14,966 Validation result (greedy) at epoch   8, step    40000: bleu:  27.13, loss: 42919.0703, ppl:   4.3692, duration: 49.1122s\n",
            "2020-01-12 21:17:30,407 Epoch   8 Step:    40100 Batch Loss:     1.672457 Tokens per Sec:    14858, Lr: 0.000300\n",
            "2020-01-12 21:17:46,237 Epoch   8 Step:    40200 Batch Loss:     1.660021 Tokens per Sec:    14771, Lr: 0.000300\n",
            "2020-01-12 21:18:01,789 Epoch   8 Step:    40300 Batch Loss:     1.719816 Tokens per Sec:    14731, Lr: 0.000300\n",
            "2020-01-12 21:18:17,468 Epoch   8 Step:    40400 Batch Loss:     1.811155 Tokens per Sec:    14668, Lr: 0.000300\n",
            "2020-01-12 21:18:33,111 Epoch   8 Step:    40500 Batch Loss:     1.458744 Tokens per Sec:    14717, Lr: 0.000300\n",
            "2020-01-12 21:18:48,801 Epoch   8 Step:    40600 Batch Loss:     1.554785 Tokens per Sec:    14546, Lr: 0.000300\n",
            "2020-01-12 21:19:04,391 Epoch   8 Step:    40700 Batch Loss:     1.625668 Tokens per Sec:    14549, Lr: 0.000300\n",
            "2020-01-12 21:19:20,002 Epoch   8 Step:    40800 Batch Loss:     1.508327 Tokens per Sec:    14736, Lr: 0.000300\n",
            "2020-01-12 21:19:25,084 Epoch   8: total training loss 8543.45\n",
            "2020-01-12 21:19:25,084 EPOCH 9\n",
            "2020-01-12 21:19:36,242 Epoch   9 Step:    40900 Batch Loss:     1.595006 Tokens per Sec:    14321, Lr: 0.000300\n",
            "2020-01-12 21:19:51,978 Epoch   9 Step:    41000 Batch Loss:     1.700033 Tokens per Sec:    14606, Lr: 0.000300\n",
            "2020-01-12 21:20:40,158 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 21:20:40,158 Saving new checkpoint.\n",
            "2020-01-12 21:20:41,709 Example #0\n",
            "2020-01-12 21:20:41,710 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 21:20:41,710 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 21:20:41,710 \tHypothesis: Ịmara na anyị nwere Nna nke eluigwe bụ́ onye hụrụ anyị n’anya ma na - eche banyere anyị bụ nzọụkwụ dị oké mkpa n’inwe nchegbu .\n",
            "2020-01-12 21:20:41,710 Example #1\n",
            "2020-01-12 21:20:41,711 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 21:20:41,711 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 21:20:41,711 \tHypothesis: Gịnị ka anyị kwesịrị ime iji nwee ihe ịga nke ọma n’alụmdi na nwunye n’agbanyeghị mgbalị Setan mere iji mee ndokwa ahụ ?\n",
            "2020-01-12 21:20:41,711 Example #2\n",
            "2020-01-12 21:20:41,711 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 21:20:41,711 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 21:20:41,711 \tHypothesis: Ha aghọwo ndị enyi nke ụmụnna Kraịst na - eguzosi ike n’ihe , na - enye aka n’ọrụ nkwusa “ ozi ọma a nke alaeze ” maka ịgba àmà nye mba nile .\n",
            "2020-01-12 21:20:41,712 Example #3\n",
            "2020-01-12 21:20:41,712 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 21:20:41,712 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 21:20:41,712 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ n’ịchụso mgbaghara Chineke .\n",
            "2020-01-12 21:20:41,712 Validation result (greedy) at epoch   9, step    41000: bleu:  27.08, loss: 42691.5898, ppl:   4.3352, duration: 49.7335s\n",
            "2020-01-12 21:20:57,266 Epoch   9 Step:    41100 Batch Loss:     1.576014 Tokens per Sec:    14850, Lr: 0.000300\n",
            "2020-01-12 21:21:12,810 Epoch   9 Step:    41200 Batch Loss:     1.383647 Tokens per Sec:    14625, Lr: 0.000300\n",
            "2020-01-12 21:21:28,420 Epoch   9 Step:    41300 Batch Loss:     1.844190 Tokens per Sec:    14642, Lr: 0.000300\n",
            "2020-01-12 21:21:43,964 Epoch   9 Step:    41400 Batch Loss:     1.586089 Tokens per Sec:    14521, Lr: 0.000300\n",
            "2020-01-12 21:21:59,610 Epoch   9 Step:    41500 Batch Loss:     1.416789 Tokens per Sec:    14788, Lr: 0.000300\n",
            "2020-01-12 21:22:14,920 Epoch   9 Step:    41600 Batch Loss:     1.464863 Tokens per Sec:    14614, Lr: 0.000300\n",
            "2020-01-12 21:22:30,607 Epoch   9 Step:    41700 Batch Loss:     1.622128 Tokens per Sec:    14798, Lr: 0.000300\n",
            "2020-01-12 21:22:46,117 Epoch   9 Step:    41800 Batch Loss:     1.475816 Tokens per Sec:    14755, Lr: 0.000300\n",
            "2020-01-12 21:23:01,741 Epoch   9 Step:    41900 Batch Loss:     1.591409 Tokens per Sec:    14631, Lr: 0.000300\n",
            "2020-01-12 21:23:17,338 Epoch   9 Step:    42000 Batch Loss:     1.496505 Tokens per Sec:    14620, Lr: 0.000300\n",
            "2020-01-12 21:24:05,435 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 21:24:05,435 Saving new checkpoint.\n",
            "2020-01-12 21:24:06,702 Example #0\n",
            "2020-01-12 21:24:06,703 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 21:24:06,703 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 21:24:06,703 \tHypothesis: Ịmata na anyị nwere Nna nke eluigwe nke hụrụ anyị n’anya ma na - eche banyere anyị bụ nzọụkwụ dị oké mkpa n’inwe nchegbu .\n",
            "2020-01-12 21:24:06,704 Example #1\n",
            "2020-01-12 21:24:06,704 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 21:24:06,704 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 21:24:06,704 \tHypothesis: Gịnị ka ọ dị mkpa ime ka alụmdi na nwunye bụrụ ihe ịga nke ọma n’agbanyeghị mgbalị Setan mere iji mee ka e mee ndokwa ahụ ?\n",
            "2020-01-12 21:24:06,704 Example #2\n",
            "2020-01-12 21:24:06,705 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 21:24:06,705 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 21:24:06,705 \tHypothesis: Ha aghọwo ndị òtù ụmụnna Kraịst na - eguzosi ike n’ihe , na - enye aka n’ọrụ nkwusa dị oké ọnụ ahịa n’ikwusa “ ozi ọma nke a nke alaeze ” maka ịgba àmà nye mba nile .\n",
            "2020-01-12 21:24:06,705 Example #3\n",
            "2020-01-12 21:24:06,706 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 21:24:06,706 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 21:24:06,706 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ iji nweta mgbaghara Chineke .\n",
            "2020-01-12 21:24:06,706 Validation result (greedy) at epoch   9, step    42000: bleu:  27.20, loss: 42611.5781, ppl:   4.3233, duration: 49.3676s\n",
            "2020-01-12 21:24:22,452 Epoch   9 Step:    42100 Batch Loss:     1.689489 Tokens per Sec:    14672, Lr: 0.000300\n",
            "2020-01-12 21:24:37,908 Epoch   9 Step:    42200 Batch Loss:     1.629198 Tokens per Sec:    14771, Lr: 0.000300\n",
            "2020-01-12 21:24:53,733 Epoch   9 Step:    42300 Batch Loss:     1.480716 Tokens per Sec:    14922, Lr: 0.000300\n",
            "2020-01-12 21:25:09,246 Epoch   9 Step:    42400 Batch Loss:     1.391306 Tokens per Sec:    14428, Lr: 0.000300\n",
            "2020-01-12 21:25:24,821 Epoch   9 Step:    42500 Batch Loss:     2.052684 Tokens per Sec:    14569, Lr: 0.000300\n",
            "2020-01-12 21:25:40,490 Epoch   9 Step:    42600 Batch Loss:     1.802234 Tokens per Sec:    14677, Lr: 0.000300\n",
            "2020-01-12 21:25:56,402 Epoch   9 Step:    42700 Batch Loss:     1.773178 Tokens per Sec:    14750, Lr: 0.000300\n",
            "2020-01-12 21:26:11,944 Epoch   9 Step:    42800 Batch Loss:     1.451049 Tokens per Sec:    14545, Lr: 0.000300\n",
            "2020-01-12 21:26:27,617 Epoch   9 Step:    42900 Batch Loss:     1.634992 Tokens per Sec:    14958, Lr: 0.000300\n",
            "2020-01-12 21:26:43,158 Epoch   9 Step:    43000 Batch Loss:     1.591648 Tokens per Sec:    14372, Lr: 0.000300\n",
            "2020-01-12 21:27:31,287 Hooray! New best validation result [ppl]!\n",
            "2020-01-12 21:27:31,288 Saving new checkpoint.\n",
            "2020-01-12 21:27:32,528 Example #0\n",
            "2020-01-12 21:27:32,529 \tSource:     Knowing that we have a heavenly Father who loves and cares for us is a vital step in overcoming feelings of anxiety .\n",
            "2020-01-12 21:27:32,529 \tReference:  Otu ihe dị ezigbo mkpa ga - eme ka obi na - eru anyị ala bụ ịmata na anyị nwere Nna bí n’eluigwe , onye hụrụ anyị n’anya ma na - eche gbasara anyị .\n",
            "2020-01-12 21:27:32,529 \tHypothesis: Ịmata na anyị nwere Nna nke eluigwe nke hụrụ anyị n’anya , na - eche banyere anyị bụ nzọụkwụ dị oké mkpa n’ileghara nchegbu nke nchegbu anya .\n",
            "2020-01-12 21:27:32,529 Example #1\n",
            "2020-01-12 21:27:32,529 \tSource:     What is needed to make a success of marriage in spite of Satan’s efforts to undermine that arrangement ?\n",
            "2020-01-12 21:27:32,530 \tReference:  Olee ihe dị mkpa iji mee ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị ndị Setan na - eme iji mebie ndokwa a ?\n",
            "2020-01-12 21:27:32,530 \tHypothesis: Gịnị ka ọ dị mkpa ime ka alụmdi na nwunye nwee ihe ịga nke ọma n’agbanyeghị mgbalị Setan mere ime iji mee ndokwa ahụ ?\n",
            "2020-01-12 21:27:32,530 Example #2\n",
            "2020-01-12 21:27:32,530 \tSource:     They have become the loyal companions of Christ’s brothers , giving valuable assistance in preaching “ this good news of the kingdom ” for a witness to all the nations .\n",
            "2020-01-12 21:27:32,530 \tReference:  Ha aghọwo ezi ndị enyi nke ụmụnna Kraịst , na - enyere ha nnọọ aka n’ikwusa “ ozi ọma nke a nke alaeze ” ka ọ bụrụ àmà nye mba nile .\n",
            "2020-01-12 21:27:32,530 \tHypothesis: Ha aghọwo ndị enyi nke ụmụnna Kraịst na - eguzosi ike n’ihe , na - enye aka n’ikwusa “ ozi ọma nke alaeze ahụ ” maka ịgba àmà nye mba nile .\n",
            "2020-01-12 21:27:32,531 Example #3\n",
            "2020-01-12 21:27:32,531 \tSource:     That is the second step toward gaining God’s forgiveness .\n",
            "2020-01-12 21:27:32,531 \tReference:  Nke ahụ bụ ihe nke abụọ mmadụ ga - eme ka Chineke wee gbaghara ya mmehie ya .\n",
            "2020-01-12 21:27:32,531 \tHypothesis: Nke ahụ bụ nzọụkwụ nke abụọ iji nweta mgbaghara Chineke .\n",
            "2020-01-12 21:27:32,531 Validation result (greedy) at epoch   9, step    43000: bleu:  28.13, loss: 42403.3984, ppl:   4.2925, duration: 49.3729s\n",
            "2020-01-12 21:27:48,244 Epoch   9 Step:    43100 Batch Loss:     1.547266 Tokens per Sec:    14820, Lr: 0.000300\n",
            "2020-01-12 21:28:03,823 Epoch   9 Step:    43200 Batch Loss:     1.929423 Tokens per Sec:    14652, Lr: 0.000300\n",
            "2020-01-12 21:28:19,245 Epoch   9 Step:    43300 Batch Loss:     1.527496 Tokens per Sec:    14532, Lr: 0.000300\n",
            "2020-01-12 21:28:35,093 Epoch   9 Step:    43400 Batch Loss:     1.788403 Tokens per Sec:    14724, Lr: 0.000300\n",
            "2020-01-12 21:28:50,595 Epoch   9 Step:    43500 Batch Loss:     2.251348 Tokens per Sec:    14310, Lr: 0.000300\n",
            "2020-01-12 21:29:06,387 Epoch   9 Step:    43600 Batch Loss:     1.597214 Tokens per Sec:    14627, Lr: 0.000300\n",
            "2020-01-12 21:29:21,960 Epoch   9 Step:    43700 Batch Loss:     1.625542 Tokens per Sec:    14496, Lr: 0.000300\n",
            "2020-01-12 21:29:37,642 Epoch   9 Step:    43800 Batch Loss:     1.622900 Tokens per Sec:    14858, Lr: 0.000300\n",
            "2020-01-12 21:29:53,317 Epoch   9 Step:    43900 Batch Loss:     1.755046 Tokens per Sec:    14460, Lr: 0.000300\n",
            "2020-01-12 21:30:09,012 Epoch   9 Step:    44000 Batch Loss:     1.583094 Tokens per Sec:    14681, Lr: 0.000300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MBoDS09JM807",
        "colab": {}
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "!cp -r joeynmt/models/${src}${tgt}_transformer/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n94wlrCjVc17",
        "colab": {}
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "66WhRE9lIhoD",
        "colab": {}
      },
      "source": [
        "# Test our model\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}