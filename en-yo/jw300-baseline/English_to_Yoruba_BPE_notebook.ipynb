{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "starter_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Igc5itf-xMGj"
      },
      "source": [
        "# Masakhane - Machine Translation for African Languages (Using JoeyNMT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x4fXCKCf36IK"
      },
      "source": [
        "## Note before beginning:\n",
        "### - The idea is that you should be able to make minimal changes to this in order to get SOME result for your own translation corpus. \n",
        "\n",
        "### - The tl;dr: Go to the **\"TODO\"** comments which will tell you what to update to get up and running\n",
        "\n",
        "### - If you actually want to have a clue what you're doing, read the text and peek at the links\n",
        "\n",
        "### - With 100 epochs, it should take around 7 hours to run in Google Colab\n",
        "\n",
        "### - Once you've gotten a result for your language, please attach and email your notebook that generated it to masakhanetranslation@gmail.com\n",
        "\n",
        "### - If you care enough and get a chance, doing a brief background on your language would be amazing. See examples in  [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l929HimrxS0a"
      },
      "source": [
        "## Retrieve your data & make a parallel corpus\n",
        "\n",
        "If you are wanting to use the JW300 data referenced on the Masakhane website or in our GitHub repo, you can use `opus-tools` to convert the data into a convenient format. `opus_read` from that package provides a convenient tool for reading the native aligned XML files and to convert them to TMX format. The tool can also be used to fetch relevant files from OPUS on the fly and to filter the data as necessary. [Read the documentation](https://pypi.org/project/opustools-pkg/) for more details.\n",
        "\n",
        "Once you have your corpus files in TMX format (an xml structure which will include the sentences in your target language and your source language in a single file), we recommend reading them into a pandas dataframe. Thankfully, Jade wrote a silly `tmx2dataframe` package which converts your tmx file to a pandas dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oGRmDELn7Az0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "d7b10285-b8cb-4bdf-8d2d-edcc60748912"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cn3tgQLzUxwn",
        "colab": {}
      },
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\n",
        "import os\n",
        "source_language = \"en\"\n",
        "target_language = \"yo\" \n",
        "lc = False  # If True, lowercase the data.\n",
        "seed = 42  # Random seed for shuffling.\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = tag\n",
        "\n",
        "# This will save it to a folder in our gdrive instead!\n",
        "!mkdir -p \"/content/drive/My Drive/masakhane/$src-$tgt-$tag\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/masakhane/%s-%s-%s\" % (source_language, target_language, tag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kBSgJHEw7Nvx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a46198b-6208-4662-b9fa-9c606248578c"
      },
      "source": [
        "!echo $gdrive_path"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masakhane/en-yo-baseline\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gA75Fs9ys8Y9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "3d3007b5-5539-4266-8282-dc7af376a13f"
      },
      "source": [
        "# Install opus-tools\n",
        "! pip install opustools-pkg"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opustools-pkg\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/9f/e829a0cceccc603450cd18e1ff80807b6237a88d9a8df2c0bb320796e900/opustools_pkg-0.0.52-py3-none-any.whl (80kB)\n",
            "\r\u001b[K     |████                            | 10kB 26.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 20kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 51kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 71kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 5.5MB/s \n",
            "\u001b[?25hInstalling collected packages: opustools-pkg\n",
            "Successfully installed opustools-pkg-0.0.52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xq-tDZVks7ZD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "7bc867b4-33e1-49b6-b95c-c0541fa8b3ff"
      },
      "source": [
        "# Downloading our corpus\n",
        "! opus_read -d JW300 -s $src -t $tgt -wm moses -w jw300.$src jw300.$tgt -q\n",
        "\n",
        "# extract the corpus file\n",
        "! gunzip JW300_latest_xml_$src-$tgt.xml.gz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Alignment file /proj/nlpl/data/OPUS/JW300/latest/xml/en-yo.xml.gz not found. The following files are available for downloading:\n",
            "\n",
            "   4 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/en-yo.xml.gz\n",
            " 263 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/en.zip\n",
            "  58 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/yo.zip\n",
            "\n",
            " 325 MB Total size\n",
            "./JW300_latest_xml_en-yo.xml.gz ... 100% of 4 MB\n",
            "./JW300_latest_xml_en.zip ... 100% of 263 MB\n",
            "./JW300_latest_xml_yo.zip ... 100% of 58 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n48GDRnP8y2G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "62cdd198-6dda-47ee-85fe-21b15fa07702"
      },
      "source": [
        "# Download the global test set.\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
        "  \n",
        "# And the specific test set for this language pair.\n",
        "os.environ[\"trg\"] = target_language \n",
        "os.environ[\"src\"] = source_language \n",
        "\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.en \n",
        "! mv test.en-$trg.en test.en\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.$trg \n",
        "! mv test.en-$trg.$trg test.$trg"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-17 17:19:16--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 277791 (271K) [text/plain]\n",
            "Saving to: ‘test.en-any.en’\n",
            "\n",
            "\rtest.en-any.en        0%[                    ]       0  --.-KB/s               \rtest.en-any.en      100%[===================>] 271.28K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-01-17 17:19:16 (13.9 MB/s) - ‘test.en-any.en’ saved [277791/277791]\n",
            "\n",
            "--2020-01-17 17:19:18--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-yo.en\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 201994 (197K) [text/plain]\n",
            "Saving to: ‘test.en-yo.en’\n",
            "\n",
            "test.en-yo.en       100%[===================>] 197.26K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-01-17 17:19:19 (14.4 MB/s) - ‘test.en-yo.en’ saved [201994/201994]\n",
            "\n",
            "--2020-01-17 17:19:22--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-yo.yo\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 280073 (274K) [text/plain]\n",
            "Saving to: ‘test.en-yo.yo’\n",
            "\n",
            "test.en-yo.yo       100%[===================>] 273.51K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-01-17 17:19:23 (15.3 MB/s) - ‘test.en-yo.yo’ saved [280073/280073]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqDG-CI28y2L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "083c9138-212e-4959-9765-c1848b337998"
      },
      "source": [
        "# Read the test data to filter from train and dev splits.\n",
        "# Store english portion in set for quick filtering checks.\n",
        "en_test_sents = set()\n",
        "filter_test_sents = \"test.en-any.en\"\n",
        "j = 0\n",
        "with open(filter_test_sents) as f:\n",
        "  for line in f:\n",
        "    en_test_sents.add(line.strip())\n",
        "    j += 1\n",
        "print('Loaded {} global test sentences to filter from the training/dev data.'.format(j))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 3571 global test sentences to filter from the training/dev data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3CNdwLBCfSIl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "2a141021-fb26-4c74-81f0-44c39fdb748c"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# TMX file to dataframe\n",
        "source_file = 'jw300.' + source_language\n",
        "target_file = 'jw300.' + target_language\n",
        "\n",
        "source = []\n",
        "target = []\n",
        "skip_lines = []  # Collect the line numbers of the source portion to skip the same lines for the target portion.\n",
        "with open(source_file) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        # Skip sentences that are contained in the test set.\n",
        "        if line.strip() not in en_test_sents:\n",
        "            source.append(line.strip())\n",
        "        else:\n",
        "            skip_lines.append(i)             \n",
        "with open(target_file) as f:\n",
        "    for j, line in enumerate(f):\n",
        "        # Only add to corpus if corresponding source was not skipped.\n",
        "        if j not in skip_lines:\n",
        "            target.append(line.strip())\n",
        "    \n",
        "print('Loaded data and skipped {}/{} lines since contained in test set.'.format(len(skip_lines), i))\n",
        "    \n",
        "df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n",
        "# if you get TypeError: data argument can't be an iterator is because of your zip version run this below\n",
        "#df = pd.DataFrame(list(zip(source, target)), columns=['source_sentence', 'target_sentence'])\n",
        "df.head(5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded data and skipped 5663/474986 lines since contained in test set.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_sentence</th>\n",
              "      <th>target_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Using Ladders — Do You Make These Safety Checks ?</td>\n",
              "      <td>Lílo Àkàbà — Ǹjẹ́ O Máa Ń Ṣe Àyẹ̀wò Wọ̀nyí Tó...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>By Awake !</td>\n",
              "      <td>Látọwọ́ akọ̀ròyìn Jí !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>correspondent in Ireland</td>\n",
              "      <td>ní Ireland</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PAUL needed to change a bulb in an outside lig...</td>\n",
              "      <td>PAUL fẹ́ pààrọ̀ gílóòbù iná tó wà lóde ilé ẹ̀ .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>He also needed to clean the outside upstairs w...</td>\n",
              "      <td>Ó tún fẹ́ nu àwọn fèrèsé pẹ̀tẹ́ẹ̀sì lọ́wọ́ ita...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     source_sentence                                    target_sentence\n",
              "0  Using Ladders — Do You Make These Safety Checks ?  Lílo Àkàbà — Ǹjẹ́ O Máa Ń Ṣe Àyẹ̀wò Wọ̀nyí Tó...\n",
              "1                                         By Awake !                             Látọwọ́ akọ̀ròyìn Jí !\n",
              "2                           correspondent in Ireland                                         ní Ireland\n",
              "3  PAUL needed to change a bulb in an outside lig...    PAUL fẹ́ pààrọ̀ gílóòbù iná tó wà lóde ilé ẹ̀ .\n",
              "4  He also needed to clean the outside upstairs w...  Ó tún fẹ́ nu àwọn fèrèsé pẹ̀tẹ́ẹ̀sì lọ́wọ́ ita..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YkuK3B4p2AkN"
      },
      "source": [
        "## Pre-processing and export\n",
        "\n",
        "It is generally a good idea to remove duplicate translations and conflicting translations from the corpus. In practice, these public corpora include some number of these that need to be cleaned.\n",
        "\n",
        "In addition we will split our data into dev/test/train and export to the filesystem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M_2ouEOH1_1q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "a7309847-e165-4e5d-c83f-8caf21e72fd2"
      },
      "source": [
        "# drop duplicate translations\n",
        "df_pp = df.drop_duplicates()\n",
        "\n",
        "# drop conflicting translations\n",
        "# (this is optional and something that you might want to comment out \n",
        "# depending on the size of your corpus)\n",
        "df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n",
        "df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n",
        "\n",
        "# Shuffle the data to remove bias in dev set selection.\n",
        "df_pp = df_pp.sample(frac=1, random_state=seed).reset_index(drop=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_1BwAApEtMk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ae159d3f-27b6-40cd-cd1d-70c10a241cee"
      },
      "source": [
        "# Install fuzzy wuzzy to remove \"almost duplicate\" sentences in the\n",
        "# test and training sets.\n",
        "! pip install fuzzywuzzy\n",
        "! pip install python-Levenshtein\n",
        "import time\n",
        "from fuzzywuzzy import process\n",
        "import numpy as np\n",
        "\n",
        "# reset the index of the training set after previous filtering\n",
        "df_pp.reset_index(drop=False, inplace=True)\n",
        "\n",
        "# Remove samples from the training data set if they \"almost overlap\" with the\n",
        "# samples in the test set.\n",
        "\n",
        "# Filtering function. Adjust pad to narrow down the candidate matches to\n",
        "# within a certain length of characters of the given sample.\n",
        "def fuzzfilter(sample, candidates, pad):\n",
        "  candidates = [x for x in candidates if len(x) <= len(sample)+pad and len(x) >= len(sample)-pad] \n",
        "  if len(candidates) > 0:\n",
        "    return process.extractOne(sample, candidates)[1]\n",
        "  else:\n",
        "    return np.nan\n",
        "\n",
        "# NOTE - This might run slow depending on the size of your training set. We are\n",
        "# printing some information to help you track how long it would take. \n",
        "scores = []\n",
        "start_time = time.time()\n",
        "for idx, row in df_pp.iterrows():\n",
        "  scores.append(fuzzfilter(row['source_sentence'], list(en_test_sents), 5))\n",
        "  if idx % 1000 == 0:\n",
        "    hours, rem = divmod(time.time() - start_time, 3600)\n",
        "    minutes, seconds = divmod(rem, 60)\n",
        "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds), \"%0.2f percent complete\" % (100.0*float(idx)/float(len(df_pp))))\n",
        "\n",
        "# Filter out \"almost overlapping samples\"\n",
        "df_pp['scores'] = scores\n",
        "df_pp = df_pp[df_pp['scores'] < 95]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading https://files.pythonhosted.org/packages/d8/f1/5a267addb30ab7eaa1beab2b9323073815da4551076554ecc890a3595ec9/fuzzywuzzy-0.17.0-py2.py3-none-any.whl\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.17.0\n",
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (42.0.2)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144676 sha256=9ea934a16ebb4c02728ca1af20466e179eacf3412c08ad8bb0bde144bf075211\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.0\n",
            "00:00:00.13 0.00 percent complete\n",
            "00:00:18.96 0.24 percent complete\n",
            "00:00:38.11 0.48 percent complete\n",
            "00:00:57.26 0.72 percent complete\n",
            "00:01:16.08 0.96 percent complete\n",
            "00:01:35.18 1.20 percent complete\n",
            "00:01:53.15 1.43 percent complete\n",
            "00:02:12.25 1.67 percent complete\n",
            "00:02:30.91 1.91 percent complete\n",
            "00:02:49.58 2.15 percent complete\n",
            "00:03:09.08 2.39 percent complete\n",
            "00:03:27.03 2.63 percent complete\n",
            "00:03:46.28 2.87 percent complete\n",
            "00:04:05.31 3.11 percent complete\n",
            "00:04:24.30 3.35 percent complete\n",
            "00:04:43.81 3.59 percent complete\n",
            "00:05:02.76 3.82 percent complete\n",
            "00:05:21.41 4.06 percent complete\n",
            "00:05:41.61 4.30 percent complete\n",
            "00:06:00.13 4.54 percent complete\n",
            "00:06:18.94 4.78 percent complete\n",
            "00:06:37.73 5.02 percent complete\n",
            "00:06:56.34 5.26 percent complete\n",
            "00:07:15.43 5.50 percent complete\n",
            "00:07:34.39 5.74 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '↓']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:07:53.90 5.98 percent complete\n",
            "00:08:14.12 6.22 percent complete\n",
            "00:08:32.88 6.45 percent complete\n",
            "00:08:52.02 6.69 percent complete\n",
            "00:09:10.59 6.93 percent complete\n",
            "00:09:29.52 7.17 percent complete\n",
            "00:09:49.28 7.41 percent complete\n",
            "00:10:07.99 7.65 percent complete\n",
            "00:10:26.80 7.89 percent complete\n",
            "00:10:46.56 8.13 percent complete\n",
            "00:11:05.10 8.37 percent complete\n",
            "00:11:23.38 8.61 percent complete\n",
            "00:11:42.67 8.84 percent complete\n",
            "00:12:01.24 9.08 percent complete\n",
            "00:12:20.04 9.32 percent complete\n",
            "00:12:39.18 9.56 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '” *']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:12:58.06 9.80 percent complete\n",
            "00:13:17.51 10.04 percent complete\n",
            "00:13:36.53 10.28 percent complete\n",
            "00:13:55.26 10.52 percent complete\n",
            "00:14:13.70 10.76 percent complete\n",
            "00:14:32.82 11.00 percent complete\n",
            "00:14:52.21 11.24 percent complete\n",
            "00:15:11.09 11.47 percent complete\n",
            "00:15:29.42 11.71 percent complete\n",
            "00:15:49.31 11.95 percent complete\n",
            "00:16:08.94 12.19 percent complete\n",
            "00:16:28.33 12.43 percent complete\n",
            "00:16:47.36 12.67 percent complete\n",
            "00:17:06.35 12.91 percent complete\n",
            "00:17:24.93 13.15 percent complete\n",
            "00:17:43.95 13.39 percent complete\n",
            "00:18:02.92 13.63 percent complete\n",
            "00:18:22.58 13.86 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '. .']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:18:42.03 14.10 percent complete\n",
            "00:19:00.45 14.34 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:19:18.89 14.58 percent complete\n",
            "00:19:37.86 14.82 percent complete\n",
            "00:19:56.60 15.06 percent complete\n",
            "00:20:15.05 15.30 percent complete\n",
            "00:20:33.81 15.54 percent complete\n",
            "00:20:53.12 15.78 percent complete\n",
            "00:21:13.15 16.02 percent complete\n",
            "00:21:31.56 16.26 percent complete\n",
            "00:21:50.40 16.49 percent complete\n",
            "00:22:09.28 16.73 percent complete\n",
            "00:22:28.38 16.97 percent complete\n",
            "00:22:47.48 17.21 percent complete\n",
            "00:23:06.20 17.45 percent complete\n",
            "00:23:25.05 17.69 percent complete\n",
            "00:23:44.09 17.93 percent complete\n",
            "00:24:02.94 18.17 percent complete\n",
            "00:24:20.85 18.41 percent complete\n",
            "00:24:39.77 18.65 percent complete\n",
            "00:24:58.11 18.88 percent complete\n",
            "00:25:16.43 19.12 percent complete\n",
            "00:25:34.91 19.36 percent complete\n",
            "00:25:54.92 19.60 percent complete\n",
            "00:26:13.85 19.84 percent complete\n",
            "00:26:32.60 20.08 percent complete\n",
            "00:26:50.91 20.32 percent complete\n",
            "00:27:09.33 20.56 percent complete\n",
            "00:27:28.38 20.80 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '․ ․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:27:47.62 21.04 percent complete\n",
            "00:28:06.21 21.28 percent complete\n",
            "00:28:25.74 21.51 percent complete\n",
            "00:28:44.64 21.75 percent complete\n",
            "00:29:03.09 21.99 percent complete\n",
            "00:29:21.67 22.23 percent complete\n",
            "00:29:41.07 22.47 percent complete\n",
            "00:29:59.48 22.71 percent complete\n",
            "00:30:18.55 22.95 percent complete\n",
            "00:30:37.63 23.19 percent complete\n",
            "00:30:56.07 23.43 percent complete\n",
            "00:31:14.98 23.67 percent complete\n",
            "00:31:33.62 23.90 percent complete\n",
            "00:31:52.49 24.14 percent complete\n",
            "00:32:11.47 24.38 percent complete\n",
            "00:32:30.89 24.62 percent complete\n",
            "00:32:49.68 24.86 percent complete\n",
            "00:33:08.94 25.10 percent complete\n",
            "00:33:28.09 25.34 percent complete\n",
            "00:33:47.39 25.58 percent complete\n",
            "00:34:06.30 25.82 percent complete\n",
            "00:34:25.05 26.06 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '→ →']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:34:43.52 26.29 percent complete\n",
            "00:35:03.01 26.53 percent complete\n",
            "00:35:22.19 26.77 percent complete\n",
            "00:35:41.12 27.01 percent complete\n",
            "00:36:00.24 27.25 percent complete\n",
            "00:36:19.38 27.49 percent complete\n",
            "00:36:38.62 27.73 percent complete\n",
            "00:36:57.83 27.97 percent complete\n",
            "00:37:16.87 28.21 percent complete\n",
            "00:37:35.67 28.45 percent complete\n",
            "00:37:54.56 28.69 percent complete\n",
            "00:38:13.46 28.92 percent complete\n",
            "00:38:32.24 29.16 percent complete\n",
            "00:38:52.46 29.40 percent complete\n",
            "00:39:11.30 29.64 percent complete\n",
            "00:39:30.26 29.88 percent complete\n",
            "00:39:49.02 30.12 percent complete\n",
            "00:40:08.17 30.36 percent complete\n",
            "00:40:27.34 30.60 percent complete\n",
            "00:40:46.49 30.84 percent complete\n",
            "00:41:05.98 31.08 percent complete\n",
            "00:41:25.67 31.31 percent complete\n",
            "00:41:44.98 31.55 percent complete\n",
            "00:42:04.15 31.79 percent complete\n",
            "00:42:23.10 32.03 percent complete\n",
            "00:42:42.60 32.27 percent complete\n",
            "00:43:02.08 32.51 percent complete\n",
            "00:43:20.58 32.75 percent complete\n",
            "00:43:39.59 32.99 percent complete\n",
            "00:43:59.78 33.23 percent complete\n",
            "00:44:18.22 33.47 percent complete\n",
            "00:44:36.92 33.71 percent complete\n",
            "00:44:55.89 33.94 percent complete\n",
            "00:45:15.22 34.18 percent complete\n",
            "00:45:34.09 34.42 percent complete\n",
            "00:45:53.00 34.66 percent complete\n",
            "00:46:12.20 34.90 percent complete\n",
            "00:46:32.46 35.14 percent complete\n",
            "00:46:51.63 35.38 percent complete\n",
            "00:47:11.47 35.62 percent complete\n",
            "00:47:31.19 35.86 percent complete\n",
            "00:47:50.82 36.10 percent complete\n",
            "00:48:10.56 36.33 percent complete\n",
            "00:48:29.06 36.57 percent complete\n",
            "00:48:48.01 36.81 percent complete\n",
            "00:49:08.65 37.05 percent complete\n",
            "00:49:27.33 37.29 percent complete\n",
            "00:49:46.92 37.53 percent complete\n",
            "00:50:06.74 37.77 percent complete\n",
            "00:50:25.03 38.01 percent complete\n",
            "00:50:44.28 38.25 percent complete\n",
            "00:51:03.30 38.49 percent complete\n",
            "00:51:22.01 38.73 percent complete\n",
            "00:51:41.84 38.96 percent complete\n",
            "00:52:01.86 39.20 percent complete\n",
            "00:52:20.60 39.44 percent complete\n",
            "00:52:40.24 39.68 percent complete\n",
            "00:52:59.99 39.92 percent complete\n",
            "00:53:19.31 40.16 percent complete\n",
            "00:53:38.87 40.40 percent complete\n",
            "00:53:57.67 40.64 percent complete\n",
            "00:54:17.58 40.88 percent complete\n",
            "00:54:37.27 41.12 percent complete\n",
            "00:54:56.85 41.35 percent complete\n",
            "00:55:16.09 41.59 percent complete\n",
            "00:55:34.92 41.83 percent complete\n",
            "00:55:54.40 42.07 percent complete\n",
            "00:56:13.20 42.31 percent complete\n",
            "00:56:32.47 42.55 percent complete\n",
            "00:56:52.97 42.79 percent complete\n",
            "00:57:12.86 43.03 percent complete\n",
            "00:57:31.80 43.27 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '↓ ↓']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:57:50.99 43.51 percent complete\n",
            "00:58:10.45 43.75 percent complete\n",
            "00:58:29.33 43.98 percent complete\n",
            "00:58:48.02 44.22 percent complete\n",
            "00:59:07.60 44.46 percent complete\n",
            "00:59:27.40 44.70 percent complete\n",
            "00:59:46.39 44.94 percent complete\n",
            "01:00:06.10 45.18 percent complete\n",
            "01:00:25.25 45.42 percent complete\n",
            "01:00:44.37 45.66 percent complete\n",
            "01:01:04.18 45.90 percent complete\n",
            "01:01:23.16 46.14 percent complete\n",
            "01:01:42.29 46.37 percent complete\n",
            "01:02:02.47 46.61 percent complete\n",
            "01:02:22.09 46.85 percent complete\n",
            "01:02:41.57 47.09 percent complete\n",
            "01:03:02.37 47.33 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '*']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:03:21.77 47.57 percent complete\n",
            "01:03:41.54 47.81 percent complete\n",
            "01:04:01.57 48.05 percent complete\n",
            "01:04:21.18 48.29 percent complete\n",
            "01:04:41.17 48.53 percent complete\n",
            "01:05:02.06 48.77 percent complete\n",
            "01:05:21.61 49.00 percent complete\n",
            "01:05:40.79 49.24 percent complete\n",
            "01:06:00.40 49.48 percent complete\n",
            "01:06:20.19 49.72 percent complete\n",
            "01:06:40.09 49.96 percent complete\n",
            "01:06:59.06 50.20 percent complete\n",
            "01:07:18.48 50.44 percent complete\n",
            "01:07:38.71 50.68 percent complete\n",
            "01:07:58.19 50.92 percent complete\n",
            "01:08:17.88 51.16 percent complete\n",
            "01:08:37.17 51.39 percent complete\n",
            "01:08:56.63 51.63 percent complete\n",
            "01:09:15.72 51.87 percent complete\n",
            "01:09:34.98 52.11 percent complete\n",
            "01:09:55.14 52.35 percent complete\n",
            "01:10:16.13 52.59 percent complete\n",
            "01:10:35.61 52.83 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '⇩']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:10:55.15 53.07 percent complete\n",
            "01:11:14.39 53.31 percent complete\n",
            "01:11:33.44 53.55 percent complete\n",
            "01:11:52.69 53.79 percent complete\n",
            "01:12:12.70 54.02 percent complete\n",
            "01:12:31.71 54.26 percent complete\n",
            "01:12:51.76 54.50 percent complete\n",
            "01:13:11.31 54.74 percent complete\n",
            "01:13:30.38 54.98 percent complete\n",
            "01:13:49.07 55.22 percent complete\n",
            "01:14:08.17 55.46 percent complete\n",
            "01:14:26.81 55.70 percent complete\n",
            "01:14:45.81 55.94 percent complete\n",
            "01:15:05.04 56.18 percent complete\n",
            "01:15:24.01 56.41 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '”']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:15:43.29 56.65 percent complete\n",
            "01:16:02.70 56.89 percent complete\n",
            "01:16:21.25 57.13 percent complete\n",
            "01:16:39.79 57.37 percent complete\n",
            "01:16:59.25 57.61 percent complete\n",
            "01:17:18.26 57.85 percent complete\n",
            "01:17:36.90 58.09 percent complete\n",
            "01:17:56.92 58.33 percent complete\n",
            "01:18:15.99 58.57 percent complete\n",
            "01:18:35.55 58.81 percent complete\n",
            "01:18:55.16 59.04 percent complete\n",
            "01:19:14.43 59.28 percent complete\n",
            "01:19:33.41 59.52 percent complete\n",
            "01:19:52.49 59.76 percent complete\n",
            "01:20:11.67 60.00 percent complete\n",
            "01:20:31.49 60.24 percent complete\n",
            "01:20:49.98 60.48 percent complete\n",
            "01:21:09.51 60.72 percent complete\n",
            "01:21:28.79 60.96 percent complete\n",
            "01:21:47.80 61.20 percent complete\n",
            "01:22:07.35 61.43 percent complete\n",
            "01:22:25.53 61.67 percent complete\n",
            "01:22:44.93 61.91 percent complete\n",
            "01:23:05.21 62.15 percent complete\n",
            "01:23:24.77 62.39 percent complete\n",
            "01:23:43.93 62.63 percent complete\n",
            "01:24:02.98 62.87 percent complete\n",
            "01:24:21.94 63.11 percent complete\n",
            "01:24:41.24 63.35 percent complete\n",
            "01:25:00.98 63.59 percent complete\n",
            "01:25:20.16 63.83 percent complete\n",
            "01:25:40.41 64.06 percent complete\n",
            "01:25:59.73 64.30 percent complete\n",
            "01:26:18.65 64.54 percent complete\n",
            "01:26:37.70 64.78 percent complete\n",
            "01:26:57.33 65.02 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '↓ ↓ ↓ ↓']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:27:16.25 65.26 percent complete\n",
            "01:27:35.07 65.50 percent complete\n",
            "01:27:53.82 65.74 percent complete\n",
            "01:28:14.82 65.98 percent complete\n",
            "01:28:33.90 66.22 percent complete\n",
            "01:28:53.09 66.45 percent complete\n",
            "01:29:12.53 66.69 percent complete\n",
            "01:29:31.70 66.93 percent complete\n",
            "01:29:50.70 67.17 percent complete\n",
            "01:30:09.57 67.41 percent complete\n",
            "01:30:28.05 67.65 percent complete\n",
            "01:30:47.67 67.89 percent complete\n",
            "01:31:06.40 68.13 percent complete\n",
            "01:31:25.13 68.37 percent complete\n",
            "01:31:43.73 68.61 percent complete\n",
            "01:32:02.41 68.85 percent complete\n",
            "01:32:20.92 69.08 percent complete\n",
            "01:32:39.97 69.32 percent complete\n",
            "01:32:58.86 69.56 percent complete\n",
            "01:33:16.89 69.80 percent complete\n",
            "01:33:36.22 70.04 percent complete\n",
            "01:33:55.05 70.28 percent complete\n",
            "01:34:14.51 70.52 percent complete\n",
            "01:34:33.52 70.76 percent complete\n",
            "01:34:52.90 71.00 percent complete\n",
            "01:35:11.97 71.24 percent complete\n",
            "01:35:31.07 71.47 percent complete\n",
            "01:35:49.71 71.71 percent complete\n",
            "01:36:09.44 71.95 percent complete\n",
            "01:36:28.17 72.19 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '\\']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:36:47.44 72.43 percent complete\n",
            "01:37:06.27 72.67 percent complete\n",
            "01:37:25.66 72.91 percent complete\n",
            "01:37:45.56 73.15 percent complete\n",
            "01:38:05.01 73.39 percent complete\n",
            "01:38:24.37 73.63 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '●']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:38:45.38 73.86 percent complete\n",
            "01:39:04.99 74.10 percent complete\n",
            "01:39:23.72 74.34 percent complete\n",
            "01:39:42.85 74.58 percent complete\n",
            "01:40:02.80 74.82 percent complete\n",
            "01:40:21.95 75.06 percent complete\n",
            "01:40:41.18 75.30 percent complete\n",
            "01:41:00.42 75.54 percent complete\n",
            "01:41:20.16 75.78 percent complete\n",
            "01:41:39.20 76.02 percent complete\n",
            "01:41:57.89 76.26 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '․ ․ ․ ․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:42:16.49 76.49 percent complete\n",
            "01:42:35.86 76.73 percent complete\n",
            "01:42:55.06 76.97 percent complete\n",
            "01:43:13.79 77.21 percent complete\n",
            "01:43:33.18 77.45 percent complete\n",
            "01:43:53.04 77.69 percent complete\n",
            "01:44:11.99 77.93 percent complete\n",
            "01:44:31.28 78.17 percent complete\n",
            "01:44:50.42 78.41 percent complete\n",
            "01:45:09.68 78.65 percent complete\n",
            "01:45:28.95 78.88 percent complete\n",
            "01:45:47.14 79.12 percent complete\n",
            "01:46:06.22 79.36 percent complete\n",
            "01:46:26.52 79.60 percent complete\n",
            "01:46:45.23 79.84 percent complete\n",
            "01:47:03.71 80.08 percent complete\n",
            "01:47:22.21 80.32 percent complete\n",
            "01:47:41.35 80.56 percent complete\n",
            "01:48:00.15 80.80 percent complete\n",
            "01:48:18.93 81.04 percent complete\n",
            "01:48:37.48 81.28 percent complete\n",
            "01:48:57.24 81.51 percent complete\n",
            "01:49:16.20 81.75 percent complete\n",
            "01:49:35.25 81.99 percent complete\n",
            "01:49:54.10 82.23 percent complete\n",
            "01:50:12.83 82.47 percent complete\n",
            "01:50:31.86 82.71 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '□ ․ ․ ․ ․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:50:51.58 82.95 percent complete\n",
            "01:51:10.16 83.19 percent complete\n",
            "01:51:30.08 83.43 percent complete\n",
            "01:51:49.02 83.67 percent complete\n",
            "01:52:08.52 83.90 percent complete\n",
            "01:52:27.83 84.14 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '— ― ― ― ― ― ― ―']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:52:46.89 84.38 percent complete\n",
            "01:53:05.78 84.62 percent complete\n",
            "01:53:24.71 84.86 percent complete\n",
            "01:53:43.96 85.10 percent complete\n",
            "01:54:04.22 85.34 percent complete\n",
            "01:54:23.32 85.58 percent complete\n",
            "01:54:43.29 85.82 percent complete\n",
            "01:55:02.79 86.06 percent complete\n",
            "01:55:22.21 86.30 percent complete\n",
            "01:55:41.33 86.53 percent complete\n",
            "01:56:00.23 86.77 percent complete\n",
            "01:56:19.12 87.01 percent complete\n",
            "01:56:38.77 87.25 percent complete\n",
            "01:56:57.61 87.49 percent complete\n",
            "01:57:15.84 87.73 percent complete\n",
            "01:57:34.60 87.97 percent complete\n",
            "01:57:53.46 88.21 percent complete\n",
            "01:58:11.84 88.45 percent complete\n",
            "01:58:30.89 88.69 percent complete\n",
            "01:58:49.28 88.92 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '⇧']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:59:08.05 89.16 percent complete\n",
            "01:59:27.61 89.40 percent complete\n",
            "01:59:46.54 89.64 percent complete\n",
            "02:00:05.68 89.88 percent complete\n",
            "02:00:24.65 90.12 percent complete\n",
            "02:00:43.91 90.36 percent complete\n",
            "02:01:02.03 90.60 percent complete\n",
            "02:01:20.91 90.84 percent complete\n",
            "02:01:40.07 91.08 percent complete\n",
            "02:01:59.19 91.32 percent complete\n",
            "02:02:18.00 91.55 percent complete\n",
            "02:02:36.68 91.79 percent complete\n",
            "02:02:55.43 92.03 percent complete\n",
            "02:03:14.24 92.27 percent complete\n",
            "02:03:33.75 92.51 percent complete\n",
            "02:03:52.57 92.75 percent complete\n",
            "02:04:11.93 92.99 percent complete\n",
            "02:04:31.85 93.23 percent complete\n",
            "02:04:51.23 93.47 percent complete\n",
            "02:05:10.88 93.71 percent complete\n",
            "02:05:29.89 93.94 percent complete\n",
            "02:05:48.95 94.18 percent complete\n",
            "02:06:07.35 94.42 percent complete\n",
            "02:06:26.48 94.66 percent complete\n",
            "02:06:45.32 94.90 percent complete\n",
            "02:07:05.28 95.14 percent complete\n",
            "02:07:23.59 95.38 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '․ ․ ․ ․ ․ ․ ․ ․ ․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "02:07:42.84 95.62 percent complete\n",
            "02:08:01.06 95.86 percent complete\n",
            "02:08:19.53 96.10 percent complete\n",
            "02:08:38.57 96.34 percent complete\n",
            "02:08:57.26 96.57 percent complete\n",
            "02:09:15.59 96.81 percent complete\n",
            "02:09:35.51 97.05 percent complete\n",
            "02:09:54.22 97.29 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "02:10:13.11 97.53 percent complete\n",
            "02:10:32.11 97.77 percent complete\n",
            "02:10:50.97 98.01 percent complete\n",
            "02:11:09.38 98.25 percent complete\n",
            "02:11:28.00 98.49 percent complete\n",
            "02:11:46.41 98.73 percent complete\n",
            "02:12:06.18 98.96 percent complete\n",
            "02:12:24.79 99.20 percent complete\n",
            "02:12:43.89 99.44 percent complete\n",
            "02:13:02.72 99.68 percent complete\n",
            "02:13:21.36 99.92 percent complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hxxBOCA-xXhy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "cc6cc5c7-3b9f-46c1-df7d-baa8b256ba81"
      },
      "source": [
        "# This section does the split between train/dev for the parallel corpora then saves them as separate files\n",
        "# We use 1000 dev test and the given test set.\n",
        "import csv\n",
        "\n",
        "# Do the split between dev/train and create parallel corpora\n",
        "num_dev_patterns = 1000\n",
        "\n",
        "# Optional: lower case the corpora - this will make it easier to generalize, but without proper casing.\n",
        "if lc:  # Julia: making lowercasing optional\n",
        "    df_pp[\"source_sentence\"] = df_pp[\"source_sentence\"].str.lower()\n",
        "    df_pp[\"target_sentence\"] = df_pp[\"target_sentence\"].str.lower()\n",
        "\n",
        "# Julia: test sets are already generated\n",
        "dev = df_pp.tail(num_dev_patterns) # Herman: Error in original\n",
        "stripped = df_pp.drop(df_pp.tail(num_dev_patterns).index)\n",
        "\n",
        "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in stripped.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "    \n",
        "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in dev.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "\n",
        "#stripped[[\"source_sentence\"]].to_csv(\"train.\"+source_language, header=False, index=False)  # Herman: Added `header=False` everywhere\n",
        "#stripped[[\"target_sentence\"]].to_csv(\"train.\"+target_language, header=False, index=False)  # Julia: Problematic handling of quotation marks.\n",
        "\n",
        "#dev[[\"source_sentence\"]].to_csv(\"dev.\"+source_language, header=False, index=False)\n",
        "#dev[[\"target_sentence\"]].to_csv(\"dev.\"+target_language, header=False, index=False)\n",
        "\n",
        "# Doublecheck the format below. There should be no extra quotation marks or weird characters.\n",
        "! head train.*\n",
        "! head dev.*"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> train.en <==\n",
            "TRAIN YOUR CHILDREN : “ I teach my children to check the expiration date of any packaged food items , such as snacks , before they buy them . ” ​ — Ruth , Nigeria\n",
            "When she replied that she was , he explained that he and his mother were trying to assist his sister with a school report on Canadians .\n",
            "Through the prophet Zephaniah , Jehovah answers : “ That day is a day of fury , a day of distress and of anguish , a day of storm and of desolation , a day of darkness and of gloominess , a day of clouds and of thick gloom . ”\n",
            "We do not require that people simply do as we tell them , but we give them convincing reasons to obey Christ’s command .\n",
            "Still , Jehovah can annihilate any rebel in the lake of fire , denying him any hope of a resurrection .\n",
            "Lucaris was arrested , and on July 27 , 1638 , he was taken on board a small boat as if for banishment .\n",
            "Yes , Jehovah remembered their faithful course .\n",
            "□ To appear tough\n",
            "During the ensuing confrontation , we Witnesses had to make our position clear to the agitated rebels and also explain our stand to the military guards .\n",
            "( See opening image . ) ( c ) Why should this Bible account about Samuel be of special interest to elders today ?\n",
            "\n",
            "==> train.yo <==\n",
            "KỌ́ ÀWỌN ỌMỌ RẸ : “ Mo kọ́ àwọn ọmọ mi pé kí wọ́n tó ra oúnjẹ bí ìpápánu , tó wà nínú agolo , ike , bébà , tàbí ọ̀rá , kí wọ́n máa yẹ ara oúnjẹ náà wò kí wọ́n lè mọ déètì tó máa bà jẹ́ . ” — Ruth , Nàìjíríà\n",
            "Nígbà tó sọ fún ọ̀dọ́kùnrin náà pé Kánádà lòun ti wá , ọ̀dọ́kùnrin náà sọ fún un pé òun àti màmá òun fẹ́ ran àbúrò òun obìnrin kan lọ́wọ́ láti kó ọ̀rọ̀ kan jọ nípa àwọn ará Kánádà , èyí tó fẹ́ mu lọ sílé ìwé .\n",
            "Jèhófà gbẹnu wòlíì Sefanáyà sọ ìdí rẹ̀ fún wa , ó ní : “ Ọjọ́ yẹn jẹ́ ọjọ́ ìbínú kíkan , ọjọ́ wàhálà àti làásìgbò , ọjọ́ ìjì àti ìsọdahoro , ọjọ́ òkùnkùn àti ìṣúdùdù , ọjọ́ àwọsánmà àti ìṣúdùdù tí ó nípọn . ”\n",
            "A ò fẹ́ káwọn èèyàn wulẹ̀ ṣe ohun tá a sọ fún wọn nìkan , àmọ́ à tún ń fún wọn ní ẹ̀rí tó dájú nípa ìdí tó fi yẹ kí wọ́n ṣègbọràn sí àṣẹ Kristi .\n",
            "Síbẹ̀ , Jèhófà lè pa ọlọ̀tẹ̀ èyíkéyìí run nípa sísọ ọ sínú adágún iná , tó túmọ̀ sí pé onítọ̀hún máa kú láìsí ìrètí kankan láti tún jíǹde .\n",
            "Ní wọ́n bá fi ọlọ́pàá mú Lucaris , nígbà tó sì di July 27 , 1638 , wọ́n fi ọkọ̀ ojú omi wà á lọ bí ẹni pé wọ́n fẹ́ gbé e lọ sí ilẹ̀ mìíràn .\n",
            "Bẹ́ẹ̀ ni o , Jèhófà kò gbàgbé ìṣòtítọ́ wọn .\n",
            "□ Kí wọ́n má bàa fojú ọ̀dẹ̀ wò mí\n",
            "Lákòókò tí àríyànjiyàn náà ń lọ lọ́wọ́ , àwa Ẹlẹ́rìí jẹ́ káwọn ọlọ̀tẹ̀ tínú ń bí yìí mọ̀ pé a ò lọ́wọ́ sí nǹkan tí wọ́n ń ṣe , a sì tún ṣàlàyé irú ẹni tá a jẹ́ fáwọn ológun tó ń ṣọ́ ọgbà ẹ̀wọ̀n náà .\n",
            "( Wo àwòrán tó wà níbẹ̀rẹ̀ àpilẹ̀kọ yìí . ) ( d ) Kí nìdí tó fi yẹ kí àwọn alàgbà lóde òní fún àkọsílẹ̀ Bíbélì yìí nípa Sámúẹ́lì láfiyèsí àrà ọ̀tọ̀ ?\n",
            "==> dev.en <==\n",
            "He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "Now I had to find a legitimate line of work .\n",
            "Do I value material things more than my relationship with Jehovah and with people ?\n",
            "He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "According to Harkavy’s Students ’ Hebrew and Chaldee Dictionary , ʽadh means “ duration , everlastingness , eternity , for ever . ”\n",
            "Why is rendering proper honor to elders a concern ?\n",
            "Jeremiah would rather be alone than be corrupted by bad companions .\n",
            "In years gone by , we believed that Jehovah became displeased with his people because they did not have a zealous share in the preaching work during World War I .\n",
            "Rather , they need to use a translation of the Bible in their own language .\n",
            "On a more personal level , showing honor to those to whom it is due keeps us from becoming self - centered .\n",
            "\n",
            "==> dev.yo <==\n",
            "Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "Gẹ́gẹ́ bí Harkavy’s Students ’ Hebrew and Chaldee Dictionary ṣe sọ , ʽadh túmọ̀ sí “ àkókò gígùn , àìnípẹ̀kun , títí gbére , títí láé . ”\n",
            "Kí nìdí tí kò fi yẹ ká máa gbé àwọn alàgbà gẹ̀gẹ̀ ju bó ṣe yẹ lọ ?\n",
            "Jeremáyà gbà kóun dá wà ju pé káwọn ọ̀rẹ́ burúkú wá kéèràn ran òun .\n",
            "Láwọn ọdún mélòó kan sẹ́yìn , a gbà pé inú Jèhófà ò dùn sáwọn èèyàn rẹ̀ torí pé wọn ò fìtara wàásù lásìkò Ogun Àgbáyé Kìíní .\n",
            "Àfi kí wọ́n ka Bíbélì tí wọ́n tú sí èdè wọn .\n",
            "Tó bá ti mọ́ wa lára láti máa bọlá fáwọn míì , a ò ní máa ro tara wa nìkan .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epeCydmCyS8X"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Installation of JoeyNMT\n",
        "\n",
        "JoeyNMT is a simple, minimalist NMT package which is useful for learning and teaching. Check out the documentation for JoeyNMT [here](https://joeynmt.readthedocs.io)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iBRMm4kMxZ8L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b18457c1-1b14-4dec-edd0-b9730323309c"
      },
      "source": [
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'joeynmt'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 2204 (delta 8), reused 5 (delta 3), pack-reused 2184\u001b[K\n",
            "Receiving objects: 100% (2204/2204), 2.60 MiB | 4.31 MiB/s, done.\n",
            "Resolving deltas: 100% (1529/1529), done.\n",
            "Processing /content/joeynmt\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (6.2.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.17.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (42.0.2)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.3.1)\n",
            "Requirement already satisfied: tensorflow>=1.14 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.3.1)\n",
            "Collecting sacrebleu>=1.3.6\n",
            "  Downloading https://files.pythonhosted.org/packages/45/31/1a135b964c169984b27fb2f7a50280fa7f8e6d9d404d8a9e596180487fd1/sacrebleu-1.4.3-py3-none-any.whl\n",
            "Collecting subword-nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (3.1.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.9.0)\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/d9/ea9816aea31beeadccd03f1f8b625ecf8f645bd66744484d162d84803ce5/PyYAML-5.3.tar.gz (268kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 24.3MB/s \n",
            "\u001b[?25hCollecting pylint\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/59/43fc36c5ee316bb9aeb7cf5329cdbdca89e5749c34d5602753827c0aa2dc/pylint-2.4.4-py3-none-any.whl (302kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 28.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six==1.12 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.1.8)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.10.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.33.6)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (4.28.1)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.3.6->joeynmt==0.0.1) (3.6.6)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/91/db/7bc703c0760df726839e0699b7f78a4d8217fdc9c7fcb1b51b39c5a22a4e/portalocker-1.5.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.6.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt==0.0.1) (0.25.3)\n",
            "Collecting astroid<2.4,>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/ae/86734823047962e7b8c8529186a1ac4a7ca19aaf1aa0c7713c022ef593fd/astroid-2.3.3-py3-none-any.whl (205kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 22.3MB/s \n",
            "\u001b[?25hCollecting isort<5,>=4.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/b0/c121fd1fa3419ea9bfd55c7f9c4fedfec5143208d8c7ad3ce3db6c623c21/isort-4.3.21-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.8MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7,>=0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14->joeynmt==0.0.1) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14->joeynmt==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow>=1.14->joeynmt==0.0.1) (2.8.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn->joeynmt==0.0.1) (2018.9)\n",
            "Collecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/ed/5459080d95eb87a02fe860d447197be63b6e2b5e9ff73c2b0a85622994f4/typed_ast-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (737kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 38.7MB/s \n",
            "\u001b[?25hCollecting lazy-object-proxy==1.4.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/dd/b1e3407e9e6913cf178e506cd0dee818e58694d9a5cd1984e3f6a8b9a10f/lazy_object_proxy-1.4.3-cp36-cp36m-manylinux1_x86_64.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: joeynmt, pyyaml\n",
            "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for joeynmt: filename=joeynmt-0.0.1-cp36-none-any.whl size=72136 sha256=236b35fbed374cc299b613e59279e93eefdca931f670a7a4077444f9f1ef1eb9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wgoxy3ml/wheels/db/01/db/751cc9f3e7f6faec127c43644ba250a3ea7ad200594aeda70a\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3-cp36-cp36m-linux_x86_64.whl size=44229 sha256=7261b076f714b196d51dad1e97ff1cfe2aa11d957c72c347a5e662d9ae012e82\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/76/4d/a95b8dd7b452b69e8ed4f68b69e1b55e12c9c9624dd962b191\n",
            "Successfully built joeynmt pyyaml\n",
            "Installing collected packages: portalocker, sacrebleu, subword-nmt, pyyaml, typed-ast, lazy-object-proxy, astroid, isort, mccabe, pylint, joeynmt\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed astroid-2.3.3 isort-4.3.21 joeynmt-0.0.1 lazy-object-proxy-1.4.3 mccabe-0.6.1 portalocker-1.5.2 pylint-2.4.4 pyyaml-5.3 sacrebleu-1.4.3 subword-nmt-0.3.7 typed-ast-1.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AaE77Tcppex9"
      },
      "source": [
        "# Preprocessing the Data into Subword BPE Tokens\n",
        "\n",
        "- One of the most powerful improvements for agglutinative languages (a feature of most Bantu languages) is using BPE tokenization [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n",
        "\n",
        "- It was also shown that by optimizing the number of BPE codes we significantly improve results for low-resourced languages [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021) [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)\n",
        "\n",
        "- Below we have the scripts for doing BPE tokenization of our data. We use 4000 tokens as recommended by [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021). You do not need to change anything. Simply running the below will be suitable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H-TyjtmXB1mL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "9f22832c-da2e-4cce-ae07-a60c59d50ebb"
      },
      "source": [
        "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
        "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
        "\n",
        "# Do subword NMT\n",
        "from os import path\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "\n",
        "# Learn BPEs on the training data.\n",
        "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "# Apply BPE splits to the development and test data.\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
        "\n",
        "# Create directory, move everyone we care about to the correct location\n",
        "! mkdir -p $data_path\n",
        "! cp train.* $data_path\n",
        "! cp test.* $data_path\n",
        "! cp dev.* $data_path\n",
        "! cp bpe.codes.4000 $data_path\n",
        "! ls $data_path\n",
        "\n",
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\"\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# Some output\n",
        "! echo \"BPE Xhosa Sentences\"\n",
        "! tail -n 5 test.bpe.$tgt\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! tail -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.en\t     test.bpe.yo     test.yo\t   train.en\n",
            "dev.bpe.en\tdev.yo\t     test.en\t     train.bpe.en  train.yo\n",
            "dev.bpe.yo\ttest.bpe.en  test.en-any.en  train.bpe.yo\n",
            "bpe.codes.4000\tdev.en\t     test.bpe.yo     test.yo\t   train.en\n",
            "dev.bpe.en\tdev.yo\t     test.en\t     train.bpe.en  train.yo\n",
            "dev.bpe.yo\ttest.bpe.en  test.en-any.en  train.bpe.yo\n",
            "BPE Xhosa Sentences\n",
            "A@@ p@@ at@@ a ńlá ti ìgbàgbọ́ ( Wo ìpín@@ rọ̀ 12 - 14 )\n",
            "À@@ ṣí@@ borí ìgb@@ àlà ( Wo ìpín@@ rọ̀ 15 - 18 )\n",
            "Mo ti rí i pé àwọn èèyàn máa ń fẹ́ gb@@ ọ́@@ rọ̀ wa tí wọ́n bá rí i pé a lóye Bíbélì dáadáa , a sì fẹ́ ran àwọn lọ́wọ́ . ”\n",
            "I@@ dà ẹ̀mí ( Wo ìpín@@ rọ̀ 19 - 20 )\n",
            "Ó dájú pé lọ́@@ lá ìt@@ ì@@ lẹ́yìn Jèhófà , a máa dúró gb@@ ọ@@ in - in , Èṣù ò sì ní rí wa gbé ṣe .\n",
            "Combined BPE Vocab\n",
            "œ@@\n",
            "Ísír@@\n",
            "Isra@@\n",
            "̃\n",
            "×\n",
            "ô\n",
            "ʺ\n",
            "bítì\n",
            "Pété@@\n",
            "Jóò@@\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IlMitUHR8Qy-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "6598c5de-3279-4386-903a-bbf542fcd7d6"
      },
      "source": [
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.en\t     test.bpe.yo     test.yo\t   train.en\n",
            "dev.bpe.en\tdev.yo\t     test.en\t     train.bpe.en  train.yo\n",
            "dev.bpe.yo\ttest.bpe.en  test.en-any.en  train.bpe.yo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ixmzi60WsUZ8"
      },
      "source": [
        "# Creating the JoeyNMT Config\n",
        "\n",
        "JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n",
        "\n",
        "- We used Transformer architecture \n",
        "- We set our dropout to reasonably high: 0.3 (recommended in  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
        "\n",
        "Things worth playing with:\n",
        "- The batch size (also recommended to change for low-resourced languages)\n",
        "- The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n",
        "- The decoder options (beam_size, alpha)\n",
        "- Evaluation metrics (BLEU versus Crhf4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PIs1lY2hxMsl",
        "colab": {}
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/train.bpe\"\n",
        "    dev:   \"data/{name}/dev.bpe\"\n",
        "    test:  \"data/{name}/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: False               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pIifxE3Qzuvs"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "This single line of joeynmt runs the training using the config we made above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6ZBPFwT94WpI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58d9ae09-0a37-402e-a8d1-7a9c42b3010a"
      },
      "source": [
        "# Train the model\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-17 20:02:04,366 Hello! This is Joey-NMT.\n",
            "2020-01-17 20:02:05,435 Total params: 12188160\n",
            "2020-01-17 20:02:05,436 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']\n",
            "2020-01-17 20:02:15,468 cfg.name                           : enyo_transformer\n",
            "2020-01-17 20:02:15,468 cfg.data.src                       : en\n",
            "2020-01-17 20:02:15,468 cfg.data.trg                       : yo\n",
            "2020-01-17 20:02:15,468 cfg.data.train                     : data/enyo/train.bpe\n",
            "2020-01-17 20:02:15,469 cfg.data.dev                       : data/enyo/dev.bpe\n",
            "2020-01-17 20:02:15,469 cfg.data.test                      : data/enyo/test.bpe\n",
            "2020-01-17 20:02:15,469 cfg.data.level                     : bpe\n",
            "2020-01-17 20:02:15,469 cfg.data.lowercase                 : False\n",
            "2020-01-17 20:02:15,469 cfg.data.max_sent_length           : 100\n",
            "2020-01-17 20:02:15,469 cfg.data.src_vocab                 : data/enyo/vocab.txt\n",
            "2020-01-17 20:02:15,469 cfg.data.trg_vocab                 : data/enyo/vocab.txt\n",
            "2020-01-17 20:02:15,469 cfg.testing.beam_size              : 5\n",
            "2020-01-17 20:02:15,469 cfg.testing.alpha                  : 1.0\n",
            "2020-01-17 20:02:15,469 cfg.training.random_seed           : 42\n",
            "2020-01-17 20:02:15,469 cfg.training.optimizer             : adam\n",
            "2020-01-17 20:02:15,469 cfg.training.normalization         : tokens\n",
            "2020-01-17 20:02:15,469 cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2020-01-17 20:02:15,469 cfg.training.scheduling            : plateau\n",
            "2020-01-17 20:02:15,469 cfg.training.patience              : 5\n",
            "2020-01-17 20:02:15,469 cfg.training.learning_rate_factor  : 0.5\n",
            "2020-01-17 20:02:15,469 cfg.training.learning_rate_warmup  : 1000\n",
            "2020-01-17 20:02:15,469 cfg.training.decrease_factor       : 0.7\n",
            "2020-01-17 20:02:15,470 cfg.training.loss                  : crossentropy\n",
            "2020-01-17 20:02:15,470 cfg.training.learning_rate         : 0.0003\n",
            "2020-01-17 20:02:15,470 cfg.training.learning_rate_min     : 1e-08\n",
            "2020-01-17 20:02:15,470 cfg.training.weight_decay          : 0.0\n",
            "2020-01-17 20:02:15,470 cfg.training.label_smoothing       : 0.1\n",
            "2020-01-17 20:02:15,470 cfg.training.batch_size            : 4096\n",
            "2020-01-17 20:02:15,470 cfg.training.batch_type            : token\n",
            "2020-01-17 20:02:15,470 cfg.training.eval_batch_size       : 3600\n",
            "2020-01-17 20:02:15,470 cfg.training.eval_batch_type       : token\n",
            "2020-01-17 20:02:15,470 cfg.training.batch_multiplier      : 1\n",
            "2020-01-17 20:02:15,470 cfg.training.early_stopping_metric : ppl\n",
            "2020-01-17 20:02:15,470 cfg.training.epochs                : 30\n",
            "2020-01-17 20:02:15,470 cfg.training.validation_freq       : 1000\n",
            "2020-01-17 20:02:15,470 cfg.training.logging_freq          : 100\n",
            "2020-01-17 20:02:15,470 cfg.training.eval_metric           : bleu\n",
            "2020-01-17 20:02:15,470 cfg.training.model_dir             : models/enyo_transformer\n",
            "2020-01-17 20:02:15,470 cfg.training.overwrite             : False\n",
            "2020-01-17 20:02:15,470 cfg.training.shuffle               : True\n",
            "2020-01-17 20:02:15,470 cfg.training.use_cuda              : True\n",
            "2020-01-17 20:02:15,470 cfg.training.max_output_length     : 100\n",
            "2020-01-17 20:02:15,471 cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2020-01-17 20:02:15,471 cfg.training.keep_last_ckpts       : 3\n",
            "2020-01-17 20:02:15,471 cfg.model.initializer              : xavier\n",
            "2020-01-17 20:02:15,471 cfg.model.bias_initializer         : zeros\n",
            "2020-01-17 20:02:15,471 cfg.model.init_gain                : 1.0\n",
            "2020-01-17 20:02:15,471 cfg.model.embed_initializer        : xavier\n",
            "2020-01-17 20:02:15,471 cfg.model.embed_init_gain          : 1.0\n",
            "2020-01-17 20:02:15,471 cfg.model.tied_embeddings          : True\n",
            "2020-01-17 20:02:15,471 cfg.model.tied_softmax             : True\n",
            "2020-01-17 20:02:15,471 cfg.model.encoder.type             : transformer\n",
            "2020-01-17 20:02:15,471 cfg.model.encoder.num_layers       : 6\n",
            "2020-01-17 20:02:15,471 cfg.model.encoder.num_heads        : 4\n",
            "2020-01-17 20:02:15,471 cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2020-01-17 20:02:15,471 cfg.model.encoder.embeddings.scale : True\n",
            "2020-01-17 20:02:15,471 cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2020-01-17 20:02:15,471 cfg.model.encoder.hidden_size      : 256\n",
            "2020-01-17 20:02:15,471 cfg.model.encoder.ff_size          : 1024\n",
            "2020-01-17 20:02:15,471 cfg.model.encoder.dropout          : 0.3\n",
            "2020-01-17 20:02:15,471 cfg.model.decoder.type             : transformer\n",
            "2020-01-17 20:02:15,472 cfg.model.decoder.num_layers       : 6\n",
            "2020-01-17 20:02:15,472 cfg.model.decoder.num_heads        : 4\n",
            "2020-01-17 20:02:15,472 cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2020-01-17 20:02:15,472 cfg.model.decoder.embeddings.scale : True\n",
            "2020-01-17 20:02:15,472 cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2020-01-17 20:02:15,472 cfg.model.decoder.hidden_size      : 256\n",
            "2020-01-17 20:02:15,472 cfg.model.decoder.ff_size          : 1024\n",
            "2020-01-17 20:02:15,472 cfg.model.decoder.dropout          : 0.3\n",
            "2020-01-17 20:02:15,472 Data set sizes: \n",
            "\ttrain 415100,\n",
            "\tvalid 1000,\n",
            "\ttest 2662\n",
            "2020-01-17 20:02:15,472 First training example:\n",
            "\t[SRC] T@@ R@@ A@@ IN Y@@ O@@ U@@ R C@@ H@@ I@@ L@@ D@@ R@@ E@@ N : “ I teach my children to ch@@ ec@@ k the exp@@ ir@@ ation d@@ ate of any p@@ ack@@ aged food it@@ em@@ s , such as s@@ n@@ ac@@ ks , before they bu@@ y them . ” ​ — Ru@@ th , N@@ ig@@ er@@ ia\n",
            "\t[TRG] K@@ Ọ́ ÀWỌN Ọ@@ M@@ Ọ R@@ Ẹ : “ Mo kọ́ àwọn ọmọ mi pé kí wọ́n tó ra oúnjẹ bí ìp@@ á@@ p@@ án@@ u , tó wà nínú ag@@ ol@@ o , i@@ ke , bé@@ bà , tàbí ọ̀r@@ á , kí wọ́n máa yẹ ara oúnjẹ náà wò kí wọ́n lè mọ dé@@ è@@ tì tó máa bà jẹ́ . ” — Ru@@ th , N@@ àì@@ jí@@ ríà\n",
            "2020-01-17 20:02:15,472 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) tó (8) a (9) to\n",
            "2020-01-17 20:02:15,473 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) tó (8) a (9) to\n",
            "2020-01-17 20:02:15,473 Number of Src words (types): 4406\n",
            "2020-01-17 20:02:15,473 Number of Trg words (types): 4406\n",
            "2020-01-17 20:02:15,473 Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4406),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4406))\n",
            "2020-01-17 20:02:15,477 EPOCH 1\n",
            "2020-01-17 20:02:30,608 Epoch   1 Step:      100 Batch Loss:     5.651617 Tokens per Sec:    15306, Lr: 0.000300\n",
            "2020-01-17 20:02:45,133 Epoch   1 Step:      200 Batch Loss:     5.154922 Tokens per Sec:    15676, Lr: 0.000300\n",
            "2020-01-17 20:02:59,653 Epoch   1 Step:      300 Batch Loss:     5.083961 Tokens per Sec:    15919, Lr: 0.000300\n",
            "2020-01-17 20:03:14,382 Epoch   1 Step:      400 Batch Loss:     4.941563 Tokens per Sec:    15949, Lr: 0.000300\n",
            "2020-01-17 20:03:29,304 Epoch   1 Step:      500 Batch Loss:     4.724124 Tokens per Sec:    15592, Lr: 0.000300\n",
            "2020-01-17 20:03:44,587 Epoch   1 Step:      600 Batch Loss:     4.386890 Tokens per Sec:    15194, Lr: 0.000300\n",
            "2020-01-17 20:03:59,897 Epoch   1 Step:      700 Batch Loss:     4.195787 Tokens per Sec:    15138, Lr: 0.000300\n",
            "2020-01-17 20:04:15,455 Epoch   1 Step:      800 Batch Loss:     4.005951 Tokens per Sec:    14904, Lr: 0.000300\n",
            "2020-01-17 20:04:30,886 Epoch   1 Step:      900 Batch Loss:     3.900524 Tokens per Sec:    14807, Lr: 0.000300\n",
            "2020-01-17 20:04:46,253 Epoch   1 Step:     1000 Batch Loss:     3.833878 Tokens per Sec:    15052, Lr: 0.000300\n",
            "2020-01-17 20:05:34,114 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 20:05:34,114 Saving new checkpoint.\n",
            "2020-01-17 20:05:34,371 Example #0\n",
            "2020-01-17 20:05:34,371 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 20:05:34,371 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 20:05:34,371 \tHypothesis: Àwọn tó ń jẹ́ pé àwọn èèyàn ni wọ́n ń ṣe àwọn èèyàn tó ń ṣe àwọn èèyàn .\n",
            "2020-01-17 20:05:34,371 Example #1\n",
            "2020-01-17 20:05:34,371 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 20:05:34,371 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 20:05:34,372 \tHypothesis: Ó jẹ́ pé àwọn èèyàn ni wọ́n ń ṣe àwọn èèyàn .\n",
            "2020-01-17 20:05:34,372 Example #2\n",
            "2020-01-17 20:05:34,372 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 20:05:34,372 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 20:05:34,372 \tHypothesis: Kí ni Jésù ṣe ń ṣe láti ṣe ohun tó ń ṣe láti ṣe ohun tó wà nínú Bíbélì ?\n",
            "2020-01-17 20:05:34,372 Example #3\n",
            "2020-01-17 20:05:34,372 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 20:05:34,372 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 20:05:34,372 \tHypothesis: Àmọ́ , ó sì máa ń ṣe ohun tó ń ṣe láti ṣe ohun tó ń ṣe .\n",
            "2020-01-17 20:05:34,372 Validation result (greedy) at epoch   1, step     1000: bleu:   1.80, loss: 117992.4297, ppl:  52.4857, duration: 48.1193s\n",
            "2020-01-17 20:05:50,057 Epoch   1 Step:     1100 Batch Loss:     4.160717 Tokens per Sec:    15008, Lr: 0.000300\n",
            "2020-01-17 20:06:05,300 Epoch   1 Step:     1200 Batch Loss:     3.995522 Tokens per Sec:    14797, Lr: 0.000300\n",
            "2020-01-17 20:06:20,827 Epoch   1 Step:     1300 Batch Loss:     3.775538 Tokens per Sec:    15129, Lr: 0.000300\n",
            "2020-01-17 20:06:36,204 Epoch   1 Step:     1400 Batch Loss:     3.710065 Tokens per Sec:    14990, Lr: 0.000300\n",
            "2020-01-17 20:06:51,788 Epoch   1 Step:     1500 Batch Loss:     3.837762 Tokens per Sec:    15106, Lr: 0.000300\n",
            "2020-01-17 20:07:06,916 Epoch   1 Step:     1600 Batch Loss:     3.750664 Tokens per Sec:    14774, Lr: 0.000300\n",
            "2020-01-17 20:07:22,569 Epoch   1 Step:     1700 Batch Loss:     3.606179 Tokens per Sec:    15252, Lr: 0.000300\n",
            "2020-01-17 20:07:38,005 Epoch   1 Step:     1800 Batch Loss:     3.655918 Tokens per Sec:    14835, Lr: 0.000300\n",
            "2020-01-17 20:07:53,388 Epoch   1 Step:     1900 Batch Loss:     3.706626 Tokens per Sec:    15021, Lr: 0.000300\n",
            "2020-01-17 20:08:08,754 Epoch   1 Step:     2000 Batch Loss:     3.580547 Tokens per Sec:    14806, Lr: 0.000300\n",
            "2020-01-17 20:08:56,744 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 20:08:56,745 Saving new checkpoint.\n",
            "2020-01-17 20:08:56,981 Example #0\n",
            "2020-01-17 20:08:56,981 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 20:08:56,981 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 20:08:56,981 \tHypothesis: Ó tún sọ pé àwọn èèyàn tó wà nínú rẹ̀ , ó sì máa ń ṣe àwọn èèyàn tó ń ṣe àwọn èèyàn .\n",
            "2020-01-17 20:08:56,981 Example #1\n",
            "2020-01-17 20:08:56,982 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 20:08:56,982 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 20:08:56,982 \tHypothesis: Mo tún máa ń fi àwọn ọmọ wọn sílẹ̀ .\n",
            "2020-01-17 20:08:56,982 Example #2\n",
            "2020-01-17 20:08:56,982 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 20:08:56,982 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 20:08:56,982 \tHypothesis: Ṣé mo máa ń ṣe àwọn ọmọ mi láti máa ṣe iṣẹ́ ìwàásù Jèhófà , kí wọ́n sì máa ṣe iṣẹ́ ìwàásù wa ?\n",
            "2020-01-17 20:08:56,982 Example #3\n",
            "2020-01-17 20:08:56,982 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 20:08:56,982 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 20:08:56,982 \tHypothesis: Ó dájú pé ó ṣeé ṣe kó o máa ṣe kó o sì máa bá a nìṣó , ó sì máa ń ṣe ohun tó o bá ń ṣe .\n",
            "2020-01-17 20:08:56,982 Validation result (greedy) at epoch   1, step     2000: bleu:   3.10, loss: 101322.2344, ppl:  29.9937, duration: 48.2278s\n",
            "2020-01-17 20:09:12,356 Epoch   1 Step:     2100 Batch Loss:     3.629908 Tokens per Sec:    15136, Lr: 0.000300\n",
            "2020-01-17 20:09:27,592 Epoch   1 Step:     2200 Batch Loss:     3.251465 Tokens per Sec:    14909, Lr: 0.000300\n",
            "2020-01-17 20:09:43,178 Epoch   1 Step:     2300 Batch Loss:     3.500051 Tokens per Sec:    15071, Lr: 0.000300\n",
            "2020-01-17 20:09:58,344 Epoch   1 Step:     2400 Batch Loss:     3.432413 Tokens per Sec:    15260, Lr: 0.000300\n",
            "2020-01-17 20:10:13,599 Epoch   1 Step:     2500 Batch Loss:     3.467661 Tokens per Sec:    15152, Lr: 0.000300\n",
            "2020-01-17 20:10:28,904 Epoch   1 Step:     2600 Batch Loss:     3.196687 Tokens per Sec:    14934, Lr: 0.000300\n",
            "2020-01-17 20:10:44,311 Epoch   1 Step:     2700 Batch Loss:     3.516019 Tokens per Sec:    14742, Lr: 0.000300\n",
            "2020-01-17 20:10:59,844 Epoch   1 Step:     2800 Batch Loss:     2.978670 Tokens per Sec:    15113, Lr: 0.000300\n",
            "2020-01-17 20:11:15,118 Epoch   1 Step:     2900 Batch Loss:     3.130475 Tokens per Sec:    14799, Lr: 0.000300\n",
            "2020-01-17 20:11:30,388 Epoch   1 Step:     3000 Batch Loss:     3.081049 Tokens per Sec:    15026, Lr: 0.000300\n",
            "2020-01-17 20:12:18,071 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 20:12:18,072 Saving new checkpoint.\n",
            "2020-01-17 20:12:18,315 Example #0\n",
            "2020-01-17 20:12:18,316 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 20:12:18,316 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 20:12:18,316 \tHypothesis: Ó ṣe kedere pé , ó yẹ ká máa fi ẹ̀mí mímọ́ hàn .\n",
            "2020-01-17 20:12:18,316 Example #1\n",
            "2020-01-17 20:12:18,316 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 20:12:18,316 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 20:12:18,316 \tHypothesis: Mo máa ń fi àwọn ohun tó ń ṣe .\n",
            "2020-01-17 20:12:18,316 Example #2\n",
            "2020-01-17 20:12:18,316 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 20:12:18,316 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 20:12:18,316 \tHypothesis: Ṣé mo máa ń fi àwọn èèyàn Jèhófà hàn sí i pé Jèhófà ń ṣe ?\n",
            "2020-01-17 20:12:18,316 Example #3\n",
            "2020-01-17 20:12:18,317 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 20:12:18,317 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 20:12:18,317 \tHypothesis: Ó sì tún máa ń ṣe ohun tó o bá ń ṣe , ó sì máa ń ṣe ohun tó o bá fẹ́ .\n",
            "2020-01-17 20:12:18,317 Validation result (greedy) at epoch   1, step     3000: bleu:   4.33, loss: 92681.6719, ppl:  22.4425, duration: 47.9290s\n",
            "2020-01-17 20:12:33,731 Epoch   1 Step:     3100 Batch Loss:     3.193326 Tokens per Sec:    14870, Lr: 0.000300\n",
            "2020-01-17 20:12:48,719 Epoch   1 Step:     3200 Batch Loss:     3.028764 Tokens per Sec:    14686, Lr: 0.000300\n",
            "2020-01-17 20:13:04,100 Epoch   1 Step:     3300 Batch Loss:     3.150409 Tokens per Sec:    15210, Lr: 0.000300\n",
            "2020-01-17 20:13:19,355 Epoch   1 Step:     3400 Batch Loss:     3.675951 Tokens per Sec:    15059, Lr: 0.000300\n",
            "2020-01-17 20:13:34,690 Epoch   1 Step:     3500 Batch Loss:     2.936108 Tokens per Sec:    14957, Lr: 0.000300\n",
            "2020-01-17 20:13:50,082 Epoch   1 Step:     3600 Batch Loss:     3.028289 Tokens per Sec:    15208, Lr: 0.000300\n",
            "2020-01-17 20:14:05,466 Epoch   1 Step:     3700 Batch Loss:     2.978987 Tokens per Sec:    14843, Lr: 0.000300\n",
            "2020-01-17 20:14:20,919 Epoch   1 Step:     3800 Batch Loss:     3.119967 Tokens per Sec:    15077, Lr: 0.000300\n",
            "2020-01-17 20:14:36,448 Epoch   1 Step:     3900 Batch Loss:     3.042300 Tokens per Sec:    15170, Lr: 0.000300\n",
            "2020-01-17 20:14:51,701 Epoch   1 Step:     4000 Batch Loss:     2.956565 Tokens per Sec:    14662, Lr: 0.000300\n",
            "2020-01-17 20:15:39,581 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 20:15:39,581 Saving new checkpoint.\n",
            "2020-01-17 20:15:39,842 Example #0\n",
            "2020-01-17 20:15:39,843 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 20:15:39,843 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 20:15:39,843 \tHypothesis: Ó ti ń fi hàn pé ó ń ṣe ohun tó ń ṣe , ó sì ń mú kí Jésù máa ṣe ohun tí Kristi ń ṣe .\n",
            "2020-01-17 20:15:39,843 Example #1\n",
            "2020-01-17 20:15:39,843 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 20:15:39,843 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 20:15:39,843 \tHypothesis: Nígbà tí mo ti ń ṣe iṣẹ́ ìsìn alákòókò kíkún .\n",
            "2020-01-17 20:15:39,843 Example #2\n",
            "2020-01-17 20:15:39,843 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 20:15:39,843 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 20:15:39,844 \tHypothesis: Ǹjẹ́ mo lè ṣe àwọn nǹkan tí Jèhófà ń ṣe fún àwọn èèyàn àti ìdílé mi ?\n",
            "2020-01-17 20:15:39,844 Example #3\n",
            "2020-01-17 20:15:39,844 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 20:15:39,844 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 20:15:39,844 \tHypothesis: Ó ti ń ṣe ohun tó o bá ń ṣe , ó sì ń ṣe ẹ́ bíi pé kó o máa ṣe ohun tó o bá ń ṣe .\n",
            "2020-01-17 20:15:39,844 Validation result (greedy) at epoch   1, step     4000: bleu:   5.73, loss: 86913.8125, ppl:  18.4923, duration: 48.1429s\n",
            "2020-01-17 20:15:55,182 Epoch   1 Step:     4100 Batch Loss:     3.033050 Tokens per Sec:    15142, Lr: 0.000300\n",
            "2020-01-17 20:16:10,495 Epoch   1 Step:     4200 Batch Loss:     2.982864 Tokens per Sec:    15076, Lr: 0.000300\n",
            "2020-01-17 20:16:25,903 Epoch   1 Step:     4300 Batch Loss:     2.868348 Tokens per Sec:    14974, Lr: 0.000300\n",
            "2020-01-17 20:16:41,320 Epoch   1 Step:     4400 Batch Loss:     2.923782 Tokens per Sec:    15258, Lr: 0.000300\n",
            "2020-01-17 20:16:56,718 Epoch   1 Step:     4500 Batch Loss:     3.214713 Tokens per Sec:    15451, Lr: 0.000300\n",
            "2020-01-17 20:17:11,975 Epoch   1 Step:     4600 Batch Loss:     3.060722 Tokens per Sec:    15047, Lr: 0.000300\n",
            "2020-01-17 20:17:27,356 Epoch   1 Step:     4700 Batch Loss:     2.671840 Tokens per Sec:    14904, Lr: 0.000300\n",
            "2020-01-17 20:17:42,854 Epoch   1 Step:     4800 Batch Loss:     3.109906 Tokens per Sec:    15259, Lr: 0.000300\n",
            "2020-01-17 20:17:58,247 Epoch   1 Step:     4900 Batch Loss:     2.565634 Tokens per Sec:    15130, Lr: 0.000300\n",
            "2020-01-17 20:18:13,553 Epoch   1 Step:     5000 Batch Loss:     3.394921 Tokens per Sec:    15215, Lr: 0.000300\n",
            "2020-01-17 20:19:01,404 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 20:19:01,404 Saving new checkpoint.\n",
            "2020-01-17 20:19:01,664 Example #0\n",
            "2020-01-17 20:19:01,665 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 20:19:01,665 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 20:19:01,665 \tHypothesis: Ó ń fi ìgbésí ayé rẹ̀ hàn , ó sì ń fi hàn pé a ní ìgbàgbọ́ nínú Kristi .\n",
            "2020-01-17 20:19:01,665 Example #1\n",
            "2020-01-17 20:19:01,665 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 20:19:01,665 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 20:19:01,665 \tHypothesis: Mo ti wá rí i pé mo ti ń ṣiṣẹ́ kára .\n",
            "2020-01-17 20:19:01,665 Example #2\n",
            "2020-01-17 20:19:01,665 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 20:19:01,666 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 20:19:01,666 \tHypothesis: Ṣé mo máa ń ṣe àwọn nǹkan tó ń ṣe nínú àjọṣe tímọ́tímọ́ pẹ̀lú Jèhófà àti àwọn èèyàn mi ?\n",
            "2020-01-17 20:19:01,666 Example #3\n",
            "2020-01-17 20:19:01,666 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 20:19:01,666 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 20:19:01,666 \tHypothesis: Ó ti rí i pé ó ti ń ṣe nǹkan tó o ti ń ṣe , àmọ́ ó máa ń ṣe ẹ́ bíi pé kó o máa ṣe ohun tó o bá fẹ́ .\n",
            "2020-01-17 20:19:01,666 Validation result (greedy) at epoch   1, step     5000: bleu:   7.29, loss: 81803.5938, ppl:  15.5774, duration: 48.1130s\n",
            "2020-01-17 20:19:16,984 Epoch   1 Step:     5100 Batch Loss:     3.200601 Tokens per Sec:    14955, Lr: 0.000300\n",
            "2020-01-17 20:19:32,292 Epoch   1 Step:     5200 Batch Loss:     2.810164 Tokens per Sec:    15210, Lr: 0.000300\n",
            "2020-01-17 20:19:40,442 Epoch   1: total training loss 18610.95\n",
            "2020-01-17 20:19:40,442 EPOCH 2\n",
            "2020-01-17 20:19:48,238 Epoch   2 Step:     5300 Batch Loss:     2.831245 Tokens per Sec:    13999, Lr: 0.000300\n",
            "2020-01-17 20:20:03,583 Epoch   2 Step:     5400 Batch Loss:     2.913992 Tokens per Sec:    15100, Lr: 0.000300\n",
            "2020-01-17 20:20:18,837 Epoch   2 Step:     5500 Batch Loss:     2.978886 Tokens per Sec:    14956, Lr: 0.000300\n",
            "2020-01-17 20:20:34,275 Epoch   2 Step:     5600 Batch Loss:     2.680222 Tokens per Sec:    15213, Lr: 0.000300\n",
            "2020-01-17 20:20:49,501 Epoch   2 Step:     5700 Batch Loss:     2.731855 Tokens per Sec:    15046, Lr: 0.000300\n",
            "2020-01-17 20:21:04,840 Epoch   2 Step:     5800 Batch Loss:     2.984466 Tokens per Sec:    15271, Lr: 0.000300\n",
            "2020-01-17 20:21:20,233 Epoch   2 Step:     5900 Batch Loss:     2.422177 Tokens per Sec:    14963, Lr: 0.000300\n",
            "2020-01-17 20:21:35,634 Epoch   2 Step:     6000 Batch Loss:     2.906600 Tokens per Sec:    15249, Lr: 0.000300\n",
            "2020-01-17 20:22:23,383 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 20:22:23,383 Saving new checkpoint.\n",
            "2020-01-17 20:22:23,667 Example #0\n",
            "2020-01-17 20:22:23,667 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 20:22:23,667 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 20:22:23,667 \tHypothesis: Ó ń fi hàn pé òun jẹ́ onírẹ̀lẹ̀ , ó ń fi ẹ̀mí mímọ́ hàn .\n",
            "2020-01-17 20:22:23,667 Example #1\n",
            "2020-01-17 20:22:23,667 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 20:22:23,667 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 20:22:23,667 \tHypothesis: Ní báyìí , mo ti rí i pé iṣẹ́ ìwàásù ni mo ti ń ṣe .\n",
            "2020-01-17 20:22:23,667 Example #2\n",
            "2020-01-17 20:22:23,667 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 20:22:23,668 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 20:22:23,668 \tHypothesis: Ṣé mo ti ní àwọn ohun tó dára jù lọ nínú àjọṣe tó dára pẹ̀lú Jèhófà àti àwọn èèyàn mi ?\n",
            "2020-01-17 20:22:23,668 Example #3\n",
            "2020-01-17 20:22:23,668 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 20:22:23,668 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 20:22:23,668 \tHypothesis: Ó ti rí i pé ó ti ń ṣe ẹ́ láǹfààní , àmọ́ ó máa ń ṣe ẹ́ láǹfààní láti máa bá a nìṣó ní ìtẹ́lọ́rùn .\n",
            "2020-01-17 20:22:23,668 Validation result (greedy) at epoch   2, step     6000: bleu:   9.07, loss: 78208.8594, ppl:  13.8068, duration: 48.0334s\n",
            "2020-01-17 20:22:39,015 Epoch   2 Step:     6100 Batch Loss:     2.736856 Tokens per Sec:    15176, Lr: 0.000300\n",
            "2020-01-17 20:22:54,486 Epoch   2 Step:     6200 Batch Loss:     3.217248 Tokens per Sec:    14937, Lr: 0.000300\n",
            "2020-01-17 20:23:09,760 Epoch   2 Step:     6300 Batch Loss:     2.814104 Tokens per Sec:    15314, Lr: 0.000300\n",
            "2020-01-17 20:23:24,948 Epoch   2 Step:     6400 Batch Loss:     2.617935 Tokens per Sec:    15229, Lr: 0.000300\n",
            "2020-01-17 20:23:40,311 Epoch   2 Step:     6500 Batch Loss:     2.619583 Tokens per Sec:    14899, Lr: 0.000300\n",
            "2020-01-17 20:23:55,582 Epoch   2 Step:     6600 Batch Loss:     2.511684 Tokens per Sec:    15166, Lr: 0.000300\n",
            "2020-01-17 20:24:10,989 Epoch   2 Step:     6700 Batch Loss:     2.771451 Tokens per Sec:    15184, Lr: 0.000300\n",
            "2020-01-17 20:24:26,405 Epoch   2 Step:     6800 Batch Loss:     2.865517 Tokens per Sec:    15255, Lr: 0.000300\n",
            "2020-01-17 20:24:41,819 Epoch   2 Step:     6900 Batch Loss:     2.794037 Tokens per Sec:    15140, Lr: 0.000300\n",
            "2020-01-17 20:24:57,129 Epoch   2 Step:     7000 Batch Loss:     2.867008 Tokens per Sec:    15229, Lr: 0.000300\n",
            "2020-01-17 20:25:44,819 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 20:25:44,819 Saving new checkpoint.\n",
            "2020-01-17 20:25:45,085 Example #0\n",
            "2020-01-17 20:25:45,085 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 20:25:45,085 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 20:25:45,085 \tHypothesis: Ó jẹ́ ká mọ̀ pé ó ń fi ẹ̀mí mímọ́ hàn , ó sì ń fún wa ní ẹ̀mí mímọ́ .\n",
            "2020-01-17 20:25:45,085 Example #1\n",
            "2020-01-17 20:25:45,085 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 20:25:45,085 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 20:25:45,086 \tHypothesis: Mo ti rí i pé mo ti rí iṣẹ́ tó dára jù lọ .\n",
            "2020-01-17 20:25:45,086 Example #2\n",
            "2020-01-17 20:25:45,086 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 20:25:45,086 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 20:25:45,086 \tHypothesis: Ṣé mo ní àwọn ohun tó dára jù lọ nínú àjọṣe tó dára pẹ̀lú Jèhófà àti àwọn èèyàn mi pẹ̀lú ?\n",
            "2020-01-17 20:25:45,086 Example #3\n",
            "2020-01-17 20:25:45,086 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 20:25:45,086 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 20:25:45,086 \tHypothesis: Ó ti rí i pé ó ti ń ṣe ẹ́ , àmọ́ ó máa ń ṣe ẹ́ láǹfààní láti máa rìn .\n",
            "2020-01-17 20:25:45,086 Validation result (greedy) at epoch   2, step     7000: bleu:  10.17, loss: 75345.0078, ppl:  12.5414, duration: 47.9571s\n",
            "2020-01-17 20:26:00,128 Epoch   2 Step:     7100 Batch Loss:     2.565890 Tokens per Sec:    14889, Lr: 0.000300\n",
            "2020-01-17 20:26:15,518 Epoch   2 Step:     7200 Batch Loss:     2.677002 Tokens per Sec:    15311, Lr: 0.000300\n",
            "2020-01-17 20:26:30,829 Epoch   2 Step:     7300 Batch Loss:     2.596707 Tokens per Sec:    14977, Lr: 0.000300\n",
            "2020-01-17 20:26:46,225 Epoch   2 Step:     7400 Batch Loss:     2.172575 Tokens per Sec:    15172, Lr: 0.000300\n",
            "2020-01-17 20:27:01,480 Epoch   2 Step:     7500 Batch Loss:     2.448964 Tokens per Sec:    15142, Lr: 0.000300\n",
            "2020-01-17 20:27:16,910 Epoch   2 Step:     7600 Batch Loss:     2.633640 Tokens per Sec:    15213, Lr: 0.000300\n",
            "2020-01-17 20:27:32,112 Epoch   2 Step:     7700 Batch Loss:     2.781974 Tokens per Sec:    15029, Lr: 0.000300\n",
            "2020-01-17 20:27:47,475 Epoch   2 Step:     7800 Batch Loss:     2.482935 Tokens per Sec:    14902, Lr: 0.000300\n",
            "2020-01-17 20:28:02,767 Epoch   2 Step:     7900 Batch Loss:     2.442588 Tokens per Sec:    15139, Lr: 0.000300\n",
            "2020-01-17 20:28:18,125 Epoch   2 Step:     8000 Batch Loss:     2.380605 Tokens per Sec:    15014, Lr: 0.000300\n",
            "2020-01-17 20:29:05,857 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 20:29:05,857 Saving new checkpoint.\n",
            "2020-01-17 20:29:06,125 Example #0\n",
            "2020-01-17 20:29:06,126 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 20:29:06,126 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 20:29:06,126 \tHypothesis: Ó jẹ́ ká máa fi hàn pé òun ni ẹni tó ń fúnni ní ẹ̀bùn tó yẹ ká máa fi hàn pé a jẹ́ onígbàgbọ́ .\n",
            "2020-01-17 20:29:06,126 Example #1\n",
            "2020-01-17 20:29:06,126 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 20:29:06,126 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 20:29:06,126 \tHypothesis: Ní báyìí , mo ti wá rí iṣẹ́ ìwàásù .\n",
            "2020-01-17 20:29:06,126 Example #2\n",
            "2020-01-17 20:29:06,126 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 20:29:06,126 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 20:29:06,127 \tHypothesis: Ṣé mo ní àjọṣe tó dára pẹ̀lú Jèhófà ju àwọn èèyàn lọ ?\n",
            "2020-01-17 20:29:06,127 Example #3\n",
            "2020-01-17 20:29:06,127 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 20:29:06,127 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 20:29:06,127 \tHypothesis: Ó ti rí i pé ó ti ń ṣe ẹ́ láǹfààní ju bó ṣe yẹ lọ , àmọ́ ó máa ń fi sùúrù ṣe ẹ́ .\n",
            "2020-01-17 20:29:06,127 Validation result (greedy) at epoch   2, step     8000: bleu:  11.54, loss: 72986.0781, ppl:  11.5867, duration: 48.0022s\n",
            "2020-01-17 20:29:21,456 Epoch   2 Step:     8100 Batch Loss:     2.376002 Tokens per Sec:    15258, Lr: 0.000300\n",
            "2020-01-17 20:29:36,876 Epoch   2 Step:     8200 Batch Loss:     2.403431 Tokens per Sec:    15144, Lr: 0.000300\n",
            "2020-01-17 20:29:52,128 Epoch   2 Step:     8300 Batch Loss:     2.491778 Tokens per Sec:    15167, Lr: 0.000300\n",
            "2020-01-17 20:30:07,207 Epoch   2 Step:     8400 Batch Loss:     2.651011 Tokens per Sec:    14894, Lr: 0.000300\n",
            "2020-01-17 20:30:22,661 Epoch   2 Step:     8500 Batch Loss:     2.725023 Tokens per Sec:    15151, Lr: 0.000300\n",
            "2020-01-17 20:30:38,101 Epoch   2 Step:     8600 Batch Loss:     2.843770 Tokens per Sec:    15338, Lr: 0.000300\n",
            "2020-01-17 20:30:53,371 Epoch   2 Step:     8700 Batch Loss:     2.532361 Tokens per Sec:    15023, Lr: 0.000300\n",
            "2020-01-17 20:31:08,771 Epoch   2 Step:     8800 Batch Loss:     2.650401 Tokens per Sec:    14957, Lr: 0.000300\n",
            "2020-01-17 20:31:24,245 Epoch   2 Step:     8900 Batch Loss:     2.537630 Tokens per Sec:    15449, Lr: 0.000300\n",
            "2020-01-17 20:31:39,696 Epoch   2 Step:     9000 Batch Loss:     2.415248 Tokens per Sec:    14849, Lr: 0.000300\n",
            "2020-01-17 20:32:27,486 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 20:32:27,486 Saving new checkpoint.\n",
            "2020-01-17 20:32:27,720 Example #0\n",
            "2020-01-17 20:32:27,720 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 20:32:27,720 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 20:32:27,720 \tHypothesis: Ó jẹ́ ká mọ̀ pé ó yẹ kí ìgbàgbọ́ wa lágbára , ó sì fi hàn pé ó yẹ kí Kristi máa fi ẹ̀mí mímọ́ rẹ̀ hàn .\n",
            "2020-01-17 20:32:27,720 Example #1\n",
            "2020-01-17 20:32:27,720 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 20:32:27,720 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 20:32:27,720 \tHypothesis: Mo ti wá rí i pé iṣẹ́ tó ń ṣe ni mo máa ń ṣe .\n",
            "2020-01-17 20:32:27,720 Example #2\n",
            "2020-01-17 20:32:27,721 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 20:32:27,721 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 20:32:27,721 \tHypothesis: Ǹjẹ́ mo ní àwọn nǹkan tó dára ju àjọṣe mi pẹ̀lú Jèhófà àti àwọn èèyàn ?\n",
            "2020-01-17 20:32:27,721 Example #3\n",
            "2020-01-17 20:32:27,721 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 20:32:27,721 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 20:32:27,721 \tHypothesis: Ó ti rí i pé ó ti rí i pé o ti ń ṣe nǹkan kan ju bó ṣe yẹ lọ .\n",
            "2020-01-17 20:32:27,721 Validation result (greedy) at epoch   2, step     9000: bleu:  12.26, loss: 71024.8203, ppl:  10.8485, duration: 48.0249s\n",
            "2020-01-17 20:32:43,071 Epoch   2 Step:     9100 Batch Loss:     2.828179 Tokens per Sec:    14996, Lr: 0.000300\n",
            "2020-01-17 20:32:58,360 Epoch   2 Step:     9200 Batch Loss:     2.469577 Tokens per Sec:    15135, Lr: 0.000300\n",
            "2020-01-17 20:33:13,466 Epoch   2 Step:     9300 Batch Loss:     2.305721 Tokens per Sec:    15176, Lr: 0.000300\n",
            "2020-01-17 20:33:28,667 Epoch   2 Step:     9400 Batch Loss:     2.665661 Tokens per Sec:    14775, Lr: 0.000300\n",
            "2020-01-17 20:33:43,977 Epoch   2 Step:     9500 Batch Loss:     2.132962 Tokens per Sec:    14901, Lr: 0.000300\n",
            "2020-01-17 20:33:59,350 Epoch   2 Step:     9600 Batch Loss:     2.539052 Tokens per Sec:    14885, Lr: 0.000300\n",
            "2020-01-17 20:34:14,661 Epoch   2 Step:     9700 Batch Loss:     2.515122 Tokens per Sec:    15063, Lr: 0.000300\n",
            "2020-01-17 20:34:29,792 Epoch   2 Step:     9800 Batch Loss:     2.326742 Tokens per Sec:    14893, Lr: 0.000300\n",
            "2020-01-17 20:34:45,518 Epoch   2 Step:     9900 Batch Loss:     2.361092 Tokens per Sec:    15485, Lr: 0.000300\n",
            "2020-01-17 20:35:00,809 Epoch   2 Step:    10000 Batch Loss:     2.270333 Tokens per Sec:    15149, Lr: 0.000300\n",
            "2020-01-17 20:35:48,442 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 20:35:48,442 Saving new checkpoint.\n",
            "2020-01-17 20:35:48,672 Example #0\n",
            "2020-01-17 20:35:48,672 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 20:35:48,672 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 20:35:48,673 \tHypothesis: Ó jẹ́ ẹni tó ń fúnni ní ìyè , ó ń fún ẹ̀bùn tí ó yẹ ká máa fi hàn pé òun jẹ́ ẹ̀bùn tí Kristi ní .\n",
            "2020-01-17 20:35:48,673 Example #1\n",
            "2020-01-17 20:35:48,673 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 20:35:48,673 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 20:35:48,673 \tHypothesis: Mo ti wá rí i pé iṣẹ́ tí mo ṣe yìí ti ń ṣe .\n",
            "2020-01-17 20:35:48,673 Example #2\n",
            "2020-01-17 20:35:48,673 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 20:35:48,673 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 20:35:48,673 \tHypothesis: Ṣé mo ní àjọṣe tó dára ju àjọṣe mi pẹ̀lú Jèhófà àti àwọn èèyàn lọ ?\n",
            "2020-01-17 20:35:48,673 Example #3\n",
            "2020-01-17 20:35:48,673 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 20:35:48,674 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 20:35:48,674 \tHypothesis: Ó ti rí i pé ó ti ń ṣe nǹkan tó pọ̀ jù , àmọ́ ó máa ń ṣe ẹ́ láǹfààní .\n",
            "2020-01-17 20:35:48,674 Validation result (greedy) at epoch   2, step    10000: bleu:  13.36, loss: 68561.9844, ppl:   9.9877, duration: 47.8643s\n",
            "2020-01-17 20:36:03,852 Epoch   2 Step:    10100 Batch Loss:     2.420507 Tokens per Sec:    14918, Lr: 0.000300\n",
            "2020-01-17 20:36:19,146 Epoch   2 Step:    10200 Batch Loss:     2.442957 Tokens per Sec:    15163, Lr: 0.000300\n",
            "2020-01-17 20:36:34,392 Epoch   2 Step:    10300 Batch Loss:     2.467040 Tokens per Sec:    15039, Lr: 0.000300\n",
            "2020-01-17 20:36:49,875 Epoch   2 Step:    10400 Batch Loss:     2.365289 Tokens per Sec:    15063, Lr: 0.000300\n",
            "2020-01-17 20:37:04,985 Epoch   2 Step:    10500 Batch Loss:     2.246313 Tokens per Sec:    14901, Lr: 0.000300\n",
            "2020-01-17 20:37:04,998 Epoch   2: total training loss 13787.70\n",
            "2020-01-17 20:37:04,998 EPOCH 3\n",
            "2020-01-17 20:37:20,727 Epoch   3 Step:    10600 Batch Loss:     2.483065 Tokens per Sec:    14522, Lr: 0.000300\n",
            "2020-01-17 20:37:36,064 Epoch   3 Step:    10700 Batch Loss:     2.353754 Tokens per Sec:    14866, Lr: 0.000300\n",
            "2020-01-17 20:37:51,342 Epoch   3 Step:    10800 Batch Loss:     2.209525 Tokens per Sec:    15000, Lr: 0.000300\n",
            "2020-01-17 20:38:06,693 Epoch   3 Step:    10900 Batch Loss:     2.543043 Tokens per Sec:    15293, Lr: 0.000300\n",
            "2020-01-17 20:38:22,255 Epoch   3 Step:    11000 Batch Loss:     2.531735 Tokens per Sec:    14972, Lr: 0.000300\n",
            "2020-01-17 20:39:10,395 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 20:39:10,396 Saving new checkpoint.\n",
            "2020-01-17 20:39:10,653 Example #0\n",
            "2020-01-17 20:39:10,654 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 20:39:10,654 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 20:39:10,654 \tHypothesis: Ó jẹ́ ọ̀kan lára ohun tó wà nínú ìgbésí ayé , ẹni tó yẹ kó máa fi ẹ̀bùn fún Kristi .\n",
            "2020-01-17 20:39:10,654 Example #1\n",
            "2020-01-17 20:39:10,654 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 20:39:10,654 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 20:39:10,654 \tHypothesis: Ní báyìí , mo ti wá rí i pé iṣẹ́ tí mo ṣe yìí ti ń ṣe .\n",
            "2020-01-17 20:39:10,654 Example #2\n",
            "2020-01-17 20:39:10,654 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 20:39:10,654 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 20:39:10,654 \tHypothesis: Ǹjẹ́ mo máa ń fi àwọn nǹkan tó dára ju àjọṣe mi pẹ̀lú Jèhófà àti àwọn èèyàn mi pẹ̀lú ?\n",
            "2020-01-17 20:39:10,654 Example #3\n",
            "2020-01-17 20:39:10,655 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 20:39:10,655 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 20:39:10,655 \tHypothesis: Ó ti rí ọ̀pọ̀ ìrírí àti ìrírí tó o ní , àmọ́ ó máa ń fi sùúrù rìn .\n",
            "2020-01-17 20:39:10,655 Validation result (greedy) at epoch   3, step    11000: bleu:  13.71, loss: 66886.6172, ppl:   9.4415, duration: 48.3991s\n",
            "2020-01-17 20:39:25,942 Epoch   3 Step:    11100 Batch Loss:     2.248968 Tokens per Sec:    14965, Lr: 0.000300\n",
            "2020-01-17 20:39:41,131 Epoch   3 Step:    11200 Batch Loss:     2.652482 Tokens per Sec:    14989, Lr: 0.000300\n",
            "2020-01-17 20:39:56,517 Epoch   3 Step:    11300 Batch Loss:     2.559486 Tokens per Sec:    15142, Lr: 0.000300\n",
            "2020-01-17 20:40:11,851 Epoch   3 Step:    11400 Batch Loss:     2.799168 Tokens per Sec:    14966, Lr: 0.000300\n",
            "2020-01-17 20:40:27,186 Epoch   3 Step:    11500 Batch Loss:     2.232553 Tokens per Sec:    15203, Lr: 0.000300\n",
            "2020-01-17 20:40:42,535 Epoch   3 Step:    11600 Batch Loss:     2.220369 Tokens per Sec:    15171, Lr: 0.000300\n",
            "2020-01-17 20:40:57,976 Epoch   3 Step:    11700 Batch Loss:     2.621756 Tokens per Sec:    15346, Lr: 0.000300\n",
            "2020-01-17 20:41:13,222 Epoch   3 Step:    11800 Batch Loss:     2.150871 Tokens per Sec:    14971, Lr: 0.000300\n",
            "2020-01-17 20:41:28,445 Epoch   3 Step:    11900 Batch Loss:     2.271373 Tokens per Sec:    15122, Lr: 0.000300\n",
            "2020-01-17 20:41:43,790 Epoch   3 Step:    12000 Batch Loss:     2.064146 Tokens per Sec:    15106, Lr: 0.000300\n",
            "2020-01-17 20:42:31,534 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 20:42:31,534 Saving new checkpoint.\n",
            "2020-01-17 20:42:31,770 Example #0\n",
            "2020-01-17 20:42:31,770 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 20:42:31,770 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 20:42:31,770 \tHypothesis: Ó jẹ́ ọ̀kan lára àwọn tó ń fúnni ní ẹ̀mí mímọ́ , ó sì ń fún wa ní ẹ̀bùn tí kò tọ́ .\n",
            "2020-01-17 20:42:31,770 Example #1\n",
            "2020-01-17 20:42:31,770 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 20:42:31,770 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 20:42:31,770 \tHypothesis: Mo ti wá rí i pé iṣẹ́ àṣekára ni mò ń ṣe .\n",
            "2020-01-17 20:42:31,770 Example #2\n",
            "2020-01-17 20:42:31,771 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 20:42:31,771 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 20:42:31,771 \tHypothesis: Ǹjẹ́ mo mọyì àwọn nǹkan tó ṣe pàtàkì ju àjọṣe mi pẹ̀lú Jèhófà àti àwọn èèyàn ?\n",
            "2020-01-17 20:42:31,771 Example #3\n",
            "2020-01-17 20:42:31,771 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 20:42:31,771 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 20:42:31,771 \tHypothesis: Ó ti rí ọ̀pọ̀ ìrírí àti oorun tó o ní , àmọ́ ó máa ń fi sùúrù rìn .\n",
            "2020-01-17 20:42:31,771 Validation result (greedy) at epoch   3, step    12000: bleu:  15.33, loss: 65310.1211, ppl:   8.9549, duration: 47.9804s\n",
            "2020-01-17 20:42:47,076 Epoch   3 Step:    12100 Batch Loss:     2.299537 Tokens per Sec:    15071, Lr: 0.000300\n",
            "2020-01-17 20:43:02,289 Epoch   3 Step:    12200 Batch Loss:     2.311770 Tokens per Sec:    15145, Lr: 0.000300\n",
            "2020-01-17 20:43:17,680 Epoch   3 Step:    12300 Batch Loss:     2.197074 Tokens per Sec:    14960, Lr: 0.000300\n",
            "2020-01-17 20:43:32,787 Epoch   3 Step:    12400 Batch Loss:     2.015906 Tokens per Sec:    15166, Lr: 0.000300\n",
            "2020-01-17 20:43:48,177 Epoch   3 Step:    12500 Batch Loss:     2.499927 Tokens per Sec:    14791, Lr: 0.000300\n",
            "2020-01-17 20:44:03,446 Epoch   3 Step:    12600 Batch Loss:     2.316360 Tokens per Sec:    15255, Lr: 0.000300\n",
            "2020-01-17 20:44:18,835 Epoch   3 Step:    12700 Batch Loss:     2.883192 Tokens per Sec:    15203, Lr: 0.000300\n",
            "2020-01-17 20:44:34,156 Epoch   3 Step:    12800 Batch Loss:     2.272120 Tokens per Sec:    14870, Lr: 0.000300\n",
            "2020-01-17 20:44:49,342 Epoch   3 Step:    12900 Batch Loss:     2.319587 Tokens per Sec:    15078, Lr: 0.000300\n",
            "2020-01-17 20:45:04,656 Epoch   3 Step:    13000 Batch Loss:     2.233084 Tokens per Sec:    15203, Lr: 0.000300\n",
            "2020-01-17 20:45:52,543 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 20:45:52,543 Saving new checkpoint.\n",
            "2020-01-17 20:45:52,791 Example #0\n",
            "2020-01-17 20:45:52,791 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 20:45:52,791 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 20:45:52,791 \tHypothesis: Ó jẹ́ ọ̀kan lára àwọn tó ń fúnni ní ẹ̀bùn tí Kristi ní fún un .\n",
            "2020-01-17 20:45:52,791 Example #1\n",
            "2020-01-17 20:45:52,791 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 20:45:52,791 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 20:45:52,791 \tHypothesis: Ní báyìí , mo ti rí i pé iṣẹ́ àṣekára ni mò ń ṣe .\n",
            "2020-01-17 20:45:52,791 Example #2\n",
            "2020-01-17 20:45:52,792 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 20:45:52,792 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 20:45:52,792 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àti àwọn èèyàn ?\n",
            "2020-01-17 20:45:52,792 Example #3\n",
            "2020-01-17 20:45:52,792 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 20:45:52,792 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 20:45:52,792 \tHypothesis: Ó ti rí ọ̀pọ̀ ìrírí àti okun tó o ní , àmọ́ ó ń fi sùúrù rìn .\n",
            "2020-01-17 20:45:52,792 Validation result (greedy) at epoch   3, step    13000: bleu:  16.21, loss: 63689.7305, ppl:   8.4809, duration: 48.1359s\n",
            "2020-01-17 20:46:07,893 Epoch   3 Step:    13100 Batch Loss:     2.255358 Tokens per Sec:    15032, Lr: 0.000300\n",
            "2020-01-17 20:46:23,374 Epoch   3 Step:    13200 Batch Loss:     2.365994 Tokens per Sec:    15334, Lr: 0.000300\n",
            "2020-01-17 20:46:38,799 Epoch   3 Step:    13300 Batch Loss:     2.311499 Tokens per Sec:    15218, Lr: 0.000300\n",
            "2020-01-17 20:46:54,163 Epoch   3 Step:    13400 Batch Loss:     2.238747 Tokens per Sec:    15143, Lr: 0.000300\n",
            "2020-01-17 20:47:09,614 Epoch   3 Step:    13500 Batch Loss:     2.339578 Tokens per Sec:    14692, Lr: 0.000300\n",
            "2020-01-17 20:47:24,925 Epoch   3 Step:    13600 Batch Loss:     1.978218 Tokens per Sec:    15029, Lr: 0.000300\n",
            "2020-01-17 20:47:40,153 Epoch   3 Step:    13700 Batch Loss:     2.411258 Tokens per Sec:    15010, Lr: 0.000300\n",
            "2020-01-17 20:47:55,412 Epoch   3 Step:    13800 Batch Loss:     2.510365 Tokens per Sec:    15114, Lr: 0.000300\n",
            "2020-01-17 20:48:10,784 Epoch   3 Step:    13900 Batch Loss:     2.097995 Tokens per Sec:    14997, Lr: 0.000300\n",
            "2020-01-17 20:48:26,218 Epoch   3 Step:    14000 Batch Loss:     2.379001 Tokens per Sec:    15165, Lr: 0.000300\n",
            "2020-01-17 20:49:14,031 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 20:49:14,031 Saving new checkpoint.\n",
            "2020-01-17 20:49:14,270 Example #0\n",
            "2020-01-17 20:49:14,271 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 20:49:14,271 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 20:49:14,271 \tHypothesis: Ó jẹ́ orísun ìyè , Ẹni tó ń fúnni ní ẹ̀bùn àìlẹ́tọ̀ọ́sí nípasẹ̀ Kristi .\n",
            "2020-01-17 20:49:14,271 Example #1\n",
            "2020-01-17 20:49:14,271 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 20:49:14,271 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 20:49:14,271 \tHypothesis: Ní báyìí , mo ti wá rí i pé iṣẹ́ kan wà tó ń ṣe mí láǹfààní .\n",
            "2020-01-17 20:49:14,271 Example #2\n",
            "2020-01-17 20:49:14,271 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 20:49:14,271 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 20:49:14,271 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tó ṣe pàtàkì ju àjọṣe mi pẹ̀lú Jèhófà àti pẹ̀lú àwọn èèyàn ?\n",
            "2020-01-17 20:49:14,271 Example #3\n",
            "2020-01-17 20:49:14,272 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 20:49:14,272 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 20:49:14,272 \tHypothesis: Ó ti rí i pé ó ti rí i pé ó ti pẹ́ tó , àmọ́ ó máa ń fi sùúrù rìn .\n",
            "2020-01-17 20:49:14,272 Validation result (greedy) at epoch   3, step    14000: bleu:  15.91, loss: 62669.9023, ppl:   8.1955, duration: 48.0530s\n",
            "2020-01-17 20:49:29,406 Epoch   3 Step:    14100 Batch Loss:     2.172721 Tokens per Sec:    14815, Lr: 0.000300\n",
            "2020-01-17 20:49:44,954 Epoch   3 Step:    14200 Batch Loss:     2.398120 Tokens per Sec:    15414, Lr: 0.000300\n",
            "2020-01-17 20:50:00,241 Epoch   3 Step:    14300 Batch Loss:     2.205026 Tokens per Sec:    15150, Lr: 0.000300\n",
            "2020-01-17 20:50:15,709 Epoch   3 Step:    14400 Batch Loss:     2.245809 Tokens per Sec:    15097, Lr: 0.000300\n",
            "2020-01-17 20:50:30,846 Epoch   3 Step:    14500 Batch Loss:     2.242214 Tokens per Sec:    14977, Lr: 0.000300\n",
            "2020-01-17 20:50:46,228 Epoch   3 Step:    14600 Batch Loss:     2.542259 Tokens per Sec:    14984, Lr: 0.000300\n",
            "2020-01-17 20:51:01,450 Epoch   3 Step:    14700 Batch Loss:     2.346295 Tokens per Sec:    15080, Lr: 0.000300\n",
            "2020-01-17 20:51:16,808 Epoch   3 Step:    14800 Batch Loss:     2.057225 Tokens per Sec:    14981, Lr: 0.000300\n",
            "2020-01-17 20:51:32,088 Epoch   3 Step:    14900 Batch Loss:     2.386183 Tokens per Sec:    15297, Lr: 0.000300\n",
            "2020-01-17 20:51:47,353 Epoch   3 Step:    15000 Batch Loss:     2.263669 Tokens per Sec:    14837, Lr: 0.000300\n",
            "2020-01-17 20:52:35,189 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 20:52:35,190 Saving new checkpoint.\n",
            "2020-01-17 20:52:35,431 Example #0\n",
            "2020-01-17 20:52:35,432 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 20:52:35,432 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 20:52:35,432 \tHypothesis: Ó jẹ́ orísun ìyè , Ẹni tó ń fúnni ní ẹ̀bùn àìlẹ́tọ̀ọ́sí nípasẹ̀ Kristi .\n",
            "2020-01-17 20:52:35,432 Example #1\n",
            "2020-01-17 20:52:35,432 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 20:52:35,432 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 20:52:35,432 \tHypothesis: Mo wá rí i pé mo ti wá rí iṣẹ́ kan tó ń ṣe .\n",
            "2020-01-17 20:52:35,432 Example #2\n",
            "2020-01-17 20:52:35,432 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 20:52:35,432 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 20:52:35,432 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àti pẹ̀lú àwọn èèyàn ?\n",
            "2020-01-17 20:52:35,432 Example #3\n",
            "2020-01-17 20:52:35,433 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 20:52:35,433 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 20:52:35,433 \tHypothesis: Ó ti rí i pé ó ti rí nǹkan tó pọ̀ ju bó ṣe ń ṣe , àmọ́ ó máa ń fi sùúrù rìn .\n",
            "2020-01-17 20:52:35,433 Validation result (greedy) at epoch   3, step    15000: bleu:  17.52, loss: 61375.0078, ppl:   7.8469, duration: 48.0798s\n",
            "2020-01-17 20:52:50,782 Epoch   3 Step:    15100 Batch Loss:     2.068762 Tokens per Sec:    15150, Lr: 0.000300\n",
            "2020-01-17 20:53:05,800 Epoch   3 Step:    15200 Batch Loss:     2.232289 Tokens per Sec:    15187, Lr: 0.000300\n",
            "2020-01-17 20:53:21,152 Epoch   3 Step:    15300 Batch Loss:     2.038440 Tokens per Sec:    14985, Lr: 0.000300\n",
            "2020-01-17 20:53:36,515 Epoch   3 Step:    15400 Batch Loss:     2.268228 Tokens per Sec:    15445, Lr: 0.000300\n",
            "2020-01-17 20:53:51,797 Epoch   3 Step:    15500 Batch Loss:     2.269814 Tokens per Sec:    15158, Lr: 0.000300\n",
            "2020-01-17 20:54:06,965 Epoch   3 Step:    15600 Batch Loss:     2.378784 Tokens per Sec:    15201, Lr: 0.000300\n",
            "2020-01-17 20:54:22,465 Epoch   3 Step:    15700 Batch Loss:     2.150067 Tokens per Sec:    15149, Lr: 0.000300\n",
            "2020-01-17 20:54:30,571 Epoch   3: total training loss 12215.91\n",
            "2020-01-17 20:54:30,571 EPOCH 4\n",
            "2020-01-17 20:54:38,244 Epoch   4 Step:    15800 Batch Loss:     2.093537 Tokens per Sec:    13633, Lr: 0.000300\n",
            "2020-01-17 20:54:53,555 Epoch   4 Step:    15900 Batch Loss:     2.932654 Tokens per Sec:    15212, Lr: 0.000300\n",
            "2020-01-17 20:55:08,732 Epoch   4 Step:    16000 Batch Loss:     2.332251 Tokens per Sec:    15129, Lr: 0.000300\n",
            "2020-01-17 20:55:56,542 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 20:55:56,542 Saving new checkpoint.\n",
            "2020-01-17 20:55:56,810 Example #0\n",
            "2020-01-17 20:55:56,810 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 20:55:56,810 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 20:55:56,810 \tHypothesis: Ó jẹ́ Orísun ìyè , Ẹni tó ń fúnni ní ẹ̀bùn tí kò lẹ́tọ̀ọ́ láti fi hàn pé a jẹ́ ẹ̀bùn tí Kristi ní .\n",
            "2020-01-17 20:55:56,810 Example #1\n",
            "2020-01-17 20:55:56,811 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 20:55:56,811 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 20:55:56,811 \tHypothesis: Ní báyìí , mo ti wá rí i pé iṣẹ́ àṣekára ni mo ti ń ṣe .\n",
            "2020-01-17 20:55:56,811 Example #2\n",
            "2020-01-17 20:55:56,811 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 20:55:56,811 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 20:55:56,811 \tHypothesis: Ṣé mo ní àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 20:55:56,811 Example #3\n",
            "2020-01-17 20:55:56,811 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 20:55:56,811 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 20:55:56,811 \tHypothesis: Ó ti rí i pé ó ti rí i pé o ti ṣe ohun tó ṣe , àmọ́ ó máa ń fi sùúrù rìn ní ìsinmi .\n",
            "2020-01-17 20:55:56,811 Validation result (greedy) at epoch   4, step    16000: bleu:  17.95, loss: 60620.3086, ppl:   7.6506, duration: 48.0792s\n",
            "2020-01-17 20:56:12,196 Epoch   4 Step:    16100 Batch Loss:     2.044674 Tokens per Sec:    14704, Lr: 0.000300\n",
            "2020-01-17 20:56:27,383 Epoch   4 Step:    16200 Batch Loss:     2.055219 Tokens per Sec:    15043, Lr: 0.000300\n",
            "2020-01-17 20:56:42,889 Epoch   4 Step:    16300 Batch Loss:     1.907203 Tokens per Sec:    15326, Lr: 0.000300\n",
            "2020-01-17 20:56:58,217 Epoch   4 Step:    16400 Batch Loss:     2.618945 Tokens per Sec:    15175, Lr: 0.000300\n",
            "2020-01-17 20:57:13,542 Epoch   4 Step:    16500 Batch Loss:     2.238926 Tokens per Sec:    14951, Lr: 0.000300\n",
            "2020-01-17 20:57:28,663 Epoch   4 Step:    16600 Batch Loss:     2.480807 Tokens per Sec:    15027, Lr: 0.000300\n",
            "2020-01-17 20:57:43,988 Epoch   4 Step:    16700 Batch Loss:     1.833062 Tokens per Sec:    15194, Lr: 0.000300\n",
            "2020-01-17 20:57:59,187 Epoch   4 Step:    16800 Batch Loss:     2.135971 Tokens per Sec:    14960, Lr: 0.000300\n",
            "2020-01-17 20:58:14,599 Epoch   4 Step:    16900 Batch Loss:     2.029055 Tokens per Sec:    15168, Lr: 0.000300\n",
            "2020-01-17 20:58:29,779 Epoch   4 Step:    17000 Batch Loss:     2.329516 Tokens per Sec:    15226, Lr: 0.000300\n",
            "2020-01-17 20:59:17,549 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 20:59:17,549 Saving new checkpoint.\n",
            "2020-01-17 20:59:17,811 Example #0\n",
            "2020-01-17 20:59:17,812 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 20:59:17,812 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 20:59:17,812 \tHypothesis: Ó jẹ́ Orísun ìyè , Ẹni tí ó bá ń fi ẹ̀bùn àfọwọ́gbà nípasẹ̀ Kristi .\n",
            "2020-01-17 20:59:17,812 Example #1\n",
            "2020-01-17 20:59:17,812 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 20:59:17,812 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 20:59:17,812 \tHypothesis: Mo ti wá rí i pé iṣẹ́ kan wà tó ń ṣe .\n",
            "2020-01-17 20:59:17,812 Example #2\n",
            "2020-01-17 20:59:17,812 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 20:59:17,813 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 20:59:17,813 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àti àwọn èèyàn ?\n",
            "2020-01-17 20:59:17,813 Example #3\n",
            "2020-01-17 20:59:17,813 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 20:59:17,813 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 20:59:17,813 \tHypothesis: Ó ti rí i pé ó ti wà ní ìrírí àti bó ṣe ń ṣe ẹ́ , àmọ́ ó máa ń fi sùúrù rìn .\n",
            "2020-01-17 20:59:17,813 Validation result (greedy) at epoch   4, step    17000: bleu:  18.38, loss: 59778.6055, ppl:   7.4375, duration: 48.0338s\n",
            "2020-01-17 20:59:32,941 Epoch   4 Step:    17100 Batch Loss:     2.297875 Tokens per Sec:    15042, Lr: 0.000300\n",
            "2020-01-17 20:59:48,379 Epoch   4 Step:    17200 Batch Loss:     2.179398 Tokens per Sec:    14874, Lr: 0.000300\n",
            "2020-01-17 21:00:03,520 Epoch   4 Step:    17300 Batch Loss:     2.303722 Tokens per Sec:    14971, Lr: 0.000300\n",
            "2020-01-17 21:00:18,757 Epoch   4 Step:    17400 Batch Loss:     2.151887 Tokens per Sec:    14985, Lr: 0.000300\n",
            "2020-01-17 21:00:34,018 Epoch   4 Step:    17500 Batch Loss:     2.378989 Tokens per Sec:    15278, Lr: 0.000300\n",
            "2020-01-17 21:00:49,366 Epoch   4 Step:    17600 Batch Loss:     2.080313 Tokens per Sec:    15241, Lr: 0.000300\n",
            "2020-01-17 21:01:04,739 Epoch   4 Step:    17700 Batch Loss:     2.263572 Tokens per Sec:    14963, Lr: 0.000300\n",
            "2020-01-17 21:01:20,032 Epoch   4 Step:    17800 Batch Loss:     2.167127 Tokens per Sec:    15144, Lr: 0.000300\n",
            "2020-01-17 21:01:35,283 Epoch   4 Step:    17900 Batch Loss:     2.469588 Tokens per Sec:    14849, Lr: 0.000300\n",
            "2020-01-17 21:01:50,608 Epoch   4 Step:    18000 Batch Loss:     2.019063 Tokens per Sec:    15216, Lr: 0.000300\n",
            "2020-01-17 21:02:38,623 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 21:02:38,624 Saving new checkpoint.\n",
            "2020-01-17 21:02:38,897 Example #0\n",
            "2020-01-17 21:02:38,898 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 21:02:38,898 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:02:38,898 \tHypothesis: Òun ni Ẹni tí ó jẹ́ ẹ̀bùn tí ó yẹ fún Kristi .\n",
            "2020-01-17 21:02:38,898 Example #1\n",
            "2020-01-17 21:02:38,898 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 21:02:38,898 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 21:02:38,898 \tHypothesis: Ní báyìí , mo ti wá rí i pé iṣẹ́ kan wà tó ń ṣe .\n",
            "2020-01-17 21:02:38,898 Example #2\n",
            "2020-01-17 21:02:38,898 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 21:02:38,899 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 21:02:38,899 \tHypothesis: Ǹjẹ́ mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn mi lọ ?\n",
            "2020-01-17 21:02:38,899 Example #3\n",
            "2020-01-17 21:02:38,899 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 21:02:38,899 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 21:02:38,899 \tHypothesis: Ó ti rí i pé ó ti ní ìrírí àti ìwọ̀nba , ṣùgbọ́n ó ń fi sùúrù rìn jìnnà sí ọ .\n",
            "2020-01-17 21:02:38,899 Validation result (greedy) at epoch   4, step    18000: bleu:  18.64, loss: 59024.8867, ppl:   7.2517, duration: 48.2909s\n",
            "2020-01-17 21:02:54,381 Epoch   4 Step:    18100 Batch Loss:     2.026363 Tokens per Sec:    15341, Lr: 0.000300\n",
            "2020-01-17 21:03:09,661 Epoch   4 Step:    18200 Batch Loss:     2.198466 Tokens per Sec:    15064, Lr: 0.000300\n",
            "2020-01-17 21:03:24,884 Epoch   4 Step:    18300 Batch Loss:     2.345448 Tokens per Sec:    15051, Lr: 0.000300\n",
            "2020-01-17 21:03:40,329 Epoch   4 Step:    18400 Batch Loss:     2.159438 Tokens per Sec:    15364, Lr: 0.000300\n",
            "2020-01-17 21:03:55,648 Epoch   4 Step:    18500 Batch Loss:     2.223084 Tokens per Sec:    15032, Lr: 0.000300\n",
            "2020-01-17 21:04:10,864 Epoch   4 Step:    18600 Batch Loss:     2.170035 Tokens per Sec:    15042, Lr: 0.000300\n",
            "2020-01-17 21:04:26,372 Epoch   4 Step:    18700 Batch Loss:     1.961124 Tokens per Sec:    15315, Lr: 0.000300\n",
            "2020-01-17 21:04:41,663 Epoch   4 Step:    18800 Batch Loss:     2.367146 Tokens per Sec:    14705, Lr: 0.000300\n",
            "2020-01-17 21:04:56,927 Epoch   4 Step:    18900 Batch Loss:     2.209584 Tokens per Sec:    14729, Lr: 0.000300\n",
            "2020-01-17 21:05:12,294 Epoch   4 Step:    19000 Batch Loss:     2.271223 Tokens per Sec:    15178, Lr: 0.000300\n",
            "2020-01-17 21:06:00,003 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 21:06:00,003 Saving new checkpoint.\n",
            "2020-01-17 21:06:00,233 Example #0\n",
            "2020-01-17 21:06:00,234 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 21:06:00,234 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:06:00,234 \tHypothesis: Ó jẹ́ Orísun ìyè , Ẹni tí ó bá ń fúnni ní ẹ̀bùn àìlẹ́tọ̀ọ́sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:06:00,234 Example #1\n",
            "2020-01-17 21:06:00,234 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 21:06:00,234 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 21:06:00,234 \tHypothesis: Mo ti wá rí i pé mo ní láti rí iṣẹ́ kan tó ń ṣe .\n",
            "2020-01-17 21:06:00,234 Example #2\n",
            "2020-01-17 21:06:00,234 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 21:06:00,234 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 21:06:00,234 \tHypothesis: Ǹjẹ́ mo ní àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 21:06:00,234 Example #3\n",
            "2020-01-17 21:06:00,235 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 21:06:00,235 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 21:06:00,235 \tHypothesis: Ó ní ìrírí púpọ̀ sí i , ó sì ń fi sùúrù bá ọ lọ , àmọ́ ó ń fi sùúrù bá ọ lọ .\n",
            "2020-01-17 21:06:00,235 Validation result (greedy) at epoch   4, step    19000: bleu:  18.96, loss: 58258.6250, ppl:   7.0675, duration: 47.9408s\n",
            "2020-01-17 21:06:15,511 Epoch   4 Step:    19100 Batch Loss:     2.026001 Tokens per Sec:    15252, Lr: 0.000300\n",
            "2020-01-17 21:06:30,736 Epoch   4 Step:    19200 Batch Loss:     2.328987 Tokens per Sec:    15081, Lr: 0.000300\n",
            "2020-01-17 21:06:46,208 Epoch   4 Step:    19300 Batch Loss:     2.322803 Tokens per Sec:    15097, Lr: 0.000300\n",
            "2020-01-17 21:07:01,492 Epoch   4 Step:    19400 Batch Loss:     2.314528 Tokens per Sec:    15064, Lr: 0.000300\n",
            "2020-01-17 21:07:16,780 Epoch   4 Step:    19500 Batch Loss:     2.028474 Tokens per Sec:    15366, Lr: 0.000300\n",
            "2020-01-17 21:07:32,044 Epoch   4 Step:    19600 Batch Loss:     2.264930 Tokens per Sec:    15430, Lr: 0.000300\n",
            "2020-01-17 21:07:47,524 Epoch   4 Step:    19700 Batch Loss:     2.085554 Tokens per Sec:    14982, Lr: 0.000300\n",
            "2020-01-17 21:08:02,938 Epoch   4 Step:    19800 Batch Loss:     2.129477 Tokens per Sec:    15208, Lr: 0.000300\n",
            "2020-01-17 21:08:18,175 Epoch   4 Step:    19900 Batch Loss:     2.351999 Tokens per Sec:    15172, Lr: 0.000300\n",
            "2020-01-17 21:08:33,348 Epoch   4 Step:    20000 Batch Loss:     2.355159 Tokens per Sec:    14916, Lr: 0.000300\n",
            "2020-01-17 21:09:21,100 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 21:09:21,101 Saving new checkpoint.\n",
            "2020-01-17 21:09:21,344 Example #0\n",
            "2020-01-17 21:09:21,344 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 21:09:21,344 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:09:21,344 \tHypothesis: Ẹni tó bá ń fi ẹ̀mí yàn , Ẹni tó ń fúnni ní ẹ̀bùn àìlẹ́tọ̀ọ́sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:09:21,344 Example #1\n",
            "2020-01-17 21:09:21,345 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 21:09:21,345 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 21:09:21,345 \tHypothesis: Ní báyìí , mo ti wá rí i pé iṣẹ́ tó ń ṣe mí láǹfààní .\n",
            "2020-01-17 21:09:21,345 Example #2\n",
            "2020-01-17 21:09:21,345 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 21:09:21,345 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 21:09:21,345 \tHypothesis: Ǹjẹ́ mo mọyì àwọn nǹkan tara tí mo ní pẹ̀lú Jèhófà àti pẹ̀lú àwọn èèyàn ?\n",
            "2020-01-17 21:09:21,345 Example #3\n",
            "2020-01-17 21:09:21,345 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 21:09:21,345 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 21:09:21,345 \tHypothesis: Ó ti ní ìrírí àti ìwà tó dára ju bó ṣe yẹ lọ , àmọ́ ó ń fi sùúrù rìn jìnnà sí yín .\n",
            "2020-01-17 21:09:21,345 Validation result (greedy) at epoch   4, step    20000: bleu:  19.13, loss: 57677.8125, ppl:   6.9311, duration: 47.9968s\n",
            "2020-01-17 21:09:36,604 Epoch   4 Step:    20100 Batch Loss:     2.031192 Tokens per Sec:    15167, Lr: 0.000300\n",
            "2020-01-17 21:09:51,934 Epoch   4 Step:    20200 Batch Loss:     2.250911 Tokens per Sec:    14954, Lr: 0.000300\n",
            "2020-01-17 21:10:07,211 Epoch   4 Step:    20300 Batch Loss:     2.063007 Tokens per Sec:    15007, Lr: 0.000300\n",
            "2020-01-17 21:10:22,416 Epoch   4 Step:    20400 Batch Loss:     2.087915 Tokens per Sec:    15125, Lr: 0.000300\n",
            "2020-01-17 21:10:37,650 Epoch   4 Step:    20500 Batch Loss:     2.108191 Tokens per Sec:    15033, Lr: 0.000300\n",
            "2020-01-17 21:10:53,109 Epoch   4 Step:    20600 Batch Loss:     2.134251 Tokens per Sec:    15150, Lr: 0.000300\n",
            "2020-01-17 21:11:08,404 Epoch   4 Step:    20700 Batch Loss:     1.963436 Tokens per Sec:    14768, Lr: 0.000300\n",
            "2020-01-17 21:11:23,782 Epoch   4 Step:    20800 Batch Loss:     2.347270 Tokens per Sec:    15207, Lr: 0.000300\n",
            "2020-01-17 21:11:39,147 Epoch   4 Step:    20900 Batch Loss:     2.049126 Tokens per Sec:    15067, Lr: 0.000300\n",
            "2020-01-17 21:11:54,426 Epoch   4 Step:    21000 Batch Loss:     2.425230 Tokens per Sec:    15189, Lr: 0.000300\n",
            "2020-01-17 21:12:42,048 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 21:12:42,048 Saving new checkpoint.\n",
            "2020-01-17 21:12:42,289 Example #0\n",
            "2020-01-17 21:12:42,290 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 21:12:42,290 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:12:42,290 \tHypothesis: Òun ni Orísun ìyè , Ẹni tí ń fúnni ní ẹ̀bùn àìlẹ́tọ̀ọ́sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:12:42,290 Example #1\n",
            "2020-01-17 21:12:42,290 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 21:12:42,290 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 21:12:42,290 \tHypothesis: Mo ti wá rí i pé iṣẹ́ kan tó ń ṣe mí láǹfààní gan - an .\n",
            "2020-01-17 21:12:42,290 Example #2\n",
            "2020-01-17 21:12:42,290 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 21:12:42,290 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 21:12:42,291 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àti pẹ̀lú àwọn èèyàn ?\n",
            "2020-01-17 21:12:42,291 Example #3\n",
            "2020-01-17 21:12:42,291 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 21:12:42,291 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 21:12:42,291 \tHypothesis: Ó ti rí i pé ó ti rí i pé ó ti ṣe ẹ́ , àmọ́ ó máa ń fi sùúrù rìn jìnnà sí ẹ .\n",
            "2020-01-17 21:12:42,291 Validation result (greedy) at epoch   4, step    21000: bleu:  19.99, loss: 57009.6328, ppl:   6.7774, duration: 47.8647s\n",
            "2020-01-17 21:12:43,957 Epoch   4: total training loss 11327.97\n",
            "2020-01-17 21:12:43,957 EPOCH 5\n",
            "2020-01-17 21:12:58,186 Epoch   5 Step:    21100 Batch Loss:     2.382644 Tokens per Sec:    14578, Lr: 0.000300\n",
            "2020-01-17 21:13:13,396 Epoch   5 Step:    21200 Batch Loss:     2.210024 Tokens per Sec:    15109, Lr: 0.000300\n",
            "2020-01-17 21:13:28,562 Epoch   5 Step:    21300 Batch Loss:     1.847348 Tokens per Sec:    15002, Lr: 0.000300\n",
            "2020-01-17 21:13:43,851 Epoch   5 Step:    21400 Batch Loss:     2.109966 Tokens per Sec:    14868, Lr: 0.000300\n",
            "2020-01-17 21:13:59,278 Epoch   5 Step:    21500 Batch Loss:     2.022361 Tokens per Sec:    15428, Lr: 0.000300\n",
            "2020-01-17 21:14:14,310 Epoch   5 Step:    21600 Batch Loss:     2.128957 Tokens per Sec:    14928, Lr: 0.000300\n",
            "2020-01-17 21:14:29,442 Epoch   5 Step:    21700 Batch Loss:     1.958193 Tokens per Sec:    15045, Lr: 0.000300\n",
            "2020-01-17 21:14:44,765 Epoch   5 Step:    21800 Batch Loss:     1.993192 Tokens per Sec:    15153, Lr: 0.000300\n",
            "2020-01-17 21:15:00,022 Epoch   5 Step:    21900 Batch Loss:     2.323786 Tokens per Sec:    15009, Lr: 0.000300\n",
            "2020-01-17 21:15:15,302 Epoch   5 Step:    22000 Batch Loss:     2.150449 Tokens per Sec:    15218, Lr: 0.000300\n",
            "2020-01-17 21:16:02,979 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 21:16:02,980 Saving new checkpoint.\n",
            "2020-01-17 21:16:03,227 Example #0\n",
            "2020-01-17 21:16:03,227 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 21:16:03,227 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:16:03,227 \tHypothesis: Ẹni tó ń fúnni ní ẹ̀bùn àìlẹ́tọ̀ọ́sí Kristi ni Ẹni tí kò lẹ́tọ̀ọ́ láti fi fúnni ní ẹ̀bùn àìlẹ́tọ̀ọ́sí .\n",
            "2020-01-17 21:16:03,228 Example #1\n",
            "2020-01-17 21:16:03,228 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 21:16:03,228 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 21:16:03,228 \tHypothesis: Ní báyìí , mo ti wá rí i pé iṣẹ́ tí mò ń ṣe .\n",
            "2020-01-17 21:16:03,228 Example #2\n",
            "2020-01-17 21:16:03,228 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 21:16:03,228 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 21:16:03,228 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àti pẹ̀lú àwọn èèyàn ?\n",
            "2020-01-17 21:16:03,228 Example #3\n",
            "2020-01-17 21:16:03,229 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 21:16:03,229 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 21:16:03,229 \tHypothesis: Ó ti rí i pé ó ti rí i pé ó ti ń ṣe ẹ́ bíi pé kó máa fi sùúrù rìn jìnnà sí ọ .\n",
            "2020-01-17 21:16:03,229 Validation result (greedy) at epoch   5, step    22000: bleu:  20.17, loss: 56531.6445, ppl:   6.6695, duration: 47.9266s\n",
            "2020-01-17 21:16:18,467 Epoch   5 Step:    22100 Batch Loss:     2.077263 Tokens per Sec:    15077, Lr: 0.000300\n",
            "2020-01-17 21:16:33,788 Epoch   5 Step:    22200 Batch Loss:     2.057514 Tokens per Sec:    14842, Lr: 0.000300\n",
            "2020-01-17 21:16:49,003 Epoch   5 Step:    22300 Batch Loss:     1.987526 Tokens per Sec:    14720, Lr: 0.000300\n",
            "2020-01-17 21:17:04,335 Epoch   5 Step:    22400 Batch Loss:     1.891994 Tokens per Sec:    15212, Lr: 0.000300\n",
            "2020-01-17 21:17:19,486 Epoch   5 Step:    22500 Batch Loss:     1.801630 Tokens per Sec:    14982, Lr: 0.000300\n",
            "2020-01-17 21:17:34,862 Epoch   5 Step:    22600 Batch Loss:     1.890063 Tokens per Sec:    15050, Lr: 0.000300\n",
            "2020-01-17 21:17:50,101 Epoch   5 Step:    22700 Batch Loss:     2.038388 Tokens per Sec:    14902, Lr: 0.000300\n",
            "2020-01-17 21:18:05,422 Epoch   5 Step:    22800 Batch Loss:     2.149551 Tokens per Sec:    15136, Lr: 0.000300\n",
            "2020-01-17 21:18:21,004 Epoch   5 Step:    22900 Batch Loss:     2.394769 Tokens per Sec:    15353, Lr: 0.000300\n",
            "2020-01-17 21:18:36,338 Epoch   5 Step:    23000 Batch Loss:     2.110105 Tokens per Sec:    15075, Lr: 0.000300\n",
            "2020-01-17 21:19:24,011 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 21:19:24,011 Saving new checkpoint.\n",
            "2020-01-17 21:19:24,276 Example #0\n",
            "2020-01-17 21:19:24,276 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 21:19:24,276 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:19:24,276 \tHypothesis: Òun ni Orísun ìyè , Ẹni tí ó fún un ní ẹ̀bùn àìlẹ́tọ̀ọ́sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:19:24,276 Example #1\n",
            "2020-01-17 21:19:24,276 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 21:19:24,276 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 21:19:24,276 \tHypothesis: Ní báyìí , mo ní láti wá iṣẹ́ kan tó ń ṣe .\n",
            "2020-01-17 21:19:24,276 Example #2\n",
            "2020-01-17 21:19:24,277 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 21:19:24,277 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 21:19:24,277 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àti àwọn èèyàn lọ ?\n",
            "2020-01-17 21:19:24,277 Example #3\n",
            "2020-01-17 21:19:24,277 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 21:19:24,277 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 21:19:24,277 \tHypothesis: Ó ti ní ìrírí àti ìwọ̀nba díẹ̀ tó o ní ju bó ṣe yẹ lọ , àmọ́ ó ń fi sùúrù rìn sún mọ́ ọ .\n",
            "2020-01-17 21:19:24,277 Validation result (greedy) at epoch   5, step    23000: bleu:  20.63, loss: 55824.0664, ppl:   6.5130, duration: 47.9390s\n",
            "2020-01-17 21:19:39,643 Epoch   5 Step:    23100 Batch Loss:     2.050174 Tokens per Sec:    15159, Lr: 0.000300\n",
            "2020-01-17 21:19:54,962 Epoch   5 Step:    23200 Batch Loss:     2.036155 Tokens per Sec:    14999, Lr: 0.000300\n",
            "2020-01-17 21:20:10,343 Epoch   5 Step:    23300 Batch Loss:     2.035548 Tokens per Sec:    15042, Lr: 0.000300\n",
            "2020-01-17 21:20:25,672 Epoch   5 Step:    23400 Batch Loss:     1.880965 Tokens per Sec:    15060, Lr: 0.000300\n",
            "2020-01-17 21:20:40,941 Epoch   5 Step:    23500 Batch Loss:     2.170383 Tokens per Sec:    14947, Lr: 0.000300\n",
            "2020-01-17 21:20:56,430 Epoch   5 Step:    23600 Batch Loss:     2.160426 Tokens per Sec:    15136, Lr: 0.000300\n",
            "2020-01-17 21:21:11,730 Epoch   5 Step:    23700 Batch Loss:     2.111062 Tokens per Sec:    15308, Lr: 0.000300\n",
            "2020-01-17 21:21:27,010 Epoch   5 Step:    23800 Batch Loss:     2.172956 Tokens per Sec:    15275, Lr: 0.000300\n",
            "2020-01-17 21:21:42,252 Epoch   5 Step:    23900 Batch Loss:     2.033293 Tokens per Sec:    15006, Lr: 0.000300\n",
            "2020-01-17 21:21:57,573 Epoch   5 Step:    24000 Batch Loss:     2.038156 Tokens per Sec:    15028, Lr: 0.000300\n",
            "2020-01-17 21:22:45,268 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 21:22:45,269 Saving new checkpoint.\n",
            "2020-01-17 21:22:45,503 Example #0\n",
            "2020-01-17 21:22:45,503 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 21:22:45,503 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:22:45,504 \tHypothesis: Òun ni Orísun ìyè , Ẹni tí ó bá ń fúnni ní ẹ̀bùn àìlẹ́tọ̀ọ́sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:22:45,504 Example #1\n",
            "2020-01-17 21:22:45,504 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 21:22:45,504 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 21:22:45,504 \tHypothesis: Ní báyìí , mo ní láti rí i pé iṣẹ́ kan wà tó ń ṣe .\n",
            "2020-01-17 21:22:45,504 Example #2\n",
            "2020-01-17 21:22:45,504 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 21:22:45,504 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 21:22:45,504 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àti àwọn èèyàn lọ ?\n",
            "2020-01-17 21:22:45,504 Example #3\n",
            "2020-01-17 21:22:45,504 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 21:22:45,505 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 21:22:45,505 \tHypothesis: Ó ní ìrírí tó pọ̀ jù lọ , ó sì ń ṣe é , àmọ́ ó ń fi sùúrù rìn sún mọ́ ọ .\n",
            "2020-01-17 21:22:45,505 Validation result (greedy) at epoch   5, step    24000: bleu:  20.72, loss: 55512.0430, ppl:   6.4451, duration: 47.9314s\n",
            "2020-01-17 21:23:00,924 Epoch   5 Step:    24100 Batch Loss:     2.105430 Tokens per Sec:    15326, Lr: 0.000300\n",
            "2020-01-17 21:23:16,297 Epoch   5 Step:    24200 Batch Loss:     2.028932 Tokens per Sec:    14771, Lr: 0.000300\n",
            "2020-01-17 21:23:31,757 Epoch   5 Step:    24300 Batch Loss:     2.073020 Tokens per Sec:    15209, Lr: 0.000300\n",
            "2020-01-17 21:23:47,216 Epoch   5 Step:    24400 Batch Loss:     2.231430 Tokens per Sec:    14912, Lr: 0.000300\n",
            "2020-01-17 21:24:02,404 Epoch   5 Step:    24500 Batch Loss:     1.830759 Tokens per Sec:    14785, Lr: 0.000300\n",
            "2020-01-17 21:24:17,929 Epoch   5 Step:    24600 Batch Loss:     1.856280 Tokens per Sec:    15209, Lr: 0.000300\n",
            "2020-01-17 21:24:33,312 Epoch   5 Step:    24700 Batch Loss:     1.946201 Tokens per Sec:    15186, Lr: 0.000300\n",
            "2020-01-17 21:24:48,672 Epoch   5 Step:    24800 Batch Loss:     1.961888 Tokens per Sec:    15054, Lr: 0.000300\n",
            "2020-01-17 21:25:03,833 Epoch   5 Step:    24900 Batch Loss:     2.158548 Tokens per Sec:    15030, Lr: 0.000300\n",
            "2020-01-17 21:25:19,307 Epoch   5 Step:    25000 Batch Loss:     1.977571 Tokens per Sec:    15150, Lr: 0.000300\n",
            "2020-01-17 21:26:06,968 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 21:26:06,968 Saving new checkpoint.\n",
            "2020-01-17 21:26:07,198 Example #0\n",
            "2020-01-17 21:26:07,198 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 21:26:07,198 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:26:07,198 \tHypothesis: Òun ni Orísun ìyè , Ẹni tí ó bá ń fúnni gẹ́gẹ́ bí ẹ̀bùn àìlẹ́tọ̀ọ́sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:26:07,198 Example #1\n",
            "2020-01-17 21:26:07,198 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 21:26:07,199 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 21:26:07,199 \tHypothesis: Ní báyìí , mo ní láti rí i pé iṣẹ́ tó dáa ni .\n",
            "2020-01-17 21:26:07,199 Example #2\n",
            "2020-01-17 21:26:07,199 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 21:26:07,199 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 21:26:07,199 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àti àwọn èèyàn lọ ?\n",
            "2020-01-17 21:26:07,199 Example #3\n",
            "2020-01-17 21:26:07,199 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 21:26:07,199 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 21:26:07,199 \tHypothesis: Ó ti ní ìrírí tó pọ̀ ju ìwọ lọ , àmọ́ ó máa ń rìn jìnnà sí yín .\n",
            "2020-01-17 21:26:07,199 Validation result (greedy) at epoch   5, step    25000: bleu:  21.10, loss: 54873.3047, ppl:   6.3084, duration: 47.8923s\n",
            "2020-01-17 21:26:22,644 Epoch   5 Step:    25100 Batch Loss:     2.142540 Tokens per Sec:    15142, Lr: 0.000300\n",
            "2020-01-17 21:26:38,085 Epoch   5 Step:    25200 Batch Loss:     1.961540 Tokens per Sec:    15222, Lr: 0.000300\n",
            "2020-01-17 21:26:53,225 Epoch   5 Step:    25300 Batch Loss:     1.960820 Tokens per Sec:    14852, Lr: 0.000300\n",
            "2020-01-17 21:27:08,552 Epoch   5 Step:    25400 Batch Loss:     2.151433 Tokens per Sec:    15215, Lr: 0.000300\n",
            "2020-01-17 21:27:23,859 Epoch   5 Step:    25500 Batch Loss:     1.892877 Tokens per Sec:    15139, Lr: 0.000300\n",
            "2020-01-17 21:27:39,195 Epoch   5 Step:    25600 Batch Loss:     2.094628 Tokens per Sec:    15294, Lr: 0.000300\n",
            "2020-01-17 21:27:54,424 Epoch   5 Step:    25700 Batch Loss:     1.823428 Tokens per Sec:    14912, Lr: 0.000300\n",
            "2020-01-17 21:28:09,811 Epoch   5 Step:    25800 Batch Loss:     1.854149 Tokens per Sec:    15336, Lr: 0.000300\n",
            "2020-01-17 21:28:25,113 Epoch   5 Step:    25900 Batch Loss:     2.111332 Tokens per Sec:    14934, Lr: 0.000300\n",
            "2020-01-17 21:28:40,477 Epoch   5 Step:    26000 Batch Loss:     1.899824 Tokens per Sec:    15158, Lr: 0.000300\n",
            "2020-01-17 21:29:28,013 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 21:29:28,013 Saving new checkpoint.\n",
            "2020-01-17 21:29:28,245 Example #0\n",
            "2020-01-17 21:29:28,246 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 21:29:28,246 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:29:28,246 \tHypothesis: Ó jẹ́ Orísun ìyè , Ẹni tí ń fúnni ní ẹ̀bùn àìlẹ́tọ̀ọ́sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:29:28,246 Example #1\n",
            "2020-01-17 21:29:28,246 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 21:29:28,246 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 21:29:28,246 \tHypothesis: Mo wá rí i pé iṣẹ́ kan tó ń ṣe mí láǹfààní .\n",
            "2020-01-17 21:29:28,246 Example #2\n",
            "2020-01-17 21:29:28,246 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 21:29:28,246 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 21:29:28,246 \tHypothesis: Ǹjẹ́ mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àti àwọn èèyàn lọ ?\n",
            "2020-01-17 21:29:28,246 Example #3\n",
            "2020-01-17 21:29:28,247 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 21:29:28,247 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 21:29:28,247 \tHypothesis: Ó ti ní ìrírí tó pọ̀ jù lọ , ó sì ní sùúrù ju bó ṣe yẹ lọ , àmọ́ ó ń fi sùúrù rìn jìnnà sí yín .\n",
            "2020-01-17 21:29:28,247 Validation result (greedy) at epoch   5, step    26000: bleu:  20.87, loss: 54459.9844, ppl:   6.2215, duration: 47.7693s\n",
            "2020-01-17 21:29:43,724 Epoch   5 Step:    26100 Batch Loss:     2.011353 Tokens per Sec:    15199, Lr: 0.000300\n",
            "2020-01-17 21:29:59,113 Epoch   5 Step:    26200 Batch Loss:     2.001849 Tokens per Sec:    15251, Lr: 0.000300\n",
            "2020-01-17 21:30:08,508 Epoch   5: total training loss 10738.43\n",
            "2020-01-17 21:30:08,508 EPOCH 6\n",
            "2020-01-17 21:30:14,866 Epoch   6 Step:    26300 Batch Loss:     1.864031 Tokens per Sec:    13920, Lr: 0.000300\n",
            "2020-01-17 21:30:30,139 Epoch   6 Step:    26400 Batch Loss:     1.975502 Tokens per Sec:    15322, Lr: 0.000300\n",
            "2020-01-17 21:30:45,449 Epoch   6 Step:    26500 Batch Loss:     1.889332 Tokens per Sec:    15084, Lr: 0.000300\n",
            "2020-01-17 21:31:00,834 Epoch   6 Step:    26600 Batch Loss:     2.022588 Tokens per Sec:    15327, Lr: 0.000300\n",
            "2020-01-17 21:31:16,202 Epoch   6 Step:    26700 Batch Loss:     1.769282 Tokens per Sec:    15330, Lr: 0.000300\n",
            "2020-01-17 21:31:31,462 Epoch   6 Step:    26800 Batch Loss:     1.935696 Tokens per Sec:    15196, Lr: 0.000300\n",
            "2020-01-17 21:31:46,755 Epoch   6 Step:    26900 Batch Loss:     2.048031 Tokens per Sec:    15044, Lr: 0.000300\n",
            "2020-01-17 21:32:02,000 Epoch   6 Step:    27000 Batch Loss:     1.989980 Tokens per Sec:    14989, Lr: 0.000300\n",
            "2020-01-17 21:32:49,680 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 21:32:49,680 Saving new checkpoint.\n",
            "2020-01-17 21:32:49,936 Example #0\n",
            "2020-01-17 21:32:49,936 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 21:32:49,937 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:32:49,937 \tHypothesis: Òun ni Orísun ìyè , Ẹni tí ó jẹ́ ẹ̀bùn àìlẹ́tọ̀ọ́sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:32:49,937 Example #1\n",
            "2020-01-17 21:32:49,937 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 21:32:49,937 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 21:32:49,937 \tHypothesis: Mo wá rí i pé iṣẹ́ tó ń ṣe mí láǹfààní gan - an nìyẹn .\n",
            "2020-01-17 21:32:49,937 Example #2\n",
            "2020-01-17 21:32:49,937 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 21:32:49,937 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 21:32:49,937 \tHypothesis: Ǹjẹ́ mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn lọ ?\n",
            "2020-01-17 21:32:49,937 Example #3\n",
            "2020-01-17 21:32:49,937 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 21:32:49,938 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 21:32:49,938 \tHypothesis: Ó ní ìrírí àti ìwọ̀nba díẹ̀ tó o ní , àmọ́ ó ń fi sùúrù bá ọ rìn .\n",
            "2020-01-17 21:32:49,938 Validation result (greedy) at epoch   6, step    27000: bleu:  21.68, loss: 54111.0195, ppl:   6.1490, duration: 47.9371s\n",
            "2020-01-17 21:33:05,085 Epoch   6 Step:    27100 Batch Loss:     2.031852 Tokens per Sec:    15128, Lr: 0.000300\n",
            "2020-01-17 21:33:20,330 Epoch   6 Step:    27200 Batch Loss:     1.934251 Tokens per Sec:    15129, Lr: 0.000300\n",
            "2020-01-17 21:33:35,563 Epoch   6 Step:    27300 Batch Loss:     1.995030 Tokens per Sec:    14804, Lr: 0.000300\n",
            "2020-01-17 21:33:50,904 Epoch   6 Step:    27400 Batch Loss:     2.112202 Tokens per Sec:    14984, Lr: 0.000300\n",
            "2020-01-17 21:34:06,208 Epoch   6 Step:    27500 Batch Loss:     2.023821 Tokens per Sec:    15253, Lr: 0.000300\n",
            "2020-01-17 21:34:21,404 Epoch   6 Step:    27600 Batch Loss:     2.008334 Tokens per Sec:    15026, Lr: 0.000300\n",
            "2020-01-17 21:34:36,685 Epoch   6 Step:    27700 Batch Loss:     1.755723 Tokens per Sec:    15067, Lr: 0.000300\n",
            "2020-01-17 21:34:51,905 Epoch   6 Step:    27800 Batch Loss:     1.993693 Tokens per Sec:    14890, Lr: 0.000300\n",
            "2020-01-17 21:35:07,157 Epoch   6 Step:    27900 Batch Loss:     2.093545 Tokens per Sec:    15294, Lr: 0.000300\n",
            "2020-01-17 21:35:22,577 Epoch   6 Step:    28000 Batch Loss:     2.019815 Tokens per Sec:    15337, Lr: 0.000300\n",
            "2020-01-17 21:36:10,354 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 21:36:10,354 Saving new checkpoint.\n",
            "2020-01-17 21:36:10,591 Example #0\n",
            "2020-01-17 21:36:10,591 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 21:36:10,591 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:36:10,591 \tHypothesis: Òun ni Orísun ìyè , Ẹni tó ń fúnni gẹ́gẹ́ bí ẹ̀bùn àìlẹ́tọ̀ọ́sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:36:10,591 Example #1\n",
            "2020-01-17 21:36:10,591 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 21:36:10,591 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 21:36:10,591 \tHypothesis: Mo wá rí i pé mo ní láti wá iṣẹ́ kan tó yẹ kí n ṣe .\n",
            "2020-01-17 21:36:10,591 Example #2\n",
            "2020-01-17 21:36:10,591 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 21:36:10,592 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 21:36:10,592 \tHypothesis: Ǹjẹ́ mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àti àwọn èèyàn lọ ?\n",
            "2020-01-17 21:36:10,592 Example #3\n",
            "2020-01-17 21:36:10,592 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 21:36:10,592 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 21:36:10,592 \tHypothesis: Ó ní ìrírí tó pọ̀ jù lọ , ó sì ní sùúrù ju bó ṣe yẹ lọ , àmọ́ ó máa ń fi sùúrù rìn jìnnà sí ọ .\n",
            "2020-01-17 21:36:10,592 Validation result (greedy) at epoch   6, step    28000: bleu:  21.70, loss: 53726.8438, ppl:   6.0702, duration: 48.0143s\n",
            "2020-01-17 21:36:25,862 Epoch   6 Step:    28100 Batch Loss:     2.072907 Tokens per Sec:    15234, Lr: 0.000300\n",
            "2020-01-17 21:36:41,204 Epoch   6 Step:    28200 Batch Loss:     2.240888 Tokens per Sec:    14966, Lr: 0.000300\n",
            "2020-01-17 21:36:56,281 Epoch   6 Step:    28300 Batch Loss:     2.010086 Tokens per Sec:    14712, Lr: 0.000300\n",
            "2020-01-17 21:37:11,609 Epoch   6 Step:    28400 Batch Loss:     2.152720 Tokens per Sec:    14870, Lr: 0.000300\n",
            "2020-01-17 21:37:26,871 Epoch   6 Step:    28500 Batch Loss:     1.785093 Tokens per Sec:    14978, Lr: 0.000300\n",
            "2020-01-17 21:37:42,244 Epoch   6 Step:    28600 Batch Loss:     1.939427 Tokens per Sec:    15158, Lr: 0.000300\n",
            "2020-01-17 21:37:57,453 Epoch   6 Step:    28700 Batch Loss:     1.964912 Tokens per Sec:    15302, Lr: 0.000300\n",
            "2020-01-17 21:38:12,851 Epoch   6 Step:    28800 Batch Loss:     1.975823 Tokens per Sec:    15278, Lr: 0.000300\n",
            "2020-01-17 21:38:28,017 Epoch   6 Step:    28900 Batch Loss:     2.132960 Tokens per Sec:    15273, Lr: 0.000300\n",
            "2020-01-17 21:38:43,179 Epoch   6 Step:    29000 Batch Loss:     1.919477 Tokens per Sec:    14689, Lr: 0.000300\n",
            "2020-01-17 21:39:30,887 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 21:39:30,887 Saving new checkpoint.\n",
            "2020-01-17 21:39:31,124 Example #0\n",
            "2020-01-17 21:39:31,124 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 21:39:31,124 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:39:31,124 \tHypothesis: Òun ni Orísun ìyè , Ẹni tí ó fi ẹ̀bùn àìlẹ́tọ̀ọ́sí hàn nípasẹ̀ Kristi .\n",
            "2020-01-17 21:39:31,124 Example #1\n",
            "2020-01-17 21:39:31,124 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 21:39:31,124 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 21:39:31,125 \tHypothesis: Ní báyìí , mo ti wá rí i pé iṣẹ́ tó ń ṣe mí láǹfààní .\n",
            "2020-01-17 21:39:31,125 Example #2\n",
            "2020-01-17 21:39:31,125 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 21:39:31,125 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 21:39:31,125 \tHypothesis: Ǹjẹ́ mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn lọ ?\n",
            "2020-01-17 21:39:31,125 Example #3\n",
            "2020-01-17 21:39:31,125 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 21:39:31,125 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 21:39:31,125 \tHypothesis: Ó ní ìrírí tó pọ̀ jù lọ , ó sì ń fi sùúrù rìn jìnnà sí ọ .\n",
            "2020-01-17 21:39:31,125 Validation result (greedy) at epoch   6, step    29000: bleu:  21.91, loss: 53420.6719, ppl:   6.0082, duration: 47.9453s\n",
            "2020-01-17 21:39:46,687 Epoch   6 Step:    29100 Batch Loss:     1.835925 Tokens per Sec:    15467, Lr: 0.000300\n",
            "2020-01-17 21:40:01,681 Epoch   6 Step:    29200 Batch Loss:     1.977145 Tokens per Sec:    15060, Lr: 0.000300\n",
            "2020-01-17 21:40:17,011 Epoch   6 Step:    29300 Batch Loss:     1.970838 Tokens per Sec:    15300, Lr: 0.000300\n",
            "2020-01-17 21:40:32,238 Epoch   6 Step:    29400 Batch Loss:     1.929649 Tokens per Sec:    14851, Lr: 0.000300\n",
            "2020-01-17 21:40:47,658 Epoch   6 Step:    29500 Batch Loss:     2.047502 Tokens per Sec:    15253, Lr: 0.000300\n",
            "2020-01-17 21:41:02,862 Epoch   6 Step:    29600 Batch Loss:     2.004019 Tokens per Sec:    15330, Lr: 0.000300\n",
            "2020-01-17 21:41:18,263 Epoch   6 Step:    29700 Batch Loss:     1.982833 Tokens per Sec:    15242, Lr: 0.000300\n",
            "2020-01-17 21:41:33,582 Epoch   6 Step:    29800 Batch Loss:     2.203838 Tokens per Sec:    15406, Lr: 0.000300\n",
            "2020-01-17 21:41:48,672 Epoch   6 Step:    29900 Batch Loss:     1.997133 Tokens per Sec:    14835, Lr: 0.000300\n",
            "2020-01-17 21:42:04,069 Epoch   6 Step:    30000 Batch Loss:     2.050083 Tokens per Sec:    15104, Lr: 0.000300\n",
            "2020-01-17 21:42:51,738 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 21:42:51,738 Saving new checkpoint.\n",
            "2020-01-17 21:42:52,048 Example #0\n",
            "2020-01-17 21:42:52,048 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 21:42:52,048 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:42:52,048 \tHypothesis: Ó jẹ́ Orísun ìyè , Ẹni tó ń fúnni gẹ́gẹ́ bí ẹ̀bùn àìlẹ́tọ̀ọ́sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:42:52,049 Example #1\n",
            "2020-01-17 21:42:52,049 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 21:42:52,049 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 21:42:52,049 \tHypothesis: Ní báyìí , mo ní láti wá iṣẹ́ kan tó máa ń ṣe .\n",
            "2020-01-17 21:42:52,049 Example #2\n",
            "2020-01-17 21:42:52,049 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 21:42:52,049 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 21:42:52,049 \tHypothesis: Ǹjẹ́ mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn lọ ?\n",
            "2020-01-17 21:42:52,049 Example #3\n",
            "2020-01-17 21:42:52,049 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 21:42:52,049 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 21:42:52,049 \tHypothesis: Ó ní ìrírí tó pọ̀ jù lọ , ó sì ń ṣe é , àmọ́ ó ń fi sùúrù rìn nítòsí rẹ .\n",
            "2020-01-17 21:42:52,050 Validation result (greedy) at epoch   6, step    30000: bleu:  21.92, loss: 53018.1250, ppl:   5.9275, duration: 47.9807s\n",
            "2020-01-17 21:43:07,465 Epoch   6 Step:    30100 Batch Loss:     1.971674 Tokens per Sec:    15171, Lr: 0.000300\n",
            "2020-01-17 21:43:22,599 Epoch   6 Step:    30200 Batch Loss:     2.115822 Tokens per Sec:    15092, Lr: 0.000300\n",
            "2020-01-17 21:43:38,018 Epoch   6 Step:    30300 Batch Loss:     2.006145 Tokens per Sec:    15228, Lr: 0.000300\n",
            "2020-01-17 21:43:53,149 Epoch   6 Step:    30400 Batch Loss:     2.394602 Tokens per Sec:    15069, Lr: 0.000300\n",
            "2020-01-17 21:44:08,449 Epoch   6 Step:    30500 Batch Loss:     1.831698 Tokens per Sec:    14986, Lr: 0.000300\n",
            "2020-01-17 21:44:23,630 Epoch   6 Step:    30600 Batch Loss:     1.892599 Tokens per Sec:    15001, Lr: 0.000300\n",
            "2020-01-17 21:44:38,944 Epoch   6 Step:    30700 Batch Loss:     1.896599 Tokens per Sec:    15001, Lr: 0.000300\n",
            "2020-01-17 21:44:54,120 Epoch   6 Step:    30800 Batch Loss:     2.029240 Tokens per Sec:    15280, Lr: 0.000300\n",
            "2020-01-17 21:45:09,282 Epoch   6 Step:    30900 Batch Loss:     1.969159 Tokens per Sec:    15101, Lr: 0.000300\n",
            "2020-01-17 21:45:24,590 Epoch   6 Step:    31000 Batch Loss:     1.972296 Tokens per Sec:    15206, Lr: 0.000300\n",
            "2020-01-17 21:46:12,150 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 21:46:12,150 Saving new checkpoint.\n",
            "2020-01-17 21:46:12,399 Example #0\n",
            "2020-01-17 21:46:12,399 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 21:46:12,399 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:46:12,399 \tHypothesis: Òun ni Orísun ìyè , Ẹni tí ó ń fúnni gẹ́gẹ́ bí ẹ̀bùn àìlẹ́tọ̀ọ́sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:46:12,399 Example #1\n",
            "2020-01-17 21:46:12,399 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 21:46:12,399 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 21:46:12,400 \tHypothesis: Ní báyìí , mo ti wá rí i pé iṣẹ́ tó ń ṣe mí láǹfààní .\n",
            "2020-01-17 21:46:12,400 Example #2\n",
            "2020-01-17 21:46:12,400 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 21:46:12,400 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 21:46:12,400 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àti àwọn èèyàn lọ ?\n",
            "2020-01-17 21:46:12,400 Example #3\n",
            "2020-01-17 21:46:12,400 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 21:46:12,400 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 21:46:12,400 \tHypothesis: Ó ní ìrírí tó pọ̀ jù lọ , ó sì ní sùúrù ju bó ṣe yẹ lọ , àmọ́ ó ń fi sùúrù rìn jìnnà sí ẹ .\n",
            "2020-01-17 21:46:12,400 Validation result (greedy) at epoch   6, step    31000: bleu:  22.32, loss: 52623.3828, ppl:   5.8495, duration: 47.8105s\n",
            "2020-01-17 21:46:27,674 Epoch   6 Step:    31100 Batch Loss:     1.841514 Tokens per Sec:    15174, Lr: 0.000300\n",
            "2020-01-17 21:46:42,961 Epoch   6 Step:    31200 Batch Loss:     1.864093 Tokens per Sec:    15133, Lr: 0.000300\n",
            "2020-01-17 21:46:58,125 Epoch   6 Step:    31300 Batch Loss:     1.816182 Tokens per Sec:    15152, Lr: 0.000300\n",
            "2020-01-17 21:47:13,503 Epoch   6 Step:    31400 Batch Loss:     1.938735 Tokens per Sec:    15029, Lr: 0.000300\n",
            "2020-01-17 21:47:28,771 Epoch   6 Step:    31500 Batch Loss:     1.904811 Tokens per Sec:    15282, Lr: 0.000300\n",
            "2020-01-17 21:47:31,237 Epoch   6: total training loss 10327.66\n",
            "2020-01-17 21:47:31,238 EPOCH 7\n",
            "2020-01-17 21:47:44,514 Epoch   7 Step:    31600 Batch Loss:     2.232427 Tokens per Sec:    14326, Lr: 0.000300\n",
            "2020-01-17 21:48:00,004 Epoch   7 Step:    31700 Batch Loss:     1.743986 Tokens per Sec:    15441, Lr: 0.000300\n",
            "2020-01-17 21:48:15,263 Epoch   7 Step:    31800 Batch Loss:     1.710633 Tokens per Sec:    15291, Lr: 0.000300\n",
            "2020-01-17 21:48:30,464 Epoch   7 Step:    31900 Batch Loss:     1.927900 Tokens per Sec:    14859, Lr: 0.000300\n",
            "2020-01-17 21:48:45,724 Epoch   7 Step:    32000 Batch Loss:     2.257879 Tokens per Sec:    14955, Lr: 0.000300\n",
            "2020-01-17 21:49:33,681 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 21:49:33,681 Saving new checkpoint.\n",
            "2020-01-17 21:49:33,980 Example #0\n",
            "2020-01-17 21:49:33,981 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 21:49:33,981 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:49:33,981 \tHypothesis: Òun ni Orísun ìyè , Ẹni tí ó fún un ní ẹ̀bùn tí kò lẹ́tọ̀ọ́ láti ọ̀dọ̀ Kristi .\n",
            "2020-01-17 21:49:33,981 Example #1\n",
            "2020-01-17 21:49:33,981 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 21:49:33,981 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 21:49:33,981 \tHypothesis: Ní báyìí , mo ní láti wá iṣẹ́ kan tó máa jẹ́ kí n lè máa ṣe .\n",
            "2020-01-17 21:49:33,981 Example #2\n",
            "2020-01-17 21:49:33,981 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 21:49:33,981 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 21:49:33,982 \tHypothesis: Ǹjẹ́ mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àti àwọn èèyàn lọ ?\n",
            "2020-01-17 21:49:33,982 Example #3\n",
            "2020-01-17 21:49:33,982 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 21:49:33,982 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 21:49:33,982 \tHypothesis: Ó ní ìrírí tó pọ̀ jù lọ , ó sì ti ṣe ẹ́ , àmọ́ ó máa ń fi sùúrù rìn jìnnà sí ẹ .\n",
            "2020-01-17 21:49:33,982 Validation result (greedy) at epoch   7, step    32000: bleu:  22.26, loss: 52473.7188, ppl:   5.8202, duration: 48.2580s\n",
            "2020-01-17 21:49:49,230 Epoch   7 Step:    32100 Batch Loss:     1.661834 Tokens per Sec:    15003, Lr: 0.000300\n",
            "2020-01-17 21:50:04,608 Epoch   7 Step:    32200 Batch Loss:     1.888324 Tokens per Sec:    15396, Lr: 0.000300\n",
            "2020-01-17 21:50:19,908 Epoch   7 Step:    32300 Batch Loss:     2.006183 Tokens per Sec:    15051, Lr: 0.000300\n",
            "2020-01-17 21:50:35,090 Epoch   7 Step:    32400 Batch Loss:     1.721595 Tokens per Sec:    14939, Lr: 0.000300\n",
            "2020-01-17 21:50:50,515 Epoch   7 Step:    32500 Batch Loss:     1.611527 Tokens per Sec:    15182, Lr: 0.000300\n",
            "2020-01-17 21:51:05,549 Epoch   7 Step:    32600 Batch Loss:     1.835779 Tokens per Sec:    14916, Lr: 0.000300\n",
            "2020-01-17 21:51:20,926 Epoch   7 Step:    32700 Batch Loss:     2.115950 Tokens per Sec:    15274, Lr: 0.000300\n",
            "2020-01-17 21:51:36,126 Epoch   7 Step:    32800 Batch Loss:     1.839787 Tokens per Sec:    15123, Lr: 0.000300\n",
            "2020-01-17 21:51:51,302 Epoch   7 Step:    32900 Batch Loss:     2.123595 Tokens per Sec:    15005, Lr: 0.000300\n",
            "2020-01-17 21:52:06,612 Epoch   7 Step:    33000 Batch Loss:     1.949946 Tokens per Sec:    15224, Lr: 0.000300\n",
            "2020-01-17 21:52:54,136 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 21:52:54,136 Saving new checkpoint.\n",
            "2020-01-17 21:52:54,398 Example #0\n",
            "2020-01-17 21:52:54,398 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 21:52:54,398 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:52:54,398 \tHypothesis: Òun ni Orísun ìyè , Ẹni tí ń fúnni ní ẹ̀bùn àìlẹ́tọ̀ọ́sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:52:54,398 Example #1\n",
            "2020-01-17 21:52:54,398 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 21:52:54,398 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 21:52:54,398 \tHypothesis: Ní báyìí , mo ní láti rí i pé iṣẹ́ kan wà tó yẹ kí n ṣe .\n",
            "2020-01-17 21:52:54,399 Example #2\n",
            "2020-01-17 21:52:54,399 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 21:52:54,399 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 21:52:54,399 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àti pẹ̀lú àwọn èèyàn lọ ?\n",
            "2020-01-17 21:52:54,399 Example #3\n",
            "2020-01-17 21:52:54,399 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 21:52:54,399 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 21:52:54,399 \tHypothesis: Ó ní ìrírí tó pọ̀ ju bó ṣe yẹ lọ , àmọ́ ó máa ń fi sùúrù rìn jìnnà sí ọ .\n",
            "2020-01-17 21:52:54,399 Validation result (greedy) at epoch   7, step    33000: bleu:  22.45, loss: 52151.9023, ppl:   5.7577, duration: 47.7872s\n",
            "2020-01-17 21:53:09,770 Epoch   7 Step:    33100 Batch Loss:     2.017200 Tokens per Sec:    15127, Lr: 0.000300\n",
            "2020-01-17 21:53:24,821 Epoch   7 Step:    33200 Batch Loss:     1.866260 Tokens per Sec:    15230, Lr: 0.000300\n",
            "2020-01-17 21:53:40,179 Epoch   7 Step:    33300 Batch Loss:     1.929390 Tokens per Sec:    15124, Lr: 0.000300\n",
            "2020-01-17 21:53:55,452 Epoch   7 Step:    33400 Batch Loss:     1.827623 Tokens per Sec:    15107, Lr: 0.000300\n",
            "2020-01-17 21:54:10,634 Epoch   7 Step:    33500 Batch Loss:     2.060180 Tokens per Sec:    15064, Lr: 0.000300\n",
            "2020-01-17 21:54:26,041 Epoch   7 Step:    33600 Batch Loss:     2.028372 Tokens per Sec:    15109, Lr: 0.000300\n",
            "2020-01-17 21:54:41,402 Epoch   7 Step:    33700 Batch Loss:     1.987738 Tokens per Sec:    15197, Lr: 0.000300\n",
            "2020-01-17 21:54:56,549 Epoch   7 Step:    33800 Batch Loss:     2.087192 Tokens per Sec:    14870, Lr: 0.000300\n",
            "2020-01-17 21:55:11,877 Epoch   7 Step:    33900 Batch Loss:     1.856023 Tokens per Sec:    15279, Lr: 0.000300\n",
            "2020-01-17 21:55:27,096 Epoch   7 Step:    34000 Batch Loss:     1.735946 Tokens per Sec:    15093, Lr: 0.000300\n",
            "2020-01-17 21:56:14,711 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 21:56:14,711 Saving new checkpoint.\n",
            "2020-01-17 21:56:14,969 Example #0\n",
            "2020-01-17 21:56:14,970 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 21:56:14,970 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:56:14,970 \tHypothesis: Òun ni Orísun ìyè , Ẹni tó ń fúnni ní ẹ̀bùn àìlẹ́tọ̀ọ́sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:56:14,970 Example #1\n",
            "2020-01-17 21:56:14,970 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 21:56:14,970 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 21:56:14,970 \tHypothesis: Ní báyìí , mo ti wá rí i pé iṣẹ́ tó ń ṣe mí láǹfààní .\n",
            "2020-01-17 21:56:14,970 Example #2\n",
            "2020-01-17 21:56:14,970 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 21:56:14,970 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 21:56:14,970 \tHypothesis: Ǹjẹ́ mo mọyì ohun ìní tara ju àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn lọ ?\n",
            "2020-01-17 21:56:14,970 Example #3\n",
            "2020-01-17 21:56:14,971 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 21:56:14,971 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 21:56:14,971 \tHypothesis: Ó ní ìrírí tó pọ̀ jù , ó sì ti ṣe é ju bó ṣe yẹ lọ , àmọ́ ó máa ń rìn jìnnà sí ọ .\n",
            "2020-01-17 21:56:14,971 Validation result (greedy) at epoch   7, step    34000: bleu:  23.14, loss: 52074.4297, ppl:   5.7427, duration: 47.8743s\n",
            "2020-01-17 21:56:30,106 Epoch   7 Step:    34100 Batch Loss:     1.697479 Tokens per Sec:    14934, Lr: 0.000300\n",
            "2020-01-17 21:56:45,522 Epoch   7 Step:    34200 Batch Loss:     1.891679 Tokens per Sec:    15027, Lr: 0.000300\n",
            "2020-01-17 21:57:00,625 Epoch   7 Step:    34300 Batch Loss:     1.827127 Tokens per Sec:    14991, Lr: 0.000300\n",
            "2020-01-17 21:57:15,877 Epoch   7 Step:    34400 Batch Loss:     1.811329 Tokens per Sec:    15248, Lr: 0.000300\n",
            "2020-01-17 21:57:31,200 Epoch   7 Step:    34500 Batch Loss:     1.926429 Tokens per Sec:    15369, Lr: 0.000300\n",
            "2020-01-17 21:57:46,542 Epoch   7 Step:    34600 Batch Loss:     1.806683 Tokens per Sec:    15226, Lr: 0.000300\n",
            "2020-01-17 21:58:01,768 Epoch   7 Step:    34700 Batch Loss:     1.871457 Tokens per Sec:    15277, Lr: 0.000300\n",
            "2020-01-17 21:58:17,018 Epoch   7 Step:    34800 Batch Loss:     1.859363 Tokens per Sec:    15081, Lr: 0.000300\n",
            "2020-01-17 21:58:32,341 Epoch   7 Step:    34900 Batch Loss:     1.992869 Tokens per Sec:    15201, Lr: 0.000300\n",
            "2020-01-17 21:58:47,620 Epoch   7 Step:    35000 Batch Loss:     2.079259 Tokens per Sec:    15063, Lr: 0.000300\n",
            "2020-01-17 21:59:35,475 Hooray! New best validation result [ppl]!\n",
            "2020-01-17 21:59:35,476 Saving new checkpoint.\n",
            "2020-01-17 21:59:35,739 Example #0\n",
            "2020-01-17 21:59:35,739 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 21:59:35,739 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 21:59:35,739 \tHypothesis: Òun ni Orísun ìyè , Ẹni tí ó fi ẹ̀bùn àìlẹ́tọ̀ọ́sí hàn nípasẹ̀ Kristi .\n",
            "2020-01-17 21:59:35,739 Example #1\n",
            "2020-01-17 21:59:35,739 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 21:59:35,740 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 21:59:35,740 \tHypothesis: Ní báyìí , mo ti wá rí i pé iṣẹ́ kan wà tó yẹ kí n ṣe .\n",
            "2020-01-17 21:59:35,740 Example #2\n",
            "2020-01-17 21:59:35,740 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 21:59:35,740 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 21:59:35,740 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara tí mo ní pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 21:59:35,740 Example #3\n",
            "2020-01-17 21:59:35,741 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 21:59:35,741 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 21:59:35,741 \tHypothesis: Ó ní ìrírí tó pọ̀ jù , ó sì ti ṣe é , àmọ́ ó ń fi sùúrù rìn jìnnà sí ọ .\n",
            "2020-01-17 21:59:35,741 Validation result (greedy) at epoch   7, step    35000: bleu:  23.59, loss: 51513.8008, ppl:   5.6357, duration: 48.1201s\n",
            "2020-01-17 21:59:51,137 Epoch   7 Step:    35100 Batch Loss:     1.943792 Tokens per Sec:    15248, Lr: 0.000300\n",
            "2020-01-17 22:00:06,489 Epoch   7 Step:    35200 Batch Loss:     1.902552 Tokens per Sec:    15330, Lr: 0.000300\n",
            "2020-01-17 22:00:21,787 Epoch   7 Step:    35300 Batch Loss:     1.708821 Tokens per Sec:    15238, Lr: 0.000300\n",
            "2020-01-17 22:00:36,953 Epoch   7 Step:    35400 Batch Loss:     2.100375 Tokens per Sec:    15007, Lr: 0.000300\n",
            "2020-01-17 22:00:52,336 Epoch   7 Step:    35500 Batch Loss:     1.836920 Tokens per Sec:    14983, Lr: 0.000300\n",
            "2020-01-17 22:01:07,820 Epoch   7 Step:    35600 Batch Loss:     1.887438 Tokens per Sec:    15651, Lr: 0.000300\n",
            "2020-01-17 22:01:23,177 Epoch   7 Step:    35700 Batch Loss:     2.103233 Tokens per Sec:    15083, Lr: 0.000300\n",
            "2020-01-17 22:01:38,303 Epoch   7 Step:    35800 Batch Loss:     1.865384 Tokens per Sec:    15021, Lr: 0.000300\n",
            "2020-01-17 22:01:53,678 Epoch   7 Step:    35900 Batch Loss:     1.899968 Tokens per Sec:    15199, Lr: 0.000300\n",
            "2020-01-17 22:02:09,019 Epoch   7 Step:    36000 Batch Loss:     1.929623 Tokens per Sec:    15084, Lr: 0.000300\n",
            "2020-01-17 22:02:56,774 Example #0\n",
            "2020-01-17 22:02:56,774 \tSource:     He is the Source of life , the One giving it as an undeserved gift through Christ .\n",
            "2020-01-17 22:02:56,774 \tReference:  Òun ni Orísun ìyè , Ẹni tí ń fi ìyè fúnni gẹ́gẹ́ bí ẹbùn tí a kò lẹ́tọ̀ọ́ sí nípasẹ̀ Kristi .\n",
            "2020-01-17 22:02:56,774 \tHypothesis: Òun ni Orísun ìyè , Ẹni tó ń fúnni ní ẹ̀bùn àìlẹ́tọ̀ọ́sí nípasẹ̀ Kristi .\n",
            "2020-01-17 22:02:56,774 Example #1\n",
            "2020-01-17 22:02:56,774 \tSource:     Now I had to find a legitimate line of work .\n",
            "2020-01-17 22:02:56,774 \tReference:  Torí náà , mo ní láti wá iṣẹ́ gidi .\n",
            "2020-01-17 22:02:56,774 \tHypothesis: Mo wá rí i pé iṣẹ́ tó ń lọ lọ́wọ́ mi ti pọ̀ jù .\n",
            "2020-01-17 22:02:56,775 Example #2\n",
            "2020-01-17 22:02:56,775 \tSource:     Do I value material things more than my relationship with Jehovah and with people ?\n",
            "2020-01-17 22:02:56,775 \tReference:  Ṣé àwọn nǹkan tara ló jẹ mí lógún jù àbí àjọṣe mi pẹ̀lú Jèhófà àtàwọn èèyàn ?\n",
            "2020-01-17 22:02:56,775 \tHypothesis: Ṣé mo mọyì àwọn nǹkan tara ju àjọṣe mi pẹ̀lú Jèhófà àti àwọn èèyàn lọ ?\n",
            "2020-01-17 22:02:56,775 Example #3\n",
            "2020-01-17 22:02:56,775 \tSource:     He has far more experience and stamina than you do , but he patiently walks near you .\n",
            "2020-01-17 22:02:56,775 \tReference:  Ẹni tẹ́ ẹ jọ ń lọ yìí mọ ọ̀nà yẹn dáadáa .\n",
            "2020-01-17 22:02:56,775 \tHypothesis: Ó ní ìrírí tó pọ̀ jù lọ , ó sì ń fi sùúrù rìn sún mọ́ ọ .\n",
            "2020-01-17 22:02:56,775 Validation result (greedy) at epoch   7, step    36000: bleu:  22.73, loss: 51559.8203, ppl:   5.6444, duration: 47.7560s\n",
            "2020-01-17 22:03:12,079 Epoch   7 Step:    36100 Batch Loss:     2.049358 Tokens per Sec:    14997, Lr: 0.000300\n",
            "2020-01-17 22:03:27,424 Epoch   7 Step:    36200 Batch Loss:     1.975194 Tokens per Sec:    15463, Lr: 0.000300\n",
            "2020-01-17 22:03:42,574 Epoch   7 Step:    36300 Batch Loss:     1.873048 Tokens per Sec:    14843, Lr: 0.000300\n",
            "2020-01-17 22:03:57,934 Epoch   7 Step:    36400 Batch Loss:     1.819185 Tokens per Sec:    15201, Lr: 0.000300\n",
            "2020-01-17 22:04:13,072 Epoch   7 Step:    36500 Batch Loss:     1.808444 Tokens per Sec:    14847, Lr: 0.000300\n",
            "2020-01-17 22:04:28,365 Epoch   7 Step:    36600 Batch Loss:     1.934447 Tokens per Sec:    15265, Lr: 0.000300\n",
            "2020-01-17 22:04:43,782 Epoch   7 Step:    36700 Batch Loss:     2.099455 Tokens per Sec:    15024, Lr: 0.000300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MBoDS09JM807",
        "colab": {}
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "!cp -r joeynmt/models/${src}${tgt}_transformer/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n94wlrCjVc17",
        "colab": {}
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "66WhRE9lIhoD",
        "colab": {}
      },
      "source": [
        "# Test our model\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}