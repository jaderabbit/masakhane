{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Igc5itf-xMGj"
   },
   "source": [
    "# Masakhane - Reverse Machine Translation for African Languages (Using JoeyNMT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmkY5eucWsJF"
   },
   "source": [
    "> ## NB\n",
    ">### - The purpose of this Notebook is to build models that translate African languages(target language) *into* English(source language). This will allow us to in future be able to make translations from one African language to the other. If you'd like to translate *from* English, please use [this](https://github.com/masakhane-io/masakhane-mt/blob/master/starter_notebook.ipynb) starter notebook instead.\n",
    "\n",
    ">### - We call this reverse training because normally we build models that make translations from the source language(English) to the target language. But in this case we are doing the reverse; building models that make translations from the target language to the source(English)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4fXCKCf36IK"
   },
   "source": [
    "## Note before beginning:\n",
    "### - The idea is that you should be able to make minimal changes to this in order to get SOME result for your own translation corpus. \n",
    "\n",
    "### - The tl;dr: Go to the **\"TODO\"** comments which will tell you what to update to get up and running\n",
    "\n",
    "### - If you actually want to have a clue what you're doing, read the text and peek at the links\n",
    "\n",
    "### - With 100 epochs, it should take around 7 hours to run in Google Colab\n",
    "\n",
    "### - Once you've gotten a result for your language, please attach and email your notebook that generated it to masakhanetranslation@gmail.com\n",
    "\n",
    "### - If you care enough and get a chance, doing a brief background on your language would be amazing. See examples in  [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l929HimrxS0a"
   },
   "source": [
    "## Retrieve your data & make a parallel corpus\n",
    "\n",
    "If you are wanting to use the JW300 data referenced on the Masakhane website or in our GitHub repo, you can use `opus-tools` to convert the data into a convenient format. `opus_read` from that package provides a convenient tool for reading the native aligned XML files and to convert them to TMX format. The tool can also be used to fetch relevant files from OPUS on the fly and to filter the data as necessary. [Read the documentation](https://pypi.org/project/opustools-pkg/) for more details.\n",
    "\n",
    "Once you have your corpus files in TMX format (an xml structure which will include the sentences in your target language and your source language in a single file), we recommend reading them into a pandas dataframe. Thankfully, Jade wrote a silly `tmx2dataframe` package which converts your tmx file to a pandas dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "\n",
    "This is a notebook for training a nr - en translation model using the JW300 dataset.\n",
    "\n",
    "We train on a relatively large configuration, which uses the suggestions and increases some of the parameters. The model was trained for 23 epochs.\n",
    "\n",
    "The results were\n",
    "```\n",
    "2021-10-10 05:59:31,826 - INFO - joeynmt.prediction -  dev bleu[13a]:  51.47 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
    "2021-10-10 06:01:47,043 - INFO - joeynmt.prediction - test bleu[13a]:  57.22 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
    "```\n",
    "\n",
    "## Nonstandard Changes\n",
    "\n",
    "While most of this notebook is the same as the original template notebook, we made a few differences, mainly with regards to the joeynmt installation, as we has some problems with that.\n",
    "\n",
    "Instead of `git clone ... && cd ... && pip install .`, we do the following:\n",
    "\n",
    "```\n",
    "! pip install joeynmt\n",
    "! pip install torch==1.9.0\n",
    "! pip install --upgrade sacrebleu==1.5.1\n",
    "\n",
    "\n",
    "# make the directories since we didn't clone\n",
    "!mkdir -p joeynmt/configs\n",
    "!mkdir -p joeynmt/data\n",
    "!mkdir -p joeynmt/scripts\n",
    "\n",
    "# get the build_vocab.py script\n",
    "\n",
    "!cd joeynmt/scripts && wget https://raw.githubusercontent.com/joeynmt/joeynmt/master/scripts/build_vocab.py\n",
    "```\n",
    "\n",
    "The git clone approach should also work however.\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "We also do not use google colab, and as such the google drive specific code (e.g. mounting the drive) is commented out. This can be uncommented as needed\n",
    "\n",
    "\n",
    "A final note, we did not run the training commands in the notebook, but rather in a terminal, as that was more stable and disconnections were less brutal. The output will be shown here, and it should be completely reproducible by running the commands in the notebook instead. The only commands we ran like this were the training procedure and the testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oGRmDELn7Az0",
    "outputId": "21888cab-76b8-4759-9957-d8aa117ef418"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Cn3tgQLzUxwn"
   },
   "outputs": [],
   "source": [
    "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
    "# These will also become the suffix's of all vocab and corpus files used throughout\n",
    "import os\n",
    "source_language = \"en\"\n",
    "target_language = \"af\" \n",
    "lc = False  # If True, lowercase the data.\n",
    "seed = 42  # Random seed for shuffling.\n",
    "tag = \"mcb_af_to_en_1007_v2_larger\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
    "\n",
    "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
    "os.environ[\"tgt\"] = target_language\n",
    "os.environ[\"tag\"] = tag\n",
    "\n",
    "# This will save it to a folder in our gdrive instead!\n",
    "!mkdir -p \"nlp_things/masakhane/$tgt-$src-$tag\"\n",
    "os.environ[\"gdrive_path\"] = \"nlp_things/masakhane/%s-%s-%s\" % (target_language, source_language, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kBSgJHEw7Nvx",
    "outputId": "479297fa-fd5b-4059-cd2e-d45a1ee77eb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp_things/masakhane/af-en-mcb_af_to_en_1007_v2_larger\n"
     ]
    }
   ],
   "source": [
    "!echo $gdrive_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gA75Fs9ys8Y9",
    "outputId": "6b6c9e4c-e9ef-4762-d761-32b49ea47dd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opustools-pkg\n",
      "  Downloading opustools_pkg-0.0.52-py3-none-any.whl (80 kB)\n",
      "\u001b[K     |████████████████████████████████| 80 kB 9.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: opustools-pkg\n",
      "Successfully installed opustools-pkg-0.0.52\n"
     ]
    }
   ],
   "source": [
    "# Install opus-tools\n",
    "! pip install opustools-pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xq-tDZVks7ZD",
    "outputId": "e438ed40-34a0-4436-8aa9-6399abfaf457"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alignment file /proj/nlpl/data/OPUS/JW300/latest/xml/af-en.xml.gz not found. The following files are available for downloading:\n",
      "\n",
      "  11 MB https://object.pouta.csc.fi/OPUS-JW300/v1c/xml/af-en.xml.gz\n",
      " 124 MB https://object.pouta.csc.fi/OPUS-JW300/v1c/xml/af.zip\n",
      " 274 MB https://object.pouta.csc.fi/OPUS-JW300/v1c/xml/en.zip\n",
      "\n",
      " 409 MB Total size\n",
      "./JW300_latest_xml_af-en.xml.gz ... 100% of 11 MB\n",
      "./JW300_latest_xml_af.zip ... 100% of 124 MB\n",
      "./JW300_latest_xml_en.zip ... 100% of 274 MB\n"
     ]
    }
   ],
   "source": [
    "# Downloading our corpus\n",
    "! opus_read -d JW300 -s $src -t $tgt -wm moses -w jw300.$src jw300.$tgt -q\n",
    "\n",
    "# extract the corpus file\n",
    "\n",
    "# I had to use this way around one because the file was af-en.\n",
    "! gunzip JW300_latest_xml_$tgt-$src.xml.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n48GDRnP8y2G",
    "outputId": "70fb6cb7-d42b-4c0b-961c-14699ad5211a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-07 13:53:38--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 277791 (271K) [text/plain]\n",
      "Saving to: ‘test.en-any.en’\n",
      "\n",
      "test.en-any.en      100%[===================>] 271.28K  --.-KB/s    in 0.006s  \n",
      "\n",
      "2021-10-07 13:53:39 (44.2 MB/s) - ‘test.en-any.en’ saved [277791/277791]\n",
      "\n",
      "--2021-10-07 13:53:39--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-af.en\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 203446 (199K) [text/plain]\n",
      "Saving to: ‘test.en-af.en’\n",
      "\n",
      "test.en-af.en       100%[===================>] 198.68K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-10-07 13:53:39 (43.4 MB/s) - ‘test.en-af.en’ saved [203446/203446]\n",
      "\n",
      "--2021-10-07 13:53:40--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-af.af\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 212484 (208K) [text/plain]\n",
      "Saving to: ‘test.en-af.af’\n",
      "\n",
      "test.en-af.af       100%[===================>] 207.50K  --.-KB/s    in 0.008s  \n",
      "\n",
      "2021-10-07 13:53:40 (24.4 MB/s) - ‘test.en-af.af’ saved [212484/212484]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the global test set.\n",
    "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
    "  \n",
    "# And the specific test set for this language pair.\n",
    "os.environ[\"trg\"] = target_language \n",
    "os.environ[\"src\"] = source_language \n",
    "\n",
    "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.en \n",
    "! mv test.en-$trg.en test.en\n",
    "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.$trg \n",
    "! mv test.en-$trg.$trg test.$trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NqDG-CI28y2L",
    "outputId": "8da9efa0-f777-4b99-f367-a005cd81592b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3571 global test sentences to filter from the training/dev data.\n"
     ]
    }
   ],
   "source": [
    "# Read the test data to filter from train and dev splits.\n",
    "# Store english portion in set for quick filtering checks.\n",
    "en_test_sents = set()\n",
    "filter_test_sents = \"test.en-any.en\"\n",
    "j = 0\n",
    "with open(filter_test_sents) as f:\n",
    "    for line in f:\n",
    "        en_test_sents.add(line.strip())\n",
    "        j += 1\n",
    "print('Loaded {} global test sentences to filter from the training/dev data.'.format(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "3CNdwLBCfSIl",
    "outputId": "af89e4f6-bfed-4bc1-9437-2562631a602e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data and skipped 5470/1104023 lines since contained in test set.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_sentence</th>\n",
       "      <th>target_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‘ They Lifted Me Out of Deep Depression ’</td>\n",
       "      <td>‘ Hulle het my uit diep depressie gehelp ’</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A woman from England wrote :</td>\n",
       "      <td>’ n Vrou van Engeland het geskryf :</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“ Dear Sir :</td>\n",
       "      <td>“ Geagte Meneer :</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             source_sentence  \\\n",
       "0  ‘ They Lifted Me Out of Deep Depression ’   \n",
       "1               A woman from England wrote :   \n",
       "2                               “ Dear Sir :   \n",
       "\n",
       "                              target_sentence  \n",
       "0  ‘ Hulle het my uit diep depressie gehelp ’  \n",
       "1         ’ n Vrou van Engeland het geskryf :  \n",
       "2                           “ Geagte Meneer :  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TMX file to dataframe\n",
    "source_file = 'jw300.' + source_language\n",
    "target_file = 'jw300.' + target_language\n",
    "\n",
    "source = []\n",
    "target = []\n",
    "skip_lines = []  # Collect the line numbers of the source portion to skip the same lines for the target portion.\n",
    "with open(source_file) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        # Skip sentences that are contained in the test set.\n",
    "        if line.strip() not in en_test_sents:\n",
    "            source.append(line.strip())\n",
    "        else:\n",
    "            skip_lines.append(i)             \n",
    "with open(target_file) as f:\n",
    "    for j, line in enumerate(f):\n",
    "        # Only add to corpus if corresponding source was not skipped.\n",
    "        if j not in skip_lines:\n",
    "            target.append(line.strip())\n",
    "    \n",
    "print('Loaded data and skipped {}/{} lines since contained in test set.'.format(len(skip_lines), i))\n",
    "    \n",
    "df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n",
    "# if you get TypeError: data argument can't be an iterator is because of your zip version run this below\n",
    "#df = pd.DataFrame(list(zip(source, target)), columns=['source_sentence', 'target_sentence'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkuK3B4p2AkN"
   },
   "source": [
    "## Pre-processing and export\n",
    "\n",
    "It is generally a good idea to remove duplicate translations and conflicting translations from the corpus. In practice, these public corpora include some number of these that need to be cleaned.\n",
    "\n",
    "In addition we will split our data into dev/test/train and export to the filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_2ouEOH1_1q",
    "outputId": "6b70c61f-6322-4473-a90e-76ed060c889c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home-mscluster/mbeukman/anaconda3/envs/nlp_q2/lib/python3.7/site-packages/pandas/util/_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# drop duplicate translations\n",
    "df_pp = df.drop_duplicates()\n",
    "\n",
    "# drop conflicting translations\n",
    "# (this is optional and something that you might want to comment out \n",
    "# depending on the size of your corpus)\n",
    "df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n",
    "df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n",
    "\n",
    "# Shuffle the data to remove bias in dev set selection.\n",
    "df_pp = df_pp.sample(frac=1, random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_1BwAApEtMk",
    "outputId": "53d0065c-3bd9-406a-c43b-0ab2ccf74bca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0\n",
      "Collecting python-Levenshtein\n",
      "  Using cached python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
      "Requirement already satisfied: setuptools in /home-mscluster/mbeukman/anaconda3/envs/nlp_q2/lib/python3.7/site-packages (from python-Levenshtein) (58.0.4)\n",
      "Building wheels for collected packages: python-Levenshtein\n",
      "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=171679 sha256=d35e14a3f1162e26f294203aaab2e486581dff940827f2c9a2258e1339642115\n",
      "  Stored in directory: /home-mscluster/mbeukman/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
      "Successfully built python-Levenshtein\n",
      "Installing collected packages: python-Levenshtein\n",
      "Successfully installed python-Levenshtein-0.12.2\n"
     ]
    }
   ],
   "source": [
    "# Install fuzzy wuzzy to remove \"almost duplicate\" sentences in the\n",
    "# test and training sets.\n",
    "! pip install fuzzywuzzy\n",
    "! pip install python-Levenshtein\n",
    "import time\n",
    "from fuzzywuzzy import process\n",
    "import numpy as np\n",
    "from os import cpu_count\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "# reset the index of the training set after previous filtering\n",
    "df_pp.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# Remove samples from the training data set if they \"almost overlap\" with the\n",
    "# samples in the test set.\n",
    "\n",
    "# Filtering function. Adjust pad to narrow down the candidate matches to\n",
    "# within a certain length of characters of the given sample.\n",
    "def fuzzfilter(sample, candidates, pad):\n",
    "    candidates = [x for x in candidates if len(x) <= len(sample)+pad and len(x) >= len(sample)-pad] \n",
    "      if len(candidates) > 0:\n",
    "        return process.extractOne(sample, candidates)[1]\n",
    "      else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "92EsgTaY3B4H"
   },
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# ### iterating over pandas dataframe rows is not recomended, let use multi processing to apply the function\n",
    "\n",
    "# with Pool(cpu_count()-1) as pool:\n",
    "#     scores = pool.map(partial(fuzzfilter, candidates=list(en_test_sents), pad=5), df_pp['source_sentence'])\n",
    "# hours, rem = divmod(time.time() - start_time, 3600)\n",
    "# minutes, seconds = divmod(rem, 60)\n",
    "# print(\"done in {}h:{}min:{}seconds\".format(hours, minutes, seconds))\n",
    "\n",
    "# # Filter out \"almost overlapping samples\"\n",
    "# df_pp = df_pp.assign(scores=scores)\n",
    "# df_pp = df_pp[df_pp['scores'] < 95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hxxBOCA-xXhy",
    "outputId": "57c82223-7c12-45a5-db96-2b8bdcda6580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> train.af <==\n",
      "Moderne tegnologie stel leerders en studente in staat om makliker en op bedrewener wyse te kul .\n",
      "Ons het eers gaan sit en die koste bereken .\n",
      "□ Wat leer die vernietiging van Sodom en Gomorra ons omtrent Jehovah se geregtigheid ?\n",
      "1948\n",
      "Die taai kakkerlak\n",
      "Jesus het groot mag in die hemel .\n",
      "14 Jehovah het ’ n vasgestelde tyd bepaal vir sy enigverwekte Seun om as die Messias aarde toe te kom .\n",
      "Ja , hulle predikingswerk is nie bloot ’ n roetine nie .\n",
      "14 DIE BYBEL SE BESKOUINGStiptelikheid\n",
      "As jy ’ n ongepaste emosionele band met iemand van die teenoorgestelde geslag smee , verswak dit jou band met jou huweliksmaat .\n",
      "\n",
      "==> train.en <==\n",
      "Modern technology enables students to cheat with new levels of ease and sophistication .\n",
      "First , we sat down to count the cost .\n",
      "□ What does the destruction of Sodom and Gomorrah teach us about Jehovah ’ s justice ?\n",
      "1948\n",
      "The Enduring Cockroach\n",
      "Jesus has great power in heaven .\n",
      "14 Jehovah had set a fixed time for his only-begotten Son to come to the earth as the Messiah .\n",
      "It is an art that requires creative teaching methods .\n",
      "14 THE BIBLE ’ S VIEWPOINTPunctuality\n",
      "When you form an improper emotional connection with a member of the opposite sex , you weaken your connection with your spouse .\n",
      "==> dev.af <==\n",
      "My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "Praag het afgevaardigdes uit 39 lande gehad , waaronder meer as 26 000 uit Duitsland , bykans 13 000 uit Pole , meer as 900 uit Italië , 570 uit Nederland , 746 uit Swede en 743 uit Japan .\n",
      "Dit is wat Keith en Erika gedoen het .\n",
      "Maar hulle kon nie saamstem oor hoe dit in hedendaagse tale weergegee moet word nie .\n",
      "Trouens , waarom was God nog altyd so geduldig ?\n",
      "Deur diegene ter dood te veroordeel wat as geloofsgrondslag slegs “ die woord van God [ aanvaar het ] soos dit in die Heilige Skrif gevind word ” , het die Inkwisisie duidelik getoon dat die Katolieke Kerk nie die Heilige Bybel as die enigste geïnspireerde bron beskou nie .\n",
      "Ja , as ’ n mens opreg jammer is , behoort jy vasbeslote te wees om nie weer dieselfde fout te maak nie .\n",
      "\n",
      "==> dev.en <==\n",
      "Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "WHY SET SPIRITUAL GOALS ?\n",
      "Prague had delegates from 39 countries , including more than 26,000 from Germany , nearly 13,000 from Poland , over 900 from Italy , 570 from the Netherlands , 746 from Sweden , and 743 from Japan .\n",
      "That is what Keith and Erika did .\n",
      "But they could not agree on how it should be rendered in modern languages .\n",
      "Indeed , why has God been so patient ?\n",
      "By condemning to death those who accepted as the basis of belief only “ the word of God expressed in the Holy Scriptures , ” the Inquisition clearly showed that the Catholic Church does not consider the Holy Bible to be the only inspired source .\n",
      "Yes , a sincere apology should be accompanied by the determination not to repeat the mistake .\n"
     ]
    }
   ],
   "source": [
    "# This section does the split between train/dev for the parallel corpora then saves them as separate files\n",
    "# We use 1000 dev test and the given test set.\n",
    "import csv\n",
    "\n",
    "# Do the split between dev/train and create parallel corpora\n",
    "num_dev_patterns = 1000\n",
    "\n",
    "# Optional: lower case the corpora - this will make it easier to generalize, but without proper casing.\n",
    "if lc:  # Julia: making lowercasing optional\n",
    "    df_pp[\"source_sentence\"] = df_pp[\"source_sentence\"].str.lower()\n",
    "    df_pp[\"target_sentence\"] = df_pp[\"target_sentence\"].str.lower()\n",
    "\n",
    "# Julia: test sets are already generated\n",
    "dev = df_pp.tail(num_dev_patterns) # Herman: Error in original\n",
    "stripped = df_pp.drop(df_pp.tail(num_dev_patterns).index)\n",
    "\n",
    "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n",
    "  for index, row in stripped.iterrows():\n",
    "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
    "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
    "    \n",
    "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n",
    "  for index, row in dev.iterrows():\n",
    "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
    "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
    "\n",
    "#stripped[[\"source_sentence\"]].to_csv(\"train.\"+source_language, header=False, index=False)  # Herman: Added `header=False` everywhere\n",
    "#stripped[[\"target_sentence\"]].to_csv(\"train.\"+target_language, header=False, index=False)  # Julia: Problematic handling of quotation marks.\n",
    "\n",
    "#dev[[\"source_sentence\"]].to_csv(\"dev.\"+source_language, header=False, index=False)\n",
    "#dev[[\"target_sentence\"]].to_csv(\"dev.\"+target_language, header=False, index=False)\n",
    "\n",
    "# Doublecheck the format below. There should be no extra quotation marks or weird characters.\n",
    "! head train.*\n",
    "! head dev.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epeCydmCyS8X"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Installation of JoeyNMT\n",
    "\n",
    "JoeyNMT is a simple, minimalist NMT package which is useful for learning and teaching. Check out the documentation for JoeyNMT [here](https://joeynmt.readthedocs.io)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had trouble with joey git installation, so I just installed it through here, and made the requisite directories. But theoretically this should work with the github installation as in the original notebook.\n",
    "\n",
    "# Install using pip\n",
    "! pip install joeynmt\n",
    "! pip install torch==1.9.0\n",
    "! pip install --upgrade sacrebleu==1.5.1\n",
    "\n",
    "\n",
    "!mkdir -p joeynmt/configs\n",
    "!mkdir -p joeynmt/data\n",
    "!mkdir -p joeynmt/scripts\n",
    "\n",
    "!cd joeynmt/scripts && wget https://raw.githubusercontent.com/joeynmt/joeynmt/master/scripts/build_vocab.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaE77Tcppex9"
   },
   "source": [
    "# Preprocessing the Data into Subword BPE Tokens\n",
    "\n",
    "- One of the most powerful improvements for agglutinative languages (a feature of most Bantu languages) is using BPE tokenization [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n",
    "\n",
    "- It was also shown that by optimizing the umber of BPE codes we significantly improve results for low-resourced languages [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021) [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)\n",
    "\n",
    "- Below we have the scripts for doing BPE tokenization of our data. We use 4000 tokens as recommended by [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021). You do not need to change anything. Simply running the below will be suitable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H-TyjtmXB1mL",
    "outputId": "4ebbb8e3-e036-46f8-9cab-bb703daf07c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpe.codes.4000\tdev.bpe.en  test.af\t test.en\t train.bpe.af\n",
      "dev.af\t\tdev.en\t    test.bpe.af  test.en-any.en  train.bpe.en\n",
      "dev.bpe.af\tscripts     test.bpe.en  train.af\t train.en\n",
      "bpe.codes.4000\tdev.bpe.en  test.bpe.af  test.en-any.en  train.bpe.en\n",
      "dev.af\t\tdev.en\t    test.bpe.en  train.af\t train.en\n",
      "dev.bpe.af\ttest.af     test.en\t train.bpe.af\n",
      "BPE Afrikaans Sentences\n",
      "Ek moes hom om@@ k@@ oop sodat hy sy oë sou sluit vir die maa@@ t@@ skappy se on@@ eer@@ likheid .\n",
      "Ge@@ volglik was ek bekend as ’ n on@@ eer@@ like man . Toe ek in die waarheid kom , het ek gewe@@ ier om daar@@ mee voor@@ t te gaan , al het ek ’ n goeie sal@@ ar@@ is ver@@ dien .\n",
      "Ek stel ’ n goeie voorbeeld vir my twee seuns , en ek het voor@@ regte in die gemeente gekry .\n",
      "Bel@@ a@@ st@@ ing@@ ou@@ di@@ te@@ ur@@ s en ander met wie ek be@@ sig@@ heid doen , weet nou dat ek ’ n eer@@ like man is . ”\n",
      "R@@ ut het na Israel ge@@ trek , waar sy die ware God kon aan@@ bid .\n",
      "Combined BPE Vocab\n",
      "̣@@\n",
      "Ì@@\n",
      "ṇ@@\n",
      "ז@@\n",
      "À@@\n",
      "Α\n",
      "ḥ\n",
      "Ο@@\n",
      "˜\n",
      "·@@\n"
     ]
    }
   ],
   "source": [
    "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
    "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
    "\n",
    "# Do subword NMT\n",
    "from os import path\n",
    "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
    "os.environ[\"tgt\"] = target_language\n",
    "\n",
    "# Learn BPEs on the training data.\n",
    "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\",target_language + source_language ) # Herman! \n",
    "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
    "\n",
    "# Apply BPE splits to the development and test data.\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
    "\n",
    "# Create directory, move everyone we care about to the correct location\n",
    "! mkdir -p $data_path\n",
    "! cp train.* $data_path\n",
    "! cp test.* $data_path\n",
    "! cp dev.* $data_path\n",
    "! cp bpe.codes.4000 $data_path\n",
    "! ls $data_path\n",
    "\n",
    "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
    "! cp train.* \"$gdrive_path\"\n",
    "! cp test.* \"$gdrive_path\"\n",
    "! cp dev.* \"$gdrive_path\"\n",
    "! cp bpe.codes.4000 \"$gdrive_path\"\n",
    "! ls \"$gdrive_path\"\n",
    "\n",
    "# Create that vocab using build_vocab\n",
    "! chmod 777 joeynmt/scripts/build_vocab.py\n",
    "! joeynmt/scripts/build_vocab.py joeynmt/data/$tgt$src/train.bpe.$src joeynmt/data/$tgt$src/train.bpe.$tgt --output_path joeynmt/data/$tgt$src/vocab.txt\n",
    "\n",
    "# Some output\n",
    "! echo \"BPE Afrikaans Sentences\"\n",
    "! tail -n 5 test.bpe.$tgt\n",
    "! echo \"Combined BPE Vocab\"\n",
    "! tail -n 10 joeynmt/data/$tgt$src/vocab.txt  # Herman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IlMitUHR8Qy-",
    "outputId": "4d3be082-c6a3-4acc-97fb-de3598c50c90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpe.codes.4000\tdev.bpe.en  test.bpe.af  test.en-any.en  train.bpe.en\n",
      "dev.af\t\tdev.en\t    test.bpe.en  train.af\t train.en\n",
      "dev.bpe.af\ttest.af     test.en\t train.bpe.af\n"
     ]
    }
   ],
   "source": [
    "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
    "! cp train.* \"$gdrive_path\"\n",
    "! cp test.* \"$gdrive_path\"\n",
    "! cp dev.* \"$gdrive_path\"\n",
    "! cp bpe.codes.4000 \"$gdrive_path\"\n",
    "! ls \"$gdrive_path\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ixmzi60WsUZ8"
   },
   "source": [
    "# Creating the JoeyNMT Config\n",
    "\n",
    "JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n",
    "\n",
    "- We used Transformer architecture \n",
    "- We set our dropout to reasonably high: 0.3 (recommended in  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
    "\n",
    "Things worth playing with:\n",
    "- The batch size (also recommended to change for low-resourced languages)\n",
    "- The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n",
    "- The decoder options (beam_size, alpha)\n",
    "- Evaluation metrics (BLEU versus Crhf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "h8TMgv1p3L1z"
   },
   "outputs": [],
   "source": [
    "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
    "# (You can of course play with all the parameters if you'd like!)\n",
    "\n",
    "name = '%s%s' % (target_language, source_language)\n",
    "# gdrive_path = os.environ[\"gdrive_path\"]\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{target_language}{source_language}_reverse_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{target_language}\"\n",
    "    trg: \"{source_language}\"\n",
    "    train: \"data/{name}/train.bpe\"\n",
    "    dev:   \"data/{name}/dev.bpe\"\n",
    "    test:  \"data/{name}/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"data/{name}/vocab.txt\"\n",
    "    trg_vocab: \"data/{name}/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    load_model: \"../nlp_things/masakhane/af-en-mcb_af_to_en_1007_v2_larger/models/enaf_reverse_transformer-large-30-epochs/125000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"noam\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 8192\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 3600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 100                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 5000          # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/{name}_reverse_transformer_michael_larger_30epochs-more-1008\"\n",
    "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp_things/masakhane/af-en-mcb_af_to_en_1007_v2_larger/models/enaf_reverse_transformer-large/5000.ckpt\n"
     ]
    }
   ],
   "source": [
    "!echo \"$gdrive_path/models/${src}${tgt}_reverse_transformer-large/5000.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEzoJtV2MIpt"
   },
   "source": [
    "# Train the Model\n",
    "\n",
    "This single line of joeynmt runs the training using the config we made above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "WzbNYNdjLgNb",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "fddbfee2-c6d2-4377-e682-8b1c78d49cc5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-08 05:32:16,822 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-10-08 05:32:16,867 - INFO - joeynmt.data - Loading training data...\n",
      "2021-10-08 05:32:36,109 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-10-08 05:32:36,365 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-10-08 05:32:36,472 - INFO - joeynmt.data - Loading test data...\n",
      "2021-10-08 05:32:36,497 - INFO - joeynmt.data - Data loaded.\n",
      "2021-10-08 05:32:36,497 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-10-08 05:32:36,778 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-10-08 05:32:36,792 - INFO - joeynmt.training - Total params: 12223488\n",
      "2021-10-08 05:32:39,458 - INFO - joeynmt.training - Loading model from ../nlp_things/masakhane/af-en-mcb_af_to_en_1007_v2_larger/models/enaf_reverse_transformer-large-30-epochs/125000.ckpt\n",
      "2021-10-08 05:32:39,791 - INFO - joeynmt.helpers - cfg.name                           : afen_reverse_transformer\n",
      "2021-10-08 05:32:39,791 - INFO - joeynmt.helpers - cfg.data.src                       : af\n",
      "2021-10-08 05:32:39,791 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-10-08 05:32:39,791 - INFO - joeynmt.helpers - cfg.data.train                     : data/afen/train.bpe\n",
      "2021-10-08 05:32:39,791 - INFO - joeynmt.helpers - cfg.data.dev                       : data/afen/dev.bpe\n",
      "2021-10-08 05:32:39,791 - INFO - joeynmt.helpers - cfg.data.test                      : data/afen/test.bpe\n",
      "2021-10-08 05:32:39,791 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-10-08 05:32:39,791 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-10-08 05:32:39,791 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-10-08 05:32:39,792 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : data/afen/vocab.txt\n",
      "2021-10-08 05:32:39,792 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : data/afen/vocab.txt\n",
      "2021-10-08 05:32:39,792 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-10-08 05:32:39,792 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-10-08 05:32:39,792 - INFO - joeynmt.helpers - cfg.training.load_model            : ../nlp_things/masakhane/af-en-mcb_af_to_en_1007_v2_larger/models/enaf_reverse_transformer-large-30-epochs/125000.ckpt\n",
      "2021-10-08 05:32:39,792 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-10-08 05:32:39,792 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-10-08 05:32:39,792 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-10-08 05:32:39,792 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-10-08 05:32:39,792 - INFO - joeynmt.helpers - cfg.training.scheduling            : noam\n",
      "2021-10-08 05:32:39,792 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-10-08 05:32:39,792 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-10-08 05:32:39,792 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-10-08 05:32:39,792 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-10-08 05:32:39,792 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-10-08 05:32:39,793 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-10-08 05:32:39,793 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-10-08 05:32:39,793 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-10-08 05:32:39,793 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-10-08 05:32:39,793 - INFO - joeynmt.helpers - cfg.training.batch_size            : 8192\n",
      "2021-10-08 05:32:39,793 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-10-08 05:32:39,793 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
      "2021-10-08 05:32:39,793 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-10-08 05:32:39,793 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-10-08 05:32:39,793 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-10-08 05:32:39,793 - INFO - joeynmt.helpers - cfg.training.epochs                : 100\n",
      "2021-10-08 05:32:39,793 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
      "2021-10-08 05:32:39,793 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-10-08 05:32:39,793 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-10-08 05:32:39,794 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/afen_reverse_transformer_michael_larger_30epochs-more-1008\n",
      "2021-10-08 05:32:39,794 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-10-08 05:32:39,794 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-10-08 05:32:39,794 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-10-08 05:32:39,794 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-10-08 05:32:39,794 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-10-08 05:32:39,794 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-10-08 05:32:39,794 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-10-08 05:32:39,794 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-10-08 05:32:39,794 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-10-08 05:32:39,794 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-10-08 05:32:39,794 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-10-08 05:32:39,794 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-10-08 05:32:39,795 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-10-08 05:32:39,795 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-10-08 05:32:39,795 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-10-08 05:32:39,795 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-10-08 05:32:39,795 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-10-08 05:32:39,795 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-10-08 05:32:39,795 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-10-08 05:32:39,795 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-10-08 05:32:39,795 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-10-08 05:32:39,795 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-10-08 05:32:39,795 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-10-08 05:32:39,795 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-10-08 05:32:39,795 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-10-08 05:32:39,795 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-10-08 05:32:39,796 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-10-08 05:32:39,796 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-10-08 05:32:39,796 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-10-08 05:32:39,796 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-10-08 05:32:39,796 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-10-08 05:32:39,796 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 959951,\n",
      "\tvalid 1000,\n",
      "\ttest 2682\n",
      "2021-10-08 05:32:39,796 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Mo@@ der@@ ne te@@ g@@ no@@ lo@@ gie stel leer@@ ders en studen@@ te in staat om ma@@ k@@ liker en op be@@ dre@@ wen@@ er wyse te k@@ ul .\n",
      "\t[TRG] Mo@@ der@@ n te@@ ch@@ no@@ lo@@ gy en@@ ab@@ les studen@@ ts to che@@ at with new le@@ vel@@ s of e@@ ase and s@@ op@@ hi@@ sti@@ cation .\n",
      "2021-10-08 05:32:39,796 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) die (7) the (8) in (9) ’\n",
      "2021-10-08 05:32:39,796 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) die (7) the (8) in (9) ’\n",
      "2021-10-08 05:32:39,796 - INFO - joeynmt.helpers - Number of Src words (types): 4544\n",
      "2021-10-08 05:32:39,796 - INFO - joeynmt.helpers - Number of Trg words (types): 4544\n",
      "2021-10-08 05:32:39,797 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4544),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4544))\n",
      "2021-10-08 05:32:39,806 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 8192\n",
      "\ttotal batch size (w. parallel & accumulation): 8192\n",
      "2021-10-08 05:32:39,806 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-10-08 05:33:19,371 - INFO - joeynmt.training - Epoch   1, Step:   125100, Batch Loss:     1.413539, Tokens per Sec:    10881, Lr: 0.000088\n",
      "2021-10-08 05:33:54,974 - INFO - joeynmt.training - Epoch   1, Step:   125200, Batch Loss:     1.412540, Tokens per Sec:    11569, Lr: 0.000088\n",
      "2021-10-08 05:34:30,935 - INFO - joeynmt.training - Epoch   1, Step:   125300, Batch Loss:     1.436924, Tokens per Sec:    11569, Lr: 0.000088\n",
      "2021-10-08 05:35:06,185 - INFO - joeynmt.training - Epoch   1, Step:   125400, Batch Loss:     1.668717, Tokens per Sec:    11480, Lr: 0.000088\n",
      "2021-10-08 05:35:42,098 - INFO - joeynmt.training - Epoch   1, Step:   125500, Batch Loss:     1.370337, Tokens per Sec:    11484, Lr: 0.000088\n",
      "2021-10-08 05:36:17,825 - INFO - joeynmt.training - Epoch   1, Step:   125600, Batch Loss:     1.397329, Tokens per Sec:    11467, Lr: 0.000088\n",
      "2021-10-08 05:36:53,508 - INFO - joeynmt.training - Epoch   1, Step:   125700, Batch Loss:     1.370318, Tokens per Sec:    11543, Lr: 0.000088\n",
      "2021-10-08 05:37:29,502 - INFO - joeynmt.training - Epoch   1, Step:   125800, Batch Loss:     1.489132, Tokens per Sec:    11516, Lr: 0.000088\n",
      "2021-10-08 05:38:05,898 - INFO - joeynmt.training - Epoch   1, Step:   125900, Batch Loss:     1.548209, Tokens per Sec:    11712, Lr: 0.000088\n",
      "2021-10-08 05:38:41,662 - INFO - joeynmt.training - Epoch   1, Step:   126000, Batch Loss:     1.494447, Tokens per Sec:    11542, Lr: 0.000088\n",
      "2021-10-08 05:39:17,569 - INFO - joeynmt.training - Epoch   1, Step:   126100, Batch Loss:     1.523443, Tokens per Sec:    11578, Lr: 0.000088\n",
      "2021-10-08 05:39:53,277 - INFO - joeynmt.training - Epoch   1, Step:   126200, Batch Loss:     1.490488, Tokens per Sec:    11577, Lr: 0.000088\n",
      "2021-10-08 05:40:28,751 - INFO - joeynmt.training - Epoch   1, Step:   126300, Batch Loss:     1.546647, Tokens per Sec:    11459, Lr: 0.000088\n",
      "2021-10-08 05:41:04,716 - INFO - joeynmt.training - Epoch   1, Step:   126400, Batch Loss:     1.446124, Tokens per Sec:    11823, Lr: 0.000088\n",
      "2021-10-08 05:41:40,582 - INFO - joeynmt.training - Epoch   1, Step:   126500, Batch Loss:     1.382672, Tokens per Sec:    11442, Lr: 0.000088\n",
      "2021-10-08 05:42:16,634 - INFO - joeynmt.training - Epoch   1, Step:   126600, Batch Loss:     1.401849, Tokens per Sec:    11811, Lr: 0.000088\n",
      "2021-10-08 05:42:52,121 - INFO - joeynmt.training - Epoch   1, Step:   126700, Batch Loss:     1.442336, Tokens per Sec:    11631, Lr: 0.000088\n",
      "2021-10-08 05:43:27,947 - INFO - joeynmt.training - Epoch   1, Step:   126800, Batch Loss:     1.465478, Tokens per Sec:    11676, Lr: 0.000088\n",
      "2021-10-08 05:44:04,103 - INFO - joeynmt.training - Epoch   1, Step:   126900, Batch Loss:     1.352560, Tokens per Sec:    11764, Lr: 0.000088\n",
      "2021-10-08 05:44:39,846 - INFO - joeynmt.training - Epoch   1, Step:   127000, Batch Loss:     1.489174, Tokens per Sec:    11537, Lr: 0.000088\n",
      "2021-10-08 05:45:15,712 - INFO - joeynmt.training - Epoch   1, Step:   127100, Batch Loss:     1.490780, Tokens per Sec:    11611, Lr: 0.000088\n",
      "2021-10-08 05:45:51,075 - INFO - joeynmt.training - Epoch   1, Step:   127200, Batch Loss:     1.369360, Tokens per Sec:    11476, Lr: 0.000088\n",
      "2021-10-08 05:46:26,877 - INFO - joeynmt.training - Epoch   1, Step:   127300, Batch Loss:     1.263061, Tokens per Sec:    11656, Lr: 0.000088\n",
      "2021-10-08 05:47:02,490 - INFO - joeynmt.training - Epoch   1, Step:   127400, Batch Loss:     1.366721, Tokens per Sec:    11469, Lr: 0.000088\n",
      "2021-10-08 05:47:38,357 - INFO - joeynmt.training - Epoch   1, Step:   127500, Batch Loss:     1.498312, Tokens per Sec:    11674, Lr: 0.000088\n",
      "2021-10-08 05:48:14,353 - INFO - joeynmt.training - Epoch   1, Step:   127600, Batch Loss:     1.379341, Tokens per Sec:    11711, Lr: 0.000087\n",
      "2021-10-08 05:48:50,481 - INFO - joeynmt.training - Epoch   1, Step:   127700, Batch Loss:     1.291417, Tokens per Sec:    11776, Lr: 0.000087\n",
      "2021-10-08 05:49:26,317 - INFO - joeynmt.training - Epoch   1, Step:   127800, Batch Loss:     1.427380, Tokens per Sec:    11667, Lr: 0.000087\n",
      "2021-10-08 05:50:02,404 - INFO - joeynmt.training - Epoch   1, Step:   127900, Batch Loss:     1.477086, Tokens per Sec:    11432, Lr: 0.000087\n",
      "2021-10-08 05:50:38,229 - INFO - joeynmt.training - Epoch   1, Step:   128000, Batch Loss:     1.465415, Tokens per Sec:    11578, Lr: 0.000087\n",
      "2021-10-08 05:51:13,750 - INFO - joeynmt.training - Epoch   1, Step:   128100, Batch Loss:     1.410416, Tokens per Sec:    11651, Lr: 0.000087\n",
      "2021-10-08 05:51:49,827 - INFO - joeynmt.training - Epoch   1, Step:   128200, Batch Loss:     1.253405, Tokens per Sec:    11626, Lr: 0.000087\n",
      "2021-10-08 05:52:26,009 - INFO - joeynmt.training - Epoch   1, Step:   128300, Batch Loss:     1.484412, Tokens per Sec:    11593, Lr: 0.000087\n",
      "2021-10-08 05:53:01,663 - INFO - joeynmt.training - Epoch   1, Step:   128400, Batch Loss:     1.295190, Tokens per Sec:    11265, Lr: 0.000087\n",
      "2021-10-08 05:53:37,515 - INFO - joeynmt.training - Epoch   1, Step:   128500, Batch Loss:     1.354049, Tokens per Sec:    11533, Lr: 0.000087\n",
      "2021-10-08 05:54:13,706 - INFO - joeynmt.training - Epoch   1, Step:   128600, Batch Loss:     1.511420, Tokens per Sec:    11640, Lr: 0.000087\n",
      "2021-10-08 05:54:49,729 - INFO - joeynmt.training - Epoch   1, Step:   128700, Batch Loss:     1.325682, Tokens per Sec:    11680, Lr: 0.000087\n",
      "2021-10-08 05:55:26,109 - INFO - joeynmt.training - Epoch   1, Step:   128800, Batch Loss:     1.308920, Tokens per Sec:    11784, Lr: 0.000087\n",
      "2021-10-08 05:56:02,321 - INFO - joeynmt.training - Epoch   1, Step:   128900, Batch Loss:     1.291547, Tokens per Sec:    11703, Lr: 0.000087\n",
      "2021-10-08 05:56:38,291 - INFO - joeynmt.training - Epoch   1, Step:   129000, Batch Loss:     1.318004, Tokens per Sec:    11673, Lr: 0.000087\n",
      "2021-10-08 05:57:14,701 - INFO - joeynmt.training - Epoch   1, Step:   129100, Batch Loss:     1.413264, Tokens per Sec:    11658, Lr: 0.000087\n",
      "2021-10-08 05:57:43,084 - INFO - joeynmt.training - Epoch   1: total training loss 5915.72\n",
      "2021-10-08 05:57:43,084 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-10-08 05:57:52,882 - INFO - joeynmt.training - Epoch   2, Step:   129200, Batch Loss:     1.473388, Tokens per Sec:    10210, Lr: 0.000087\n",
      "2021-10-08 05:58:28,655 - INFO - joeynmt.training - Epoch   2, Step:   129300, Batch Loss:     1.503674, Tokens per Sec:    11462, Lr: 0.000087\n",
      "2021-10-08 05:59:04,536 - INFO - joeynmt.training - Epoch   2, Step:   129400, Batch Loss:     1.429016, Tokens per Sec:    11540, Lr: 0.000087\n",
      "2021-10-08 05:59:40,428 - INFO - joeynmt.training - Epoch   2, Step:   129500, Batch Loss:     1.562510, Tokens per Sec:    11423, Lr: 0.000087\n",
      "2021-10-08 06:00:16,316 - INFO - joeynmt.training - Epoch   2, Step:   129600, Batch Loss:     1.430344, Tokens per Sec:    11611, Lr: 0.000087\n",
      "2021-10-08 06:00:52,181 - INFO - joeynmt.training - Epoch   2, Step:   129700, Batch Loss:     1.302521, Tokens per Sec:    11581, Lr: 0.000087\n",
      "2021-10-08 06:01:28,120 - INFO - joeynmt.training - Epoch   2, Step:   129800, Batch Loss:     1.518005, Tokens per Sec:    11571, Lr: 0.000087\n",
      "2021-10-08 06:02:03,554 - INFO - joeynmt.training - Epoch   2, Step:   129900, Batch Loss:     1.530997, Tokens per Sec:    11638, Lr: 0.000087\n",
      "2021-10-08 06:02:39,311 - INFO - joeynmt.training - Epoch   2, Step:   130000, Batch Loss:     1.568239, Tokens per Sec:    11660, Lr: 0.000087\n",
      "2021-10-08 06:03:15,253 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 06:03:15,253 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 06:03:15,254 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 06:03:15,594 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 06:03:15,595 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 06:03:17,289 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 06:03:17,290 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 06:03:17,290 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 06:03:17,290 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-08 06:03:17,290 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 06:03:17,291 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 06:03:17,291 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 06:03:17,291 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when you get to know yourself and express your feelings in a way that speaks to others and deprives them .\n",
      "2021-10-08 06:03:17,291 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 06:03:17,292 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 06:03:17,292 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 06:03:17,292 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-08 06:03:17,293 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 06:03:17,293 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 06:03:17,293 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 06:03:17,293 - INFO - joeynmt.training - \tHypothesis: HOW DO MOUT WE WE GEESTELICE DOELVITS ?\n",
      "2021-10-08 06:03:17,294 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   130000: bleu:  44.68, loss: 36860.4023, ppl:   3.3822, duration: 37.9821s\n",
      "2021-10-08 06:03:53,469 - INFO - joeynmt.training - Epoch   2, Step:   130100, Batch Loss:     1.474256, Tokens per Sec:    11580, Lr: 0.000087\n",
      "2021-10-08 06:04:29,591 - INFO - joeynmt.training - Epoch   2, Step:   130200, Batch Loss:     1.325375, Tokens per Sec:    11486, Lr: 0.000087\n",
      "2021-10-08 06:05:05,575 - INFO - joeynmt.training - Epoch   2, Step:   130300, Batch Loss:     1.241252, Tokens per Sec:    11435, Lr: 0.000087\n",
      "2021-10-08 06:05:41,669 - INFO - joeynmt.training - Epoch   2, Step:   130400, Batch Loss:     1.464743, Tokens per Sec:    11469, Lr: 0.000087\n",
      "2021-10-08 06:06:17,901 - INFO - joeynmt.training - Epoch   2, Step:   130500, Batch Loss:     1.476368, Tokens per Sec:    11353, Lr: 0.000087\n",
      "2021-10-08 06:06:54,064 - INFO - joeynmt.training - Epoch   2, Step:   130600, Batch Loss:     1.384910, Tokens per Sec:    11586, Lr: 0.000086\n",
      "2021-10-08 06:07:30,175 - INFO - joeynmt.training - Epoch   2, Step:   130700, Batch Loss:     1.322495, Tokens per Sec:    11404, Lr: 0.000086\n",
      "2021-10-08 06:08:06,383 - INFO - joeynmt.training - Epoch   2, Step:   130800, Batch Loss:     1.387259, Tokens per Sec:    11532, Lr: 0.000086\n",
      "2021-10-08 06:08:42,620 - INFO - joeynmt.training - Epoch   2, Step:   130900, Batch Loss:     1.291883, Tokens per Sec:    11535, Lr: 0.000086\n",
      "2021-10-08 06:09:18,835 - INFO - joeynmt.training - Epoch   2, Step:   131000, Batch Loss:     1.499368, Tokens per Sec:    11715, Lr: 0.000086\n",
      "2021-10-08 06:09:55,190 - INFO - joeynmt.training - Epoch   2, Step:   131100, Batch Loss:     1.505075, Tokens per Sec:    11358, Lr: 0.000086\n",
      "2021-10-08 06:10:31,252 - INFO - joeynmt.training - Epoch   2, Step:   131200, Batch Loss:     1.458091, Tokens per Sec:    11357, Lr: 0.000086\n",
      "2021-10-08 06:11:07,616 - INFO - joeynmt.training - Epoch   2, Step:   131300, Batch Loss:     1.329261, Tokens per Sec:    11603, Lr: 0.000086\n",
      "2021-10-08 06:11:43,831 - INFO - joeynmt.training - Epoch   2, Step:   131400, Batch Loss:     1.253603, Tokens per Sec:    11430, Lr: 0.000086\n",
      "2021-10-08 06:12:20,260 - INFO - joeynmt.training - Epoch   2, Step:   131500, Batch Loss:     1.492805, Tokens per Sec:    11586, Lr: 0.000086\n",
      "2021-10-08 06:12:56,505 - INFO - joeynmt.training - Epoch   2, Step:   131600, Batch Loss:     1.456183, Tokens per Sec:    11523, Lr: 0.000086\n",
      "2021-10-08 06:13:32,848 - INFO - joeynmt.training - Epoch   2, Step:   131700, Batch Loss:     1.273978, Tokens per Sec:    11605, Lr: 0.000086\n",
      "2021-10-08 06:14:08,715 - INFO - joeynmt.training - Epoch   2, Step:   131800, Batch Loss:     1.403180, Tokens per Sec:    11382, Lr: 0.000086\n",
      "2021-10-08 06:14:44,892 - INFO - joeynmt.training - Epoch   2, Step:   131900, Batch Loss:     1.365404, Tokens per Sec:    11563, Lr: 0.000086\n",
      "2021-10-08 06:15:21,136 - INFO - joeynmt.training - Epoch   2, Step:   132000, Batch Loss:     1.406883, Tokens per Sec:    11508, Lr: 0.000086\n",
      "2021-10-08 06:15:56,915 - INFO - joeynmt.training - Epoch   2, Step:   132100, Batch Loss:     1.662481, Tokens per Sec:    11496, Lr: 0.000086\n",
      "2021-10-08 06:16:33,291 - INFO - joeynmt.training - Epoch   2, Step:   132200, Batch Loss:     1.449376, Tokens per Sec:    11553, Lr: 0.000086\n",
      "2021-10-08 06:17:09,121 - INFO - joeynmt.training - Epoch   2, Step:   132300, Batch Loss:     1.524617, Tokens per Sec:    11385, Lr: 0.000086\n",
      "2021-10-08 06:17:45,649 - INFO - joeynmt.training - Epoch   2, Step:   132400, Batch Loss:     1.290180, Tokens per Sec:    11782, Lr: 0.000086\n",
      "2021-10-08 06:18:21,996 - INFO - joeynmt.training - Epoch   2, Step:   132500, Batch Loss:     1.442397, Tokens per Sec:    11468, Lr: 0.000086\n",
      "2021-10-08 06:18:58,607 - INFO - joeynmt.training - Epoch   2, Step:   132600, Batch Loss:     1.363748, Tokens per Sec:    11765, Lr: 0.000086\n",
      "2021-10-08 06:19:34,544 - INFO - joeynmt.training - Epoch   2, Step:   132700, Batch Loss:     1.395999, Tokens per Sec:    11431, Lr: 0.000086\n",
      "2021-10-08 06:20:10,647 - INFO - joeynmt.training - Epoch   2, Step:   132800, Batch Loss:     1.365562, Tokens per Sec:    11195, Lr: 0.000086\n",
      "2021-10-08 06:20:46,705 - INFO - joeynmt.training - Epoch   2, Step:   132900, Batch Loss:     1.376438, Tokens per Sec:    11453, Lr: 0.000086\n",
      "2021-10-08 06:21:23,347 - INFO - joeynmt.training - Epoch   2, Step:   133000, Batch Loss:     1.447560, Tokens per Sec:    11536, Lr: 0.000086\n",
      "2021-10-08 06:22:00,001 - INFO - joeynmt.training - Epoch   2, Step:   133100, Batch Loss:     1.352593, Tokens per Sec:    11603, Lr: 0.000086\n",
      "2021-10-08 06:22:36,451 - INFO - joeynmt.training - Epoch   2, Step:   133200, Batch Loss:     1.509831, Tokens per Sec:    11574, Lr: 0.000086\n",
      "2021-10-08 06:23:12,912 - INFO - joeynmt.training - Epoch   2, Step:   133300, Batch Loss:     1.431795, Tokens per Sec:    11515, Lr: 0.000086\n",
      "2021-10-08 06:23:48,780 - INFO - joeynmt.training - Epoch   2, Step:   133400, Batch Loss:     1.488050, Tokens per Sec:    11459, Lr: 0.000086\n",
      "2021-10-08 06:24:24,519 - INFO - joeynmt.training - Epoch   2, Step:   133500, Batch Loss:     1.408458, Tokens per Sec:    11273, Lr: 0.000086\n",
      "2021-10-08 06:25:00,916 - INFO - joeynmt.training - Epoch   2, Step:   133600, Batch Loss:     1.523013, Tokens per Sec:    11758, Lr: 0.000085\n",
      "2021-10-08 06:25:37,067 - INFO - joeynmt.training - Epoch   2, Step:   133700, Batch Loss:     1.348805, Tokens per Sec:    11402, Lr: 0.000085\n",
      "2021-10-08 06:26:13,448 - INFO - joeynmt.training - Epoch   2, Step:   133800, Batch Loss:     1.467219, Tokens per Sec:    11741, Lr: 0.000085\n",
      "2021-10-08 06:26:49,651 - INFO - joeynmt.training - Epoch   2, Step:   133900, Batch Loss:     1.237801, Tokens per Sec:    11512, Lr: 0.000085\n",
      "2021-10-08 06:27:25,637 - INFO - joeynmt.training - Epoch   2, Step:   134000, Batch Loss:     1.403382, Tokens per Sec:    11465, Lr: 0.000085\n",
      "2021-10-08 06:28:01,757 - INFO - joeynmt.training - Epoch   2, Step:   134100, Batch Loss:     1.412681, Tokens per Sec:    11709, Lr: 0.000085\n",
      "2021-10-08 06:28:37,655 - INFO - joeynmt.training - Epoch   2, Step:   134200, Batch Loss:     1.389919, Tokens per Sec:    11426, Lr: 0.000085\n",
      "2021-10-08 06:29:13,602 - INFO - joeynmt.training - Epoch   2, Step:   134300, Batch Loss:     1.395308, Tokens per Sec:    11328, Lr: 0.000085\n",
      "2021-10-08 06:29:49,564 - INFO - joeynmt.training - Epoch   2, Step:   134400, Batch Loss:     1.281474, Tokens per Sec:    11505, Lr: 0.000085\n",
      "2021-10-08 06:30:25,761 - INFO - joeynmt.training - Epoch   2, Step:   134500, Batch Loss:     1.362579, Tokens per Sec:    11512, Lr: 0.000085\n",
      "2021-10-08 06:31:02,010 - INFO - joeynmt.training - Epoch   2, Step:   134600, Batch Loss:     1.480150, Tokens per Sec:    11696, Lr: 0.000085\n",
      "2021-10-08 06:31:38,479 - INFO - joeynmt.training - Epoch   2, Step:   134700, Batch Loss:     1.241527, Tokens per Sec:    11667, Lr: 0.000085\n",
      "2021-10-08 06:32:14,388 - INFO - joeynmt.training - Epoch   2, Step:   134800, Batch Loss:     1.478880, Tokens per Sec:    11368, Lr: 0.000085\n",
      "2021-10-08 06:32:50,561 - INFO - joeynmt.training - Epoch   2, Step:   134900, Batch Loss:     1.315069, Tokens per Sec:    11575, Lr: 0.000085\n",
      "2021-10-08 06:33:26,860 - INFO - joeynmt.training - Epoch   2, Step:   135000, Batch Loss:     1.464247, Tokens per Sec:    11613, Lr: 0.000085\n",
      "2021-10-08 06:34:03,521 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 06:34:03,522 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 06:34:03,522 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 06:34:03,862 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 06:34:03,863 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 06:34:05,546 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 06:34:05,546 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 06:34:05,547 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 06:34:05,547 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-08 06:34:05,547 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 06:34:05,547 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 06:34:05,548 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 06:34:05,548 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when you get to know yourself and express your feelings in a way that speaks to others and deprives them .\n",
      "2021-10-08 06:34:05,548 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 06:34:05,549 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 06:34:05,549 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 06:34:05,549 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” to remove himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-08 06:34:05,549 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 06:34:05,550 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 06:34:05,550 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 06:34:05,550 - INFO - joeynmt.training - \tHypothesis: HOW DO MOSE WE WE WILLD DOELVITS ?\n",
      "2021-10-08 06:34:05,550 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   135000: bleu:  44.53, loss: 36715.7109, ppl:   3.3661, duration: 38.6893s\n",
      "2021-10-08 06:34:41,912 - INFO - joeynmt.training - Epoch   2, Step:   135100, Batch Loss:     1.373803, Tokens per Sec:    11675, Lr: 0.000085\n",
      "2021-10-08 06:35:18,140 - INFO - joeynmt.training - Epoch   2, Step:   135200, Batch Loss:     1.443646, Tokens per Sec:    11546, Lr: 0.000085\n",
      "2021-10-08 06:35:54,205 - INFO - joeynmt.training - Epoch   2, Step:   135300, Batch Loss:     1.584707, Tokens per Sec:    11555, Lr: 0.000085\n",
      "2021-10-08 06:36:30,243 - INFO - joeynmt.training - Epoch   2, Step:   135400, Batch Loss:     1.396731, Tokens per Sec:    11651, Lr: 0.000085\n",
      "2021-10-08 06:37:06,909 - INFO - joeynmt.training - Epoch   2, Step:   135500, Batch Loss:     1.442143, Tokens per Sec:    11886, Lr: 0.000085\n",
      "2021-10-08 06:37:42,636 - INFO - joeynmt.training - Epoch   2, Step:   135600, Batch Loss:     1.423524, Tokens per Sec:    11567, Lr: 0.000085\n",
      "2021-10-08 06:38:18,228 - INFO - joeynmt.training - Epoch   2, Step:   135700, Batch Loss:     1.448222, Tokens per Sec:    11367, Lr: 0.000085\n",
      "2021-10-08 06:38:54,081 - INFO - joeynmt.training - Epoch   2, Step:   135800, Batch Loss:     1.347739, Tokens per Sec:    11282, Lr: 0.000085\n",
      "2021-10-08 06:39:30,428 - INFO - joeynmt.training - Epoch   2, Step:   135900, Batch Loss:     1.241281, Tokens per Sec:    11612, Lr: 0.000085\n",
      "2021-10-08 06:39:56,607 - INFO - joeynmt.training - Epoch   2: total training loss 9580.07\n",
      "2021-10-08 06:39:56,608 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-10-08 06:40:08,716 - INFO - joeynmt.training - Epoch   3, Step:   136000, Batch Loss:     1.297912, Tokens per Sec:    10059, Lr: 0.000085\n",
      "2021-10-08 06:40:44,477 - INFO - joeynmt.training - Epoch   3, Step:   136100, Batch Loss:     1.448065, Tokens per Sec:    11462, Lr: 0.000085\n",
      "2021-10-08 06:41:20,808 - INFO - joeynmt.training - Epoch   3, Step:   136200, Batch Loss:     1.272965, Tokens per Sec:    11532, Lr: 0.000085\n",
      "2021-10-08 06:41:56,681 - INFO - joeynmt.training - Epoch   3, Step:   136300, Batch Loss:     1.351872, Tokens per Sec:    11675, Lr: 0.000085\n",
      "2021-10-08 06:42:32,545 - INFO - joeynmt.training - Epoch   3, Step:   136400, Batch Loss:     1.432392, Tokens per Sec:    11518, Lr: 0.000085\n",
      "2021-10-08 06:43:08,171 - INFO - joeynmt.training - Epoch   3, Step:   136500, Batch Loss:     1.378729, Tokens per Sec:    11412, Lr: 0.000085\n",
      "2021-10-08 06:43:44,444 - INFO - joeynmt.training - Epoch   3, Step:   136600, Batch Loss:     1.365762, Tokens per Sec:    11756, Lr: 0.000085\n",
      "2021-10-08 06:44:20,520 - INFO - joeynmt.training - Epoch   3, Step:   136700, Batch Loss:     1.250222, Tokens per Sec:    11550, Lr: 0.000085\n",
      "2021-10-08 06:44:56,237 - INFO - joeynmt.training - Epoch   3, Step:   136800, Batch Loss:     1.363295, Tokens per Sec:    11741, Lr: 0.000084\n",
      "2021-10-08 06:45:33,300 - INFO - joeynmt.training - Epoch   3, Step:   136900, Batch Loss:     1.470622, Tokens per Sec:    11494, Lr: 0.000084\n",
      "2021-10-08 06:46:09,384 - INFO - joeynmt.training - Epoch   3, Step:   137000, Batch Loss:     1.417326, Tokens per Sec:    11599, Lr: 0.000084\n",
      "2021-10-08 06:46:45,260 - INFO - joeynmt.training - Epoch   3, Step:   137100, Batch Loss:     1.418443, Tokens per Sec:    11554, Lr: 0.000084\n",
      "2021-10-08 06:47:20,748 - INFO - joeynmt.training - Epoch   3, Step:   137200, Batch Loss:     1.391544, Tokens per Sec:    11433, Lr: 0.000084\n",
      "2021-10-08 06:47:56,831 - INFO - joeynmt.training - Epoch   3, Step:   137300, Batch Loss:     1.395365, Tokens per Sec:    11654, Lr: 0.000084\n",
      "2021-10-08 06:48:33,036 - INFO - joeynmt.training - Epoch   3, Step:   137400, Batch Loss:     1.421476, Tokens per Sec:    11583, Lr: 0.000084\n",
      "2021-10-08 06:49:08,888 - INFO - joeynmt.training - Epoch   3, Step:   137500, Batch Loss:     1.396337, Tokens per Sec:    11575, Lr: 0.000084\n",
      "2021-10-08 06:49:44,617 - INFO - joeynmt.training - Epoch   3, Step:   137600, Batch Loss:     1.608001, Tokens per Sec:    11509, Lr: 0.000084\n",
      "2021-10-08 06:50:20,325 - INFO - joeynmt.training - Epoch   3, Step:   137700, Batch Loss:     1.423374, Tokens per Sec:    11562, Lr: 0.000084\n",
      "2021-10-08 06:50:56,660 - INFO - joeynmt.training - Epoch   3, Step:   137800, Batch Loss:     1.512706, Tokens per Sec:    11740, Lr: 0.000084\n",
      "2021-10-08 06:51:32,361 - INFO - joeynmt.training - Epoch   3, Step:   137900, Batch Loss:     1.300044, Tokens per Sec:    11673, Lr: 0.000084\n",
      "2021-10-08 06:52:08,171 - INFO - joeynmt.training - Epoch   3, Step:   138000, Batch Loss:     1.556313, Tokens per Sec:    11481, Lr: 0.000084\n",
      "2021-10-08 06:52:44,422 - INFO - joeynmt.training - Epoch   3, Step:   138100, Batch Loss:     1.505501, Tokens per Sec:    11670, Lr: 0.000084\n",
      "2021-10-08 06:53:19,910 - INFO - joeynmt.training - Epoch   3, Step:   138200, Batch Loss:     1.404258, Tokens per Sec:    11499, Lr: 0.000084\n",
      "2021-10-08 06:53:54,948 - INFO - joeynmt.training - Epoch   3, Step:   138300, Batch Loss:     1.354363, Tokens per Sec:    11410, Lr: 0.000084\n",
      "2021-10-08 06:54:31,241 - INFO - joeynmt.training - Epoch   3, Step:   138400, Batch Loss:     1.340749, Tokens per Sec:    11756, Lr: 0.000084\n",
      "2021-10-08 06:55:07,540 - INFO - joeynmt.training - Epoch   3, Step:   138500, Batch Loss:     1.480689, Tokens per Sec:    11608, Lr: 0.000084\n",
      "2021-10-08 06:55:43,360 - INFO - joeynmt.training - Epoch   3, Step:   138600, Batch Loss:     1.499177, Tokens per Sec:    11442, Lr: 0.000084\n",
      "2021-10-08 06:56:19,691 - INFO - joeynmt.training - Epoch   3, Step:   138700, Batch Loss:     1.376684, Tokens per Sec:    11834, Lr: 0.000084\n",
      "2021-10-08 06:56:56,033 - INFO - joeynmt.training - Epoch   3, Step:   138800, Batch Loss:     1.491553, Tokens per Sec:    11748, Lr: 0.000084\n",
      "2021-10-08 06:57:32,110 - INFO - joeynmt.training - Epoch   3, Step:   138900, Batch Loss:     1.522816, Tokens per Sec:    11625, Lr: 0.000084\n",
      "2021-10-08 06:58:08,144 - INFO - joeynmt.training - Epoch   3, Step:   139000, Batch Loss:     1.447341, Tokens per Sec:    11549, Lr: 0.000084\n",
      "2021-10-08 06:58:44,131 - INFO - joeynmt.training - Epoch   3, Step:   139100, Batch Loss:     1.485271, Tokens per Sec:    11452, Lr: 0.000084\n",
      "2021-10-08 06:59:20,841 - INFO - joeynmt.training - Epoch   3, Step:   139200, Batch Loss:     1.328694, Tokens per Sec:    11880, Lr: 0.000084\n",
      "2021-10-08 06:59:56,357 - INFO - joeynmt.training - Epoch   3, Step:   139300, Batch Loss:     1.250390, Tokens per Sec:    11327, Lr: 0.000084\n",
      "2021-10-08 07:00:32,214 - INFO - joeynmt.training - Epoch   3, Step:   139400, Batch Loss:     1.448365, Tokens per Sec:    11571, Lr: 0.000084\n",
      "2021-10-08 07:01:08,062 - INFO - joeynmt.training - Epoch   3, Step:   139500, Batch Loss:     1.393793, Tokens per Sec:    11662, Lr: 0.000084\n",
      "2021-10-08 07:01:43,877 - INFO - joeynmt.training - Epoch   3, Step:   139600, Batch Loss:     1.454592, Tokens per Sec:    11752, Lr: 0.000084\n",
      "2021-10-08 07:02:19,716 - INFO - joeynmt.training - Epoch   3, Step:   139700, Batch Loss:     1.420280, Tokens per Sec:    11720, Lr: 0.000084\n",
      "2021-10-08 07:02:55,380 - INFO - joeynmt.training - Epoch   3, Step:   139800, Batch Loss:     1.447801, Tokens per Sec:    11443, Lr: 0.000084\n",
      "2021-10-08 07:03:31,287 - INFO - joeynmt.training - Epoch   3, Step:   139900, Batch Loss:     1.402863, Tokens per Sec:    11417, Lr: 0.000084\n",
      "2021-10-08 07:04:07,391 - INFO - joeynmt.training - Epoch   3, Step:   140000, Batch Loss:     1.474044, Tokens per Sec:    11666, Lr: 0.000084\n",
      "2021-10-08 07:04:42,594 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 07:04:42,595 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 07:04:42,595 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 07:04:42,937 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 07:04:42,937 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 07:04:45,339 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 07:04:45,340 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 07:04:45,340 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 07:04:45,340 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-08 07:04:45,340 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 07:04:45,341 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 07:04:45,341 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 07:04:45,341 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when you get to know yourself and express your feelings in a way that speaks to others and deprives them .\n",
      "2021-10-08 07:04:45,341 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 07:04:45,342 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 07:04:45,342 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 07:04:45,342 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-08 07:04:45,342 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 07:04:45,343 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 07:04:45,343 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 07:04:45,343 - INFO - joeynmt.training - \tHypothesis: HOW DO MOUR FEECTLICE DOELVITS ?\n",
      "2021-10-08 07:04:45,343 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   140000: bleu:  44.41, loss: 36628.3164, ppl:   3.3563, duration: 37.9512s\n",
      "2021-10-08 07:05:21,530 - INFO - joeynmt.training - Epoch   3, Step:   140100, Batch Loss:     1.370504, Tokens per Sec:    11549, Lr: 0.000083\n",
      "2021-10-08 07:05:57,483 - INFO - joeynmt.training - Epoch   3, Step:   140200, Batch Loss:     1.312300, Tokens per Sec:    11595, Lr: 0.000083\n",
      "2021-10-08 07:06:33,521 - INFO - joeynmt.training - Epoch   3, Step:   140300, Batch Loss:     1.527239, Tokens per Sec:    11871, Lr: 0.000083\n",
      "2021-10-08 07:07:09,403 - INFO - joeynmt.training - Epoch   3, Step:   140400, Batch Loss:     1.334411, Tokens per Sec:    11589, Lr: 0.000083\n",
      "2021-10-08 07:07:45,390 - INFO - joeynmt.training - Epoch   3, Step:   140500, Batch Loss:     1.531393, Tokens per Sec:    11595, Lr: 0.000083\n",
      "2021-10-08 07:08:21,495 - INFO - joeynmt.training - Epoch   3, Step:   140600, Batch Loss:     1.386880, Tokens per Sec:    11580, Lr: 0.000083\n",
      "2021-10-08 07:08:57,500 - INFO - joeynmt.training - Epoch   3, Step:   140700, Batch Loss:     1.473265, Tokens per Sec:    11376, Lr: 0.000083\n",
      "2021-10-08 07:09:33,443 - INFO - joeynmt.training - Epoch   3, Step:   140800, Batch Loss:     1.445636, Tokens per Sec:    11411, Lr: 0.000083\n",
      "2021-10-08 07:10:08,854 - INFO - joeynmt.training - Epoch   3, Step:   140900, Batch Loss:     1.316571, Tokens per Sec:    11327, Lr: 0.000083\n",
      "2021-10-08 07:10:45,038 - INFO - joeynmt.training - Epoch   3, Step:   141000, Batch Loss:     1.324658, Tokens per Sec:    11860, Lr: 0.000083\n",
      "2021-10-08 07:11:21,214 - INFO - joeynmt.training - Epoch   3, Step:   141100, Batch Loss:     1.439580, Tokens per Sec:    11662, Lr: 0.000083\n",
      "2021-10-08 07:11:57,434 - INFO - joeynmt.training - Epoch   3, Step:   141200, Batch Loss:     1.506155, Tokens per Sec:    11685, Lr: 0.000083\n",
      "2021-10-08 07:12:33,775 - INFO - joeynmt.training - Epoch   3, Step:   141300, Batch Loss:     1.429047, Tokens per Sec:    11610, Lr: 0.000083\n",
      "2021-10-08 07:13:09,583 - INFO - joeynmt.training - Epoch   3, Step:   141400, Batch Loss:     1.406235, Tokens per Sec:    11423, Lr: 0.000083\n",
      "2021-10-08 07:13:45,415 - INFO - joeynmt.training - Epoch   3, Step:   141500, Batch Loss:     1.405740, Tokens per Sec:    11652, Lr: 0.000083\n",
      "2021-10-08 07:14:21,493 - INFO - joeynmt.training - Epoch   3, Step:   141600, Batch Loss:     1.479339, Tokens per Sec:    11742, Lr: 0.000083\n",
      "2021-10-08 07:14:57,215 - INFO - joeynmt.training - Epoch   3, Step:   141700, Batch Loss:     1.351427, Tokens per Sec:    11679, Lr: 0.000083\n",
      "2021-10-08 07:15:33,059 - INFO - joeynmt.training - Epoch   3, Step:   141800, Batch Loss:     1.312610, Tokens per Sec:    11737, Lr: 0.000083\n",
      "2021-10-08 07:16:09,242 - INFO - joeynmt.training - Epoch   3, Step:   141900, Batch Loss:     1.406922, Tokens per Sec:    11704, Lr: 0.000083\n",
      "2021-10-08 07:16:45,331 - INFO - joeynmt.training - Epoch   3, Step:   142000, Batch Loss:     1.330827, Tokens per Sec:    11715, Lr: 0.000083\n",
      "2021-10-08 07:17:21,340 - INFO - joeynmt.training - Epoch   3, Step:   142100, Batch Loss:     1.478304, Tokens per Sec:    11741, Lr: 0.000083\n",
      "2021-10-08 07:17:56,956 - INFO - joeynmt.training - Epoch   3, Step:   142200, Batch Loss:     1.327902, Tokens per Sec:    11439, Lr: 0.000083\n",
      "2021-10-08 07:18:32,552 - INFO - joeynmt.training - Epoch   3, Step:   142300, Batch Loss:     1.472542, Tokens per Sec:    11426, Lr: 0.000083\n",
      "2021-10-08 07:19:08,726 - INFO - joeynmt.training - Epoch   3, Step:   142400, Batch Loss:     1.381954, Tokens per Sec:    11441, Lr: 0.000083\n",
      "2021-10-08 07:19:44,649 - INFO - joeynmt.training - Epoch   3, Step:   142500, Batch Loss:     1.361657, Tokens per Sec:    11490, Lr: 0.000083\n",
      "2021-10-08 07:20:20,373 - INFO - joeynmt.training - Epoch   3, Step:   142600, Batch Loss:     1.447960, Tokens per Sec:    11276, Lr: 0.000083\n",
      "2021-10-08 07:20:56,447 - INFO - joeynmt.training - Epoch   3, Step:   142700, Batch Loss:     1.416493, Tokens per Sec:    11684, Lr: 0.000083\n",
      "2021-10-08 07:21:20,510 - INFO - joeynmt.training - Epoch   3: total training loss 9547.47\n",
      "2021-10-08 07:21:20,511 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-10-08 07:21:34,021 - INFO - joeynmt.training - Epoch   4, Step:   142800, Batch Loss:     1.462343, Tokens per Sec:    10097, Lr: 0.000083\n",
      "2021-10-08 07:22:10,421 - INFO - joeynmt.training - Epoch   4, Step:   142900, Batch Loss:     1.375470, Tokens per Sec:    11536, Lr: 0.000083\n",
      "2021-10-08 07:22:46,595 - INFO - joeynmt.training - Epoch   4, Step:   143000, Batch Loss:     1.512718, Tokens per Sec:    11839, Lr: 0.000083\n",
      "2021-10-08 07:23:22,488 - INFO - joeynmt.training - Epoch   4, Step:   143100, Batch Loss:     1.458994, Tokens per Sec:    11621, Lr: 0.000083\n",
      "2021-10-08 07:23:58,002 - INFO - joeynmt.training - Epoch   4, Step:   143200, Batch Loss:     1.426272, Tokens per Sec:    11392, Lr: 0.000083\n",
      "2021-10-08 07:24:34,051 - INFO - joeynmt.training - Epoch   4, Step:   143300, Batch Loss:     1.509680, Tokens per Sec:    11476, Lr: 0.000083\n",
      "2021-10-08 07:25:10,031 - INFO - joeynmt.training - Epoch   4, Step:   143400, Batch Loss:     1.419484, Tokens per Sec:    11565, Lr: 0.000083\n",
      "2021-10-08 07:25:46,306 - INFO - joeynmt.training - Epoch   4, Step:   143500, Batch Loss:     1.401592, Tokens per Sec:    11711, Lr: 0.000082\n",
      "2021-10-08 07:26:22,103 - INFO - joeynmt.training - Epoch   4, Step:   143600, Batch Loss:     1.522582, Tokens per Sec:    11568, Lr: 0.000082\n",
      "2021-10-08 07:26:57,954 - INFO - joeynmt.training - Epoch   4, Step:   143700, Batch Loss:     1.381254, Tokens per Sec:    11304, Lr: 0.000082\n",
      "2021-10-08 07:27:33,986 - INFO - joeynmt.training - Epoch   4, Step:   143800, Batch Loss:     1.434427, Tokens per Sec:    11742, Lr: 0.000082\n",
      "2021-10-08 07:28:10,017 - INFO - joeynmt.training - Epoch   4, Step:   143900, Batch Loss:     1.471604, Tokens per Sec:    11597, Lr: 0.000082\n",
      "2021-10-08 07:28:45,807 - INFO - joeynmt.training - Epoch   4, Step:   144000, Batch Loss:     1.428317, Tokens per Sec:    11472, Lr: 0.000082\n",
      "2021-10-08 07:29:22,049 - INFO - joeynmt.training - Epoch   4, Step:   144100, Batch Loss:     1.366580, Tokens per Sec:    11506, Lr: 0.000082\n",
      "2021-10-08 07:29:58,187 - INFO - joeynmt.training - Epoch   4, Step:   144200, Batch Loss:     1.261485, Tokens per Sec:    11465, Lr: 0.000082\n",
      "2021-10-08 07:30:34,611 - INFO - joeynmt.training - Epoch   4, Step:   144300, Batch Loss:     1.438389, Tokens per Sec:    11915, Lr: 0.000082\n",
      "2021-10-08 07:31:10,716 - INFO - joeynmt.training - Epoch   4, Step:   144400, Batch Loss:     1.360738, Tokens per Sec:    11608, Lr: 0.000082\n",
      "2021-10-08 07:31:46,143 - INFO - joeynmt.training - Epoch   4, Step:   144500, Batch Loss:     1.235868, Tokens per Sec:    11409, Lr: 0.000082\n",
      "2021-10-08 07:32:22,074 - INFO - joeynmt.training - Epoch   4, Step:   144600, Batch Loss:     1.309556, Tokens per Sec:    11883, Lr: 0.000082\n",
      "2021-10-08 07:32:58,088 - INFO - joeynmt.training - Epoch   4, Step:   144700, Batch Loss:     1.438078, Tokens per Sec:    11669, Lr: 0.000082\n",
      "2021-10-08 07:33:34,282 - INFO - joeynmt.training - Epoch   4, Step:   144800, Batch Loss:     1.449603, Tokens per Sec:    11512, Lr: 0.000082\n",
      "2021-10-08 07:34:10,286 - INFO - joeynmt.training - Epoch   4, Step:   144900, Batch Loss:     1.401100, Tokens per Sec:    11871, Lr: 0.000082\n",
      "2021-10-08 07:34:46,035 - INFO - joeynmt.training - Epoch   4, Step:   145000, Batch Loss:     1.462764, Tokens per Sec:    11518, Lr: 0.000082\n",
      "2021-10-08 07:35:21,391 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 07:35:21,392 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 07:35:21,392 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 07:35:21,733 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 07:35:21,733 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 07:35:23,480 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 07:35:23,480 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 07:35:23,480 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 07:35:23,480 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-08 07:35:23,480 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 07:35:23,481 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 07:35:23,481 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 07:35:23,481 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when you get to know yourself and express your feelings in a way that speaks to others and deprives them .\n",
      "2021-10-08 07:35:23,481 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 07:35:23,481 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 07:35:23,481 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 07:35:23,481 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-08 07:35:23,481 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 07:35:23,482 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 07:35:23,482 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 07:35:23,482 - INFO - joeynmt.training - \tHypothesis: HOW DO MOUT WE WILL DOELVITS ?\n",
      "2021-10-08 07:35:23,482 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   145000: bleu:  44.44, loss: 36507.6719, ppl:   3.3430, duration: 37.4466s\n",
      "2021-10-08 07:36:00,134 - INFO - joeynmt.training - Epoch   4, Step:   145100, Batch Loss:     1.516845, Tokens per Sec:    11314, Lr: 0.000082\n",
      "2021-10-08 07:36:36,778 - INFO - joeynmt.training - Epoch   4, Step:   145200, Batch Loss:     1.374120, Tokens per Sec:    11801, Lr: 0.000082\n",
      "2021-10-08 07:37:12,415 - INFO - joeynmt.training - Epoch   4, Step:   145300, Batch Loss:     1.409994, Tokens per Sec:    11280, Lr: 0.000082\n",
      "2021-10-08 07:37:48,600 - INFO - joeynmt.training - Epoch   4, Step:   145400, Batch Loss:     1.358155, Tokens per Sec:    11705, Lr: 0.000082\n",
      "2021-10-08 07:38:25,001 - INFO - joeynmt.training - Epoch   4, Step:   145500, Batch Loss:     1.355584, Tokens per Sec:    11739, Lr: 0.000082\n",
      "2021-10-08 07:39:00,388 - INFO - joeynmt.training - Epoch   4, Step:   145600, Batch Loss:     1.274542, Tokens per Sec:    11410, Lr: 0.000082\n",
      "2021-10-08 07:39:36,136 - INFO - joeynmt.training - Epoch   4, Step:   145700, Batch Loss:     1.375826, Tokens per Sec:    11539, Lr: 0.000082\n",
      "2021-10-08 07:40:12,079 - INFO - joeynmt.training - Epoch   4, Step:   145800, Batch Loss:     1.352955, Tokens per Sec:    11475, Lr: 0.000082\n",
      "2021-10-08 07:40:48,320 - INFO - joeynmt.training - Epoch   4, Step:   145900, Batch Loss:     1.363818, Tokens per Sec:    11539, Lr: 0.000082\n",
      "2021-10-08 07:41:24,205 - INFO - joeynmt.training - Epoch   4, Step:   146000, Batch Loss:     1.371413, Tokens per Sec:    11516, Lr: 0.000082\n",
      "2021-10-08 07:41:59,981 - INFO - joeynmt.training - Epoch   4, Step:   146100, Batch Loss:     1.518921, Tokens per Sec:    11636, Lr: 0.000082\n",
      "2021-10-08 07:42:36,274 - INFO - joeynmt.training - Epoch   4, Step:   146200, Batch Loss:     1.497288, Tokens per Sec:    11544, Lr: 0.000082\n",
      "2021-10-08 07:43:11,964 - INFO - joeynmt.training - Epoch   4, Step:   146300, Batch Loss:     1.418594, Tokens per Sec:    11517, Lr: 0.000082\n",
      "2021-10-08 07:43:47,859 - INFO - joeynmt.training - Epoch   4, Step:   146400, Batch Loss:     1.379541, Tokens per Sec:    11462, Lr: 0.000082\n",
      "2021-10-08 07:44:24,012 - INFO - joeynmt.training - Epoch   4, Step:   146500, Batch Loss:     1.485663, Tokens per Sec:    11677, Lr: 0.000082\n",
      "2021-10-08 07:44:59,829 - INFO - joeynmt.training - Epoch   4, Step:   146600, Batch Loss:     1.331866, Tokens per Sec:    11486, Lr: 0.000082\n",
      "2021-10-08 07:45:35,235 - INFO - joeynmt.training - Epoch   4, Step:   146700, Batch Loss:     1.290047, Tokens per Sec:    11381, Lr: 0.000082\n",
      "2021-10-08 07:46:11,136 - INFO - joeynmt.training - Epoch   4, Step:   146800, Batch Loss:     1.322006, Tokens per Sec:    11535, Lr: 0.000082\n",
      "2021-10-08 07:46:47,204 - INFO - joeynmt.training - Epoch   4, Step:   146900, Batch Loss:     1.439833, Tokens per Sec:    11688, Lr: 0.000082\n",
      "2021-10-08 07:47:23,219 - INFO - joeynmt.training - Epoch   4, Step:   147000, Batch Loss:     1.398768, Tokens per Sec:    11214, Lr: 0.000082\n",
      "2021-10-08 07:47:58,945 - INFO - joeynmt.training - Epoch   4, Step:   147100, Batch Loss:     1.238730, Tokens per Sec:    11496, Lr: 0.000081\n",
      "2021-10-08 07:48:34,316 - INFO - joeynmt.training - Epoch   4, Step:   147200, Batch Loss:     1.371113, Tokens per Sec:    11386, Lr: 0.000081\n",
      "2021-10-08 07:49:10,446 - INFO - joeynmt.training - Epoch   4, Step:   147300, Batch Loss:     1.413390, Tokens per Sec:    11652, Lr: 0.000081\n",
      "2021-10-08 07:49:46,459 - INFO - joeynmt.training - Epoch   4, Step:   147400, Batch Loss:     1.346157, Tokens per Sec:    11572, Lr: 0.000081\n",
      "2021-10-08 07:50:22,480 - INFO - joeynmt.training - Epoch   4, Step:   147500, Batch Loss:     1.461645, Tokens per Sec:    11635, Lr: 0.000081\n",
      "2021-10-08 07:50:58,129 - INFO - joeynmt.training - Epoch   4, Step:   147600, Batch Loss:     1.350260, Tokens per Sec:    11405, Lr: 0.000081\n",
      "2021-10-08 07:51:34,198 - INFO - joeynmt.training - Epoch   4, Step:   147700, Batch Loss:     1.341265, Tokens per Sec:    11638, Lr: 0.000081\n",
      "2021-10-08 07:52:10,240 - INFO - joeynmt.training - Epoch   4, Step:   147800, Batch Loss:     1.391196, Tokens per Sec:    11618, Lr: 0.000081\n",
      "2021-10-08 07:52:45,911 - INFO - joeynmt.training - Epoch   4, Step:   147900, Batch Loss:     1.453718, Tokens per Sec:    11502, Lr: 0.000081\n",
      "2021-10-08 07:53:22,063 - INFO - joeynmt.training - Epoch   4, Step:   148000, Batch Loss:     1.166337, Tokens per Sec:    11764, Lr: 0.000081\n",
      "2021-10-08 07:53:58,158 - INFO - joeynmt.training - Epoch   4, Step:   148100, Batch Loss:     1.503983, Tokens per Sec:    11675, Lr: 0.000081\n",
      "2021-10-08 07:54:34,211 - INFO - joeynmt.training - Epoch   4, Step:   148200, Batch Loss:     1.461782, Tokens per Sec:    11432, Lr: 0.000081\n",
      "2021-10-08 07:55:09,880 - INFO - joeynmt.training - Epoch   4, Step:   148300, Batch Loss:     1.413463, Tokens per Sec:    11523, Lr: 0.000081\n",
      "2021-10-08 07:55:46,084 - INFO - joeynmt.training - Epoch   4, Step:   148400, Batch Loss:     1.316125, Tokens per Sec:    11564, Lr: 0.000081\n",
      "2021-10-08 07:56:22,242 - INFO - joeynmt.training - Epoch   4, Step:   148500, Batch Loss:     1.431396, Tokens per Sec:    11603, Lr: 0.000081\n",
      "2021-10-08 07:56:57,938 - INFO - joeynmt.training - Epoch   4, Step:   148600, Batch Loss:     1.421861, Tokens per Sec:    11750, Lr: 0.000081\n",
      "2021-10-08 07:57:33,605 - INFO - joeynmt.training - Epoch   4, Step:   148700, Batch Loss:     1.415084, Tokens per Sec:    11314, Lr: 0.000081\n",
      "2021-10-08 07:58:09,804 - INFO - joeynmt.training - Epoch   4, Step:   148800, Batch Loss:     1.329976, Tokens per Sec:    11691, Lr: 0.000081\n",
      "2021-10-08 07:58:45,890 - INFO - joeynmt.training - Epoch   4, Step:   148900, Batch Loss:     1.356027, Tokens per Sec:    11716, Lr: 0.000081\n",
      "2021-10-08 07:59:21,835 - INFO - joeynmt.training - Epoch   4, Step:   149000, Batch Loss:     1.512063, Tokens per Sec:    11512, Lr: 0.000081\n",
      "2021-10-08 07:59:57,558 - INFO - joeynmt.training - Epoch   4, Step:   149100, Batch Loss:     1.301220, Tokens per Sec:    11592, Lr: 0.000081\n",
      "2021-10-08 08:00:33,429 - INFO - joeynmt.training - Epoch   4, Step:   149200, Batch Loss:     1.368499, Tokens per Sec:    11862, Lr: 0.000081\n",
      "2021-10-08 08:01:09,525 - INFO - joeynmt.training - Epoch   4, Step:   149300, Batch Loss:     1.444369, Tokens per Sec:    11572, Lr: 0.000081\n",
      "2021-10-08 08:01:45,426 - INFO - joeynmt.training - Epoch   4, Step:   149400, Batch Loss:     1.321947, Tokens per Sec:    11356, Lr: 0.000081\n",
      "2021-10-08 08:02:21,366 - INFO - joeynmt.training - Epoch   4, Step:   149500, Batch Loss:     1.367523, Tokens per Sec:    11580, Lr: 0.000081\n",
      "2021-10-08 08:02:47,846 - INFO - joeynmt.training - Epoch   4: total training loss 9522.62\n",
      "2021-10-08 08:02:47,847 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-10-08 08:02:59,276 - INFO - joeynmt.training - Epoch   5, Step:   149600, Batch Loss:     1.482434, Tokens per Sec:    10275, Lr: 0.000081\n",
      "2021-10-08 08:03:35,388 - INFO - joeynmt.training - Epoch   5, Step:   149700, Batch Loss:     1.452538, Tokens per Sec:    11553, Lr: 0.000081\n",
      "2021-10-08 08:04:11,375 - INFO - joeynmt.training - Epoch   5, Step:   149800, Batch Loss:     1.550784, Tokens per Sec:    11589, Lr: 0.000081\n",
      "2021-10-08 08:04:47,963 - INFO - joeynmt.training - Epoch   5, Step:   149900, Batch Loss:     1.361749, Tokens per Sec:    11481, Lr: 0.000081\n",
      "2021-10-08 08:05:23,883 - INFO - joeynmt.training - Epoch   5, Step:   150000, Batch Loss:     1.390757, Tokens per Sec:    11832, Lr: 0.000081\n",
      "2021-10-08 08:05:59,357 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 08:05:59,357 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 08:05:59,358 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 08:05:59,697 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 08:05:59,697 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 08:06:01,437 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 08:06:01,437 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 08:06:01,438 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 08:06:01,438 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-08 08:06:01,438 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 08:06:01,438 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 08:06:01,438 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 08:06:01,438 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when you get to know yourself and express your feelings in a way that speak to others and deprive them .\n",
      "2021-10-08 08:06:01,438 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 08:06:01,438 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 08:06:01,439 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 08:06:01,439 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-08 08:06:01,439 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 08:06:01,439 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 08:06:01,439 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 08:06:01,439 - INFO - joeynmt.training - \tHypothesis: HOW DO MOY WE WE WITH DOELVITS ?\n",
      "2021-10-08 08:06:01,439 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   150000: bleu:  44.53, loss: 36433.3750, ppl:   3.3348, duration: 37.5566s\n",
      "2021-10-08 08:06:37,173 - INFO - joeynmt.training - Epoch   5, Step:   150100, Batch Loss:     1.244258, Tokens per Sec:    11793, Lr: 0.000081\n",
      "2021-10-08 08:07:13,169 - INFO - joeynmt.training - Epoch   5, Step:   150200, Batch Loss:     1.420995, Tokens per Sec:    11629, Lr: 0.000081\n",
      "2021-10-08 08:07:48,673 - INFO - joeynmt.training - Epoch   5, Step:   150300, Batch Loss:     1.542956, Tokens per Sec:    11486, Lr: 0.000081\n",
      "2021-10-08 08:08:24,399 - INFO - joeynmt.training - Epoch   5, Step:   150400, Batch Loss:     1.261969, Tokens per Sec:    11436, Lr: 0.000081\n",
      "2021-10-08 08:09:00,917 - INFO - joeynmt.training - Epoch   5, Step:   150500, Batch Loss:     1.352023, Tokens per Sec:    11308, Lr: 0.000081\n",
      "2021-10-08 08:09:36,890 - INFO - joeynmt.training - Epoch   5, Step:   150600, Batch Loss:     1.313682, Tokens per Sec:    11855, Lr: 0.000081\n",
      "2021-10-08 08:10:12,773 - INFO - joeynmt.training - Epoch   5, Step:   150700, Batch Loss:     1.341158, Tokens per Sec:    11743, Lr: 0.000080\n",
      "2021-10-08 08:10:48,883 - INFO - joeynmt.training - Epoch   5, Step:   150800, Batch Loss:     1.341788, Tokens per Sec:    11555, Lr: 0.000080\n",
      "2021-10-08 08:11:24,928 - INFO - joeynmt.training - Epoch   5, Step:   150900, Batch Loss:     1.349412, Tokens per Sec:    11428, Lr: 0.000080\n",
      "2021-10-08 08:12:00,726 - INFO - joeynmt.training - Epoch   5, Step:   151000, Batch Loss:     1.470644, Tokens per Sec:    11350, Lr: 0.000080\n",
      "2021-10-08 08:12:36,583 - INFO - joeynmt.training - Epoch   5, Step:   151100, Batch Loss:     1.468528, Tokens per Sec:    11468, Lr: 0.000080\n",
      "2021-10-08 08:13:12,190 - INFO - joeynmt.training - Epoch   5, Step:   151200, Batch Loss:     1.390052, Tokens per Sec:    11390, Lr: 0.000080\n",
      "2021-10-08 08:13:48,740 - INFO - joeynmt.training - Epoch   5, Step:   151300, Batch Loss:     1.452239, Tokens per Sec:    11795, Lr: 0.000080\n",
      "2021-10-08 08:14:24,551 - INFO - joeynmt.training - Epoch   5, Step:   151400, Batch Loss:     1.400139, Tokens per Sec:    11572, Lr: 0.000080\n",
      "2021-10-08 08:15:00,244 - INFO - joeynmt.training - Epoch   5, Step:   151500, Batch Loss:     1.261491, Tokens per Sec:    11561, Lr: 0.000080\n",
      "2021-10-08 08:15:36,648 - INFO - joeynmt.training - Epoch   5, Step:   151600, Batch Loss:     1.455951, Tokens per Sec:    11670, Lr: 0.000080\n",
      "2021-10-08 08:16:12,519 - INFO - joeynmt.training - Epoch   5, Step:   151700, Batch Loss:     1.334871, Tokens per Sec:    11791, Lr: 0.000080\n",
      "2021-10-08 08:16:48,680 - INFO - joeynmt.training - Epoch   5, Step:   151800, Batch Loss:     1.503069, Tokens per Sec:    11654, Lr: 0.000080\n",
      "2021-10-08 08:17:24,527 - INFO - joeynmt.training - Epoch   5, Step:   151900, Batch Loss:     1.353012, Tokens per Sec:    11515, Lr: 0.000080\n",
      "2021-10-08 08:18:00,544 - INFO - joeynmt.training - Epoch   5, Step:   152000, Batch Loss:     1.442906, Tokens per Sec:    11535, Lr: 0.000080\n",
      "2021-10-08 08:18:36,276 - INFO - joeynmt.training - Epoch   5, Step:   152100, Batch Loss:     1.408815, Tokens per Sec:    11223, Lr: 0.000080\n",
      "2021-10-08 08:19:12,252 - INFO - joeynmt.training - Epoch   5, Step:   152200, Batch Loss:     1.379588, Tokens per Sec:    11688, Lr: 0.000080\n",
      "2021-10-08 08:19:48,303 - INFO - joeynmt.training - Epoch   5, Step:   152300, Batch Loss:     1.407684, Tokens per Sec:    11682, Lr: 0.000080\n",
      "2021-10-08 08:20:24,144 - INFO - joeynmt.training - Epoch   5, Step:   152400, Batch Loss:     1.609094, Tokens per Sec:    11622, Lr: 0.000080\n",
      "2021-10-08 08:21:00,050 - INFO - joeynmt.training - Epoch   5, Step:   152500, Batch Loss:     1.344838, Tokens per Sec:    11452, Lr: 0.000080\n",
      "2021-10-08 08:21:35,968 - INFO - joeynmt.training - Epoch   5, Step:   152600, Batch Loss:     1.265886, Tokens per Sec:    11609, Lr: 0.000080\n",
      "2021-10-08 08:22:12,204 - INFO - joeynmt.training - Epoch   5, Step:   152700, Batch Loss:     1.374220, Tokens per Sec:    11508, Lr: 0.000080\n",
      "2021-10-08 08:22:48,280 - INFO - joeynmt.training - Epoch   5, Step:   152800, Batch Loss:     1.452630, Tokens per Sec:    11466, Lr: 0.000080\n",
      "2021-10-08 08:23:24,058 - INFO - joeynmt.training - Epoch   5, Step:   152900, Batch Loss:     1.375323, Tokens per Sec:    11271, Lr: 0.000080\n",
      "2021-10-08 08:24:00,425 - INFO - joeynmt.training - Epoch   5, Step:   153000, Batch Loss:     1.422153, Tokens per Sec:    11614, Lr: 0.000080\n",
      "2021-10-08 08:24:36,461 - INFO - joeynmt.training - Epoch   5, Step:   153100, Batch Loss:     1.318920, Tokens per Sec:    11549, Lr: 0.000080\n",
      "2021-10-08 08:25:12,961 - INFO - joeynmt.training - Epoch   5, Step:   153200, Batch Loss:     1.318494, Tokens per Sec:    11856, Lr: 0.000080\n",
      "2021-10-08 08:25:49,224 - INFO - joeynmt.training - Epoch   5, Step:   153300, Batch Loss:     1.272489, Tokens per Sec:    11676, Lr: 0.000080\n",
      "2021-10-08 08:26:25,025 - INFO - joeynmt.training - Epoch   5, Step:   153400, Batch Loss:     1.553808, Tokens per Sec:    11506, Lr: 0.000080\n",
      "2021-10-08 08:27:01,447 - INFO - joeynmt.training - Epoch   5, Step:   153500, Batch Loss:     1.468763, Tokens per Sec:    11668, Lr: 0.000080\n",
      "2021-10-08 08:27:37,099 - INFO - joeynmt.training - Epoch   5, Step:   153600, Batch Loss:     1.185552, Tokens per Sec:    11161, Lr: 0.000080\n",
      "2021-10-08 08:28:13,450 - INFO - joeynmt.training - Epoch   5, Step:   153700, Batch Loss:     1.529818, Tokens per Sec:    11685, Lr: 0.000080\n",
      "2021-10-08 08:28:49,200 - INFO - joeynmt.training - Epoch   5, Step:   153800, Batch Loss:     1.434330, Tokens per Sec:    11318, Lr: 0.000080\n",
      "2021-10-08 08:29:25,320 - INFO - joeynmt.training - Epoch   5, Step:   153900, Batch Loss:     1.452904, Tokens per Sec:    11703, Lr: 0.000080\n",
      "2021-10-08 08:30:01,665 - INFO - joeynmt.training - Epoch   5, Step:   154000, Batch Loss:     1.437480, Tokens per Sec:    11702, Lr: 0.000080\n",
      "2021-10-08 08:30:37,603 - INFO - joeynmt.training - Epoch   5, Step:   154100, Batch Loss:     1.303213, Tokens per Sec:    11532, Lr: 0.000080\n",
      "2021-10-08 08:31:13,747 - INFO - joeynmt.training - Epoch   5, Step:   154200, Batch Loss:     1.302389, Tokens per Sec:    11553, Lr: 0.000080\n",
      "2021-10-08 08:31:49,905 - INFO - joeynmt.training - Epoch   5, Step:   154300, Batch Loss:     1.405077, Tokens per Sec:    11652, Lr: 0.000080\n",
      "2021-10-08 08:32:25,579 - INFO - joeynmt.training - Epoch   5, Step:   154400, Batch Loss:     1.394811, Tokens per Sec:    11392, Lr: 0.000080\n",
      "2021-10-08 08:33:01,927 - INFO - joeynmt.training - Epoch   5, Step:   154500, Batch Loss:     1.370133, Tokens per Sec:    11641, Lr: 0.000080\n",
      "2021-10-08 08:33:37,744 - INFO - joeynmt.training - Epoch   5, Step:   154600, Batch Loss:     1.525270, Tokens per Sec:    11225, Lr: 0.000079\n",
      "2021-10-08 08:34:13,952 - INFO - joeynmt.training - Epoch   5, Step:   154700, Batch Loss:     1.369216, Tokens per Sec:    11529, Lr: 0.000079\n",
      "2021-10-08 08:34:50,052 - INFO - joeynmt.training - Epoch   5, Step:   154800, Batch Loss:     1.409184, Tokens per Sec:    11569, Lr: 0.000079\n",
      "2021-10-08 08:35:26,014 - INFO - joeynmt.training - Epoch   5, Step:   154900, Batch Loss:     1.413606, Tokens per Sec:    11491, Lr: 0.000079\n",
      "2021-10-08 08:36:01,988 - INFO - joeynmt.training - Epoch   5, Step:   155000, Batch Loss:     1.322091, Tokens per Sec:    11575, Lr: 0.000079\n",
      "2021-10-08 08:36:38,110 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 08:36:38,111 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 08:36:38,111 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 08:36:38,453 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 08:36:38,454 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 08:36:40,200 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 08:36:40,200 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 08:36:40,200 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 08:36:40,200 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at that time for the brother who took the lead in a congregation .\n",
      "2021-10-08 08:36:40,200 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 08:36:40,201 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 08:36:40,201 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 08:36:40,201 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when you get to know yourself and express your feelings in a way that speak to others and deprive them .\n",
      "2021-10-08 08:36:40,201 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 08:36:40,201 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 08:36:40,201 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 08:36:40,201 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-08 08:36:40,201 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 08:36:40,202 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 08:36:40,202 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 08:36:40,202 - INFO - joeynmt.training - \tHypothesis: HOW DO MOUT WE WE WILLD DOELVITS ?\n",
      "2021-10-08 08:36:40,202 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   155000: bleu:  44.61, loss: 36232.9023, ppl:   3.3128, duration: 38.2133s\n",
      "2021-10-08 08:37:16,253 - INFO - joeynmt.training - Epoch   5, Step:   155100, Batch Loss:     1.456220, Tokens per Sec:    11628, Lr: 0.000079\n",
      "2021-10-08 08:37:52,452 - INFO - joeynmt.training - Epoch   5, Step:   155200, Batch Loss:     1.489261, Tokens per Sec:    11682, Lr: 0.000079\n",
      "2021-10-08 08:38:27,856 - INFO - joeynmt.training - Epoch   5, Step:   155300, Batch Loss:     1.546924, Tokens per Sec:    11007, Lr: 0.000079\n",
      "2021-10-08 08:39:03,608 - INFO - joeynmt.training - Epoch   5, Step:   155400, Batch Loss:     1.339912, Tokens per Sec:    11513, Lr: 0.000079\n",
      "2021-10-08 08:39:39,449 - INFO - joeynmt.training - Epoch   5, Step:   155500, Batch Loss:     1.402373, Tokens per Sec:    11353, Lr: 0.000079\n",
      "2021-10-08 08:40:15,852 - INFO - joeynmt.training - Epoch   5, Step:   155600, Batch Loss:     1.174324, Tokens per Sec:    11790, Lr: 0.000079\n",
      "2021-10-08 08:40:51,554 - INFO - joeynmt.training - Epoch   5, Step:   155700, Batch Loss:     1.361487, Tokens per Sec:    11526, Lr: 0.000079\n",
      "2021-10-08 08:41:27,062 - INFO - joeynmt.training - Epoch   5, Step:   155800, Batch Loss:     1.339803, Tokens per Sec:    11369, Lr: 0.000079\n",
      "2021-10-08 08:42:03,032 - INFO - joeynmt.training - Epoch   5, Step:   155900, Batch Loss:     1.438109, Tokens per Sec:    11203, Lr: 0.000079\n",
      "2021-10-08 08:42:39,375 - INFO - joeynmt.training - Epoch   5, Step:   156000, Batch Loss:     1.403742, Tokens per Sec:    11716, Lr: 0.000079\n",
      "2021-10-08 08:43:15,087 - INFO - joeynmt.training - Epoch   5, Step:   156100, Batch Loss:     1.503864, Tokens per Sec:    11454, Lr: 0.000079\n",
      "2021-10-08 08:43:50,967 - INFO - joeynmt.training - Epoch   5, Step:   156200, Batch Loss:     1.348228, Tokens per Sec:    11547, Lr: 0.000079\n",
      "2021-10-08 08:44:27,093 - INFO - joeynmt.training - Epoch   5, Step:   156300, Batch Loss:     1.460298, Tokens per Sec:    11702, Lr: 0.000079\n",
      "2021-10-08 08:44:57,677 - INFO - joeynmt.training - Epoch   5: total training loss 9493.34\n",
      "2021-10-08 08:44:57,678 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-10-08 08:45:04,596 - INFO - joeynmt.training - Epoch   6, Step:   156400, Batch Loss:     1.517753, Tokens per Sec:     9756, Lr: 0.000079\n",
      "2021-10-08 08:45:40,650 - INFO - joeynmt.training - Epoch   6, Step:   156500, Batch Loss:     1.385382, Tokens per Sec:    11698, Lr: 0.000079\n",
      "2021-10-08 08:46:16,560 - INFO - joeynmt.training - Epoch   6, Step:   156600, Batch Loss:     1.408148, Tokens per Sec:    11248, Lr: 0.000079\n",
      "2021-10-08 08:46:52,746 - INFO - joeynmt.training - Epoch   6, Step:   156700, Batch Loss:     1.368391, Tokens per Sec:    11628, Lr: 0.000079\n",
      "2021-10-08 08:47:28,598 - INFO - joeynmt.training - Epoch   6, Step:   156800, Batch Loss:     1.270540, Tokens per Sec:    11514, Lr: 0.000079\n",
      "2021-10-08 08:48:04,291 - INFO - joeynmt.training - Epoch   6, Step:   156900, Batch Loss:     1.349103, Tokens per Sec:    11473, Lr: 0.000079\n",
      "2021-10-08 08:48:40,362 - INFO - joeynmt.training - Epoch   6, Step:   157000, Batch Loss:     1.511313, Tokens per Sec:    11553, Lr: 0.000079\n",
      "2021-10-08 08:49:16,018 - INFO - joeynmt.training - Epoch   6, Step:   157100, Batch Loss:     1.360416, Tokens per Sec:    11474, Lr: 0.000079\n",
      "2021-10-08 08:49:52,078 - INFO - joeynmt.training - Epoch   6, Step:   157200, Batch Loss:     1.315191, Tokens per Sec:    11822, Lr: 0.000079\n",
      "2021-10-08 08:50:27,827 - INFO - joeynmt.training - Epoch   6, Step:   157300, Batch Loss:     1.402327, Tokens per Sec:    11530, Lr: 0.000079\n",
      "2021-10-08 08:51:03,597 - INFO - joeynmt.training - Epoch   6, Step:   157400, Batch Loss:     1.444554, Tokens per Sec:    11592, Lr: 0.000079\n",
      "2021-10-08 08:51:39,933 - INFO - joeynmt.training - Epoch   6, Step:   157500, Batch Loss:     1.327244, Tokens per Sec:    11694, Lr: 0.000079\n",
      "2021-10-08 08:52:15,788 - INFO - joeynmt.training - Epoch   6, Step:   157600, Batch Loss:     1.454343, Tokens per Sec:    11587, Lr: 0.000079\n",
      "2021-10-08 08:52:51,952 - INFO - joeynmt.training - Epoch   6, Step:   157700, Batch Loss:     1.223427, Tokens per Sec:    11538, Lr: 0.000079\n",
      "2021-10-08 08:53:28,105 - INFO - joeynmt.training - Epoch   6, Step:   157800, Batch Loss:     1.516668, Tokens per Sec:    11602, Lr: 0.000079\n",
      "2021-10-08 08:54:03,631 - INFO - joeynmt.training - Epoch   6, Step:   157900, Batch Loss:     1.610456, Tokens per Sec:    11286, Lr: 0.000079\n",
      "2021-10-08 08:54:39,428 - INFO - joeynmt.training - Epoch   6, Step:   158000, Batch Loss:     1.258896, Tokens per Sec:    11645, Lr: 0.000079\n",
      "2021-10-08 08:55:15,813 - INFO - joeynmt.training - Epoch   6, Step:   158100, Batch Loss:     1.358707, Tokens per Sec:    11690, Lr: 0.000079\n",
      "2021-10-08 08:55:51,175 - INFO - joeynmt.training - Epoch   6, Step:   158200, Batch Loss:     1.488961, Tokens per Sec:    11419, Lr: 0.000079\n",
      "2021-10-08 08:56:26,900 - INFO - joeynmt.training - Epoch   6, Step:   158300, Batch Loss:     1.399688, Tokens per Sec:    11648, Lr: 0.000079\n",
      "2021-10-08 08:57:02,871 - INFO - joeynmt.training - Epoch   6, Step:   158400, Batch Loss:     1.397113, Tokens per Sec:    11568, Lr: 0.000079\n",
      "2021-10-08 08:57:38,874 - INFO - joeynmt.training - Epoch   6, Step:   158500, Batch Loss:     1.543259, Tokens per Sec:    11622, Lr: 0.000078\n",
      "2021-10-08 08:58:14,971 - INFO - joeynmt.training - Epoch   6, Step:   158600, Batch Loss:     1.345991, Tokens per Sec:    11686, Lr: 0.000078\n",
      "2021-10-08 08:58:50,712 - INFO - joeynmt.training - Epoch   6, Step:   158700, Batch Loss:     1.416095, Tokens per Sec:    11583, Lr: 0.000078\n",
      "2021-10-08 08:59:26,428 - INFO - joeynmt.training - Epoch   6, Step:   158800, Batch Loss:     1.419283, Tokens per Sec:    11698, Lr: 0.000078\n",
      "2021-10-08 09:00:02,832 - INFO - joeynmt.training - Epoch   6, Step:   158900, Batch Loss:     1.498041, Tokens per Sec:    11716, Lr: 0.000078\n",
      "2021-10-08 09:00:39,220 - INFO - joeynmt.training - Epoch   6, Step:   159000, Batch Loss:     1.330488, Tokens per Sec:    11670, Lr: 0.000078\n",
      "2021-10-08 09:01:15,245 - INFO - joeynmt.training - Epoch   6, Step:   159100, Batch Loss:     1.428470, Tokens per Sec:    11617, Lr: 0.000078\n",
      "2021-10-08 09:01:51,115 - INFO - joeynmt.training - Epoch   6, Step:   159200, Batch Loss:     1.420917, Tokens per Sec:    11308, Lr: 0.000078\n",
      "2021-10-08 09:02:26,834 - INFO - joeynmt.training - Epoch   6, Step:   159300, Batch Loss:     1.398337, Tokens per Sec:    11689, Lr: 0.000078\n",
      "2021-10-08 09:03:03,086 - INFO - joeynmt.training - Epoch   6, Step:   159400, Batch Loss:     1.456114, Tokens per Sec:    11747, Lr: 0.000078\n",
      "2021-10-08 09:03:39,059 - INFO - joeynmt.training - Epoch   6, Step:   159500, Batch Loss:     1.391660, Tokens per Sec:    11651, Lr: 0.000078\n",
      "2021-10-08 09:04:14,560 - INFO - joeynmt.training - Epoch   6, Step:   159600, Batch Loss:     1.427492, Tokens per Sec:    11660, Lr: 0.000078\n",
      "2021-10-08 09:04:50,208 - INFO - joeynmt.training - Epoch   6, Step:   159700, Batch Loss:     1.535338, Tokens per Sec:    11574, Lr: 0.000078\n",
      "2021-10-08 09:05:26,429 - INFO - joeynmt.training - Epoch   6, Step:   159800, Batch Loss:     1.459573, Tokens per Sec:    11923, Lr: 0.000078\n",
      "2021-10-08 09:06:02,283 - INFO - joeynmt.training - Epoch   6, Step:   159900, Batch Loss:     1.349902, Tokens per Sec:    11397, Lr: 0.000078\n",
      "2021-10-08 09:06:38,396 - INFO - joeynmt.training - Epoch   6, Step:   160000, Batch Loss:     1.413723, Tokens per Sec:    11696, Lr: 0.000078\n",
      "2021-10-08 09:07:14,282 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 09:07:14,283 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 09:07:14,283 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 09:07:14,627 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 09:07:14,628 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 09:07:16,372 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 09:07:16,372 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 09:07:16,372 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 09:07:16,372 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-08 09:07:16,372 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 09:07:16,373 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 09:07:16,373 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 09:07:16,373 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when you get to know yourself and express your feelings in a way that speak to others and deprive them .\n",
      "2021-10-08 09:07:16,373 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 09:07:16,373 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 09:07:16,373 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 09:07:16,373 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-08 09:07:16,374 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 09:07:16,374 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 09:07:16,374 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 09:07:16,374 - INFO - joeynmt.training - \tHypothesis: HOW DO MOUR FECTED DOLD SILL ?\n",
      "2021-10-08 09:07:16,374 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   160000: bleu:  44.47, loss: 36215.2227, ppl:   3.3108, duration: 37.9772s\n",
      "2021-10-08 09:07:52,550 - INFO - joeynmt.training - Epoch   6, Step:   160100, Batch Loss:     1.396500, Tokens per Sec:    11631, Lr: 0.000078\n",
      "2021-10-08 09:08:28,215 - INFO - joeynmt.training - Epoch   6, Step:   160200, Batch Loss:     1.314950, Tokens per Sec:    11462, Lr: 0.000078\n",
      "2021-10-08 09:09:04,050 - INFO - joeynmt.training - Epoch   6, Step:   160300, Batch Loss:     1.382194, Tokens per Sec:    11578, Lr: 0.000078\n",
      "2021-10-08 09:09:40,040 - INFO - joeynmt.training - Epoch   6, Step:   160400, Batch Loss:     1.213471, Tokens per Sec:    11647, Lr: 0.000078\n",
      "2021-10-08 09:10:15,939 - INFO - joeynmt.training - Epoch   6, Step:   160500, Batch Loss:     1.276729, Tokens per Sec:    11521, Lr: 0.000078\n",
      "2021-10-08 09:10:51,603 - INFO - joeynmt.training - Epoch   6, Step:   160600, Batch Loss:     1.458872, Tokens per Sec:    11455, Lr: 0.000078\n",
      "2021-10-08 09:11:27,961 - INFO - joeynmt.training - Epoch   6, Step:   160700, Batch Loss:     1.325655, Tokens per Sec:    11709, Lr: 0.000078\n",
      "2021-10-08 09:12:03,681 - INFO - joeynmt.training - Epoch   6, Step:   160800, Batch Loss:     1.426918, Tokens per Sec:    11340, Lr: 0.000078\n",
      "2021-10-08 09:12:39,707 - INFO - joeynmt.training - Epoch   6, Step:   160900, Batch Loss:     1.392969, Tokens per Sec:    11607, Lr: 0.000078\n",
      "2021-10-08 09:13:16,114 - INFO - joeynmt.training - Epoch   6, Step:   161000, Batch Loss:     1.420332, Tokens per Sec:    11769, Lr: 0.000078\n",
      "2021-10-08 09:13:52,202 - INFO - joeynmt.training - Epoch   6, Step:   161100, Batch Loss:     1.357083, Tokens per Sec:    11623, Lr: 0.000078\n",
      "2021-10-08 09:14:28,155 - INFO - joeynmt.training - Epoch   6, Step:   161200, Batch Loss:     1.241013, Tokens per Sec:    11551, Lr: 0.000078\n",
      "2021-10-08 09:15:04,610 - INFO - joeynmt.training - Epoch   6, Step:   161300, Batch Loss:     1.384906, Tokens per Sec:    11927, Lr: 0.000078\n",
      "2021-10-08 09:15:40,405 - INFO - joeynmt.training - Epoch   6, Step:   161400, Batch Loss:     1.209789, Tokens per Sec:    11274, Lr: 0.000078\n",
      "2021-10-08 09:16:16,457 - INFO - joeynmt.training - Epoch   6, Step:   161500, Batch Loss:     1.294657, Tokens per Sec:    11657, Lr: 0.000078\n",
      "2021-10-08 09:16:52,548 - INFO - joeynmt.training - Epoch   6, Step:   161600, Batch Loss:     1.371016, Tokens per Sec:    11407, Lr: 0.000078\n",
      "2021-10-08 09:17:28,924 - INFO - joeynmt.training - Epoch   6, Step:   161700, Batch Loss:     1.457171, Tokens per Sec:    11819, Lr: 0.000078\n",
      "2021-10-08 09:18:04,540 - INFO - joeynmt.training - Epoch   6, Step:   161800, Batch Loss:     1.379892, Tokens per Sec:    11456, Lr: 0.000078\n",
      "2021-10-08 09:18:40,303 - INFO - joeynmt.training - Epoch   6, Step:   161900, Batch Loss:     1.232446, Tokens per Sec:    11415, Lr: 0.000078\n",
      "2021-10-08 09:19:16,556 - INFO - joeynmt.training - Epoch   6, Step:   162000, Batch Loss:     1.553185, Tokens per Sec:    11591, Lr: 0.000078\n",
      "2021-10-08 09:19:52,402 - INFO - joeynmt.training - Epoch   6, Step:   162100, Batch Loss:     1.378102, Tokens per Sec:    11710, Lr: 0.000078\n",
      "2021-10-08 09:20:28,457 - INFO - joeynmt.training - Epoch   6, Step:   162200, Batch Loss:     1.480067, Tokens per Sec:    11623, Lr: 0.000078\n",
      "2021-10-08 09:21:04,252 - INFO - joeynmt.training - Epoch   6, Step:   162300, Batch Loss:     1.375347, Tokens per Sec:    11390, Lr: 0.000078\n",
      "2021-10-08 09:21:39,982 - INFO - joeynmt.training - Epoch   6, Step:   162400, Batch Loss:     1.440488, Tokens per Sec:    11356, Lr: 0.000078\n",
      "2021-10-08 09:22:15,702 - INFO - joeynmt.training - Epoch   6, Step:   162500, Batch Loss:     1.485978, Tokens per Sec:    11523, Lr: 0.000078\n",
      "2021-10-08 09:22:51,441 - INFO - joeynmt.training - Epoch   6, Step:   162600, Batch Loss:     1.262956, Tokens per Sec:    11515, Lr: 0.000077\n",
      "2021-10-08 09:23:27,239 - INFO - joeynmt.training - Epoch   6, Step:   162700, Batch Loss:     1.439833, Tokens per Sec:    11446, Lr: 0.000077\n",
      "2021-10-08 09:24:03,150 - INFO - joeynmt.training - Epoch   6, Step:   162800, Batch Loss:     1.528089, Tokens per Sec:    11460, Lr: 0.000077\n",
      "2021-10-08 09:24:39,331 - INFO - joeynmt.training - Epoch   6, Step:   162900, Batch Loss:     1.521771, Tokens per Sec:    11539, Lr: 0.000077\n",
      "2021-10-08 09:25:15,380 - INFO - joeynmt.training - Epoch   6, Step:   163000, Batch Loss:     1.336239, Tokens per Sec:    11641, Lr: 0.000077\n",
      "2021-10-08 09:25:51,493 - INFO - joeynmt.training - Epoch   6, Step:   163100, Batch Loss:     1.428020, Tokens per Sec:    11687, Lr: 0.000077\n",
      "2021-10-08 09:26:22,771 - INFO - joeynmt.training - Epoch   6: total training loss 9445.90\n",
      "2021-10-08 09:26:22,772 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-10-08 09:26:29,668 - INFO - joeynmt.training - Epoch   7, Step:   163200, Batch Loss:     1.324053, Tokens per Sec:     9185, Lr: 0.000077\n",
      "2021-10-08 09:27:05,667 - INFO - joeynmt.training - Epoch   7, Step:   163300, Batch Loss:     1.355270, Tokens per Sec:    11657, Lr: 0.000077\n",
      "2021-10-08 09:27:41,740 - INFO - joeynmt.training - Epoch   7, Step:   163400, Batch Loss:     1.453068, Tokens per Sec:    11567, Lr: 0.000077\n",
      "2021-10-08 09:28:17,872 - INFO - joeynmt.training - Epoch   7, Step:   163500, Batch Loss:     1.464030, Tokens per Sec:    11398, Lr: 0.000077\n",
      "2021-10-08 09:28:54,022 - INFO - joeynmt.training - Epoch   7, Step:   163600, Batch Loss:     1.421501, Tokens per Sec:    11702, Lr: 0.000077\n",
      "2021-10-08 09:29:30,033 - INFO - joeynmt.training - Epoch   7, Step:   163700, Batch Loss:     1.384008, Tokens per Sec:    11559, Lr: 0.000077\n",
      "2021-10-08 09:30:06,087 - INFO - joeynmt.training - Epoch   7, Step:   163800, Batch Loss:     1.442827, Tokens per Sec:    11492, Lr: 0.000077\n",
      "2021-10-08 09:30:42,161 - INFO - joeynmt.training - Epoch   7, Step:   163900, Batch Loss:     1.454614, Tokens per Sec:    11514, Lr: 0.000077\n",
      "2021-10-08 09:31:17,611 - INFO - joeynmt.training - Epoch   7, Step:   164000, Batch Loss:     1.434318, Tokens per Sec:    11378, Lr: 0.000077\n",
      "2021-10-08 09:31:54,090 - INFO - joeynmt.training - Epoch   7, Step:   164100, Batch Loss:     1.536975, Tokens per Sec:    11655, Lr: 0.000077\n",
      "2021-10-08 09:32:29,879 - INFO - joeynmt.training - Epoch   7, Step:   164200, Batch Loss:     1.247002, Tokens per Sec:    11491, Lr: 0.000077\n",
      "2021-10-08 09:33:06,804 - INFO - joeynmt.training - Epoch   7, Step:   164300, Batch Loss:     1.332451, Tokens per Sec:    11873, Lr: 0.000077\n",
      "2021-10-08 09:33:42,634 - INFO - joeynmt.training - Epoch   7, Step:   164400, Batch Loss:     1.390647, Tokens per Sec:    11563, Lr: 0.000077\n",
      "2021-10-08 09:34:18,988 - INFO - joeynmt.training - Epoch   7, Step:   164500, Batch Loss:     1.415067, Tokens per Sec:    11682, Lr: 0.000077\n",
      "2021-10-08 09:34:55,092 - INFO - joeynmt.training - Epoch   7, Step:   164600, Batch Loss:     1.408170, Tokens per Sec:    11555, Lr: 0.000077\n",
      "2021-10-08 09:35:31,417 - INFO - joeynmt.training - Epoch   7, Step:   164700, Batch Loss:     1.512175, Tokens per Sec:    11595, Lr: 0.000077\n",
      "2021-10-08 09:36:07,625 - INFO - joeynmt.training - Epoch   7, Step:   164800, Batch Loss:     1.432102, Tokens per Sec:    11725, Lr: 0.000077\n",
      "2021-10-08 09:36:44,057 - INFO - joeynmt.training - Epoch   7, Step:   164900, Batch Loss:     1.399347, Tokens per Sec:    11381, Lr: 0.000077\n",
      "2021-10-08 09:37:20,681 - INFO - joeynmt.training - Epoch   7, Step:   165000, Batch Loss:     1.356899, Tokens per Sec:    11775, Lr: 0.000077\n",
      "2021-10-08 09:37:56,215 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 09:37:56,216 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 09:37:56,216 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 09:37:56,558 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 09:37:56,558 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 09:37:59,037 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 09:37:59,037 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 09:37:59,037 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 09:37:59,037 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-08 09:37:59,038 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 09:37:59,038 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 09:37:59,038 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 09:37:59,038 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when you get to know yourself and express your feelings in a way that speaks to others and deprive them .\n",
      "2021-10-08 09:37:59,038 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 09:37:59,038 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 09:37:59,038 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 09:37:59,038 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-08 09:37:59,039 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 09:37:59,039 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 09:37:59,039 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 09:37:59,039 - INFO - joeynmt.training - \tHypothesis: HOW DO MOY WE WE WILL PROPLE ?\n",
      "2021-10-08 09:37:59,039 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   165000: bleu:  44.74, loss: 36132.1289, ppl:   3.3017, duration: 38.3572s\n",
      "2021-10-08 09:38:35,119 - INFO - joeynmt.training - Epoch   7, Step:   165100, Batch Loss:     1.328982, Tokens per Sec:    11394, Lr: 0.000077\n",
      "2021-10-08 09:39:11,730 - INFO - joeynmt.training - Epoch   7, Step:   165200, Batch Loss:     1.374165, Tokens per Sec:    11697, Lr: 0.000077\n",
      "2021-10-08 09:39:47,735 - INFO - joeynmt.training - Epoch   7, Step:   165300, Batch Loss:     1.363819, Tokens per Sec:    11493, Lr: 0.000077\n",
      "2021-10-08 09:40:24,421 - INFO - joeynmt.training - Epoch   7, Step:   165400, Batch Loss:     1.595534, Tokens per Sec:    11604, Lr: 0.000077\n",
      "2021-10-08 09:41:00,108 - INFO - joeynmt.training - Epoch   7, Step:   165500, Batch Loss:     1.414121, Tokens per Sec:    11276, Lr: 0.000077\n",
      "2021-10-08 09:41:35,915 - INFO - joeynmt.training - Epoch   7, Step:   165600, Batch Loss:     1.450317, Tokens per Sec:    11417, Lr: 0.000077\n",
      "2021-10-08 09:42:12,107 - INFO - joeynmt.training - Epoch   7, Step:   165700, Batch Loss:     1.393167, Tokens per Sec:    11458, Lr: 0.000077\n",
      "2021-10-08 09:42:48,097 - INFO - joeynmt.training - Epoch   7, Step:   165800, Batch Loss:     1.335472, Tokens per Sec:    11315, Lr: 0.000077\n",
      "2021-10-08 09:43:24,534 - INFO - joeynmt.training - Epoch   7, Step:   165900, Batch Loss:     1.344875, Tokens per Sec:    11924, Lr: 0.000077\n",
      "2021-10-08 09:44:00,375 - INFO - joeynmt.training - Epoch   7, Step:   166000, Batch Loss:     1.471480, Tokens per Sec:    11441, Lr: 0.000077\n",
      "2021-10-08 09:44:36,604 - INFO - joeynmt.training - Epoch   7, Step:   166100, Batch Loss:     1.417220, Tokens per Sec:    11509, Lr: 0.000077\n",
      "2021-10-08 09:45:13,241 - INFO - joeynmt.training - Epoch   7, Step:   166200, Batch Loss:     1.339122, Tokens per Sec:    11849, Lr: 0.000077\n",
      "2021-10-08 09:45:49,110 - INFO - joeynmt.training - Epoch   7, Step:   166300, Batch Loss:     1.349081, Tokens per Sec:    11346, Lr: 0.000077\n",
      "2021-10-08 09:46:25,799 - INFO - joeynmt.training - Epoch   7, Step:   166400, Batch Loss:     1.375738, Tokens per Sec:    11603, Lr: 0.000077\n",
      "2021-10-08 09:47:01,348 - INFO - joeynmt.training - Epoch   7, Step:   166500, Batch Loss:     1.356266, Tokens per Sec:    11599, Lr: 0.000077\n",
      "2021-10-08 09:47:37,365 - INFO - joeynmt.training - Epoch   7, Step:   166600, Batch Loss:     1.396704, Tokens per Sec:    11394, Lr: 0.000077\n",
      "2021-10-08 09:48:13,426 - INFO - joeynmt.training - Epoch   7, Step:   166700, Batch Loss:     1.196965, Tokens per Sec:    11569, Lr: 0.000077\n",
      "2021-10-08 09:48:49,751 - INFO - joeynmt.training - Epoch   7, Step:   166800, Batch Loss:     1.383320, Tokens per Sec:    11531, Lr: 0.000077\n",
      "2021-10-08 09:49:25,643 - INFO - joeynmt.training - Epoch   7, Step:   166900, Batch Loss:     1.428555, Tokens per Sec:    11726, Lr: 0.000076\n",
      "2021-10-08 09:50:01,549 - INFO - joeynmt.training - Epoch   7, Step:   167000, Batch Loss:     1.394523, Tokens per Sec:    11512, Lr: 0.000076\n",
      "2021-10-08 09:50:37,417 - INFO - joeynmt.training - Epoch   7, Step:   167100, Batch Loss:     1.412249, Tokens per Sec:    11558, Lr: 0.000076\n",
      "2021-10-08 09:51:13,821 - INFO - joeynmt.training - Epoch   7, Step:   167200, Batch Loss:     1.363761, Tokens per Sec:    11637, Lr: 0.000076\n",
      "2021-10-08 09:51:49,958 - INFO - joeynmt.training - Epoch   7, Step:   167300, Batch Loss:     1.283624, Tokens per Sec:    11560, Lr: 0.000076\n",
      "2021-10-08 09:52:25,901 - INFO - joeynmt.training - Epoch   7, Step:   167400, Batch Loss:     1.365945, Tokens per Sec:    11346, Lr: 0.000076\n",
      "2021-10-08 09:53:02,002 - INFO - joeynmt.training - Epoch   7, Step:   167500, Batch Loss:     1.410773, Tokens per Sec:    11648, Lr: 0.000076\n",
      "2021-10-08 09:53:37,765 - INFO - joeynmt.training - Epoch   7, Step:   167600, Batch Loss:     1.327342, Tokens per Sec:    11222, Lr: 0.000076\n",
      "2021-10-08 09:54:14,356 - INFO - joeynmt.training - Epoch   7, Step:   167700, Batch Loss:     1.473809, Tokens per Sec:    11697, Lr: 0.000076\n",
      "2021-10-08 09:54:50,388 - INFO - joeynmt.training - Epoch   7, Step:   167800, Batch Loss:     1.277518, Tokens per Sec:    11548, Lr: 0.000076\n",
      "2021-10-08 09:55:26,457 - INFO - joeynmt.training - Epoch   7, Step:   167900, Batch Loss:     1.466290, Tokens per Sec:    11591, Lr: 0.000076\n",
      "2021-10-08 09:56:02,737 - INFO - joeynmt.training - Epoch   7, Step:   168000, Batch Loss:     1.411229, Tokens per Sec:    11331, Lr: 0.000076\n",
      "2021-10-08 09:56:38,825 - INFO - joeynmt.training - Epoch   7, Step:   168100, Batch Loss:     1.476169, Tokens per Sec:    11520, Lr: 0.000076\n",
      "2021-10-08 09:57:14,339 - INFO - joeynmt.training - Epoch   7, Step:   168200, Batch Loss:     1.425569, Tokens per Sec:    11472, Lr: 0.000076\n",
      "2021-10-08 09:57:50,520 - INFO - joeynmt.training - Epoch   7, Step:   168300, Batch Loss:     1.310351, Tokens per Sec:    11691, Lr: 0.000076\n",
      "2021-10-08 09:58:26,633 - INFO - joeynmt.training - Epoch   7, Step:   168400, Batch Loss:     1.446703, Tokens per Sec:    11668, Lr: 0.000076\n",
      "2021-10-08 09:59:02,687 - INFO - joeynmt.training - Epoch   7, Step:   168500, Batch Loss:     1.437750, Tokens per Sec:    11448, Lr: 0.000076\n",
      "2021-10-08 09:59:38,196 - INFO - joeynmt.training - Epoch   7, Step:   168600, Batch Loss:     1.382190, Tokens per Sec:    11485, Lr: 0.000076\n",
      "2021-10-08 10:00:14,401 - INFO - joeynmt.training - Epoch   7, Step:   168700, Batch Loss:     1.267567, Tokens per Sec:    11504, Lr: 0.000076\n",
      "2021-10-08 10:00:50,749 - INFO - joeynmt.training - Epoch   7, Step:   168800, Batch Loss:     1.357723, Tokens per Sec:    11698, Lr: 0.000076\n",
      "2021-10-08 10:01:26,481 - INFO - joeynmt.training - Epoch   7, Step:   168900, Batch Loss:     1.504960, Tokens per Sec:    11760, Lr: 0.000076\n",
      "2021-10-08 10:02:02,748 - INFO - joeynmt.training - Epoch   7, Step:   169000, Batch Loss:     1.328865, Tokens per Sec:    11643, Lr: 0.000076\n",
      "2021-10-08 10:02:38,576 - INFO - joeynmt.training - Epoch   7, Step:   169100, Batch Loss:     1.320359, Tokens per Sec:    11456, Lr: 0.000076\n",
      "2021-10-08 10:03:14,751 - INFO - joeynmt.training - Epoch   7, Step:   169200, Batch Loss:     1.376896, Tokens per Sec:    11577, Lr: 0.000076\n",
      "2021-10-08 10:03:51,008 - INFO - joeynmt.training - Epoch   7, Step:   169300, Batch Loss:     1.359165, Tokens per Sec:    11821, Lr: 0.000076\n",
      "2021-10-08 10:04:27,008 - INFO - joeynmt.training - Epoch   7, Step:   169400, Batch Loss:     1.347537, Tokens per Sec:    11532, Lr: 0.000076\n",
      "2021-10-08 10:05:02,922 - INFO - joeynmt.training - Epoch   7, Step:   169500, Batch Loss:     1.309069, Tokens per Sec:    11589, Lr: 0.000076\n",
      "2021-10-08 10:05:38,683 - INFO - joeynmt.training - Epoch   7, Step:   169600, Batch Loss:     1.386466, Tokens per Sec:    11412, Lr: 0.000076\n",
      "2021-10-08 10:06:14,927 - INFO - joeynmt.training - Epoch   7, Step:   169700, Batch Loss:     1.512872, Tokens per Sec:    11713, Lr: 0.000076\n",
      "2021-10-08 10:06:51,014 - INFO - joeynmt.training - Epoch   7, Step:   169800, Batch Loss:     1.301850, Tokens per Sec:    11514, Lr: 0.000076\n",
      "2021-10-08 10:07:27,054 - INFO - joeynmt.training - Epoch   7, Step:   169900, Batch Loss:     1.348685, Tokens per Sec:    11458, Lr: 0.000076\n",
      "2021-10-08 10:07:53,167 - INFO - joeynmt.training - Epoch   7: total training loss 9401.09\n",
      "2021-10-08 10:07:53,168 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-10-08 10:08:04,042 - INFO - joeynmt.training - Epoch   8, Step:   170000, Batch Loss:     1.285256, Tokens per Sec:    10198, Lr: 0.000076\n",
      "2021-10-08 10:08:39,650 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 10:08:39,651 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 10:08:39,651 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 10:08:39,993 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 10:08:39,993 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 10:08:41,821 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 10:08:41,821 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 10:08:41,821 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 10:08:41,821 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-08 10:08:41,821 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 10:08:41,821 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 10:08:41,822 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 10:08:41,822 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when you get to know yourself and express your feelings in a way that speak to others and deprive them .\n",
      "2021-10-08 10:08:41,822 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 10:08:41,822 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 10:08:41,822 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 10:08:41,822 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-08 10:08:41,822 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 10:08:41,822 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 10:08:41,823 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 10:08:41,823 - INFO - joeynmt.training - \tHypothesis: HOW DO MOY WE WE WILL SECTED ?\n",
      "2021-10-08 10:08:41,823 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   170000: bleu:  44.90, loss: 36024.7188, ppl:   3.2900, duration: 37.7802s\n",
      "2021-10-08 10:09:17,758 - INFO - joeynmt.training - Epoch   8, Step:   170100, Batch Loss:     1.396517, Tokens per Sec:    11568, Lr: 0.000076\n",
      "2021-10-08 10:09:54,080 - INFO - joeynmt.training - Epoch   8, Step:   170200, Batch Loss:     1.456003, Tokens per Sec:    11842, Lr: 0.000076\n",
      "2021-10-08 10:10:29,785 - INFO - joeynmt.training - Epoch   8, Step:   170300, Batch Loss:     1.438469, Tokens per Sec:    11454, Lr: 0.000076\n",
      "2021-10-08 10:11:06,445 - INFO - joeynmt.training - Epoch   8, Step:   170400, Batch Loss:     1.318653, Tokens per Sec:    11779, Lr: 0.000076\n",
      "2021-10-08 10:11:42,200 - INFO - joeynmt.training - Epoch   8, Step:   170500, Batch Loss:     1.293564, Tokens per Sec:    11468, Lr: 0.000076\n",
      "2021-10-08 10:12:18,539 - INFO - joeynmt.training - Epoch   8, Step:   170600, Batch Loss:     1.437436, Tokens per Sec:    11663, Lr: 0.000076\n",
      "2021-10-08 10:12:54,841 - INFO - joeynmt.training - Epoch   8, Step:   170700, Batch Loss:     1.374166, Tokens per Sec:    11858, Lr: 0.000076\n",
      "2021-10-08 10:13:30,806 - INFO - joeynmt.training - Epoch   8, Step:   170800, Batch Loss:     1.298902, Tokens per Sec:    11457, Lr: 0.000076\n",
      "2021-10-08 10:14:05,913 - INFO - joeynmt.training - Epoch   8, Step:   170900, Batch Loss:     1.469866, Tokens per Sec:    11211, Lr: 0.000076\n",
      "2021-10-08 10:14:41,373 - INFO - joeynmt.training - Epoch   8, Step:   171000, Batch Loss:     1.379526, Tokens per Sec:    11413, Lr: 0.000076\n",
      "2021-10-08 10:15:17,502 - INFO - joeynmt.training - Epoch   8, Step:   171100, Batch Loss:     1.377592, Tokens per Sec:    11719, Lr: 0.000076\n",
      "2021-10-08 10:15:53,162 - INFO - joeynmt.training - Epoch   8, Step:   171200, Batch Loss:     1.436301, Tokens per Sec:    11466, Lr: 0.000076\n",
      "2021-10-08 10:16:29,146 - INFO - joeynmt.training - Epoch   8, Step:   171300, Batch Loss:     1.397976, Tokens per Sec:    11403, Lr: 0.000076\n",
      "2021-10-08 10:17:04,946 - INFO - joeynmt.training - Epoch   8, Step:   171400, Batch Loss:     1.428079, Tokens per Sec:    11436, Lr: 0.000075\n",
      "2021-10-08 10:17:40,988 - INFO - joeynmt.training - Epoch   8, Step:   171500, Batch Loss:     1.408180, Tokens per Sec:    11759, Lr: 0.000075\n",
      "2021-10-08 10:18:16,965 - INFO - joeynmt.training - Epoch   8, Step:   171600, Batch Loss:     1.411118, Tokens per Sec:    11643, Lr: 0.000075\n",
      "2021-10-08 10:18:52,815 - INFO - joeynmt.training - Epoch   8, Step:   171700, Batch Loss:     1.363862, Tokens per Sec:    11655, Lr: 0.000075\n",
      "2021-10-08 10:19:28,592 - INFO - joeynmt.training - Epoch   8, Step:   171800, Batch Loss:     1.462643, Tokens per Sec:    11742, Lr: 0.000075\n",
      "2021-10-08 10:20:04,380 - INFO - joeynmt.training - Epoch   8, Step:   171900, Batch Loss:     1.451730, Tokens per Sec:    11422, Lr: 0.000075\n",
      "2021-10-08 10:20:40,137 - INFO - joeynmt.training - Epoch   8, Step:   172000, Batch Loss:     1.300063, Tokens per Sec:    11444, Lr: 0.000075\n",
      "2021-10-08 10:21:16,200 - INFO - joeynmt.training - Epoch   8, Step:   172100, Batch Loss:     1.374744, Tokens per Sec:    11642, Lr: 0.000075\n",
      "2021-10-08 10:21:52,389 - INFO - joeynmt.training - Epoch   8, Step:   172200, Batch Loss:     1.500111, Tokens per Sec:    11734, Lr: 0.000075\n",
      "2021-10-08 10:22:28,257 - INFO - joeynmt.training - Epoch   8, Step:   172300, Batch Loss:     1.391690, Tokens per Sec:    11273, Lr: 0.000075\n",
      "2021-10-08 10:23:04,297 - INFO - joeynmt.training - Epoch   8, Step:   172400, Batch Loss:     1.504250, Tokens per Sec:    11566, Lr: 0.000075\n",
      "2021-10-08 10:23:40,211 - INFO - joeynmt.training - Epoch   8, Step:   172500, Batch Loss:     1.162338, Tokens per Sec:    11424, Lr: 0.000075\n",
      "2021-10-08 10:24:15,936 - INFO - joeynmt.training - Epoch   8, Step:   172600, Batch Loss:     1.424317, Tokens per Sec:    11371, Lr: 0.000075\n",
      "2021-10-08 10:24:52,584 - INFO - joeynmt.training - Epoch   8, Step:   172700, Batch Loss:     1.298617, Tokens per Sec:    11827, Lr: 0.000075\n",
      "2021-10-08 10:25:28,553 - INFO - joeynmt.training - Epoch   8, Step:   172800, Batch Loss:     1.384457, Tokens per Sec:    11734, Lr: 0.000075\n",
      "2021-10-08 10:26:04,804 - INFO - joeynmt.training - Epoch   8, Step:   172900, Batch Loss:     1.512384, Tokens per Sec:    11552, Lr: 0.000075\n",
      "2021-10-08 10:26:40,433 - INFO - joeynmt.training - Epoch   8, Step:   173000, Batch Loss:     1.385746, Tokens per Sec:    11341, Lr: 0.000075\n",
      "2021-10-08 10:27:16,719 - INFO - joeynmt.training - Epoch   8, Step:   173100, Batch Loss:     1.367466, Tokens per Sec:    11771, Lr: 0.000075\n",
      "2021-10-08 10:27:52,673 - INFO - joeynmt.training - Epoch   8, Step:   173200, Batch Loss:     1.399535, Tokens per Sec:    11500, Lr: 0.000075\n",
      "2021-10-08 10:28:28,841 - INFO - joeynmt.training - Epoch   8, Step:   173300, Batch Loss:     1.323070, Tokens per Sec:    11620, Lr: 0.000075\n",
      "2021-10-08 10:29:04,700 - INFO - joeynmt.training - Epoch   8, Step:   173400, Batch Loss:     1.502744, Tokens per Sec:    11778, Lr: 0.000075\n",
      "2021-10-08 10:29:40,602 - INFO - joeynmt.training - Epoch   8, Step:   173500, Batch Loss:     1.470711, Tokens per Sec:    11558, Lr: 0.000075\n",
      "2021-10-08 10:30:16,482 - INFO - joeynmt.training - Epoch   8, Step:   173600, Batch Loss:     1.301883, Tokens per Sec:    11717, Lr: 0.000075\n",
      "2021-10-08 10:30:52,727 - INFO - joeynmt.training - Epoch   8, Step:   173700, Batch Loss:     1.214017, Tokens per Sec:    11589, Lr: 0.000075\n",
      "2021-10-08 10:31:28,754 - INFO - joeynmt.training - Epoch   8, Step:   173800, Batch Loss:     1.298129, Tokens per Sec:    11609, Lr: 0.000075\n",
      "2021-10-08 10:32:04,965 - INFO - joeynmt.training - Epoch   8, Step:   173900, Batch Loss:     1.458525, Tokens per Sec:    11650, Lr: 0.000075\n",
      "2021-10-08 10:32:41,201 - INFO - joeynmt.training - Epoch   8, Step:   174000, Batch Loss:     1.379336, Tokens per Sec:    11741, Lr: 0.000075\n",
      "2021-10-08 10:33:17,202 - INFO - joeynmt.training - Epoch   8, Step:   174100, Batch Loss:     1.346170, Tokens per Sec:    11695, Lr: 0.000075\n",
      "2021-10-08 10:33:52,814 - INFO - joeynmt.training - Epoch   8, Step:   174200, Batch Loss:     1.438795, Tokens per Sec:    11486, Lr: 0.000075\n",
      "2021-10-08 10:34:29,024 - INFO - joeynmt.training - Epoch   8, Step:   174300, Batch Loss:     1.455892, Tokens per Sec:    11635, Lr: 0.000075\n",
      "2021-10-08 10:35:05,340 - INFO - joeynmt.training - Epoch   8, Step:   174400, Batch Loss:     1.482587, Tokens per Sec:    11839, Lr: 0.000075\n",
      "2021-10-08 10:35:41,073 - INFO - joeynmt.training - Epoch   8, Step:   174500, Batch Loss:     1.476307, Tokens per Sec:    11442, Lr: 0.000075\n",
      "2021-10-08 10:36:16,670 - INFO - joeynmt.training - Epoch   8, Step:   174600, Batch Loss:     1.339818, Tokens per Sec:    11463, Lr: 0.000075\n",
      "2021-10-08 10:36:52,650 - INFO - joeynmt.training - Epoch   8, Step:   174700, Batch Loss:     1.452591, Tokens per Sec:    11814, Lr: 0.000075\n",
      "2021-10-08 10:37:29,143 - INFO - joeynmt.training - Epoch   8, Step:   174800, Batch Loss:     1.454729, Tokens per Sec:    11959, Lr: 0.000075\n",
      "2021-10-08 10:38:05,322 - INFO - joeynmt.training - Epoch   8, Step:   174900, Batch Loss:     1.423861, Tokens per Sec:    11706, Lr: 0.000075\n",
      "2021-10-08 10:38:41,392 - INFO - joeynmt.training - Epoch   8, Step:   175000, Batch Loss:     1.296621, Tokens per Sec:    11435, Lr: 0.000075\n",
      "2021-10-08 10:39:17,204 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 10:39:17,205 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 10:39:17,205 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 10:39:17,547 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 10:39:17,547 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 10:39:19,312 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 10:39:19,312 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 10:39:19,312 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 10:39:19,312 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-08 10:39:19,313 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 10:39:19,313 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 10:39:19,313 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 10:39:19,313 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when you get to know yourself and express your feelings in a way that speak to others and harm them .\n",
      "2021-10-08 10:39:19,313 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 10:39:19,314 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 10:39:19,314 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 10:39:19,314 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-08 10:39:19,314 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 10:39:19,314 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 10:39:19,314 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 10:39:19,314 - INFO - joeynmt.training - \tHypothesis: HOW DO MOUR GEESTELICE DOELVITS ?\n",
      "2021-10-08 10:39:19,315 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   175000: bleu:  44.78, loss: 35937.8633, ppl:   3.2806, duration: 37.9220s\n",
      "2021-10-08 10:39:55,104 - INFO - joeynmt.training - Epoch   8, Step:   175100, Batch Loss:     1.417216, Tokens per Sec:    11410, Lr: 0.000075\n",
      "2021-10-08 10:40:30,762 - INFO - joeynmt.training - Epoch   8, Step:   175200, Batch Loss:     1.435444, Tokens per Sec:    11496, Lr: 0.000075\n",
      "2021-10-08 10:41:07,280 - INFO - joeynmt.training - Epoch   8, Step:   175300, Batch Loss:     1.371369, Tokens per Sec:    11847, Lr: 0.000075\n",
      "2021-10-08 10:41:43,353 - INFO - joeynmt.training - Epoch   8, Step:   175400, Batch Loss:     1.270149, Tokens per Sec:    11812, Lr: 0.000075\n",
      "2021-10-08 10:42:19,573 - INFO - joeynmt.training - Epoch   8, Step:   175500, Batch Loss:     1.452585, Tokens per Sec:    11423, Lr: 0.000075\n",
      "2021-10-08 10:42:55,672 - INFO - joeynmt.training - Epoch   8, Step:   175600, Batch Loss:     1.354836, Tokens per Sec:    11613, Lr: 0.000075\n",
      "2021-10-08 10:43:31,913 - INFO - joeynmt.training - Epoch   8, Step:   175700, Batch Loss:     1.401479, Tokens per Sec:    11459, Lr: 0.000075\n",
      "2021-10-08 10:44:08,127 - INFO - joeynmt.training - Epoch   8, Step:   175800, Batch Loss:     1.385523, Tokens per Sec:    11709, Lr: 0.000075\n",
      "2021-10-08 10:44:44,357 - INFO - joeynmt.training - Epoch   8, Step:   175900, Batch Loss:     1.411237, Tokens per Sec:    11616, Lr: 0.000075\n",
      "2021-10-08 10:45:19,834 - INFO - joeynmt.training - Epoch   8, Step:   176000, Batch Loss:     1.449936, Tokens per Sec:    11231, Lr: 0.000074\n",
      "2021-10-08 10:45:56,218 - INFO - joeynmt.training - Epoch   8, Step:   176100, Batch Loss:     1.423213, Tokens per Sec:    11495, Lr: 0.000074\n",
      "2021-10-08 10:46:32,321 - INFO - joeynmt.training - Epoch   8, Step:   176200, Batch Loss:     1.452669, Tokens per Sec:    11513, Lr: 0.000074\n",
      "2021-10-08 10:47:08,357 - INFO - joeynmt.training - Epoch   8, Step:   176300, Batch Loss:     1.326650, Tokens per Sec:    11619, Lr: 0.000074\n",
      "2021-10-08 10:47:43,854 - INFO - joeynmt.training - Epoch   8, Step:   176400, Batch Loss:     1.330295, Tokens per Sec:    11341, Lr: 0.000074\n",
      "2021-10-08 10:48:19,968 - INFO - joeynmt.training - Epoch   8, Step:   176500, Batch Loss:     1.503748, Tokens per Sec:    11364, Lr: 0.000074\n",
      "2021-10-08 10:48:56,015 - INFO - joeynmt.training - Epoch   8, Step:   176600, Batch Loss:     1.347280, Tokens per Sec:    11777, Lr: 0.000074\n",
      "2021-10-08 10:49:31,574 - INFO - joeynmt.training - Epoch   8, Step:   176700, Batch Loss:     1.275603, Tokens per Sec:    11492, Lr: 0.000074\n",
      "2021-10-08 10:49:54,423 - INFO - joeynmt.training - Epoch   8: total training loss 9366.97\n",
      "2021-10-08 10:49:54,423 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-10-08 10:50:09,278 - INFO - joeynmt.training - Epoch   9, Step:   176800, Batch Loss:     1.453014, Tokens per Sec:    10554, Lr: 0.000074\n",
      "2021-10-08 10:50:45,573 - INFO - joeynmt.training - Epoch   9, Step:   176900, Batch Loss:     1.428609, Tokens per Sec:    11610, Lr: 0.000074\n",
      "2021-10-08 10:51:21,101 - INFO - joeynmt.training - Epoch   9, Step:   177000, Batch Loss:     1.319507, Tokens per Sec:    11439, Lr: 0.000074\n",
      "2021-10-08 10:51:56,960 - INFO - joeynmt.training - Epoch   9, Step:   177100, Batch Loss:     1.215464, Tokens per Sec:    11657, Lr: 0.000074\n",
      "2021-10-08 10:52:32,957 - INFO - joeynmt.training - Epoch   9, Step:   177200, Batch Loss:     1.306195, Tokens per Sec:    11554, Lr: 0.000074\n",
      "2021-10-08 10:53:09,339 - INFO - joeynmt.training - Epoch   9, Step:   177300, Batch Loss:     1.409136, Tokens per Sec:    11665, Lr: 0.000074\n",
      "2021-10-08 10:53:45,615 - INFO - joeynmt.training - Epoch   9, Step:   177400, Batch Loss:     1.506578, Tokens per Sec:    11644, Lr: 0.000074\n",
      "2021-10-08 10:54:22,427 - INFO - joeynmt.training - Epoch   9, Step:   177500, Batch Loss:     1.430723, Tokens per Sec:    11331, Lr: 0.000074\n",
      "2021-10-08 10:54:58,895 - INFO - joeynmt.training - Epoch   9, Step:   177600, Batch Loss:     1.354037, Tokens per Sec:    11870, Lr: 0.000074\n",
      "2021-10-08 10:55:34,995 - INFO - joeynmt.training - Epoch   9, Step:   177700, Batch Loss:     1.381942, Tokens per Sec:    11562, Lr: 0.000074\n",
      "2021-10-08 10:56:10,779 - INFO - joeynmt.training - Epoch   9, Step:   177800, Batch Loss:     1.402082, Tokens per Sec:    11417, Lr: 0.000074\n",
      "2021-10-08 10:56:46,416 - INFO - joeynmt.training - Epoch   9, Step:   177900, Batch Loss:     1.295138, Tokens per Sec:    11570, Lr: 0.000074\n",
      "2021-10-08 10:57:22,353 - INFO - joeynmt.training - Epoch   9, Step:   178000, Batch Loss:     1.353417, Tokens per Sec:    11453, Lr: 0.000074\n",
      "2021-10-08 10:57:58,487 - INFO - joeynmt.training - Epoch   9, Step:   178100, Batch Loss:     1.316753, Tokens per Sec:    11602, Lr: 0.000074\n",
      "2021-10-08 10:58:34,772 - INFO - joeynmt.training - Epoch   9, Step:   178200, Batch Loss:     1.306353, Tokens per Sec:    11645, Lr: 0.000074\n",
      "2021-10-08 10:59:11,172 - INFO - joeynmt.training - Epoch   9, Step:   178300, Batch Loss:     1.450124, Tokens per Sec:    11710, Lr: 0.000074\n",
      "2021-10-08 10:59:46,543 - INFO - joeynmt.training - Epoch   9, Step:   178400, Batch Loss:     1.595884, Tokens per Sec:    11319, Lr: 0.000074\n",
      "2021-10-08 11:00:22,645 - INFO - joeynmt.training - Epoch   9, Step:   178500, Batch Loss:     1.435951, Tokens per Sec:    11608, Lr: 0.000074\n",
      "2021-10-08 11:00:58,285 - INFO - joeynmt.training - Epoch   9, Step:   178600, Batch Loss:     1.290005, Tokens per Sec:    11511, Lr: 0.000074\n",
      "2021-10-08 11:01:34,054 - INFO - joeynmt.training - Epoch   9, Step:   178700, Batch Loss:     1.428179, Tokens per Sec:    11397, Lr: 0.000074\n",
      "2021-10-08 11:02:09,762 - INFO - joeynmt.training - Epoch   9, Step:   178800, Batch Loss:     1.397683, Tokens per Sec:    11531, Lr: 0.000074\n",
      "2021-10-08 11:02:46,182 - INFO - joeynmt.training - Epoch   9, Step:   178900, Batch Loss:     1.416703, Tokens per Sec:    11534, Lr: 0.000074\n",
      "2021-10-08 11:03:21,966 - INFO - joeynmt.training - Epoch   9, Step:   179000, Batch Loss:     1.293162, Tokens per Sec:    11363, Lr: 0.000074\n",
      "2021-10-08 11:03:57,572 - INFO - joeynmt.training - Epoch   9, Step:   179100, Batch Loss:     1.341244, Tokens per Sec:    11428, Lr: 0.000074\n",
      "2021-10-08 11:04:33,372 - INFO - joeynmt.training - Epoch   9, Step:   179200, Batch Loss:     1.409757, Tokens per Sec:    11457, Lr: 0.000074\n",
      "2021-10-08 11:05:09,198 - INFO - joeynmt.training - Epoch   9, Step:   179300, Batch Loss:     1.327531, Tokens per Sec:    11475, Lr: 0.000074\n",
      "2021-10-08 11:05:45,561 - INFO - joeynmt.training - Epoch   9, Step:   179400, Batch Loss:     1.407325, Tokens per Sec:    11535, Lr: 0.000074\n",
      "2021-10-08 11:06:21,693 - INFO - joeynmt.training - Epoch   9, Step:   179500, Batch Loss:     1.262388, Tokens per Sec:    11349, Lr: 0.000074\n",
      "2021-10-08 11:06:57,697 - INFO - joeynmt.training - Epoch   9, Step:   179600, Batch Loss:     1.313443, Tokens per Sec:    11542, Lr: 0.000074\n",
      "2021-10-08 11:07:33,946 - INFO - joeynmt.training - Epoch   9, Step:   179700, Batch Loss:     1.451907, Tokens per Sec:    11593, Lr: 0.000074\n",
      "2021-10-08 11:08:10,040 - INFO - joeynmt.training - Epoch   9, Step:   179800, Batch Loss:     1.318500, Tokens per Sec:    11451, Lr: 0.000074\n",
      "2021-10-08 11:08:45,695 - INFO - joeynmt.training - Epoch   9, Step:   179900, Batch Loss:     1.288457, Tokens per Sec:    11406, Lr: 0.000074\n",
      "2021-10-08 11:09:22,035 - INFO - joeynmt.training - Epoch   9, Step:   180000, Batch Loss:     1.386755, Tokens per Sec:    11752, Lr: 0.000074\n",
      "2021-10-08 11:09:57,646 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 11:09:57,647 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 11:09:57,647 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 11:09:59,682 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 11:09:59,683 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 11:09:59,683 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 11:09:59,683 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-08 11:09:59,683 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 11:09:59,684 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 11:09:59,684 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 11:09:59,684 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when you get to know yourself and express your feelings in a way that speaks to others and deprive them .\n",
      "2021-10-08 11:09:59,684 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 11:09:59,685 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 11:09:59,685 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 11:09:59,685 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-08 11:09:59,685 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 11:09:59,686 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 11:09:59,686 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 11:09:59,686 - INFO - joeynmt.training - \tHypothesis: HOW DO MOY WE WE WILL GEESTELION DOELVITS ?\n",
      "2021-10-08 11:09:59,686 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   180000: bleu:  44.80, loss: 35954.1523, ppl:   3.2824, duration: 37.6507s\n",
      "2021-10-08 11:10:35,854 - INFO - joeynmt.training - Epoch   9, Step:   180100, Batch Loss:     1.368063, Tokens per Sec:    11526, Lr: 0.000074\n",
      "2021-10-08 11:11:11,527 - INFO - joeynmt.training - Epoch   9, Step:   180200, Batch Loss:     1.437380, Tokens per Sec:    11358, Lr: 0.000074\n",
      "2021-10-08 11:11:47,739 - INFO - joeynmt.training - Epoch   9, Step:   180300, Batch Loss:     1.447621, Tokens per Sec:    11583, Lr: 0.000074\n",
      "2021-10-08 11:12:23,936 - INFO - joeynmt.training - Epoch   9, Step:   180400, Batch Loss:     1.423926, Tokens per Sec:    11551, Lr: 0.000074\n",
      "2021-10-08 11:13:00,085 - INFO - joeynmt.training - Epoch   9, Step:   180500, Batch Loss:     1.388075, Tokens per Sec:    11713, Lr: 0.000074\n",
      "2021-10-08 11:13:36,693 - INFO - joeynmt.training - Epoch   9, Step:   180600, Batch Loss:     1.494041, Tokens per Sec:    11890, Lr: 0.000074\n",
      "2021-10-08 11:14:12,246 - INFO - joeynmt.training - Epoch   9, Step:   180700, Batch Loss:     1.316807, Tokens per Sec:    11360, Lr: 0.000074\n",
      "2021-10-08 11:14:48,274 - INFO - joeynmt.training - Epoch   9, Step:   180800, Batch Loss:     1.413036, Tokens per Sec:    11576, Lr: 0.000073\n",
      "2021-10-08 11:15:25,268 - INFO - joeynmt.training - Epoch   9, Step:   180900, Batch Loss:     1.435842, Tokens per Sec:    10999, Lr: 0.000073\n",
      "2021-10-08 11:16:01,572 - INFO - joeynmt.training - Epoch   9, Step:   181000, Batch Loss:     1.356983, Tokens per Sec:    11754, Lr: 0.000073\n",
      "2021-10-08 11:16:37,538 - INFO - joeynmt.training - Epoch   9, Step:   181100, Batch Loss:     1.378530, Tokens per Sec:    11346, Lr: 0.000073\n",
      "2021-10-08 11:17:13,836 - INFO - joeynmt.training - Epoch   9, Step:   181200, Batch Loss:     1.391206, Tokens per Sec:    11941, Lr: 0.000073\n",
      "2021-10-08 11:17:49,357 - INFO - joeynmt.training - Epoch   9, Step:   181300, Batch Loss:     1.397753, Tokens per Sec:    11507, Lr: 0.000073\n",
      "2021-10-08 11:18:25,260 - INFO - joeynmt.training - Epoch   9, Step:   181400, Batch Loss:     1.350244, Tokens per Sec:    11284, Lr: 0.000073\n",
      "2021-10-08 11:19:01,554 - INFO - joeynmt.training - Epoch   9, Step:   181500, Batch Loss:     1.331261, Tokens per Sec:    11801, Lr: 0.000073\n",
      "2021-10-08 11:19:37,882 - INFO - joeynmt.training - Epoch   9, Step:   181600, Batch Loss:     1.300520, Tokens per Sec:    11785, Lr: 0.000073\n",
      "2021-10-08 11:20:14,034 - INFO - joeynmt.training - Epoch   9, Step:   181700, Batch Loss:     1.423100, Tokens per Sec:    11645, Lr: 0.000073\n",
      "2021-10-08 11:20:49,972 - INFO - joeynmt.training - Epoch   9, Step:   181800, Batch Loss:     1.479212, Tokens per Sec:    11495, Lr: 0.000073\n",
      "2021-10-08 11:21:25,651 - INFO - joeynmt.training - Epoch   9, Step:   181900, Batch Loss:     1.456668, Tokens per Sec:    11717, Lr: 0.000073\n",
      "2021-10-08 11:22:01,878 - INFO - joeynmt.training - Epoch   9, Step:   182000, Batch Loss:     1.299975, Tokens per Sec:    11557, Lr: 0.000073\n",
      "2021-10-08 11:22:37,614 - INFO - joeynmt.training - Epoch   9, Step:   182100, Batch Loss:     1.328584, Tokens per Sec:    11232, Lr: 0.000073\n",
      "2021-10-08 11:23:13,952 - INFO - joeynmt.training - Epoch   9, Step:   182200, Batch Loss:     1.140073, Tokens per Sec:    11742, Lr: 0.000073\n",
      "2021-10-08 11:23:49,769 - INFO - joeynmt.training - Epoch   9, Step:   182300, Batch Loss:     1.281700, Tokens per Sec:    11421, Lr: 0.000073\n",
      "2021-10-08 11:24:25,772 - INFO - joeynmt.training - Epoch   9, Step:   182400, Batch Loss:     1.348102, Tokens per Sec:    11531, Lr: 0.000073\n",
      "2021-10-08 11:25:02,007 - INFO - joeynmt.training - Epoch   9, Step:   182500, Batch Loss:     1.305530, Tokens per Sec:    11661, Lr: 0.000073\n",
      "2021-10-08 11:25:37,679 - INFO - joeynmt.training - Epoch   9, Step:   182600, Batch Loss:     1.418860, Tokens per Sec:    11371, Lr: 0.000073\n",
      "2021-10-08 11:26:13,723 - INFO - joeynmt.training - Epoch   9, Step:   182700, Batch Loss:     1.417091, Tokens per Sec:    11569, Lr: 0.000073\n",
      "2021-10-08 11:26:50,317 - INFO - joeynmt.training - Epoch   9, Step:   182800, Batch Loss:     1.414105, Tokens per Sec:    11540, Lr: 0.000073\n",
      "2021-10-08 11:27:26,174 - INFO - joeynmt.training - Epoch   9, Step:   182900, Batch Loss:     1.489536, Tokens per Sec:    11526, Lr: 0.000073\n",
      "2021-10-08 11:28:02,509 - INFO - joeynmt.training - Epoch   9, Step:   183000, Batch Loss:     1.362116, Tokens per Sec:    11764, Lr: 0.000073\n",
      "2021-10-08 11:28:38,196 - INFO - joeynmt.training - Epoch   9, Step:   183100, Batch Loss:     1.293244, Tokens per Sec:    11456, Lr: 0.000073\n",
      "2021-10-08 11:29:14,741 - INFO - joeynmt.training - Epoch   9, Step:   183200, Batch Loss:     1.398406, Tokens per Sec:    11587, Lr: 0.000073\n",
      "2021-10-08 11:29:50,761 - INFO - joeynmt.training - Epoch   9, Step:   183300, Batch Loss:     1.515504, Tokens per Sec:    11662, Lr: 0.000073\n",
      "2021-10-08 11:30:26,468 - INFO - joeynmt.training - Epoch   9, Step:   183400, Batch Loss:     1.429845, Tokens per Sec:    11484, Lr: 0.000073\n",
      "2021-10-08 11:31:02,601 - INFO - joeynmt.training - Epoch   9, Step:   183500, Batch Loss:     1.440348, Tokens per Sec:    11520, Lr: 0.000073\n",
      "2021-10-08 11:31:26,023 - INFO - joeynmt.training - Epoch   9: total training loss 9357.98\n",
      "2021-10-08 11:31:26,024 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-10-08 11:31:41,423 - INFO - joeynmt.training - Epoch  10, Step:   183600, Batch Loss:     1.405112, Tokens per Sec:    10764, Lr: 0.000073\n",
      "2021-10-08 11:32:17,613 - INFO - joeynmt.training - Epoch  10, Step:   183700, Batch Loss:     1.185899, Tokens per Sec:    11444, Lr: 0.000073\n",
      "2021-10-08 11:32:53,686 - INFO - joeynmt.training - Epoch  10, Step:   183800, Batch Loss:     1.354724, Tokens per Sec:    11731, Lr: 0.000073\n",
      "2021-10-08 11:33:30,430 - INFO - joeynmt.training - Epoch  10, Step:   183900, Batch Loss:     1.362289, Tokens per Sec:    11253, Lr: 0.000073\n",
      "2021-10-08 11:34:06,191 - INFO - joeynmt.training - Epoch  10, Step:   184000, Batch Loss:     1.461361, Tokens per Sec:    11618, Lr: 0.000073\n",
      "2021-10-08 11:34:42,390 - INFO - joeynmt.training - Epoch  10, Step:   184100, Batch Loss:     1.523582, Tokens per Sec:    11706, Lr: 0.000073\n",
      "2021-10-08 11:35:18,542 - INFO - joeynmt.training - Epoch  10, Step:   184200, Batch Loss:     1.388733, Tokens per Sec:    11476, Lr: 0.000073\n",
      "2021-10-08 11:35:54,913 - INFO - joeynmt.training - Epoch  10, Step:   184300, Batch Loss:     1.415097, Tokens per Sec:    11812, Lr: 0.000073\n",
      "2021-10-08 11:36:30,988 - INFO - joeynmt.training - Epoch  10, Step:   184400, Batch Loss:     1.204485, Tokens per Sec:    11573, Lr: 0.000073\n",
      "2021-10-08 11:37:07,328 - INFO - joeynmt.training - Epoch  10, Step:   184500, Batch Loss:     1.388198, Tokens per Sec:    11511, Lr: 0.000073\n",
      "2021-10-08 11:37:43,192 - INFO - joeynmt.training - Epoch  10, Step:   184600, Batch Loss:     1.373008, Tokens per Sec:    11748, Lr: 0.000073\n",
      "2021-10-08 11:38:19,511 - INFO - joeynmt.training - Epoch  10, Step:   184700, Batch Loss:     1.393066, Tokens per Sec:    11508, Lr: 0.000073\n",
      "2021-10-08 11:38:55,387 - INFO - joeynmt.training - Epoch  10, Step:   184800, Batch Loss:     1.339595, Tokens per Sec:    11573, Lr: 0.000073\n",
      "2021-10-08 11:39:31,616 - INFO - joeynmt.training - Epoch  10, Step:   184900, Batch Loss:     1.294840, Tokens per Sec:    11472, Lr: 0.000073\n",
      "2021-10-08 11:40:07,799 - INFO - joeynmt.training - Epoch  10, Step:   185000, Batch Loss:     1.335646, Tokens per Sec:    11685, Lr: 0.000073\n",
      "2021-10-08 11:40:43,379 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 11:40:43,380 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 11:40:43,380 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 11:40:43,722 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 11:40:43,722 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 11:40:45,530 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 11:40:45,530 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 11:40:45,530 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 11:40:45,530 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term used at that time for the brother who took the lead in a congregation .\n",
      "2021-10-08 11:40:45,531 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 11:40:45,531 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 11:40:45,531 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 11:40:45,531 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when you get to know yourself and express your feelings in a way that speaks to others and harm them .\n",
      "2021-10-08 11:40:45,531 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 11:40:45,531 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 11:40:45,531 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 11:40:45,531 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-08 11:40:45,532 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 11:40:45,532 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 11:40:45,532 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 11:40:45,532 - INFO - joeynmt.training - \tHypothesis: HOW DO MOY WE WE WILL SECTED ?\n",
      "2021-10-08 11:40:45,532 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   185000: bleu:  44.75, loss: 35815.4648, ppl:   3.2674, duration: 37.7328s\n",
      "2021-10-08 11:41:21,867 - INFO - joeynmt.training - Epoch  10, Step:   185100, Batch Loss:     1.317090, Tokens per Sec:    11438, Lr: 0.000073\n",
      "2021-10-08 11:41:58,092 - INFO - joeynmt.training - Epoch  10, Step:   185200, Batch Loss:     1.330696, Tokens per Sec:    11368, Lr: 0.000073\n",
      "2021-10-08 11:42:34,552 - INFO - joeynmt.training - Epoch  10, Step:   185300, Batch Loss:     1.438273, Tokens per Sec:    11748, Lr: 0.000073\n",
      "2021-10-08 11:43:10,560 - INFO - joeynmt.training - Epoch  10, Step:   185400, Batch Loss:     1.331078, Tokens per Sec:    11612, Lr: 0.000073\n",
      "2021-10-08 11:43:46,814 - INFO - joeynmt.training - Epoch  10, Step:   185500, Batch Loss:     1.329068, Tokens per Sec:    11625, Lr: 0.000073\n",
      "2021-10-08 11:44:22,848 - INFO - joeynmt.training - Epoch  10, Step:   185600, Batch Loss:     1.166035, Tokens per Sec:    11658, Lr: 0.000073\n",
      "2021-10-08 11:44:58,968 - INFO - joeynmt.training - Epoch  10, Step:   185700, Batch Loss:     1.271056, Tokens per Sec:    11522, Lr: 0.000073\n",
      "2021-10-08 11:45:35,248 - INFO - joeynmt.training - Epoch  10, Step:   185800, Batch Loss:     1.458060, Tokens per Sec:    11547, Lr: 0.000072\n",
      "2021-10-08 11:46:10,886 - INFO - joeynmt.training - Epoch  10, Step:   185900, Batch Loss:     1.382238, Tokens per Sec:    11200, Lr: 0.000072\n",
      "2021-10-08 11:46:46,944 - INFO - joeynmt.training - Epoch  10, Step:   186000, Batch Loss:     1.235636, Tokens per Sec:    11652, Lr: 0.000072\n",
      "2021-10-08 11:47:22,771 - INFO - joeynmt.training - Epoch  10, Step:   186100, Batch Loss:     1.450912, Tokens per Sec:    11594, Lr: 0.000072\n",
      "2021-10-08 11:47:59,978 - INFO - joeynmt.training - Epoch  10, Step:   186200, Batch Loss:     1.246719, Tokens per Sec:    11724, Lr: 0.000072\n",
      "2021-10-08 11:48:35,762 - INFO - joeynmt.training - Epoch  10, Step:   186300, Batch Loss:     1.422771, Tokens per Sec:    11117, Lr: 0.000072\n",
      "2021-10-08 11:49:12,130 - INFO - joeynmt.training - Epoch  10, Step:   186400, Batch Loss:     1.401458, Tokens per Sec:    11706, Lr: 0.000072\n",
      "2021-10-08 11:49:48,528 - INFO - joeynmt.training - Epoch  10, Step:   186500, Batch Loss:     1.413361, Tokens per Sec:    11667, Lr: 0.000072\n",
      "2021-10-08 11:50:24,523 - INFO - joeynmt.training - Epoch  10, Step:   186600, Batch Loss:     1.336403, Tokens per Sec:    11481, Lr: 0.000072\n",
      "2021-10-08 11:51:00,465 - INFO - joeynmt.training - Epoch  10, Step:   186700, Batch Loss:     1.263493, Tokens per Sec:    11586, Lr: 0.000072\n",
      "2021-10-08 11:51:36,399 - INFO - joeynmt.training - Epoch  10, Step:   186800, Batch Loss:     1.439708, Tokens per Sec:    11496, Lr: 0.000072\n",
      "2021-10-08 11:52:12,615 - INFO - joeynmt.training - Epoch  10, Step:   186900, Batch Loss:     1.354994, Tokens per Sec:    11782, Lr: 0.000072\n",
      "2021-10-08 11:52:48,736 - INFO - joeynmt.training - Epoch  10, Step:   187000, Batch Loss:     1.326160, Tokens per Sec:    11636, Lr: 0.000072\n",
      "2021-10-08 11:53:24,789 - INFO - joeynmt.training - Epoch  10, Step:   187100, Batch Loss:     1.280741, Tokens per Sec:    11561, Lr: 0.000072\n",
      "2021-10-08 11:54:00,826 - INFO - joeynmt.training - Epoch  10, Step:   187200, Batch Loss:     1.420533, Tokens per Sec:    11288, Lr: 0.000072\n",
      "2021-10-08 11:54:36,707 - INFO - joeynmt.training - Epoch  10, Step:   187300, Batch Loss:     1.485369, Tokens per Sec:    11677, Lr: 0.000072\n",
      "2021-10-08 11:55:12,456 - INFO - joeynmt.training - Epoch  10, Step:   187400, Batch Loss:     1.365935, Tokens per Sec:    11186, Lr: 0.000072\n",
      "2021-10-08 11:55:49,020 - INFO - joeynmt.training - Epoch  10, Step:   187500, Batch Loss:     1.281578, Tokens per Sec:    11699, Lr: 0.000072\n",
      "2021-10-08 11:56:25,311 - INFO - joeynmt.training - Epoch  10, Step:   187600, Batch Loss:     1.344558, Tokens per Sec:    11595, Lr: 0.000072\n",
      "2021-10-08 11:57:01,154 - INFO - joeynmt.training - Epoch  10, Step:   187700, Batch Loss:     1.375827, Tokens per Sec:    11578, Lr: 0.000072\n",
      "2021-10-08 11:57:37,053 - INFO - joeynmt.training - Epoch  10, Step:   187800, Batch Loss:     1.336046, Tokens per Sec:    11424, Lr: 0.000072\n",
      "2021-10-08 11:58:13,152 - INFO - joeynmt.training - Epoch  10, Step:   187900, Batch Loss:     1.252686, Tokens per Sec:    11565, Lr: 0.000072\n",
      "2021-10-08 11:58:48,919 - INFO - joeynmt.training - Epoch  10, Step:   188000, Batch Loss:     1.315906, Tokens per Sec:    11411, Lr: 0.000072\n",
      "2021-10-08 11:59:25,437 - INFO - joeynmt.training - Epoch  10, Step:   188100, Batch Loss:     1.438612, Tokens per Sec:    11601, Lr: 0.000072\n",
      "2021-10-08 12:00:01,587 - INFO - joeynmt.training - Epoch  10, Step:   188200, Batch Loss:     1.446879, Tokens per Sec:    11548, Lr: 0.000072\n",
      "2021-10-08 12:00:38,025 - INFO - joeynmt.training - Epoch  10, Step:   188300, Batch Loss:     1.216649, Tokens per Sec:    11719, Lr: 0.000072\n",
      "2021-10-08 12:01:13,961 - INFO - joeynmt.training - Epoch  10, Step:   188400, Batch Loss:     1.394955, Tokens per Sec:    11507, Lr: 0.000072\n",
      "2021-10-08 12:01:49,797 - INFO - joeynmt.training - Epoch  10, Step:   188500, Batch Loss:     1.356536, Tokens per Sec:    11469, Lr: 0.000072\n",
      "2021-10-08 12:02:26,093 - INFO - joeynmt.training - Epoch  10, Step:   188600, Batch Loss:     1.494031, Tokens per Sec:    11419, Lr: 0.000072\n",
      "2021-10-08 12:03:02,268 - INFO - joeynmt.training - Epoch  10, Step:   188700, Batch Loss:     1.370868, Tokens per Sec:    11571, Lr: 0.000072\n",
      "2021-10-08 12:03:38,109 - INFO - joeynmt.training - Epoch  10, Step:   188800, Batch Loss:     1.419325, Tokens per Sec:    11335, Lr: 0.000072\n",
      "2021-10-08 12:04:14,195 - INFO - joeynmt.training - Epoch  10, Step:   188900, Batch Loss:     1.378240, Tokens per Sec:    11601, Lr: 0.000072\n",
      "2021-10-08 12:04:50,430 - INFO - joeynmt.training - Epoch  10, Step:   189000, Batch Loss:     1.485618, Tokens per Sec:    11488, Lr: 0.000072\n",
      "2021-10-08 12:05:26,432 - INFO - joeynmt.training - Epoch  10, Step:   189100, Batch Loss:     1.368068, Tokens per Sec:    11526, Lr: 0.000072\n",
      "2021-10-08 12:06:02,593 - INFO - joeynmt.training - Epoch  10, Step:   189200, Batch Loss:     1.259627, Tokens per Sec:    11737, Lr: 0.000072\n",
      "2021-10-08 12:06:37,789 - INFO - joeynmt.training - Epoch  10, Step:   189300, Batch Loss:     1.338625, Tokens per Sec:    11034, Lr: 0.000072\n",
      "2021-10-08 12:07:13,904 - INFO - joeynmt.training - Epoch  10, Step:   189400, Batch Loss:     1.298330, Tokens per Sec:    11389, Lr: 0.000072\n",
      "2021-10-08 12:07:49,924 - INFO - joeynmt.training - Epoch  10, Step:   189500, Batch Loss:     1.374902, Tokens per Sec:    11467, Lr: 0.000072\n",
      "2021-10-08 12:08:26,141 - INFO - joeynmt.training - Epoch  10, Step:   189600, Batch Loss:     1.597224, Tokens per Sec:    11683, Lr: 0.000072\n",
      "2021-10-08 12:09:02,367 - INFO - joeynmt.training - Epoch  10, Step:   189700, Batch Loss:     1.354455, Tokens per Sec:    11527, Lr: 0.000072\n",
      "2021-10-08 12:09:38,857 - INFO - joeynmt.training - Epoch  10, Step:   189800, Batch Loss:     1.354602, Tokens per Sec:    11639, Lr: 0.000072\n",
      "2021-10-08 12:10:14,534 - INFO - joeynmt.training - Epoch  10, Step:   189900, Batch Loss:     1.340138, Tokens per Sec:    11467, Lr: 0.000072\n",
      "2021-10-08 12:10:50,228 - INFO - joeynmt.training - Epoch  10, Step:   190000, Batch Loss:     1.319964, Tokens per Sec:    11350, Lr: 0.000072\n",
      "2021-10-08 12:11:26,191 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 12:11:26,192 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 12:11:26,192 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 12:11:26,533 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 12:11:26,533 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 12:11:28,277 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 12:11:28,278 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 12:11:28,278 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 12:11:28,278 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-08 12:11:28,278 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 12:11:28,278 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 12:11:28,278 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 12:11:28,278 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when you get to know yourself and express your feelings in a way that speaks to others and harm them .\n",
      "2021-10-08 12:11:28,279 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 12:11:28,279 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 12:11:28,279 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 12:11:28,279 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-08 12:11:28,279 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 12:11:28,279 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 12:11:28,279 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 12:11:28,279 - INFO - joeynmt.training - \tHypothesis: HOW DO MOUR GEESTELICE DOELWITH ?\n",
      "2021-10-08 12:11:28,280 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   190000: bleu:  44.89, loss: 35751.3047, ppl:   3.2604, duration: 38.0512s\n",
      "2021-10-08 12:12:04,459 - INFO - joeynmt.training - Epoch  10, Step:   190100, Batch Loss:     1.476294, Tokens per Sec:    11225, Lr: 0.000072\n",
      "2021-10-08 12:12:41,037 - INFO - joeynmt.training - Epoch  10, Step:   190200, Batch Loss:     1.367067, Tokens per Sec:    11708, Lr: 0.000072\n",
      "2021-10-08 12:13:16,949 - INFO - joeynmt.training - Epoch  10, Step:   190300, Batch Loss:     1.291004, Tokens per Sec:    11416, Lr: 0.000072\n",
      "2021-10-08 12:13:38,849 - INFO - joeynmt.training - Epoch  10: total training loss 9329.72\n",
      "2021-10-08 12:13:38,850 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-10-08 12:13:54,349 - INFO - joeynmt.training - Epoch  11, Step:   190400, Batch Loss:     1.343604, Tokens per Sec:    10001, Lr: 0.000072\n",
      "2021-10-08 12:14:30,803 - INFO - joeynmt.training - Epoch  11, Step:   190500, Batch Loss:     1.448341, Tokens per Sec:    11750, Lr: 0.000072\n",
      "2021-10-08 12:15:06,840 - INFO - joeynmt.training - Epoch  11, Step:   190600, Batch Loss:     1.398634, Tokens per Sec:    11193, Lr: 0.000072\n",
      "2021-10-08 12:15:43,204 - INFO - joeynmt.training - Epoch  11, Step:   190700, Batch Loss:     1.294325, Tokens per Sec:    11653, Lr: 0.000072\n",
      "2021-10-08 12:16:19,788 - INFO - joeynmt.training - Epoch  11, Step:   190800, Batch Loss:     1.325046, Tokens per Sec:    11516, Lr: 0.000072\n",
      "2021-10-08 12:16:55,528 - INFO - joeynmt.training - Epoch  11, Step:   190900, Batch Loss:     1.186840, Tokens per Sec:    11554, Lr: 0.000072\n",
      "2021-10-08 12:17:31,817 - INFO - joeynmt.training - Epoch  11, Step:   191000, Batch Loss:     1.370504, Tokens per Sec:    11455, Lr: 0.000072\n",
      "2021-10-08 12:18:07,878 - INFO - joeynmt.training - Epoch  11, Step:   191100, Batch Loss:     1.291674, Tokens per Sec:    11235, Lr: 0.000071\n",
      "2021-10-08 12:18:44,552 - INFO - joeynmt.training - Epoch  11, Step:   191200, Batch Loss:     1.350113, Tokens per Sec:    11740, Lr: 0.000071\n",
      "2021-10-08 12:19:20,499 - INFO - joeynmt.training - Epoch  11, Step:   191300, Batch Loss:     1.270493, Tokens per Sec:    11420, Lr: 0.000071\n",
      "2021-10-08 12:19:56,174 - INFO - joeynmt.training - Epoch  11, Step:   191400, Batch Loss:     1.348210, Tokens per Sec:    11323, Lr: 0.000071\n",
      "2021-10-08 12:20:32,299 - INFO - joeynmt.training - Epoch  11, Step:   191500, Batch Loss:     1.367288, Tokens per Sec:    11556, Lr: 0.000071\n",
      "2021-10-08 12:21:08,352 - INFO - joeynmt.training - Epoch  11, Step:   191600, Batch Loss:     1.274331, Tokens per Sec:    11318, Lr: 0.000071\n",
      "2021-10-08 12:21:44,857 - INFO - joeynmt.training - Epoch  11, Step:   191700, Batch Loss:     1.316087, Tokens per Sec:    11625, Lr: 0.000071\n",
      "2021-10-08 12:22:20,774 - INFO - joeynmt.training - Epoch  11, Step:   191800, Batch Loss:     1.349258, Tokens per Sec:    11398, Lr: 0.000071\n",
      "2021-10-08 12:22:57,042 - INFO - joeynmt.training - Epoch  11, Step:   191900, Batch Loss:     1.341898, Tokens per Sec:    11493, Lr: 0.000071\n",
      "2021-10-08 12:23:33,177 - INFO - joeynmt.training - Epoch  11, Step:   192000, Batch Loss:     1.343501, Tokens per Sec:    11619, Lr: 0.000071\n",
      "2021-10-08 12:24:08,428 - INFO - joeynmt.training - Epoch  11, Step:   192100, Batch Loss:     1.206894, Tokens per Sec:    11305, Lr: 0.000071\n",
      "2021-10-08 12:24:44,154 - INFO - joeynmt.training - Epoch  11, Step:   192200, Batch Loss:     1.422444, Tokens per Sec:    11217, Lr: 0.000071\n",
      "2021-10-08 12:25:20,311 - INFO - joeynmt.training - Epoch  11, Step:   192300, Batch Loss:     1.270555, Tokens per Sec:    11764, Lr: 0.000071\n",
      "2021-10-08 12:25:56,187 - INFO - joeynmt.training - Epoch  11, Step:   192400, Batch Loss:     1.400880, Tokens per Sec:    11242, Lr: 0.000071\n",
      "2021-10-08 12:26:32,697 - INFO - joeynmt.training - Epoch  11, Step:   192500, Batch Loss:     1.423241, Tokens per Sec:    11760, Lr: 0.000071\n",
      "2021-10-08 12:27:08,999 - INFO - joeynmt.training - Epoch  11, Step:   192600, Batch Loss:     1.346493, Tokens per Sec:    11482, Lr: 0.000071\n",
      "2021-10-08 12:27:45,105 - INFO - joeynmt.training - Epoch  11, Step:   192700, Batch Loss:     1.273545, Tokens per Sec:    11550, Lr: 0.000071\n",
      "2021-10-08 12:28:21,599 - INFO - joeynmt.training - Epoch  11, Step:   192800, Batch Loss:     1.501145, Tokens per Sec:    11632, Lr: 0.000071\n",
      "2021-10-08 12:28:57,541 - INFO - joeynmt.training - Epoch  11, Step:   192900, Batch Loss:     1.477224, Tokens per Sec:    11586, Lr: 0.000071\n",
      "2021-10-08 12:29:33,319 - INFO - joeynmt.training - Epoch  11, Step:   193000, Batch Loss:     1.281524, Tokens per Sec:    11538, Lr: 0.000071\n",
      "2021-10-08 12:30:09,718 - INFO - joeynmt.training - Epoch  11, Step:   193100, Batch Loss:     1.291377, Tokens per Sec:    11706, Lr: 0.000071\n",
      "2021-10-08 12:30:45,836 - INFO - joeynmt.training - Epoch  11, Step:   193200, Batch Loss:     1.428267, Tokens per Sec:    11476, Lr: 0.000071\n",
      "2021-10-08 12:31:22,500 - INFO - joeynmt.training - Epoch  11, Step:   193300, Batch Loss:     1.403616, Tokens per Sec:    11751, Lr: 0.000071\n",
      "2021-10-08 12:31:57,937 - INFO - joeynmt.training - Epoch  11, Step:   193400, Batch Loss:     1.397483, Tokens per Sec:    11322, Lr: 0.000071\n",
      "2021-10-08 12:32:34,651 - INFO - joeynmt.training - Epoch  11, Step:   193500, Batch Loss:     1.379478, Tokens per Sec:    11923, Lr: 0.000071\n",
      "2021-10-08 12:33:10,834 - INFO - joeynmt.training - Epoch  11, Step:   193600, Batch Loss:     1.291128, Tokens per Sec:    11657, Lr: 0.000071\n",
      "2021-10-08 12:33:47,154 - INFO - joeynmt.training - Epoch  11, Step:   193700, Batch Loss:     1.382060, Tokens per Sec:    11491, Lr: 0.000071\n",
      "2021-10-08 12:34:23,215 - INFO - joeynmt.training - Epoch  11, Step:   193800, Batch Loss:     1.424034, Tokens per Sec:    11575, Lr: 0.000071\n",
      "2021-10-08 12:34:59,147 - INFO - joeynmt.training - Epoch  11, Step:   193900, Batch Loss:     1.392726, Tokens per Sec:    11395, Lr: 0.000071\n",
      "2021-10-08 12:35:35,641 - INFO - joeynmt.training - Epoch  11, Step:   194000, Batch Loss:     1.373682, Tokens per Sec:    11579, Lr: 0.000071\n",
      "2021-10-08 12:36:11,726 - INFO - joeynmt.training - Epoch  11, Step:   194100, Batch Loss:     1.385909, Tokens per Sec:    11422, Lr: 0.000071\n",
      "2021-10-08 12:36:48,232 - INFO - joeynmt.training - Epoch  11, Step:   194200, Batch Loss:     1.410054, Tokens per Sec:    11662, Lr: 0.000071\n",
      "2021-10-08 12:37:25,026 - INFO - joeynmt.training - Epoch  11, Step:   194300, Batch Loss:     1.360818, Tokens per Sec:    11341, Lr: 0.000071\n",
      "2021-10-08 12:38:00,846 - INFO - joeynmt.training - Epoch  11, Step:   194400, Batch Loss:     1.369656, Tokens per Sec:    11307, Lr: 0.000071\n",
      "2021-10-08 12:38:37,099 - INFO - joeynmt.training - Epoch  11, Step:   194500, Batch Loss:     1.410080, Tokens per Sec:    11674, Lr: 0.000071\n",
      "2021-10-08 12:39:13,591 - INFO - joeynmt.training - Epoch  11, Step:   194600, Batch Loss:     1.340866, Tokens per Sec:    11692, Lr: 0.000071\n",
      "2021-10-08 12:39:49,861 - INFO - joeynmt.training - Epoch  11, Step:   194700, Batch Loss:     1.287966, Tokens per Sec:    11655, Lr: 0.000071\n",
      "2021-10-08 12:40:26,052 - INFO - joeynmt.training - Epoch  11, Step:   194800, Batch Loss:     1.470684, Tokens per Sec:    11489, Lr: 0.000071\n",
      "2021-10-08 12:41:01,780 - INFO - joeynmt.training - Epoch  11, Step:   194900, Batch Loss:     1.347092, Tokens per Sec:    11278, Lr: 0.000071\n",
      "2021-10-08 12:41:37,828 - INFO - joeynmt.training - Epoch  11, Step:   195000, Batch Loss:     1.395979, Tokens per Sec:    11752, Lr: 0.000071\n",
      "2021-10-08 12:42:13,643 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 12:42:13,644 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 12:42:13,644 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 12:42:13,985 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 12:42:13,985 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 12:42:16,466 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 12:42:16,466 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 12:42:16,466 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 12:42:16,466 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-08 12:42:16,466 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 12:42:16,467 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 12:42:16,467 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 12:42:16,467 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when you get to know yourself and express your feelings in a way that speaks to others and deprives them .\n",
      "2021-10-08 12:42:16,467 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 12:42:16,467 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 12:42:16,467 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 12:42:16,467 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-08 12:42:16,467 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 12:42:16,468 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 12:42:16,468 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 12:42:16,468 - INFO - joeynmt.training - \tHypothesis: HOW DO MOY WE WE WILD SECTED ?\n",
      "2021-10-08 12:42:16,468 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   195000: bleu:  45.08, loss: 35675.5547, ppl:   3.2523, duration: 38.6392s\n",
      "2021-10-08 12:42:52,810 - INFO - joeynmt.training - Epoch  11, Step:   195100, Batch Loss:     1.515164, Tokens per Sec:    11386, Lr: 0.000071\n",
      "2021-10-08 12:43:29,317 - INFO - joeynmt.training - Epoch  11, Step:   195200, Batch Loss:     1.389056, Tokens per Sec:    11604, Lr: 0.000071\n",
      "2021-10-08 12:44:05,152 - INFO - joeynmt.training - Epoch  11, Step:   195300, Batch Loss:     1.308946, Tokens per Sec:    11394, Lr: 0.000071\n",
      "2021-10-08 12:44:41,532 - INFO - joeynmt.training - Epoch  11, Step:   195400, Batch Loss:     1.381440, Tokens per Sec:    11504, Lr: 0.000071\n",
      "2021-10-08 12:45:17,776 - INFO - joeynmt.training - Epoch  11, Step:   195500, Batch Loss:     1.376709, Tokens per Sec:    11456, Lr: 0.000071\n",
      "2021-10-08 12:45:54,529 - INFO - joeynmt.training - Epoch  11, Step:   195600, Batch Loss:     1.369019, Tokens per Sec:    11607, Lr: 0.000071\n",
      "2021-10-08 12:46:30,487 - INFO - joeynmt.training - Epoch  11, Step:   195700, Batch Loss:     1.417058, Tokens per Sec:    11193, Lr: 0.000071\n",
      "2021-10-08 12:47:06,857 - INFO - joeynmt.training - Epoch  11, Step:   195800, Batch Loss:     1.402926, Tokens per Sec:    11401, Lr: 0.000071\n",
      "2021-10-08 12:47:42,765 - INFO - joeynmt.training - Epoch  11, Step:   195900, Batch Loss:     1.240281, Tokens per Sec:    11520, Lr: 0.000071\n",
      "2021-10-08 12:48:19,409 - INFO - joeynmt.training - Epoch  11, Step:   196000, Batch Loss:     1.387310, Tokens per Sec:    11689, Lr: 0.000071\n",
      "2021-10-08 12:48:55,405 - INFO - joeynmt.training - Epoch  11, Step:   196100, Batch Loss:     1.401744, Tokens per Sec:    11312, Lr: 0.000071\n",
      "2021-10-08 12:49:31,346 - INFO - joeynmt.training - Epoch  11, Step:   196200, Batch Loss:     1.433015, Tokens per Sec:    11585, Lr: 0.000071\n",
      "2021-10-08 12:50:07,584 - INFO - joeynmt.training - Epoch  11, Step:   196300, Batch Loss:     1.370908, Tokens per Sec:    11498, Lr: 0.000071\n",
      "2021-10-08 12:50:43,512 - INFO - joeynmt.training - Epoch  11, Step:   196400, Batch Loss:     1.451702, Tokens per Sec:    11558, Lr: 0.000071\n",
      "2021-10-08 12:51:19,761 - INFO - joeynmt.training - Epoch  11, Step:   196500, Batch Loss:     1.351409, Tokens per Sec:    11296, Lr: 0.000070\n",
      "2021-10-08 12:51:56,025 - INFO - joeynmt.training - Epoch  11, Step:   196600, Batch Loss:     1.398973, Tokens per Sec:    11511, Lr: 0.000070\n",
      "2021-10-08 12:52:32,236 - INFO - joeynmt.training - Epoch  11, Step:   196700, Batch Loss:     1.339818, Tokens per Sec:    11564, Lr: 0.000070\n",
      "2021-10-08 12:53:08,978 - INFO - joeynmt.training - Epoch  11, Step:   196800, Batch Loss:     1.519286, Tokens per Sec:    11751, Lr: 0.000070\n",
      "2021-10-08 12:53:44,632 - INFO - joeynmt.training - Epoch  11, Step:   196900, Batch Loss:     1.435721, Tokens per Sec:    11209, Lr: 0.000070\n",
      "2021-10-08 12:54:21,349 - INFO - joeynmt.training - Epoch  11, Step:   197000, Batch Loss:     1.437637, Tokens per Sec:    11539, Lr: 0.000070\n",
      "2021-10-08 12:54:57,511 - INFO - joeynmt.training - Epoch  11, Step:   197100, Batch Loss:     1.464913, Tokens per Sec:    11527, Lr: 0.000070\n",
      "2021-10-08 12:55:19,675 - INFO - joeynmt.training - Epoch  11: total training loss 9302.53\n",
      "2021-10-08 12:55:19,676 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-10-08 12:55:36,177 - INFO - joeynmt.training - Epoch  12, Step:   197200, Batch Loss:     1.323491, Tokens per Sec:    10951, Lr: 0.000070\n",
      "2021-10-08 12:56:12,623 - INFO - joeynmt.training - Epoch  12, Step:   197300, Batch Loss:     1.345021, Tokens per Sec:    11554, Lr: 0.000070\n",
      "2021-10-08 12:56:48,689 - INFO - joeynmt.training - Epoch  12, Step:   197400, Batch Loss:     1.362420, Tokens per Sec:    11426, Lr: 0.000070\n",
      "2021-10-08 12:57:24,684 - INFO - joeynmt.training - Epoch  12, Step:   197500, Batch Loss:     1.285963, Tokens per Sec:    11368, Lr: 0.000070\n",
      "2021-10-08 12:58:01,488 - INFO - joeynmt.training - Epoch  12, Step:   197600, Batch Loss:     1.350220, Tokens per Sec:    11715, Lr: 0.000070\n",
      "2021-10-08 12:58:37,649 - INFO - joeynmt.training - Epoch  12, Step:   197700, Batch Loss:     1.385928, Tokens per Sec:    11301, Lr: 0.000070\n",
      "2021-10-08 12:59:13,876 - INFO - joeynmt.training - Epoch  12, Step:   197800, Batch Loss:     1.363010, Tokens per Sec:    11448, Lr: 0.000070\n",
      "2021-10-08 12:59:49,954 - INFO - joeynmt.training - Epoch  12, Step:   197900, Batch Loss:     1.353936, Tokens per Sec:    11564, Lr: 0.000070\n",
      "2021-10-08 13:00:26,285 - INFO - joeynmt.training - Epoch  12, Step:   198000, Batch Loss:     1.212091, Tokens per Sec:    11494, Lr: 0.000070\n",
      "2021-10-08 13:01:02,377 - INFO - joeynmt.training - Epoch  12, Step:   198100, Batch Loss:     1.340398, Tokens per Sec:    11601, Lr: 0.000070\n",
      "2021-10-08 13:01:37,968 - INFO - joeynmt.training - Epoch  12, Step:   198200, Batch Loss:     1.241001, Tokens per Sec:    11078, Lr: 0.000070\n",
      "2021-10-08 13:02:14,340 - INFO - joeynmt.training - Epoch  12, Step:   198300, Batch Loss:     1.234600, Tokens per Sec:    11728, Lr: 0.000070\n",
      "2021-10-08 13:02:50,642 - INFO - joeynmt.training - Epoch  12, Step:   198400, Batch Loss:     1.478309, Tokens per Sec:    11403, Lr: 0.000070\n",
      "2021-10-08 13:03:26,756 - INFO - joeynmt.training - Epoch  12, Step:   198500, Batch Loss:     1.382976, Tokens per Sec:    11751, Lr: 0.000070\n",
      "2021-10-08 13:04:03,067 - INFO - joeynmt.training - Epoch  12, Step:   198600, Batch Loss:     1.316711, Tokens per Sec:    11274, Lr: 0.000070\n",
      "2021-10-08 13:04:39,155 - INFO - joeynmt.training - Epoch  12, Step:   198700, Batch Loss:     1.410060, Tokens per Sec:    11504, Lr: 0.000070\n",
      "2021-10-08 13:05:14,988 - INFO - joeynmt.training - Epoch  12, Step:   198800, Batch Loss:     1.448658, Tokens per Sec:    11392, Lr: 0.000070\n",
      "2021-10-08 13:05:51,224 - INFO - joeynmt.training - Epoch  12, Step:   198900, Batch Loss:     1.434215, Tokens per Sec:    11456, Lr: 0.000070\n",
      "2021-10-08 13:06:27,104 - INFO - joeynmt.training - Epoch  12, Step:   199000, Batch Loss:     1.292362, Tokens per Sec:    11440, Lr: 0.000070\n",
      "2021-10-08 13:07:03,584 - INFO - joeynmt.training - Epoch  12, Step:   199100, Batch Loss:     1.191305, Tokens per Sec:    11583, Lr: 0.000070\n",
      "2021-10-08 13:07:40,333 - INFO - joeynmt.training - Epoch  12, Step:   199200, Batch Loss:     1.299206, Tokens per Sec:    11071, Lr: 0.000070\n",
      "2021-10-08 13:08:16,508 - INFO - joeynmt.training - Epoch  12, Step:   199300, Batch Loss:     1.367628, Tokens per Sec:    11419, Lr: 0.000070\n",
      "2021-10-08 13:08:52,295 - INFO - joeynmt.training - Epoch  12, Step:   199400, Batch Loss:     1.331647, Tokens per Sec:    11461, Lr: 0.000070\n",
      "2021-10-08 13:09:28,864 - INFO - joeynmt.training - Epoch  12, Step:   199500, Batch Loss:     1.433464, Tokens per Sec:    11641, Lr: 0.000070\n",
      "2021-10-08 13:10:05,071 - INFO - joeynmt.training - Epoch  12, Step:   199600, Batch Loss:     1.217550, Tokens per Sec:    11424, Lr: 0.000070\n",
      "2021-10-08 13:10:41,676 - INFO - joeynmt.training - Epoch  12, Step:   199700, Batch Loss:     1.395194, Tokens per Sec:    11687, Lr: 0.000070\n",
      "2021-10-08 13:11:18,049 - INFO - joeynmt.training - Epoch  12, Step:   199800, Batch Loss:     1.252671, Tokens per Sec:    11499, Lr: 0.000070\n",
      "2021-10-08 13:11:54,591 - INFO - joeynmt.training - Epoch  12, Step:   199900, Batch Loss:     1.066039, Tokens per Sec:    11747, Lr: 0.000070\n",
      "2021-10-08 13:12:30,840 - INFO - joeynmt.training - Epoch  12, Step:   200000, Batch Loss:     1.421443, Tokens per Sec:    11414, Lr: 0.000070\n",
      "2021-10-08 13:13:06,576 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 13:13:06,577 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 13:13:06,577 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 13:13:06,920 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 13:13:06,920 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 13:13:08,645 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 13:13:08,645 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 13:13:08,645 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 13:13:08,645 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-08 13:13:08,645 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 13:13:08,646 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 13:13:08,646 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 13:13:08,646 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when you get to know yourself and express your feelings in a way that speaks to others and deprives them .\n",
      "2021-10-08 13:13:08,646 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 13:13:08,646 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 13:13:08,646 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 13:13:08,646 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-08 13:13:08,646 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 13:13:08,647 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 13:13:08,647 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 13:13:08,647 - INFO - joeynmt.training - \tHypothesis: HOW DO MOUR FEECTED DOELVITS ?\n",
      "2021-10-08 13:13:08,647 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   200000: bleu:  44.91, loss: 35614.3047, ppl:   3.2457, duration: 37.8065s\n",
      "2021-10-08 13:13:44,892 - INFO - joeynmt.training - Epoch  12, Step:   200100, Batch Loss:     1.358330, Tokens per Sec:    11473, Lr: 0.000070\n",
      "2021-10-08 13:14:21,334 - INFO - joeynmt.training - Epoch  12, Step:   200200, Batch Loss:     1.506533, Tokens per Sec:    11472, Lr: 0.000070\n",
      "2021-10-08 13:14:57,574 - INFO - joeynmt.training - Epoch  12, Step:   200300, Batch Loss:     1.485555, Tokens per Sec:    11710, Lr: 0.000070\n",
      "2021-10-08 13:15:33,942 - INFO - joeynmt.training - Epoch  12, Step:   200400, Batch Loss:     1.489997, Tokens per Sec:    11666, Lr: 0.000070\n",
      "2021-10-08 13:16:09,921 - INFO - joeynmt.training - Epoch  12, Step:   200500, Batch Loss:     1.365425, Tokens per Sec:    11619, Lr: 0.000070\n",
      "2021-10-08 13:16:46,308 - INFO - joeynmt.training - Epoch  12, Step:   200600, Batch Loss:     1.443932, Tokens per Sec:    11368, Lr: 0.000070\n",
      "2021-10-08 13:17:22,318 - INFO - joeynmt.training - Epoch  12, Step:   200700, Batch Loss:     1.226927, Tokens per Sec:    11369, Lr: 0.000070\n",
      "2021-10-08 13:17:58,412 - INFO - joeynmt.training - Epoch  12, Step:   200800, Batch Loss:     1.286613, Tokens per Sec:    11500, Lr: 0.000070\n",
      "2021-10-08 13:18:34,573 - INFO - joeynmt.training - Epoch  12, Step:   200900, Batch Loss:     1.230413, Tokens per Sec:    11514, Lr: 0.000070\n",
      "2021-10-08 13:19:10,441 - INFO - joeynmt.training - Epoch  12, Step:   201000, Batch Loss:     1.327969, Tokens per Sec:    11577, Lr: 0.000070\n",
      "2021-10-08 13:19:46,408 - INFO - joeynmt.training - Epoch  12, Step:   201100, Batch Loss:     1.569089, Tokens per Sec:    11640, Lr: 0.000070\n",
      "2021-10-08 13:20:22,433 - INFO - joeynmt.training - Epoch  12, Step:   201200, Batch Loss:     1.350171, Tokens per Sec:    11567, Lr: 0.000070\n",
      "2021-10-08 13:20:57,996 - INFO - joeynmt.training - Epoch  12, Step:   201300, Batch Loss:     1.325003, Tokens per Sec:    11461, Lr: 0.000070\n",
      "2021-10-08 13:21:34,544 - INFO - joeynmt.training - Epoch  12, Step:   201400, Batch Loss:     1.250777, Tokens per Sec:    11597, Lr: 0.000070\n",
      "2021-10-08 13:22:10,724 - INFO - joeynmt.training - Epoch  12, Step:   201500, Batch Loss:     1.335498, Tokens per Sec:    11361, Lr: 0.000070\n",
      "2021-10-08 13:22:46,861 - INFO - joeynmt.training - Epoch  12, Step:   201600, Batch Loss:     1.348061, Tokens per Sec:    11614, Lr: 0.000070\n",
      "2021-10-08 13:23:22,850 - INFO - joeynmt.training - Epoch  12, Step:   201700, Batch Loss:     1.350560, Tokens per Sec:    11331, Lr: 0.000070\n",
      "2021-10-08 13:23:58,812 - INFO - joeynmt.training - Epoch  12, Step:   201800, Batch Loss:     1.322476, Tokens per Sec:    11348, Lr: 0.000070\n",
      "2021-10-08 13:24:35,153 - INFO - joeynmt.training - Epoch  12, Step:   201900, Batch Loss:     1.134611, Tokens per Sec:    11628, Lr: 0.000070\n",
      "2021-10-08 13:25:11,046 - INFO - joeynmt.training - Epoch  12, Step:   202000, Batch Loss:     1.415018, Tokens per Sec:    11510, Lr: 0.000070\n",
      "2021-10-08 13:25:47,464 - INFO - joeynmt.training - Epoch  12, Step:   202100, Batch Loss:     1.373626, Tokens per Sec:    11721, Lr: 0.000070\n",
      "2021-10-08 13:26:23,477 - INFO - joeynmt.training - Epoch  12, Step:   202200, Batch Loss:     1.262835, Tokens per Sec:    11627, Lr: 0.000069\n",
      "2021-10-08 13:26:59,166 - INFO - joeynmt.training - Epoch  12, Step:   202300, Batch Loss:     1.486282, Tokens per Sec:    11483, Lr: 0.000069\n",
      "2021-10-08 13:27:35,085 - INFO - joeynmt.training - Epoch  12, Step:   202400, Batch Loss:     1.454686, Tokens per Sec:    11629, Lr: 0.000069\n",
      "2021-10-08 13:28:11,255 - INFO - joeynmt.training - Epoch  12, Step:   202500, Batch Loss:     1.323354, Tokens per Sec:    11767, Lr: 0.000069\n",
      "2021-10-08 13:28:47,019 - INFO - joeynmt.training - Epoch  12, Step:   202600, Batch Loss:     1.411200, Tokens per Sec:    11548, Lr: 0.000069\n",
      "2021-10-08 13:29:23,018 - INFO - joeynmt.training - Epoch  12, Step:   202700, Batch Loss:     1.459336, Tokens per Sec:    11633, Lr: 0.000069\n",
      "2021-10-08 13:29:58,891 - INFO - joeynmt.training - Epoch  12, Step:   202800, Batch Loss:     1.295043, Tokens per Sec:    11422, Lr: 0.000069\n",
      "2021-10-08 13:30:35,166 - INFO - joeynmt.training - Epoch  12, Step:   202900, Batch Loss:     1.439460, Tokens per Sec:    11630, Lr: 0.000069\n",
      "2021-10-08 13:31:11,107 - INFO - joeynmt.training - Epoch  12, Step:   203000, Batch Loss:     1.330800, Tokens per Sec:    11405, Lr: 0.000069\n",
      "2021-10-08 13:31:47,278 - INFO - joeynmt.training - Epoch  12, Step:   203100, Batch Loss:     1.454551, Tokens per Sec:    11471, Lr: 0.000069\n",
      "2021-10-08 13:32:23,559 - INFO - joeynmt.training - Epoch  12, Step:   203200, Batch Loss:     1.323631, Tokens per Sec:    11557, Lr: 0.000069\n",
      "2021-10-08 13:32:59,119 - INFO - joeynmt.training - Epoch  12, Step:   203300, Batch Loss:     1.334818, Tokens per Sec:    11426, Lr: 0.000069\n",
      "2021-10-08 13:33:35,337 - INFO - joeynmt.training - Epoch  12, Step:   203400, Batch Loss:     1.484226, Tokens per Sec:    11769, Lr: 0.000069\n",
      "2021-10-08 13:34:11,724 - INFO - joeynmt.training - Epoch  12, Step:   203500, Batch Loss:     1.326811, Tokens per Sec:    11636, Lr: 0.000069\n",
      "2021-10-08 13:34:47,525 - INFO - joeynmt.training - Epoch  12, Step:   203600, Batch Loss:     1.489926, Tokens per Sec:    11621, Lr: 0.000069\n",
      "2021-10-08 13:35:23,586 - INFO - joeynmt.training - Epoch  12, Step:   203700, Batch Loss:     1.375033, Tokens per Sec:    11505, Lr: 0.000069\n",
      "2021-10-08 13:35:59,756 - INFO - joeynmt.training - Epoch  12, Step:   203800, Batch Loss:     1.423084, Tokens per Sec:    11593, Lr: 0.000069\n",
      "2021-10-08 13:36:35,859 - INFO - joeynmt.training - Epoch  12, Step:   203900, Batch Loss:     1.420133, Tokens per Sec:    11666, Lr: 0.000069\n",
      "2021-10-08 13:36:55,660 - INFO - joeynmt.training - Epoch  12: total training loss 9273.97\n",
      "2021-10-08 13:36:55,661 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-10-08 13:37:13,730 - INFO - joeynmt.training - Epoch  13, Step:   204000, Batch Loss:     1.427375, Tokens per Sec:    10227, Lr: 0.000069\n",
      "2021-10-08 13:37:49,604 - INFO - joeynmt.training - Epoch  13, Step:   204100, Batch Loss:     1.287815, Tokens per Sec:    11512, Lr: 0.000069\n",
      "2021-10-08 13:38:26,047 - INFO - joeynmt.training - Epoch  13, Step:   204200, Batch Loss:     1.404624, Tokens per Sec:    11757, Lr: 0.000069\n",
      "2021-10-08 13:39:01,841 - INFO - joeynmt.training - Epoch  13, Step:   204300, Batch Loss:     1.371620, Tokens per Sec:    11140, Lr: 0.000069\n",
      "2021-10-08 13:39:37,891 - INFO - joeynmt.training - Epoch  13, Step:   204400, Batch Loss:     1.271839, Tokens per Sec:    11574, Lr: 0.000069\n",
      "2021-10-08 13:40:13,831 - INFO - joeynmt.training - Epoch  13, Step:   204500, Batch Loss:     1.338546, Tokens per Sec:    11282, Lr: 0.000069\n",
      "2021-10-08 13:40:50,139 - INFO - joeynmt.training - Epoch  13, Step:   204600, Batch Loss:     1.225050, Tokens per Sec:    11764, Lr: 0.000069\n",
      "2021-10-08 13:41:26,262 - INFO - joeynmt.training - Epoch  13, Step:   204700, Batch Loss:     1.269889, Tokens per Sec:    11563, Lr: 0.000069\n",
      "2021-10-08 13:42:02,546 - INFO - joeynmt.training - Epoch  13, Step:   204800, Batch Loss:     1.124935, Tokens per Sec:    11604, Lr: 0.000069\n",
      "2021-10-08 13:42:38,696 - INFO - joeynmt.training - Epoch  13, Step:   204900, Batch Loss:     1.401063, Tokens per Sec:    11442, Lr: 0.000069\n",
      "2021-10-08 13:43:14,892 - INFO - joeynmt.training - Epoch  13, Step:   205000, Batch Loss:     1.343339, Tokens per Sec:    11554, Lr: 0.000069\n",
      "2021-10-08 13:43:52,002 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 13:43:52,003 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 13:43:52,003 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 13:43:52,349 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 13:43:52,349 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 13:43:54,096 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 13:43:54,096 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 13:43:54,096 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 13:43:54,096 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-08 13:43:54,096 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 13:43:54,096 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 13:43:54,097 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 13:43:54,097 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when you get to know yourself and express your feelings in a way that speaks to others and harm them .\n",
      "2021-10-08 13:43:54,097 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 13:43:54,097 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 13:43:54,097 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 13:43:54,097 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-08 13:43:54,097 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 13:43:54,097 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 13:43:54,098 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 13:43:54,098 - INFO - joeynmt.training - \tHypothesis: HOW DO MOY WE WE WILD SECTECTURY ?\n",
      "2021-10-08 13:43:54,098 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step   205000: bleu:  45.28, loss: 35517.9414, ppl:   3.2354, duration: 39.2053s\n",
      "2021-10-08 13:44:30,367 - INFO - joeynmt.training - Epoch  13, Step:   205100, Batch Loss:     1.368016, Tokens per Sec:    11677, Lr: 0.000069\n",
      "2021-10-08 13:45:06,175 - INFO - joeynmt.training - Epoch  13, Step:   205200, Batch Loss:     1.411145, Tokens per Sec:    11423, Lr: 0.000069\n",
      "2021-10-08 13:45:42,382 - INFO - joeynmt.training - Epoch  13, Step:   205300, Batch Loss:     1.471636, Tokens per Sec:    11523, Lr: 0.000069\n",
      "2021-10-08 13:46:18,131 - INFO - joeynmt.training - Epoch  13, Step:   205400, Batch Loss:     1.290562, Tokens per Sec:    11291, Lr: 0.000069\n",
      "2021-10-08 13:46:54,142 - INFO - joeynmt.training - Epoch  13, Step:   205500, Batch Loss:     1.329284, Tokens per Sec:    11524, Lr: 0.000069\n",
      "2021-10-08 13:47:30,446 - INFO - joeynmt.training - Epoch  13, Step:   205600, Batch Loss:     1.306621, Tokens per Sec:    11627, Lr: 0.000069\n",
      "2021-10-08 13:48:07,559 - INFO - joeynmt.training - Epoch  13, Step:   205700, Batch Loss:     1.379683, Tokens per Sec:    11268, Lr: 0.000069\n",
      "2021-10-08 13:48:43,811 - INFO - joeynmt.training - Epoch  13, Step:   205800, Batch Loss:     1.375211, Tokens per Sec:    11377, Lr: 0.000069\n",
      "2021-10-08 13:49:19,696 - INFO - joeynmt.training - Epoch  13, Step:   205900, Batch Loss:     1.406630, Tokens per Sec:    11599, Lr: 0.000069\n",
      "2021-10-08 13:49:56,321 - INFO - joeynmt.training - Epoch  13, Step:   206000, Batch Loss:     1.395606, Tokens per Sec:    11777, Lr: 0.000069\n",
      "2021-10-08 13:50:32,353 - INFO - joeynmt.training - Epoch  13, Step:   206100, Batch Loss:     1.228695, Tokens per Sec:    11297, Lr: 0.000069\n",
      "2021-10-08 13:51:08,023 - INFO - joeynmt.training - Epoch  13, Step:   206200, Batch Loss:     1.427859, Tokens per Sec:    11353, Lr: 0.000069\n",
      "2021-10-08 13:51:44,169 - INFO - joeynmt.training - Epoch  13, Step:   206300, Batch Loss:     1.270800, Tokens per Sec:    11601, Lr: 0.000069\n",
      "2021-10-08 13:52:20,530 - INFO - joeynmt.training - Epoch  13, Step:   206400, Batch Loss:     1.335334, Tokens per Sec:    11807, Lr: 0.000069\n",
      "2021-10-08 13:52:56,590 - INFO - joeynmt.training - Epoch  13, Step:   206500, Batch Loss:     1.448464, Tokens per Sec:    11489, Lr: 0.000069\n",
      "2021-10-08 13:53:32,696 - INFO - joeynmt.training - Epoch  13, Step:   206600, Batch Loss:     1.394191, Tokens per Sec:    11699, Lr: 0.000069\n",
      "2021-10-08 13:54:08,710 - INFO - joeynmt.training - Epoch  13, Step:   206700, Batch Loss:     1.455884, Tokens per Sec:    11614, Lr: 0.000069\n",
      "2021-10-08 13:54:44,830 - INFO - joeynmt.training - Epoch  13, Step:   206800, Batch Loss:     1.321460, Tokens per Sec:    11626, Lr: 0.000069\n",
      "2021-10-08 13:55:20,576 - INFO - joeynmt.training - Epoch  13, Step:   206900, Batch Loss:     1.285450, Tokens per Sec:    11076, Lr: 0.000069\n",
      "2021-10-08 13:55:56,969 - INFO - joeynmt.training - Epoch  13, Step:   207000, Batch Loss:     1.237174, Tokens per Sec:    11383, Lr: 0.000069\n",
      "2021-10-08 13:56:33,797 - INFO - joeynmt.training - Epoch  13, Step:   207100, Batch Loss:     1.305641, Tokens per Sec:    11581, Lr: 0.000069\n",
      "2021-10-08 13:57:09,508 - INFO - joeynmt.training - Epoch  13, Step:   207200, Batch Loss:     1.346263, Tokens per Sec:    11321, Lr: 0.000069\n",
      "2021-10-08 13:57:45,897 - INFO - joeynmt.training - Epoch  13, Step:   207300, Batch Loss:     1.336434, Tokens per Sec:    11606, Lr: 0.000069\n",
      "2021-10-08 13:58:22,449 - INFO - joeynmt.training - Epoch  13, Step:   207400, Batch Loss:     1.428743, Tokens per Sec:    11488, Lr: 0.000069\n",
      "2021-10-08 13:58:58,269 - INFO - joeynmt.training - Epoch  13, Step:   207500, Batch Loss:     1.336917, Tokens per Sec:    11654, Lr: 0.000069\n",
      "2021-10-08 13:59:34,484 - INFO - joeynmt.training - Epoch  13, Step:   207600, Batch Loss:     1.353204, Tokens per Sec:    11451, Lr: 0.000069\n",
      "2021-10-08 14:00:11,277 - INFO - joeynmt.training - Epoch  13, Step:   207700, Batch Loss:     1.427175, Tokens per Sec:    11680, Lr: 0.000069\n",
      "2021-10-08 14:00:47,324 - INFO - joeynmt.training - Epoch  13, Step:   207800, Batch Loss:     1.391368, Tokens per Sec:    11541, Lr: 0.000069\n",
      "2021-10-08 14:01:23,799 - INFO - joeynmt.training - Epoch  13, Step:   207900, Batch Loss:     1.451503, Tokens per Sec:    11421, Lr: 0.000069\n",
      "2021-10-08 14:01:59,960 - INFO - joeynmt.training - Epoch  13, Step:   208000, Batch Loss:     1.375213, Tokens per Sec:    11539, Lr: 0.000069\n",
      "2021-10-08 14:02:36,464 - INFO - joeynmt.training - Epoch  13, Step:   208100, Batch Loss:     1.402577, Tokens per Sec:    11375, Lr: 0.000069\n",
      "2021-10-08 14:03:12,880 - INFO - joeynmt.training - Epoch  13, Step:   208200, Batch Loss:     1.316830, Tokens per Sec:    11487, Lr: 0.000068\n",
      "2021-10-08 14:03:49,047 - INFO - joeynmt.training - Epoch  13, Step:   208300, Batch Loss:     1.444927, Tokens per Sec:    11342, Lr: 0.000068\n",
      "2021-10-08 14:04:25,328 - INFO - joeynmt.training - Epoch  13, Step:   208400, Batch Loss:     1.330889, Tokens per Sec:    11483, Lr: 0.000068\n",
      "2021-10-08 14:05:01,380 - INFO - joeynmt.training - Epoch  13, Step:   208500, Batch Loss:     1.416686, Tokens per Sec:    11441, Lr: 0.000068\n",
      "2021-10-08 14:05:37,414 - INFO - joeynmt.training - Epoch  13, Step:   208600, Batch Loss:     1.291559, Tokens per Sec:    11343, Lr: 0.000068\n",
      "2021-10-08 14:06:13,859 - INFO - joeynmt.training - Epoch  13, Step:   208700, Batch Loss:     1.299309, Tokens per Sec:    11459, Lr: 0.000068\n",
      "2021-10-08 14:06:49,958 - INFO - joeynmt.training - Epoch  13, Step:   208800, Batch Loss:     1.332725, Tokens per Sec:    11228, Lr: 0.000068\n",
      "2021-10-08 14:07:26,324 - INFO - joeynmt.training - Epoch  13, Step:   208900, Batch Loss:     1.393658, Tokens per Sec:    11533, Lr: 0.000068\n",
      "2021-10-08 14:08:02,644 - INFO - joeynmt.training - Epoch  13, Step:   209000, Batch Loss:     1.504551, Tokens per Sec:    11725, Lr: 0.000068\n",
      "2021-10-08 14:08:38,973 - INFO - joeynmt.training - Epoch  13, Step:   209100, Batch Loss:     1.255504, Tokens per Sec:    11471, Lr: 0.000068\n",
      "2021-10-08 14:09:15,415 - INFO - joeynmt.training - Epoch  13, Step:   209200, Batch Loss:     1.422372, Tokens per Sec:    11545, Lr: 0.000068\n",
      "2021-10-08 14:09:51,716 - INFO - joeynmt.training - Epoch  13, Step:   209300, Batch Loss:     1.380416, Tokens per Sec:    11432, Lr: 0.000068\n",
      "2021-10-08 14:10:27,785 - INFO - joeynmt.training - Epoch  13, Step:   209400, Batch Loss:     1.321888, Tokens per Sec:    11233, Lr: 0.000068\n",
      "2021-10-08 14:11:04,132 - INFO - joeynmt.training - Epoch  13, Step:   209500, Batch Loss:     1.363813, Tokens per Sec:    11843, Lr: 0.000068\n",
      "2021-10-08 14:11:40,582 - INFO - joeynmt.training - Epoch  13, Step:   209600, Batch Loss:     1.252944, Tokens per Sec:    11642, Lr: 0.000068\n",
      "2021-10-08 14:12:17,212 - INFO - joeynmt.training - Epoch  13, Step:   209700, Batch Loss:     1.323959, Tokens per Sec:    11504, Lr: 0.000068\n",
      "2021-10-08 14:12:52,956 - INFO - joeynmt.training - Epoch  13, Step:   209800, Batch Loss:     1.432271, Tokens per Sec:    11391, Lr: 0.000068\n",
      "2021-10-08 14:13:29,093 - INFO - joeynmt.training - Epoch  13, Step:   209900, Batch Loss:     1.397933, Tokens per Sec:    11568, Lr: 0.000068\n",
      "2021-10-08 14:14:05,611 - INFO - joeynmt.training - Epoch  13, Step:   210000, Batch Loss:     1.363274, Tokens per Sec:    11620, Lr: 0.000068\n",
      "2021-10-08 14:14:41,022 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 14:14:41,022 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 14:14:41,022 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 14:14:41,361 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 14:14:41,362 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 14:14:43,125 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 14:14:43,126 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 14:14:43,126 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 14:14:43,126 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-08 14:14:43,126 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 14:14:43,126 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 14:14:43,126 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 14:14:43,126 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when you get to know yourself and express your feelings in a way that speaks to others and deprives them .\n",
      "2021-10-08 14:14:43,126 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 14:14:43,127 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 14:14:43,127 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 14:14:43,127 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-08 14:14:43,127 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 14:14:43,127 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 14:14:43,127 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 14:14:43,127 - INFO - joeynmt.training - \tHypothesis: HOW DO MOY WE WE WILL DOELVITH ?\n",
      "2021-10-08 14:14:43,127 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step   210000: bleu:  45.03, loss: 35488.4648, ppl:   3.2322, duration: 37.5151s\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_$tgt$src.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp_things/masakhane/af-en-mcb_af_to_en_1007_v2_larger/models/enaf_reverse_transformer-large-30-epochs-and-20-more/\n",
      "130000.hyps  160000.hyps  190000.hyps  210000.ckpt    tensorboard\n",
      "135000.hyps  165000.hyps  195000.hyps  210000.hyps    train.log\n",
      "140000.hyps  170000.hyps  200000.ckpt  best.ckpt      trg_vocab.txt\n",
      "145000.hyps  175000.hyps  200000.hyps  config.yaml    validations.txt\n",
      "150000.hyps  180000.hyps  205000.ckpt  latest.ckpt\n",
      "155000.hyps  185000.hyps  205000.hyps  src_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "os.environ['FILE'] = f\"{os.environ['gdrive_path']}/models/{os.environ['src']}{os.environ['tgt']}_reverse_transformer-large-30-epochs-and-20-more/\"\n",
    "\n",
    "!echo $FILE\n",
    "!ls $FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nlp_things/masakhane/af-en-mcb_af_to_en_1007_v2_larger'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['gdrive_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "MBoDS09JM807"
   },
   "outputs": [],
   "source": [
    "# Copy the created models from the notebook storage to google drive for persistant storage\n",
    "!mkdir -p \"$FILE\"\n",
    "\n",
    "!cp -r joeynmt/models/${tgt}${src}_reverse_transformer_michael_larger_30epochs-more-1008/* \"$FILE\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n94wlrCjVc17",
    "outputId": "bb8462a5-241e-4b73-e9a8-3096816d0151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 130000\tLoss: 36860.40234\tPPL: 3.38220\tbleu: 44.68351\tLR: 0.00008667\t*\n",
      "Steps: 135000\tLoss: 36715.71094\tPPL: 3.36606\tbleu: 44.52790\tLR: 0.00008505\t*\n",
      "Steps: 140000\tLoss: 36628.31641\tPPL: 3.35635\tbleu: 44.40698\tLR: 0.00008352\t*\n",
      "Steps: 145000\tLoss: 36507.67188\tPPL: 3.34299\tbleu: 44.44326\tLR: 0.00008207\t*\n",
      "Steps: 150000\tLoss: 36433.37500\tPPL: 3.33479\tbleu: 44.53037\tLR: 0.00008069\t*\n",
      "Steps: 155000\tLoss: 36232.90234\tPPL: 3.31276\tbleu: 44.60506\tLR: 0.00007938\t*\n",
      "Steps: 160000\tLoss: 36215.22266\tPPL: 3.31083\tbleu: 44.46702\tLR: 0.00007813\t*\n",
      "Steps: 165000\tLoss: 36132.12891\tPPL: 3.30174\tbleu: 44.74444\tLR: 0.00007693\t*\n",
      "Steps: 170000\tLoss: 36024.71875\tPPL: 3.29004\tbleu: 44.90198\tLR: 0.00007579\t*\n",
      "Steps: 175000\tLoss: 35937.86328\tPPL: 3.28061\tbleu: 44.77740\tLR: 0.00007470\t*\n",
      "Steps: 180000\tLoss: 35954.15234\tPPL: 3.28237\tbleu: 44.80381\tLR: 0.00007366\t\n",
      "Steps: 185000\tLoss: 35815.46484\tPPL: 3.26736\tbleu: 44.75359\tLR: 0.00007265\t*\n",
      "Steps: 190000\tLoss: 35751.30469\tPPL: 3.26044\tbleu: 44.88682\tLR: 0.00007169\t*\n",
      "Steps: 195000\tLoss: 35675.55469\tPPL: 3.25228\tbleu: 45.08195\tLR: 0.00007077\t*\n",
      "Steps: 200000\tLoss: 35614.30469\tPPL: 3.24570\tbleu: 44.91332\tLR: 0.00006988\t*\n",
      "Steps: 205000\tLoss: 35517.94141\tPPL: 3.23538\tbleu: 45.28465\tLR: 0.00006902\t*\n",
      "Steps: 210000\tLoss: 35488.46484\tPPL: 3.23223\tbleu: 45.03143\tLR: 0.00006819\t*\n"
     ]
    }
   ],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"$FILE/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66WhRE9lIhoD",
    "outputId": "b4297dba-67ae-48be-9068-4a0e92c4f98e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-08 14:52:12,417 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-10-08 14:52:12,418 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-10-08 14:52:12,699 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-10-08 14:52:12,713 - INFO - joeynmt.data - Loading test data...\n",
      "2021-10-08 14:52:12,739 - INFO - joeynmt.data - Data loaded.\n",
      "2021-10-08 14:52:12,787 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
      "2021-10-08 14:52:15,375 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-10-08 14:52:15,608 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-10-08 14:52:15,714 - INFO - joeynmt.prediction - Decoding on dev set (data/afen/dev.bpe.en)...\n",
      "2021-10-08 14:52:59,280 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 14:52:59,281 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 14:52:59,281 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 14:52:59,621 - INFO - joeynmt.prediction -  dev bleu[13a]:  45.43 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-10-08 14:52:59,621 - INFO - joeynmt.prediction - Decoding on test set (data/afen/test.bpe.en)...\n",
      "2021-10-08 14:54:01,624 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 14:54:01,625 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 14:54:01,625 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 14:54:02,419 - INFO - joeynmt.prediction - test bleu[13a]:  53.33 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Test our model\n",
    "! cd joeynmt; python3 -m joeynmt test ../\"$FILE/config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "nPl9_zYClIbP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130000.hyps  160000.hyps  190000.hyps  210000.ckpt    tensorboard\n",
      "135000.hyps  165000.hyps  195000.hyps  210000.hyps    test.log\n",
      "140000.hyps  170000.hyps  200000.ckpt  best.ckpt      train.log\n",
      "145000.hyps  175000.hyps  200000.hyps  config.yaml    trg_vocab.txt\n",
      "150000.hyps  180000.hyps  205000.ckpt  latest.ckpt    validations.txt\n",
      "155000.hyps  185000.hyps  205000.hyps  src_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "! ls \"$FILE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try and experiment with the parameters => Make the model bigger, and train for less epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
    "# (You can of course play with all the parameters if you'd like!)\n",
    "\n",
    "name = '%s%s' % (target_language, source_language)\n",
    "# gdrive_path = os.environ[\"gdrive_path\"]\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{target_language}{source_language}_reverse_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{target_language}\"\n",
    "    trg: \"{source_language}\"\n",
    "    train: \"data/{name}/train.bpe\"\n",
    "    dev:   \"data/{name}/dev.bpe\"\n",
    "    test:  \"data/{name}/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"data/{name}/vocab.txt\"\n",
    "    trg_vocab: \"data/{name}/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    load_model: \"joeynmt/models/${tgt}${src}_reverse_transformer_big-model-end1008/\"\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 3600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 20                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 5000          # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/{name}_reverse_transformer_big-model-end1008\"\n",
    "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 8             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 512   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 512         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 2048            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 8              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 512    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 512         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 2048            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-08 15:08:59,840 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-10-08 15:08:59,892 - INFO - joeynmt.data - Loading training data...\n",
      "2021-10-08 15:09:19,421 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-10-08 15:09:19,676 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-10-08 15:09:19,781 - INFO - joeynmt.data - Loading test data...\n",
      "2021-10-08 15:09:19,806 - INFO - joeynmt.data - Data loaded.\n",
      "2021-10-08 15:09:19,806 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-10-08 15:09:20,573 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-10-08 15:09:20,585 - INFO - joeynmt.training - Total params: 46467072\n",
      "2021-10-08 15:09:23,190 - INFO - joeynmt.helpers - cfg.name                           : afen_reverse_transformer\n",
      "2021-10-08 15:09:23,190 - INFO - joeynmt.helpers - cfg.data.src                       : af\n",
      "2021-10-08 15:09:23,190 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-10-08 15:09:23,190 - INFO - joeynmt.helpers - cfg.data.train                     : data/afen/train.bpe\n",
      "2021-10-08 15:09:23,190 - INFO - joeynmt.helpers - cfg.data.dev                       : data/afen/dev.bpe\n",
      "2021-10-08 15:09:23,190 - INFO - joeynmt.helpers - cfg.data.test                      : data/afen/test.bpe\n",
      "2021-10-08 15:09:23,191 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-10-08 15:09:23,191 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-10-08 15:09:23,191 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-10-08 15:09:23,191 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : data/afen/vocab.txt\n",
      "2021-10-08 15:09:23,191 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : data/afen/vocab.txt\n",
      "2021-10-08 15:09:23,191 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-10-08 15:09:23,191 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-10-08 15:09:23,192 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-10-08 15:09:23,192 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-10-08 15:09:23,192 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-10-08 15:09:23,192 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-10-08 15:09:23,192 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-10-08 15:09:23,192 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-10-08 15:09:23,192 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-10-08 15:09:23,192 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-10-08 15:09:23,193 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-10-08 15:09:23,193 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-10-08 15:09:23,193 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-10-08 15:09:23,193 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-10-08 15:09:23,193 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-10-08 15:09:23,193 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-10-08 15:09:23,193 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-10-08 15:09:23,194 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-10-08 15:09:23,194 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
      "2021-10-08 15:09:23,194 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-10-08 15:09:23,194 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-10-08 15:09:23,194 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-10-08 15:09:23,194 - INFO - joeynmt.helpers - cfg.training.epochs                : 20\n",
      "2021-10-08 15:09:23,194 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
      "2021-10-08 15:09:23,194 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-10-08 15:09:23,195 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-10-08 15:09:23,195 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/afen_reverse_transformer_big-model-end1008\n",
      "2021-10-08 15:09:23,195 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-10-08 15:09:23,195 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-10-08 15:09:23,195 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-10-08 15:09:23,195 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-10-08 15:09:23,195 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-10-08 15:09:23,195 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-10-08 15:09:23,196 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-10-08 15:09:23,196 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-10-08 15:09:23,196 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-10-08 15:09:23,196 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-10-08 15:09:23,196 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-10-08 15:09:23,196 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-10-08 15:09:23,196 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-10-08 15:09:23,197 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-10-08 15:09:23,197 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-10-08 15:09:23,197 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 8\n",
      "2021-10-08 15:09:23,197 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 512\n",
      "2021-10-08 15:09:23,197 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-10-08 15:09:23,197 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-10-08 15:09:23,197 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 512\n",
      "2021-10-08 15:09:23,197 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 2048\n",
      "2021-10-08 15:09:23,198 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-10-08 15:09:23,198 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-10-08 15:09:23,198 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-10-08 15:09:23,198 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 8\n",
      "2021-10-08 15:09:23,198 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512\n",
      "2021-10-08 15:09:23,198 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-10-08 15:09:23,198 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-10-08 15:09:23,199 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 512\n",
      "2021-10-08 15:09:23,199 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 2048\n",
      "2021-10-08 15:09:23,199 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-10-08 15:09:23,199 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 959951,\n",
      "\tvalid 1000,\n",
      "\ttest 2682\n",
      "2021-10-08 15:09:23,199 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Mo@@ der@@ ne te@@ g@@ no@@ lo@@ gie stel leer@@ ders en studen@@ te in staat om ma@@ k@@ liker en op be@@ dre@@ wen@@ er wyse te k@@ ul .\n",
      "\t[TRG] Mo@@ der@@ n te@@ ch@@ no@@ lo@@ gy en@@ ab@@ les studen@@ ts to che@@ at with new le@@ vel@@ s of e@@ ase and s@@ op@@ hi@@ sti@@ cation .\n",
      "2021-10-08 15:09:23,199 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) die (7) the (8) in (9) ’\n",
      "2021-10-08 15:09:23,199 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) die (7) the (8) in (9) ’\n",
      "2021-10-08 15:09:23,199 - INFO - joeynmt.helpers - Number of Src words (types): 4544\n",
      "2021-10-08 15:09:23,200 - INFO - joeynmt.helpers - Number of Trg words (types): 4544\n",
      "2021-10-08 15:09:23,200 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=8),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=8),\n",
      "\tsrc_embed=Embeddings(embedding_dim=512, vocab_size=4544),\n",
      "\ttrg_embed=Embeddings(embedding_dim=512, vocab_size=4544))\n",
      "2021-10-08 15:09:23,207 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-10-08 15:09:23,207 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-10-08 15:10:08,883 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     5.627722, Tokens per Sec:     4854, Lr: 0.000300\n",
      "2021-10-08 15:10:53,527 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.186247, Tokens per Sec:     5176, Lr: 0.000300\n",
      "2021-10-08 15:11:38,568 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     5.104353, Tokens per Sec:     5050, Lr: 0.000300\n",
      "2021-10-08 15:12:23,131 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     4.905414, Tokens per Sec:     5101, Lr: 0.000300\n",
      "2021-10-08 15:13:07,483 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     4.599307, Tokens per Sec:     5023, Lr: 0.000300\n",
      "2021-10-08 15:13:53,284 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     4.639038, Tokens per Sec:     5267, Lr: 0.000300\n",
      "2021-10-08 15:14:37,920 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     4.588988, Tokens per Sec:     5007, Lr: 0.000300\n",
      "2021-10-08 15:15:22,047 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     4.225783, Tokens per Sec:     5072, Lr: 0.000300\n",
      "2021-10-08 15:16:06,719 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     4.089215, Tokens per Sec:     5052, Lr: 0.000300\n",
      "2021-10-08 15:16:51,989 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     4.531202, Tokens per Sec:     5177, Lr: 0.000300\n",
      "2021-10-08 15:17:36,592 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     4.337191, Tokens per Sec:     5037, Lr: 0.000300\n",
      "2021-10-08 15:18:21,648 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     3.856300, Tokens per Sec:     5151, Lr: 0.000300\n",
      "2021-10-08 15:19:06,783 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     4.026577, Tokens per Sec:     5079, Lr: 0.000300\n",
      "2021-10-08 15:19:52,146 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.025913, Tokens per Sec:     5114, Lr: 0.000300\n",
      "2021-10-08 15:20:37,064 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     3.830742, Tokens per Sec:     5205, Lr: 0.000300\n",
      "2021-10-08 15:21:21,696 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     3.771031, Tokens per Sec:     5036, Lr: 0.000300\n",
      "2021-10-08 15:22:06,804 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     3.827639, Tokens per Sec:     5093, Lr: 0.000300\n",
      "2021-10-08 15:22:50,815 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     3.509480, Tokens per Sec:     5055, Lr: 0.000300\n",
      "2021-10-08 15:23:35,523 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     3.156528, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-08 15:24:20,254 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     3.802053, Tokens per Sec:     5074, Lr: 0.000300\n",
      "2021-10-08 15:25:05,216 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     3.137592, Tokens per Sec:     5099, Lr: 0.000300\n",
      "2021-10-08 15:25:49,991 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     3.652179, Tokens per Sec:     5032, Lr: 0.000300\n",
      "2021-10-08 15:26:34,323 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     3.220804, Tokens per Sec:     5124, Lr: 0.000300\n",
      "2021-10-08 15:27:18,667 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     3.437496, Tokens per Sec:     5123, Lr: 0.000300\n",
      "2021-10-08 15:28:02,831 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     3.160991, Tokens per Sec:     5029, Lr: 0.000300\n",
      "2021-10-08 15:28:46,877 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     3.280961, Tokens per Sec:     5092, Lr: 0.000300\n",
      "2021-10-08 15:29:31,679 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     2.806158, Tokens per Sec:     5097, Lr: 0.000300\n",
      "2021-10-08 15:30:16,764 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     3.235935, Tokens per Sec:     5116, Lr: 0.000300\n",
      "2021-10-08 15:31:01,922 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     3.016075, Tokens per Sec:     5180, Lr: 0.000300\n",
      "2021-10-08 15:31:46,263 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     3.178673, Tokens per Sec:     5046, Lr: 0.000300\n",
      "2021-10-08 15:32:31,381 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     3.030807, Tokens per Sec:     5084, Lr: 0.000300\n",
      "2021-10-08 15:33:16,108 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     3.017969, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-08 15:34:02,433 - INFO - joeynmt.training - Epoch   1, Step:     3300, Batch Loss:     2.729814, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-08 15:34:47,328 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     2.731026, Tokens per Sec:     4812, Lr: 0.000300\n",
      "2021-10-08 15:35:33,081 - INFO - joeynmt.training - Epoch   1, Step:     3500, Batch Loss:     2.741908, Tokens per Sec:     4890, Lr: 0.000300\n",
      "2021-10-08 15:36:19,484 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     2.982136, Tokens per Sec:     4922, Lr: 0.000300\n",
      "2021-10-08 15:37:04,088 - INFO - joeynmt.training - Epoch   1, Step:     3700, Batch Loss:     2.953096, Tokens per Sec:     4919, Lr: 0.000300\n",
      "2021-10-08 15:37:49,509 - INFO - joeynmt.training - Epoch   1, Step:     3800, Batch Loss:     2.753795, Tokens per Sec:     4941, Lr: 0.000300\n",
      "2021-10-08 15:38:34,640 - INFO - joeynmt.training - Epoch   1, Step:     3900, Batch Loss:     2.889534, Tokens per Sec:     5050, Lr: 0.000300\n",
      "2021-10-08 15:39:19,591 - INFO - joeynmt.training - Epoch   1, Step:     4000, Batch Loss:     2.632142, Tokens per Sec:     5025, Lr: 0.000300\n",
      "2021-10-08 15:40:05,602 - INFO - joeynmt.training - Epoch   1, Step:     4100, Batch Loss:     2.521113, Tokens per Sec:     5045, Lr: 0.000300\n",
      "2021-10-08 15:40:50,184 - INFO - joeynmt.training - Epoch   1, Step:     4200, Batch Loss:     2.925468, Tokens per Sec:     4902, Lr: 0.000300\n",
      "2021-10-08 15:41:34,559 - INFO - joeynmt.training - Epoch   1, Step:     4300, Batch Loss:     2.577061, Tokens per Sec:     4897, Lr: 0.000300\n",
      "2021-10-08 15:42:19,463 - INFO - joeynmt.training - Epoch   1, Step:     4400, Batch Loss:     2.492663, Tokens per Sec:     5069, Lr: 0.000300\n",
      "2021-10-08 15:43:04,479 - INFO - joeynmt.training - Epoch   1, Step:     4500, Batch Loss:     2.603375, Tokens per Sec:     4934, Lr: 0.000300\n",
      "2021-10-08 15:43:50,089 - INFO - joeynmt.training - Epoch   1, Step:     4600, Batch Loss:     2.980994, Tokens per Sec:     5038, Lr: 0.000300\n",
      "2021-10-08 15:44:34,832 - INFO - joeynmt.training - Epoch   1, Step:     4700, Batch Loss:     2.332850, Tokens per Sec:     4867, Lr: 0.000300\n",
      "2021-10-08 15:45:21,122 - INFO - joeynmt.training - Epoch   1, Step:     4800, Batch Loss:     2.656529, Tokens per Sec:     4951, Lr: 0.000300\n",
      "2021-10-08 15:46:06,159 - INFO - joeynmt.training - Epoch   1, Step:     4900, Batch Loss:     2.501938, Tokens per Sec:     4915, Lr: 0.000300\n",
      "2021-10-08 15:46:51,812 - INFO - joeynmt.training - Epoch   1, Step:     5000, Batch Loss:     2.753390, Tokens per Sec:     4973, Lr: 0.000300\n",
      "2021-10-08 15:48:15,741 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 15:48:15,741 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 15:48:15,741 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 15:48:16,069 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 15:48:16,070 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 15:48:22,050 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 15:48:22,051 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 15:48:22,051 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 15:48:22,051 - INFO - joeynmt.training - \tHypothesis: My father was the newspaper , the term used for the brother who had taken in a congregation .\n",
      "2021-10-08 15:48:22,051 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 15:48:22,052 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 15:48:22,052 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 15:48:22,052 - INFO - joeynmt.training - \tHypothesis: The computation is independent , for adolescency is a time when a person knows you and your feelings that speak to others and other depression .\n",
      "2021-10-08 15:48:22,052 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 15:48:22,052 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 15:48:22,053 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 15:48:22,053 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son ” with himself , perhaps to admit that Nabbal was even the words of his son .\n",
      "2021-10-08 15:48:22,053 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 15:48:22,053 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 15:48:22,054 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 15:48:22,054 - INFO - joeynmt.training - \tHypothesis: HOW COMME OF GOUTTO TO TO ?\n",
      "2021-10-08 15:48:22,054 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     5000: bleu:  22.71, loss: 68910.3984, ppl:   9.7574, duration: 90.2408s\n",
      "2021-10-08 15:49:07,889 - INFO - joeynmt.training - Epoch   1, Step:     5100, Batch Loss:     2.378584, Tokens per Sec:     4937, Lr: 0.000300\n",
      "2021-10-08 15:49:53,438 - INFO - joeynmt.training - Epoch   1, Step:     5200, Batch Loss:     2.288031, Tokens per Sec:     5066, Lr: 0.000300\n",
      "2021-10-08 15:50:38,236 - INFO - joeynmt.training - Epoch   1, Step:     5300, Batch Loss:     2.548720, Tokens per Sec:     4976, Lr: 0.000300\n",
      "2021-10-08 15:51:24,135 - INFO - joeynmt.training - Epoch   1, Step:     5400, Batch Loss:     2.468014, Tokens per Sec:     5049, Lr: 0.000300\n",
      "2021-10-08 15:52:09,008 - INFO - joeynmt.training - Epoch   1, Step:     5500, Batch Loss:     2.407727, Tokens per Sec:     4997, Lr: 0.000300\n",
      "2021-10-08 15:52:55,159 - INFO - joeynmt.training - Epoch   1, Step:     5600, Batch Loss:     2.100550, Tokens per Sec:     4897, Lr: 0.000300\n",
      "2021-10-08 15:53:42,197 - INFO - joeynmt.training - Epoch   1, Step:     5700, Batch Loss:     2.243864, Tokens per Sec:     5014, Lr: 0.000300\n",
      "2021-10-08 15:54:27,677 - INFO - joeynmt.training - Epoch   1, Step:     5800, Batch Loss:     2.249702, Tokens per Sec:     4886, Lr: 0.000300\n",
      "2021-10-08 15:55:13,101 - INFO - joeynmt.training - Epoch   1, Step:     5900, Batch Loss:     2.182600, Tokens per Sec:     4888, Lr: 0.000300\n",
      "2021-10-08 15:55:59,062 - INFO - joeynmt.training - Epoch   1, Step:     6000, Batch Loss:     2.333374, Tokens per Sec:     4715, Lr: 0.000300\n",
      "2021-10-08 15:56:45,286 - INFO - joeynmt.training - Epoch   1, Step:     6100, Batch Loss:     2.110469, Tokens per Sec:     4955, Lr: 0.000300\n",
      "2021-10-08 15:57:31,149 - INFO - joeynmt.training - Epoch   1, Step:     6200, Batch Loss:     2.408173, Tokens per Sec:     4879, Lr: 0.000300\n",
      "2021-10-08 15:58:17,661 - INFO - joeynmt.training - Epoch   1, Step:     6300, Batch Loss:     2.176417, Tokens per Sec:     5012, Lr: 0.000300\n",
      "2021-10-08 15:59:03,864 - INFO - joeynmt.training - Epoch   1, Step:     6400, Batch Loss:     2.287139, Tokens per Sec:     4944, Lr: 0.000300\n",
      "2021-10-08 15:59:49,646 - INFO - joeynmt.training - Epoch   1, Step:     6500, Batch Loss:     2.167066, Tokens per Sec:     4803, Lr: 0.000300\n",
      "2021-10-08 16:00:36,170 - INFO - joeynmt.training - Epoch   1, Step:     6600, Batch Loss:     2.091972, Tokens per Sec:     4967, Lr: 0.000300\n",
      "2021-10-08 16:01:22,047 - INFO - joeynmt.training - Epoch   1, Step:     6700, Batch Loss:     1.882239, Tokens per Sec:     4879, Lr: 0.000300\n",
      "2021-10-08 16:02:08,983 - INFO - joeynmt.training - Epoch   1, Step:     6800, Batch Loss:     2.421760, Tokens per Sec:     5072, Lr: 0.000300\n",
      "2021-10-08 16:02:55,263 - INFO - joeynmt.training - Epoch   1, Step:     6900, Batch Loss:     2.131123, Tokens per Sec:     4878, Lr: 0.000300\n",
      "2021-10-08 16:03:40,859 - INFO - joeynmt.training - Epoch   1, Step:     7000, Batch Loss:     2.242683, Tokens per Sec:     4926, Lr: 0.000300\n",
      "2021-10-08 16:04:27,644 - INFO - joeynmt.training - Epoch   1, Step:     7100, Batch Loss:     2.527423, Tokens per Sec:     4911, Lr: 0.000300\n",
      "2021-10-08 16:05:12,941 - INFO - joeynmt.training - Epoch   1, Step:     7200, Batch Loss:     1.952227, Tokens per Sec:     5059, Lr: 0.000300\n",
      "2021-10-08 16:05:57,324 - INFO - joeynmt.training - Epoch   1, Step:     7300, Batch Loss:     2.049403, Tokens per Sec:     4808, Lr: 0.000300\n",
      "2021-10-08 16:06:43,204 - INFO - joeynmt.training - Epoch   1, Step:     7400, Batch Loss:     2.314870, Tokens per Sec:     5075, Lr: 0.000300\n",
      "2021-10-08 16:07:28,938 - INFO - joeynmt.training - Epoch   1, Step:     7500, Batch Loss:     2.060629, Tokens per Sec:     4866, Lr: 0.000300\n",
      "2021-10-08 16:08:14,560 - INFO - joeynmt.training - Epoch   1, Step:     7600, Batch Loss:     1.910100, Tokens per Sec:     4745, Lr: 0.000300\n",
      "2021-10-08 16:09:00,304 - INFO - joeynmt.training - Epoch   1, Step:     7700, Batch Loss:     1.938871, Tokens per Sec:     4865, Lr: 0.000300\n",
      "2021-10-08 16:09:46,551 - INFO - joeynmt.training - Epoch   1, Step:     7800, Batch Loss:     2.298982, Tokens per Sec:     4872, Lr: 0.000300\n",
      "2021-10-08 16:10:32,956 - INFO - joeynmt.training - Epoch   1, Step:     7900, Batch Loss:     1.991081, Tokens per Sec:     4866, Lr: 0.000300\n",
      "2021-10-08 16:11:19,512 - INFO - joeynmt.training - Epoch   1, Step:     8000, Batch Loss:     2.331794, Tokens per Sec:     4962, Lr: 0.000300\n",
      "2021-10-08 16:12:05,441 - INFO - joeynmt.training - Epoch   1, Step:     8100, Batch Loss:     2.251518, Tokens per Sec:     4900, Lr: 0.000300\n",
      "2021-10-08 16:12:52,747 - INFO - joeynmt.training - Epoch   1, Step:     8200, Batch Loss:     1.859388, Tokens per Sec:     4888, Lr: 0.000300\n",
      "2021-10-08 16:13:39,059 - INFO - joeynmt.training - Epoch   1, Step:     8300, Batch Loss:     1.980004, Tokens per Sec:     4925, Lr: 0.000300\n",
      "2021-10-08 16:14:25,280 - INFO - joeynmt.training - Epoch   1, Step:     8400, Batch Loss:     2.067598, Tokens per Sec:     4954, Lr: 0.000300\n",
      "2021-10-08 16:15:10,552 - INFO - joeynmt.training - Epoch   1, Step:     8500, Batch Loss:     2.146407, Tokens per Sec:     4899, Lr: 0.000300\n",
      "2021-10-08 16:15:55,928 - INFO - joeynmt.training - Epoch   1, Step:     8600, Batch Loss:     2.056742, Tokens per Sec:     4986, Lr: 0.000300\n",
      "2021-10-08 16:16:41,912 - INFO - joeynmt.training - Epoch   1, Step:     8700, Batch Loss:     2.051942, Tokens per Sec:     4900, Lr: 0.000300\n",
      "2021-10-08 16:17:27,908 - INFO - joeynmt.training - Epoch   1, Step:     8800, Batch Loss:     1.953642, Tokens per Sec:     4877, Lr: 0.000300\n",
      "2021-10-08 16:18:13,322 - INFO - joeynmt.training - Epoch   1, Step:     8900, Batch Loss:     2.112827, Tokens per Sec:     4985, Lr: 0.000300\n",
      "2021-10-08 16:18:57,905 - INFO - joeynmt.training - Epoch   1, Step:     9000, Batch Loss:     1.836805, Tokens per Sec:     4973, Lr: 0.000300\n",
      "2021-10-08 16:19:43,481 - INFO - joeynmt.training - Epoch   1, Step:     9100, Batch Loss:     2.071235, Tokens per Sec:     4869, Lr: 0.000300\n",
      "2021-10-08 16:20:29,960 - INFO - joeynmt.training - Epoch   1, Step:     9200, Batch Loss:     2.606380, Tokens per Sec:     4914, Lr: 0.000300\n",
      "2021-10-08 16:21:15,735 - INFO - joeynmt.training - Epoch   1, Step:     9300, Batch Loss:     1.977485, Tokens per Sec:     4827, Lr: 0.000300\n",
      "2021-10-08 16:22:01,491 - INFO - joeynmt.training - Epoch   1, Step:     9400, Batch Loss:     1.840990, Tokens per Sec:     4888, Lr: 0.000300\n",
      "2021-10-08 16:22:47,889 - INFO - joeynmt.training - Epoch   1, Step:     9500, Batch Loss:     1.884142, Tokens per Sec:     4864, Lr: 0.000300\n",
      "2021-10-08 16:23:34,345 - INFO - joeynmt.training - Epoch   1, Step:     9600, Batch Loss:     2.059023, Tokens per Sec:     4915, Lr: 0.000300\n",
      "2021-10-08 16:24:20,884 - INFO - joeynmt.training - Epoch   1, Step:     9700, Batch Loss:     1.846369, Tokens per Sec:     4927, Lr: 0.000300\n",
      "2021-10-08 16:25:06,434 - INFO - joeynmt.training - Epoch   1, Step:     9800, Batch Loss:     2.219491, Tokens per Sec:     4840, Lr: 0.000300\n",
      "2021-10-08 16:25:51,184 - INFO - joeynmt.training - Epoch   1, Step:     9900, Batch Loss:     1.901308, Tokens per Sec:     4842, Lr: 0.000300\n",
      "2021-10-08 16:26:36,584 - INFO - joeynmt.training - Epoch   1, Step:    10000, Batch Loss:     1.924379, Tokens per Sec:     4998, Lr: 0.000300\n",
      "2021-10-08 16:27:49,382 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 16:27:49,382 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 16:27:49,382 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 16:27:49,710 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 16:27:49,710 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 16:27:55,690 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 16:27:55,691 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 16:27:55,691 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 16:27:55,691 - INFO - joeynmt.training - \tHypothesis: My father was the growth servant , the term used for the brother who took the lead in a congregation .\n",
      "2021-10-08 16:27:55,692 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 16:27:55,692 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 16:27:55,692 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 16:27:55,692 - INFO - joeynmt.training - \tHypothesis: The training is recommended , for adolescency is a time when a man teaches yourself and your feelings in a way that express others and disappears .\n",
      "2021-10-08 16:27:55,692 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 16:27:55,693 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 16:27:55,693 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 16:27:55,693 - INFO - joeynmt.training - \tHypothesis: He even used the words “ Your son ” to himself , perhaps to admit that Nabal parent was as older .\n",
      "2021-10-08 16:27:55,693 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 16:27:55,694 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 16:27:55,694 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 16:27:55,694 - INFO - joeynmt.training - \tHypothesis: HOW COMOMOME OF GITY DOLD WITY ?\n",
      "2021-10-08 16:27:55,694 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step    10000: bleu:  33.09, loss: 52362.6367, ppl:   5.6463, duration: 79.1096s\n",
      "2021-10-08 16:28:41,443 - INFO - joeynmt.training - Epoch   1, Step:    10100, Batch Loss:     1.817947, Tokens per Sec:     5003, Lr: 0.000300\n",
      "2021-10-08 16:29:27,086 - INFO - joeynmt.training - Epoch   1, Step:    10200, Batch Loss:     1.930728, Tokens per Sec:     5029, Lr: 0.000300\n",
      "2021-10-08 16:30:12,110 - INFO - joeynmt.training - Epoch   1, Step:    10300, Batch Loss:     1.892913, Tokens per Sec:     4899, Lr: 0.000300\n",
      "2021-10-08 16:30:58,737 - INFO - joeynmt.training - Epoch   1, Step:    10400, Batch Loss:     1.825811, Tokens per Sec:     4918, Lr: 0.000300\n",
      "2021-10-08 16:31:43,776 - INFO - joeynmt.training - Epoch   1, Step:    10500, Batch Loss:     1.812703, Tokens per Sec:     5062, Lr: 0.000300\n",
      "2021-10-08 16:32:29,600 - INFO - joeynmt.training - Epoch   1, Step:    10600, Batch Loss:     1.826792, Tokens per Sec:     4956, Lr: 0.000300\n",
      "2021-10-08 16:33:14,595 - INFO - joeynmt.training - Epoch   1, Step:    10700, Batch Loss:     1.776096, Tokens per Sec:     5051, Lr: 0.000300\n",
      "2021-10-08 16:34:00,152 - INFO - joeynmt.training - Epoch   1, Step:    10800, Batch Loss:     1.943326, Tokens per Sec:     4950, Lr: 0.000300\n",
      "2021-10-08 16:34:45,076 - INFO - joeynmt.training - Epoch   1, Step:    10900, Batch Loss:     1.908359, Tokens per Sec:     5002, Lr: 0.000300\n",
      "2021-10-08 16:35:31,022 - INFO - joeynmt.training - Epoch   1, Step:    11000, Batch Loss:     1.707419, Tokens per Sec:     4817, Lr: 0.000300\n",
      "2021-10-08 16:36:17,660 - INFO - joeynmt.training - Epoch   1, Step:    11100, Batch Loss:     1.864314, Tokens per Sec:     4967, Lr: 0.000300\n",
      "2021-10-08 16:37:03,020 - INFO - joeynmt.training - Epoch   1, Step:    11200, Batch Loss:     2.039667, Tokens per Sec:     4902, Lr: 0.000300\n",
      "2021-10-08 16:37:47,960 - INFO - joeynmt.training - Epoch   1, Step:    11300, Batch Loss:     1.761360, Tokens per Sec:     4895, Lr: 0.000300\n",
      "2021-10-08 16:38:33,609 - INFO - joeynmt.training - Epoch   1, Step:    11400, Batch Loss:     1.811291, Tokens per Sec:     4955, Lr: 0.000300\n",
      "2021-10-08 16:39:19,161 - INFO - joeynmt.training - Epoch   1, Step:    11500, Batch Loss:     1.965382, Tokens per Sec:     5025, Lr: 0.000300\n",
      "2021-10-08 16:40:03,508 - INFO - joeynmt.training - Epoch   1, Step:    11600, Batch Loss:     1.703912, Tokens per Sec:     4930, Lr: 0.000300\n",
      "2021-10-08 16:40:48,452 - INFO - joeynmt.training - Epoch   1, Step:    11700, Batch Loss:     1.998969, Tokens per Sec:     5011, Lr: 0.000300\n",
      "2021-10-08 16:41:34,593 - INFO - joeynmt.training - Epoch   1, Step:    11800, Batch Loss:     1.833171, Tokens per Sec:     4887, Lr: 0.000300\n",
      "2021-10-08 16:42:19,664 - INFO - joeynmt.training - Epoch   1, Step:    11900, Batch Loss:     1.588279, Tokens per Sec:     5048, Lr: 0.000300\n",
      "2021-10-08 16:43:05,880 - INFO - joeynmt.training - Epoch   1, Step:    12000, Batch Loss:     2.036506, Tokens per Sec:     4929, Lr: 0.000300\n",
      "2021-10-08 16:43:51,549 - INFO - joeynmt.training - Epoch   1, Step:    12100, Batch Loss:     1.882959, Tokens per Sec:     4905, Lr: 0.000300\n",
      "2021-10-08 16:44:38,213 - INFO - joeynmt.training - Epoch   1, Step:    12200, Batch Loss:     1.772855, Tokens per Sec:     4838, Lr: 0.000300\n",
      "2021-10-08 16:45:23,988 - INFO - joeynmt.training - Epoch   1, Step:    12300, Batch Loss:     1.666396, Tokens per Sec:     4828, Lr: 0.000300\n",
      "2021-10-08 16:46:09,433 - INFO - joeynmt.training - Epoch   1, Step:    12400, Batch Loss:     1.603302, Tokens per Sec:     4910, Lr: 0.000300\n",
      "2021-10-08 16:46:55,698 - INFO - joeynmt.training - Epoch   1, Step:    12500, Batch Loss:     2.002309, Tokens per Sec:     4969, Lr: 0.000300\n",
      "2021-10-08 16:47:09,255 - INFO - joeynmt.training - Epoch   1: total training loss 33089.30\n",
      "2021-10-08 16:47:09,255 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-10-08 16:47:43,158 - INFO - joeynmt.training - Epoch   2, Step:    12600, Batch Loss:     1.754744, Tokens per Sec:     4796, Lr: 0.000300\n",
      "2021-10-08 16:48:28,620 - INFO - joeynmt.training - Epoch   2, Step:    12700, Batch Loss:     1.839152, Tokens per Sec:     4947, Lr: 0.000300\n",
      "2021-10-08 16:49:13,727 - INFO - joeynmt.training - Epoch   2, Step:    12800, Batch Loss:     1.979335, Tokens per Sec:     4915, Lr: 0.000300\n",
      "2021-10-08 16:49:59,107 - INFO - joeynmt.training - Epoch   2, Step:    12900, Batch Loss:     1.970362, Tokens per Sec:     4961, Lr: 0.000300\n",
      "2021-10-08 16:50:44,769 - INFO - joeynmt.training - Epoch   2, Step:    13000, Batch Loss:     1.842429, Tokens per Sec:     4926, Lr: 0.000300\n",
      "2021-10-08 16:51:29,863 - INFO - joeynmt.training - Epoch   2, Step:    13100, Batch Loss:     1.771226, Tokens per Sec:     4846, Lr: 0.000300\n",
      "2021-10-08 16:52:16,154 - INFO - joeynmt.training - Epoch   2, Step:    13200, Batch Loss:     1.612960, Tokens per Sec:     4941, Lr: 0.000300\n",
      "2021-10-08 16:53:02,036 - INFO - joeynmt.training - Epoch   2, Step:    13300, Batch Loss:     1.730959, Tokens per Sec:     5064, Lr: 0.000300\n",
      "2021-10-08 16:53:47,444 - INFO - joeynmt.training - Epoch   2, Step:    13400, Batch Loss:     1.673115, Tokens per Sec:     4894, Lr: 0.000300\n",
      "2021-10-08 16:54:33,288 - INFO - joeynmt.training - Epoch   2, Step:    13500, Batch Loss:     1.760071, Tokens per Sec:     4973, Lr: 0.000300\n",
      "2021-10-08 16:55:19,015 - INFO - joeynmt.training - Epoch   2, Step:    13600, Batch Loss:     1.744612, Tokens per Sec:     4970, Lr: 0.000300\n",
      "2021-10-08 16:56:04,680 - INFO - joeynmt.training - Epoch   2, Step:    13700, Batch Loss:     1.671747, Tokens per Sec:     4920, Lr: 0.000300\n",
      "2021-10-08 16:56:50,390 - INFO - joeynmt.training - Epoch   2, Step:    13800, Batch Loss:     1.735681, Tokens per Sec:     4988, Lr: 0.000300\n",
      "2021-10-08 16:57:36,551 - INFO - joeynmt.training - Epoch   2, Step:    13900, Batch Loss:     1.711282, Tokens per Sec:     4919, Lr: 0.000300\n",
      "2021-10-08 16:58:22,381 - INFO - joeynmt.training - Epoch   2, Step:    14000, Batch Loss:     2.058901, Tokens per Sec:     5001, Lr: 0.000300\n",
      "2021-10-08 16:59:08,428 - INFO - joeynmt.training - Epoch   2, Step:    14100, Batch Loss:     1.807811, Tokens per Sec:     4883, Lr: 0.000300\n",
      "2021-10-08 16:59:54,440 - INFO - joeynmt.training - Epoch   2, Step:    14200, Batch Loss:     1.866136, Tokens per Sec:     5035, Lr: 0.000300\n",
      "2021-10-08 17:00:40,697 - INFO - joeynmt.training - Epoch   2, Step:    14300, Batch Loss:     1.601465, Tokens per Sec:     5031, Lr: 0.000300\n",
      "2021-10-08 17:01:25,566 - INFO - joeynmt.training - Epoch   2, Step:    14400, Batch Loss:     1.648311, Tokens per Sec:     4902, Lr: 0.000300\n",
      "2021-10-08 17:02:12,023 - INFO - joeynmt.training - Epoch   2, Step:    14500, Batch Loss:     1.767056, Tokens per Sec:     4914, Lr: 0.000300\n",
      "2021-10-08 17:02:56,908 - INFO - joeynmt.training - Epoch   2, Step:    14600, Batch Loss:     1.718446, Tokens per Sec:     4812, Lr: 0.000300\n",
      "2021-10-08 17:03:42,090 - INFO - joeynmt.training - Epoch   2, Step:    14700, Batch Loss:     1.581563, Tokens per Sec:     4916, Lr: 0.000300\n",
      "2021-10-08 17:04:27,412 - INFO - joeynmt.training - Epoch   2, Step:    14800, Batch Loss:     1.644889, Tokens per Sec:     4986, Lr: 0.000300\n",
      "2021-10-08 17:05:13,585 - INFO - joeynmt.training - Epoch   2, Step:    14900, Batch Loss:     2.152880, Tokens per Sec:     4913, Lr: 0.000300\n",
      "2021-10-08 17:05:58,301 - INFO - joeynmt.training - Epoch   2, Step:    15000, Batch Loss:     1.828032, Tokens per Sec:     4857, Lr: 0.000300\n",
      "2021-10-08 17:07:17,622 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 17:07:17,622 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 17:07:17,623 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 17:07:17,953 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 17:07:17,953 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 17:07:23,963 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 17:07:23,963 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 17:07:23,964 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 17:07:23,964 - INFO - joeynmt.training - \tHypothesis: My father was the growing servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-08 17:07:23,964 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 17:07:23,964 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 17:07:23,964 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 17:07:23,965 - INFO - joeynmt.training - \tHypothesis: The training is restricted , for adolescence is a time when a person teaches yourself and express your feelings in a way that speaks to others and rescues them .\n",
      "2021-10-08 17:07:23,965 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 17:07:23,965 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 17:07:23,965 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 17:07:23,965 - INFO - joeynmt.training - \tHypothesis: He even used the words “ Your son David ” to himself , perhaps to recognize that Nabal parent was he .\n",
      "2021-10-08 17:07:23,965 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 17:07:23,966 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 17:07:23,966 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 17:07:23,966 - INFO - joeynmt.training - \tHypothesis: HOW COME MOTHEW WITH WITH ?\n",
      "2021-10-08 17:07:23,966 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    15000: bleu:  36.30, loss: 47018.3984, ppl:   4.7319, duration: 85.6643s\n",
      "2021-10-08 17:08:09,903 - INFO - joeynmt.training - Epoch   2, Step:    15100, Batch Loss:     1.629141, Tokens per Sec:     4936, Lr: 0.000300\n",
      "2021-10-08 17:08:56,071 - INFO - joeynmt.training - Epoch   2, Step:    15200, Batch Loss:     1.910130, Tokens per Sec:     4938, Lr: 0.000300\n",
      "2021-10-08 17:09:41,747 - INFO - joeynmt.training - Epoch   2, Step:    15300, Batch Loss:     1.578918, Tokens per Sec:     4998, Lr: 0.000300\n",
      "2021-10-08 17:10:28,455 - INFO - joeynmt.training - Epoch   2, Step:    15400, Batch Loss:     1.734805, Tokens per Sec:     4953, Lr: 0.000300\n",
      "2021-10-08 17:11:14,257 - INFO - joeynmt.training - Epoch   2, Step:    15500, Batch Loss:     1.673989, Tokens per Sec:     4912, Lr: 0.000300\n",
      "2021-10-08 17:12:00,380 - INFO - joeynmt.training - Epoch   2, Step:    15600, Batch Loss:     1.540273, Tokens per Sec:     4972, Lr: 0.000300\n",
      "2021-10-08 17:12:46,102 - INFO - joeynmt.training - Epoch   2, Step:    15700, Batch Loss:     1.756224, Tokens per Sec:     5027, Lr: 0.000300\n",
      "2021-10-08 17:13:31,088 - INFO - joeynmt.training - Epoch   2, Step:    15800, Batch Loss:     1.854647, Tokens per Sec:     4858, Lr: 0.000300\n",
      "2021-10-08 17:14:16,860 - INFO - joeynmt.training - Epoch   2, Step:    15900, Batch Loss:     1.664961, Tokens per Sec:     4985, Lr: 0.000300\n",
      "2021-10-08 17:15:02,083 - INFO - joeynmt.training - Epoch   2, Step:    16000, Batch Loss:     1.568494, Tokens per Sec:     4986, Lr: 0.000300\n",
      "2021-10-08 17:15:47,276 - INFO - joeynmt.training - Epoch   2, Step:    16100, Batch Loss:     1.639108, Tokens per Sec:     4810, Lr: 0.000300\n",
      "2021-10-08 17:16:34,941 - INFO - joeynmt.training - Epoch   2, Step:    16200, Batch Loss:     1.831371, Tokens per Sec:     5059, Lr: 0.000300\n",
      "2021-10-08 17:17:20,858 - INFO - joeynmt.training - Epoch   2, Step:    16300, Batch Loss:     1.805769, Tokens per Sec:     4906, Lr: 0.000300\n",
      "2021-10-08 17:18:06,895 - INFO - joeynmt.training - Epoch   2, Step:    16400, Batch Loss:     1.627196, Tokens per Sec:     4892, Lr: 0.000300\n",
      "2021-10-08 17:18:53,588 - INFO - joeynmt.training - Epoch   2, Step:    16500, Batch Loss:     1.813591, Tokens per Sec:     5051, Lr: 0.000300\n",
      "2021-10-08 17:19:39,114 - INFO - joeynmt.training - Epoch   2, Step:    16600, Batch Loss:     1.624227, Tokens per Sec:     4899, Lr: 0.000300\n",
      "2021-10-08 17:20:24,750 - INFO - joeynmt.training - Epoch   2, Step:    16700, Batch Loss:     1.610903, Tokens per Sec:     4914, Lr: 0.000300\n",
      "2021-10-08 17:21:10,046 - INFO - joeynmt.training - Epoch   2, Step:    16800, Batch Loss:     1.549825, Tokens per Sec:     4936, Lr: 0.000300\n",
      "2021-10-08 17:21:55,812 - INFO - joeynmt.training - Epoch   2, Step:    16900, Batch Loss:     1.531623, Tokens per Sec:     4942, Lr: 0.000300\n",
      "2021-10-08 17:22:41,789 - INFO - joeynmt.training - Epoch   2, Step:    17000, Batch Loss:     1.796597, Tokens per Sec:     4927, Lr: 0.000300\n",
      "2021-10-08 17:23:28,415 - INFO - joeynmt.training - Epoch   2, Step:    17100, Batch Loss:     1.705910, Tokens per Sec:     4936, Lr: 0.000300\n",
      "2021-10-08 17:24:14,336 - INFO - joeynmt.training - Epoch   2, Step:    17200, Batch Loss:     1.476400, Tokens per Sec:     4985, Lr: 0.000300\n",
      "2021-10-08 17:24:59,920 - INFO - joeynmt.training - Epoch   2, Step:    17300, Batch Loss:     1.648577, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-08 17:25:45,182 - INFO - joeynmt.training - Epoch   2, Step:    17400, Batch Loss:     1.495429, Tokens per Sec:     4922, Lr: 0.000300\n",
      "2021-10-08 17:26:31,587 - INFO - joeynmt.training - Epoch   2, Step:    17500, Batch Loss:     1.590339, Tokens per Sec:     4972, Lr: 0.000300\n",
      "2021-10-08 17:27:16,678 - INFO - joeynmt.training - Epoch   2, Step:    17600, Batch Loss:     1.698677, Tokens per Sec:     4973, Lr: 0.000300\n",
      "2021-10-08 17:28:02,481 - INFO - joeynmt.training - Epoch   2, Step:    17700, Batch Loss:     1.588415, Tokens per Sec:     4924, Lr: 0.000300\n",
      "2021-10-08 17:28:48,153 - INFO - joeynmt.training - Epoch   2, Step:    17800, Batch Loss:     1.734063, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-08 17:29:32,929 - INFO - joeynmt.training - Epoch   2, Step:    17900, Batch Loss:     1.664967, Tokens per Sec:     4896, Lr: 0.000300\n",
      "2021-10-08 17:30:17,701 - INFO - joeynmt.training - Epoch   2, Step:    18000, Batch Loss:     1.768015, Tokens per Sec:     4959, Lr: 0.000300\n",
      "2021-10-08 17:31:02,999 - INFO - joeynmt.training - Epoch   2, Step:    18100, Batch Loss:     1.746449, Tokens per Sec:     4920, Lr: 0.000300\n",
      "2021-10-08 17:31:49,900 - INFO - joeynmt.training - Epoch   2, Step:    18200, Batch Loss:     1.631435, Tokens per Sec:     4966, Lr: 0.000300\n",
      "2021-10-08 17:32:35,748 - INFO - joeynmt.training - Epoch   2, Step:    18300, Batch Loss:     1.698442, Tokens per Sec:     4903, Lr: 0.000300\n",
      "2021-10-08 17:33:21,162 - INFO - joeynmt.training - Epoch   2, Step:    18400, Batch Loss:     1.413664, Tokens per Sec:     4936, Lr: 0.000300\n",
      "2021-10-08 17:34:06,912 - INFO - joeynmt.training - Epoch   2, Step:    18500, Batch Loss:     1.754932, Tokens per Sec:     4945, Lr: 0.000300\n",
      "2021-10-08 17:34:52,327 - INFO - joeynmt.training - Epoch   2, Step:    18600, Batch Loss:     1.597001, Tokens per Sec:     4847, Lr: 0.000300\n",
      "2021-10-08 17:35:37,769 - INFO - joeynmt.training - Epoch   2, Step:    18700, Batch Loss:     1.775567, Tokens per Sec:     5110, Lr: 0.000300\n",
      "2021-10-08 17:36:23,775 - INFO - joeynmt.training - Epoch   2, Step:    18800, Batch Loss:     1.715400, Tokens per Sec:     5047, Lr: 0.000300\n",
      "2021-10-08 17:37:08,620 - INFO - joeynmt.training - Epoch   2, Step:    18900, Batch Loss:     1.688239, Tokens per Sec:     4931, Lr: 0.000300\n",
      "2021-10-08 17:37:54,669 - INFO - joeynmt.training - Epoch   2, Step:    19000, Batch Loss:     1.676178, Tokens per Sec:     4919, Lr: 0.000300\n",
      "2021-10-08 17:38:40,353 - INFO - joeynmt.training - Epoch   2, Step:    19100, Batch Loss:     1.585383, Tokens per Sec:     4884, Lr: 0.000300\n",
      "2021-10-08 17:39:26,085 - INFO - joeynmt.training - Epoch   2, Step:    19200, Batch Loss:     1.614223, Tokens per Sec:     4859, Lr: 0.000300\n",
      "2021-10-08 17:40:12,072 - INFO - joeynmt.training - Epoch   2, Step:    19300, Batch Loss:     1.767508, Tokens per Sec:     4912, Lr: 0.000300\n",
      "2021-10-08 17:40:57,721 - INFO - joeynmt.training - Epoch   2, Step:    19400, Batch Loss:     1.678939, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2021-10-08 17:41:42,807 - INFO - joeynmt.training - Epoch   2, Step:    19500, Batch Loss:     1.590191, Tokens per Sec:     4883, Lr: 0.000300\n",
      "2021-10-08 17:42:28,292 - INFO - joeynmt.training - Epoch   2, Step:    19600, Batch Loss:     1.487506, Tokens per Sec:     4938, Lr: 0.000300\n",
      "2021-10-08 17:43:14,348 - INFO - joeynmt.training - Epoch   2, Step:    19700, Batch Loss:     1.594469, Tokens per Sec:     5021, Lr: 0.000300\n",
      "2021-10-08 17:43:59,839 - INFO - joeynmt.training - Epoch   2, Step:    19800, Batch Loss:     1.650254, Tokens per Sec:     4921, Lr: 0.000300\n",
      "2021-10-08 17:44:45,084 - INFO - joeynmt.training - Epoch   2, Step:    19900, Batch Loss:     1.647317, Tokens per Sec:     4877, Lr: 0.000300\n",
      "2021-10-08 17:45:30,704 - INFO - joeynmt.training - Epoch   2, Step:    20000, Batch Loss:     1.689973, Tokens per Sec:     4889, Lr: 0.000300\n",
      "2021-10-08 17:46:51,553 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 17:46:51,554 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 17:46:51,554 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 17:46:51,885 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 17:46:51,885 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 17:46:58,681 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 17:46:58,682 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 17:46:58,682 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 17:46:58,682 - INFO - joeynmt.training - \tHypothesis: My father was the gross servant , the term used then for the brother who took the lead in a congregation .\n",
      "2021-10-08 17:46:58,682 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 17:46:58,683 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 17:46:58,683 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 17:46:58,683 - INFO - joeynmt.training - \tHypothesis: The attractive treatment is destined , for adolescence is a time when one learns yourself and your feelings expressed in a way that speaks to others and remove them .\n",
      "2021-10-08 17:46:58,683 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 17:46:58,683 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 17:46:58,683 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 17:46:58,683 - INFO - joeynmt.training - \tHypothesis: He even used the words “ Your son David ” with reference to himself , perhaps to recognize that Nabal parent was he .\n",
      "2021-10-08 17:46:58,683 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 17:46:58,684 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 17:46:58,684 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 17:46:58,684 - INFO - joeynmt.training - \tHypothesis: HOW MOLD MOLD WILE GECTLES DOLD STEL ?\n",
      "2021-10-08 17:46:58,684 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    20000: bleu:  39.27, loss: 42779.0352, ppl:   4.1131, duration: 87.9793s\n",
      "2021-10-08 17:47:45,351 - INFO - joeynmt.training - Epoch   2, Step:    20100, Batch Loss:     1.742357, Tokens per Sec:     4905, Lr: 0.000300\n",
      "2021-10-08 17:48:31,108 - INFO - joeynmt.training - Epoch   2, Step:    20200, Batch Loss:     1.655272, Tokens per Sec:     4955, Lr: 0.000300\n",
      "2021-10-08 17:49:17,031 - INFO - joeynmt.training - Epoch   2, Step:    20300, Batch Loss:     1.685872, Tokens per Sec:     4946, Lr: 0.000300\n",
      "2021-10-08 17:50:03,491 - INFO - joeynmt.training - Epoch   2, Step:    20400, Batch Loss:     1.532435, Tokens per Sec:     4921, Lr: 0.000300\n",
      "2021-10-08 17:50:48,215 - INFO - joeynmt.training - Epoch   2, Step:    20500, Batch Loss:     1.355665, Tokens per Sec:     4927, Lr: 0.000300\n",
      "2021-10-08 17:51:33,789 - INFO - joeynmt.training - Epoch   2, Step:    20600, Batch Loss:     1.511675, Tokens per Sec:     5156, Lr: 0.000300\n",
      "2021-10-08 17:52:19,593 - INFO - joeynmt.training - Epoch   2, Step:    20700, Batch Loss:     1.414673, Tokens per Sec:     4963, Lr: 0.000300\n",
      "2021-10-08 17:53:05,760 - INFO - joeynmt.training - Epoch   2, Step:    20800, Batch Loss:     1.567518, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-08 17:53:50,303 - INFO - joeynmt.training - Epoch   2, Step:    20900, Batch Loss:     1.575450, Tokens per Sec:     4829, Lr: 0.000300\n",
      "2021-10-08 17:54:36,584 - INFO - joeynmt.training - Epoch   2, Step:    21000, Batch Loss:     1.616017, Tokens per Sec:     4971, Lr: 0.000300\n",
      "2021-10-08 17:55:22,568 - INFO - joeynmt.training - Epoch   2, Step:    21100, Batch Loss:     1.533929, Tokens per Sec:     4900, Lr: 0.000300\n",
      "2021-10-08 17:56:08,856 - INFO - joeynmt.training - Epoch   2, Step:    21200, Batch Loss:     1.893821, Tokens per Sec:     4930, Lr: 0.000300\n",
      "2021-10-08 17:56:54,969 - INFO - joeynmt.training - Epoch   2, Step:    21300, Batch Loss:     1.668218, Tokens per Sec:     4923, Lr: 0.000300\n",
      "2021-10-08 17:57:40,088 - INFO - joeynmt.training - Epoch   2, Step:    21400, Batch Loss:     1.569091, Tokens per Sec:     4971, Lr: 0.000300\n",
      "2021-10-08 17:58:25,085 - INFO - joeynmt.training - Epoch   2, Step:    21500, Batch Loss:     1.788267, Tokens per Sec:     4887, Lr: 0.000300\n",
      "2021-10-08 17:59:10,995 - INFO - joeynmt.training - Epoch   2, Step:    21600, Batch Loss:     1.786189, Tokens per Sec:     4946, Lr: 0.000300\n",
      "2021-10-08 17:59:57,224 - INFO - joeynmt.training - Epoch   2, Step:    21700, Batch Loss:     1.693674, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-08 18:00:43,516 - INFO - joeynmt.training - Epoch   2, Step:    21800, Batch Loss:     1.588920, Tokens per Sec:     4902, Lr: 0.000300\n",
      "2021-10-08 18:01:30,200 - INFO - joeynmt.training - Epoch   2, Step:    21900, Batch Loss:     1.544889, Tokens per Sec:     5034, Lr: 0.000300\n",
      "2021-10-08 18:02:16,817 - INFO - joeynmt.training - Epoch   2, Step:    22000, Batch Loss:     1.372396, Tokens per Sec:     4925, Lr: 0.000300\n",
      "2021-10-08 18:03:04,118 - INFO - joeynmt.training - Epoch   2, Step:    22100, Batch Loss:     1.589274, Tokens per Sec:     5090, Lr: 0.000300\n",
      "2021-10-08 18:03:48,420 - INFO - joeynmt.training - Epoch   2, Step:    22200, Batch Loss:     1.702499, Tokens per Sec:     4778, Lr: 0.000300\n",
      "2021-10-08 18:04:33,332 - INFO - joeynmt.training - Epoch   2, Step:    22300, Batch Loss:     1.612203, Tokens per Sec:     4889, Lr: 0.000300\n",
      "2021-10-08 18:05:18,782 - INFO - joeynmt.training - Epoch   2, Step:    22400, Batch Loss:     1.450242, Tokens per Sec:     4845, Lr: 0.000300\n",
      "2021-10-08 18:06:04,150 - INFO - joeynmt.training - Epoch   2, Step:    22500, Batch Loss:     1.429395, Tokens per Sec:     4923, Lr: 0.000300\n",
      "2021-10-08 18:06:49,805 - INFO - joeynmt.training - Epoch   2, Step:    22600, Batch Loss:     1.507344, Tokens per Sec:     4932, Lr: 0.000300\n",
      "2021-10-08 18:07:35,837 - INFO - joeynmt.training - Epoch   2, Step:    22700, Batch Loss:     1.512106, Tokens per Sec:     4935, Lr: 0.000300\n",
      "2021-10-08 18:08:21,667 - INFO - joeynmt.training - Epoch   2, Step:    22800, Batch Loss:     1.614050, Tokens per Sec:     4980, Lr: 0.000300\n",
      "2021-10-08 18:09:07,392 - INFO - joeynmt.training - Epoch   2, Step:    22900, Batch Loss:     1.443450, Tokens per Sec:     4962, Lr: 0.000300\n",
      "2021-10-08 18:09:53,554 - INFO - joeynmt.training - Epoch   2, Step:    23000, Batch Loss:     1.572700, Tokens per Sec:     4926, Lr: 0.000300\n",
      "2021-10-08 18:10:39,387 - INFO - joeynmt.training - Epoch   2, Step:    23100, Batch Loss:     1.502598, Tokens per Sec:     4924, Lr: 0.000300\n",
      "2021-10-08 18:11:25,421 - INFO - joeynmt.training - Epoch   2, Step:    23200, Batch Loss:     1.497961, Tokens per Sec:     4956, Lr: 0.000300\n",
      "2021-10-08 18:12:10,846 - INFO - joeynmt.training - Epoch   2, Step:    23300, Batch Loss:     1.585789, Tokens per Sec:     5021, Lr: 0.000300\n",
      "2021-10-08 18:12:57,857 - INFO - joeynmt.training - Epoch   2, Step:    23400, Batch Loss:     1.720978, Tokens per Sec:     4964, Lr: 0.000300\n",
      "2021-10-08 18:13:43,843 - INFO - joeynmt.training - Epoch   2, Step:    23500, Batch Loss:     1.460390, Tokens per Sec:     4973, Lr: 0.000300\n",
      "2021-10-08 18:14:30,157 - INFO - joeynmt.training - Epoch   2, Step:    23600, Batch Loss:     1.582311, Tokens per Sec:     5034, Lr: 0.000300\n",
      "2021-10-08 18:15:15,579 - INFO - joeynmt.training - Epoch   2, Step:    23700, Batch Loss:     1.412348, Tokens per Sec:     5011, Lr: 0.000300\n",
      "2021-10-08 18:16:01,152 - INFO - joeynmt.training - Epoch   2, Step:    23800, Batch Loss:     1.607974, Tokens per Sec:     4953, Lr: 0.000300\n",
      "2021-10-08 18:16:46,572 - INFO - joeynmt.training - Epoch   2, Step:    23900, Batch Loss:     1.456440, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-08 18:17:31,521 - INFO - joeynmt.training - Epoch   2, Step:    24000, Batch Loss:     1.327500, Tokens per Sec:     4940, Lr: 0.000300\n",
      "2021-10-08 18:18:17,600 - INFO - joeynmt.training - Epoch   2, Step:    24100, Batch Loss:     1.337023, Tokens per Sec:     5024, Lr: 0.000300\n",
      "2021-10-08 18:19:02,980 - INFO - joeynmt.training - Epoch   2, Step:    24200, Batch Loss:     1.414384, Tokens per Sec:     4845, Lr: 0.000300\n",
      "2021-10-08 18:19:49,321 - INFO - joeynmt.training - Epoch   2, Step:    24300, Batch Loss:     1.388271, Tokens per Sec:     4970, Lr: 0.000300\n",
      "2021-10-08 18:20:34,868 - INFO - joeynmt.training - Epoch   2, Step:    24400, Batch Loss:     1.786905, Tokens per Sec:     4974, Lr: 0.000300\n",
      "2021-10-08 18:21:21,267 - INFO - joeynmt.training - Epoch   2, Step:    24500, Batch Loss:     1.569918, Tokens per Sec:     4995, Lr: 0.000300\n",
      "2021-10-08 18:22:06,823 - INFO - joeynmt.training - Epoch   2, Step:    24600, Batch Loss:     1.567524, Tokens per Sec:     4904, Lr: 0.000300\n",
      "2021-10-08 18:22:52,671 - INFO - joeynmt.training - Epoch   2, Step:    24700, Batch Loss:     1.384169, Tokens per Sec:     5063, Lr: 0.000300\n",
      "2021-10-08 18:23:37,788 - INFO - joeynmt.training - Epoch   2, Step:    24800, Batch Loss:     1.367683, Tokens per Sec:     4907, Lr: 0.000300\n",
      "2021-10-08 18:24:22,904 - INFO - joeynmt.training - Epoch   2, Step:    24900, Batch Loss:     1.481054, Tokens per Sec:     5006, Lr: 0.000300\n",
      "2021-10-08 18:25:07,839 - INFO - joeynmt.training - Epoch   2, Step:    25000, Batch Loss:     1.401767, Tokens per Sec:     4932, Lr: 0.000300\n",
      "2021-10-08 18:26:30,127 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 18:26:30,128 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 18:26:30,128 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 18:26:30,458 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 18:26:30,459 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 18:26:36,532 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 18:26:36,533 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 18:26:36,533 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 18:26:36,533 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at that time for the brother who took the lead in a congregation .\n",
      "2021-10-08 18:26:36,533 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 18:26:36,533 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 18:26:36,533 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 18:26:36,534 - INFO - joeynmt.training - \tHypothesis: The attraction is amazing , for adolescence is a time when one learns yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-08 18:26:36,534 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 18:26:36,534 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 18:26:36,534 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 18:26:36,534 - INFO - joeynmt.training - \tHypothesis: He even used the words “ Your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-08 18:26:36,534 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 18:26:36,534 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 18:26:36,535 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 18:26:36,535 - INFO - joeynmt.training - \tHypothesis: HOW WHAT MOLD WITH DO SECTH ?\n",
      "2021-10-08 18:26:36,535 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    25000: bleu:  40.13, loss: 40448.2461, ppl:   3.8081, duration: 88.6947s\n",
      "2021-10-08 18:26:56,857 - INFO - joeynmt.training - Epoch   2: total training loss 20384.61\n",
      "2021-10-08 18:26:56,858 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-10-08 18:27:23,952 - INFO - joeynmt.training - Epoch   3, Step:    25100, Batch Loss:     1.477413, Tokens per Sec:     4688, Lr: 0.000300\n",
      "2021-10-08 18:28:09,781 - INFO - joeynmt.training - Epoch   3, Step:    25200, Batch Loss:     1.370476, Tokens per Sec:     4977, Lr: 0.000300\n",
      "2021-10-08 18:28:55,563 - INFO - joeynmt.training - Epoch   3, Step:    25300, Batch Loss:     1.351816, Tokens per Sec:     4907, Lr: 0.000300\n",
      "2021-10-08 18:29:41,221 - INFO - joeynmt.training - Epoch   3, Step:    25400, Batch Loss:     1.318870, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-08 18:30:27,816 - INFO - joeynmt.training - Epoch   3, Step:    25500, Batch Loss:     1.606638, Tokens per Sec:     4924, Lr: 0.000300\n",
      "2021-10-08 18:31:13,091 - INFO - joeynmt.training - Epoch   3, Step:    25600, Batch Loss:     1.363184, Tokens per Sec:     4956, Lr: 0.000300\n",
      "2021-10-08 18:31:57,785 - INFO - joeynmt.training - Epoch   3, Step:    25700, Batch Loss:     1.753247, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-08 18:32:43,201 - INFO - joeynmt.training - Epoch   3, Step:    25800, Batch Loss:     1.459906, Tokens per Sec:     5056, Lr: 0.000300\n",
      "2021-10-08 18:33:27,465 - INFO - joeynmt.training - Epoch   3, Step:    25900, Batch Loss:     1.353177, Tokens per Sec:     4989, Lr: 0.000300\n",
      "2021-10-08 18:34:12,460 - INFO - joeynmt.training - Epoch   3, Step:    26000, Batch Loss:     1.567926, Tokens per Sec:     5098, Lr: 0.000300\n",
      "2021-10-08 18:34:57,078 - INFO - joeynmt.training - Epoch   3, Step:    26100, Batch Loss:     1.518478, Tokens per Sec:     5043, Lr: 0.000300\n",
      "2021-10-08 18:35:41,744 - INFO - joeynmt.training - Epoch   3, Step:    26200, Batch Loss:     1.422727, Tokens per Sec:     5051, Lr: 0.000300\n",
      "2021-10-08 18:36:26,957 - INFO - joeynmt.training - Epoch   3, Step:    26300, Batch Loss:     1.550717, Tokens per Sec:     5190, Lr: 0.000300\n",
      "2021-10-08 18:37:11,833 - INFO - joeynmt.training - Epoch   3, Step:    26400, Batch Loss:     1.583298, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-08 18:37:57,007 - INFO - joeynmt.training - Epoch   3, Step:    26500, Batch Loss:     1.340286, Tokens per Sec:     5036, Lr: 0.000300\n",
      "2021-10-08 18:38:41,977 - INFO - joeynmt.training - Epoch   3, Step:    26600, Batch Loss:     1.389462, Tokens per Sec:     5107, Lr: 0.000300\n",
      "2021-10-08 18:39:26,620 - INFO - joeynmt.training - Epoch   3, Step:    26700, Batch Loss:     1.317948, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-08 18:40:12,032 - INFO - joeynmt.training - Epoch   3, Step:    26800, Batch Loss:     1.222462, Tokens per Sec:     5089, Lr: 0.000300\n",
      "2021-10-08 18:40:56,915 - INFO - joeynmt.training - Epoch   3, Step:    26900, Batch Loss:     1.441694, Tokens per Sec:     5015, Lr: 0.000300\n",
      "2021-10-08 18:41:41,963 - INFO - joeynmt.training - Epoch   3, Step:    27000, Batch Loss:     1.379176, Tokens per Sec:     5070, Lr: 0.000300\n",
      "2021-10-08 18:42:26,392 - INFO - joeynmt.training - Epoch   3, Step:    27100, Batch Loss:     1.755992, Tokens per Sec:     4978, Lr: 0.000300\n",
      "2021-10-08 18:43:10,910 - INFO - joeynmt.training - Epoch   3, Step:    27200, Batch Loss:     1.422554, Tokens per Sec:     4967, Lr: 0.000300\n",
      "2021-10-08 18:43:55,981 - INFO - joeynmt.training - Epoch   3, Step:    27300, Batch Loss:     1.335758, Tokens per Sec:     5028, Lr: 0.000300\n",
      "2021-10-08 18:44:40,625 - INFO - joeynmt.training - Epoch   3, Step:    27400, Batch Loss:     1.522257, Tokens per Sec:     4954, Lr: 0.000300\n",
      "2021-10-08 18:45:24,841 - INFO - joeynmt.training - Epoch   3, Step:    27500, Batch Loss:     1.607294, Tokens per Sec:     4939, Lr: 0.000300\n",
      "2021-10-08 18:46:09,753 - INFO - joeynmt.training - Epoch   3, Step:    27600, Batch Loss:     1.580792, Tokens per Sec:     5074, Lr: 0.000300\n",
      "2021-10-08 18:46:54,463 - INFO - joeynmt.training - Epoch   3, Step:    27700, Batch Loss:     1.584773, Tokens per Sec:     5066, Lr: 0.000300\n",
      "2021-10-08 18:47:39,252 - INFO - joeynmt.training - Epoch   3, Step:    27800, Batch Loss:     1.725594, Tokens per Sec:     4985, Lr: 0.000300\n",
      "2021-10-08 18:48:24,281 - INFO - joeynmt.training - Epoch   3, Step:    27900, Batch Loss:     1.520544, Tokens per Sec:     5072, Lr: 0.000300\n",
      "2021-10-08 18:49:08,411 - INFO - joeynmt.training - Epoch   3, Step:    28000, Batch Loss:     1.396695, Tokens per Sec:     4899, Lr: 0.000300\n",
      "2021-10-08 18:49:54,074 - INFO - joeynmt.training - Epoch   3, Step:    28100, Batch Loss:     1.571009, Tokens per Sec:     5088, Lr: 0.000300\n",
      "2021-10-08 18:50:39,722 - INFO - joeynmt.training - Epoch   3, Step:    28200, Batch Loss:     1.443357, Tokens per Sec:     5047, Lr: 0.000300\n",
      "2021-10-08 18:51:24,936 - INFO - joeynmt.training - Epoch   3, Step:    28300, Batch Loss:     1.545571, Tokens per Sec:     5045, Lr: 0.000300\n",
      "2021-10-08 18:52:10,235 - INFO - joeynmt.training - Epoch   3, Step:    28400, Batch Loss:     1.302287, Tokens per Sec:     5023, Lr: 0.000300\n",
      "2021-10-08 18:52:54,925 - INFO - joeynmt.training - Epoch   3, Step:    28500, Batch Loss:     1.411098, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-08 18:53:39,483 - INFO - joeynmt.training - Epoch   3, Step:    28600, Batch Loss:     1.399622, Tokens per Sec:     5026, Lr: 0.000300\n",
      "2021-10-08 18:54:24,904 - INFO - joeynmt.training - Epoch   3, Step:    28700, Batch Loss:     1.538191, Tokens per Sec:     5089, Lr: 0.000300\n",
      "2021-10-08 18:55:10,684 - INFO - joeynmt.training - Epoch   3, Step:    28800, Batch Loss:     1.561612, Tokens per Sec:     5125, Lr: 0.000300\n",
      "2021-10-08 18:55:55,897 - INFO - joeynmt.training - Epoch   3, Step:    28900, Batch Loss:     1.525250, Tokens per Sec:     5107, Lr: 0.000300\n",
      "2021-10-08 18:56:40,177 - INFO - joeynmt.training - Epoch   3, Step:    29000, Batch Loss:     1.411534, Tokens per Sec:     4920, Lr: 0.000300\n",
      "2021-10-08 18:57:25,748 - INFO - joeynmt.training - Epoch   3, Step:    29100, Batch Loss:     1.396762, Tokens per Sec:     5018, Lr: 0.000300\n",
      "2021-10-08 18:58:11,475 - INFO - joeynmt.training - Epoch   3, Step:    29200, Batch Loss:     1.459709, Tokens per Sec:     5021, Lr: 0.000300\n",
      "2021-10-08 18:58:56,796 - INFO - joeynmt.training - Epoch   3, Step:    29300, Batch Loss:     1.426899, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-08 18:59:41,448 - INFO - joeynmt.training - Epoch   3, Step:    29400, Batch Loss:     1.468934, Tokens per Sec:     4972, Lr: 0.000300\n",
      "2021-10-08 19:00:27,179 - INFO - joeynmt.training - Epoch   3, Step:    29500, Batch Loss:     1.375224, Tokens per Sec:     5052, Lr: 0.000300\n",
      "2021-10-08 19:01:12,006 - INFO - joeynmt.training - Epoch   3, Step:    29600, Batch Loss:     1.580989, Tokens per Sec:     5100, Lr: 0.000300\n",
      "2021-10-08 19:01:57,163 - INFO - joeynmt.training - Epoch   3, Step:    29700, Batch Loss:     1.496373, Tokens per Sec:     5056, Lr: 0.000300\n",
      "2021-10-08 19:02:42,045 - INFO - joeynmt.training - Epoch   3, Step:    29800, Batch Loss:     1.390630, Tokens per Sec:     5017, Lr: 0.000300\n",
      "2021-10-08 19:03:26,922 - INFO - joeynmt.training - Epoch   3, Step:    29900, Batch Loss:     1.388145, Tokens per Sec:     4942, Lr: 0.000300\n",
      "2021-10-08 19:04:12,119 - INFO - joeynmt.training - Epoch   3, Step:    30000, Batch Loss:     1.377680, Tokens per Sec:     5018, Lr: 0.000300\n",
      "2021-10-08 19:05:35,434 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 19:05:35,435 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 19:05:35,435 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 19:05:35,766 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 19:05:35,766 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 19:05:41,981 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 19:05:41,982 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 19:05:41,982 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 19:05:41,982 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at that time for the brother who took the lead in a congregation .\n",
      "2021-10-08 19:05:41,982 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 19:05:41,983 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 19:05:41,983 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 19:05:41,983 - INFO - joeynmt.training - \tHypothesis: The attraction is supposedly , for adolescence is a time when one learns yourself and express your feelings in a way that speaks to others and is devastating them .\n",
      "2021-10-08 19:05:41,983 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 19:05:41,984 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 19:05:41,984 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 19:05:41,984 - INFO - joeynmt.training - \tHypothesis: He even used the words “ Your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he .\n",
      "2021-10-08 19:05:41,984 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 19:05:41,984 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 19:05:41,984 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 19:05:41,984 - INFO - joeynmt.training - \tHypothesis: HOW WHAT MOLD WITH WITH ?\n",
      "2021-10-08 19:05:41,984 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    30000: bleu:  41.79, loss: 38351.9688, ppl:   3.5531, duration: 89.8647s\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home-mscluster/mbeukman/anaconda3/envs/nlp_q2/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home-mscluster/mbeukman/anaconda3/envs/nlp_q2/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home-mscluster/mbeukman/anaconda3/envs/nlp_q2/lib/python3.7/site-packages/joeynmt/__main__.py\", line 48, in <module>\n",
      "    main()\n",
      "  File \"/home-mscluster/mbeukman/anaconda3/envs/nlp_q2/lib/python3.7/site-packages/joeynmt/__main__.py\", line 35, in main\n",
      "    train(cfg_file=args.config_path, skip_test=args.skip_test)\n",
      "  File \"/home-mscluster/mbeukman/anaconda3/envs/nlp_q2/lib/python3.7/site-packages/joeynmt/training.py\", line 804, in train\n",
      "    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)\n",
      "  File \"/home-mscluster/mbeukman/anaconda3/envs/nlp_q2/lib/python3.7/site-packages/joeynmt/training.py\", line 427, in train_and_validate\n",
      "    batch_loss += self._train_step(batch)\n",
      "  File \"/home-mscluster/mbeukman/anaconda3/envs/nlp_q2/lib/python3.7/site-packages/joeynmt/training.py\", line 536, in _train_step\n",
      "    norm_batch_loss.backward()\n",
      "  File \"/home-mscluster/mbeukman/anaconda3/envs/nlp_q2/lib/python3.7/site-packages/torch/tensor.py\", line 245, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/home-mscluster/mbeukman/anaconda3/envs/nlp_q2/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 147, in backward\n",
      "    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_$tgt$src.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp_things/masakhane/af-en-mcb_af_to_en_1007_v2_larger/models/enaf_bigger-model/\n",
      "ls: cannot access 'nlp_things/masakhane/af-en-mcb_af_to_en_1007_v2_larger/models/enaf_bigger-model/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "os.environ['FILE'] = f\"{os.environ['gdrive_path']}/models/{os.environ['src']}{os.environ['tgt']}_bigger-model/\"\n",
    "\n",
    "!echo $FILE\n",
    "!ls $FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "MBoDS09JM807"
   },
   "outputs": [],
   "source": [
    "# Copy the created models from the notebook storage to google drive for persistant storage\n",
    "!mkdir -p \"$FILE\"\n",
    "\n",
    "# !cp -r joeynmt/models/${tgt}${src}_reverse_transformer_michael_larger_30epochs/* \"$FILE\"\n",
    "!cp -r joeynmt/models/${tgt}${src}_reverse_transformer_big-model-end1008/* \"$FILE\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n94wlrCjVc17",
    "outputId": "bb8462a5-241e-4b73-e9a8-3096816d0151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 5000\tLoss: 68910.39844\tPPL: 9.75744\tbleu: 22.70799\tLR: 0.00030000\t*\n",
      "Steps: 10000\tLoss: 52362.63672\tPPL: 5.64628\tbleu: 33.08803\tLR: 0.00030000\t*\n",
      "Steps: 15000\tLoss: 47018.39844\tPPL: 4.73190\tbleu: 36.29706\tLR: 0.00030000\t*\n",
      "Steps: 20000\tLoss: 42779.03516\tPPL: 4.11312\tbleu: 39.27454\tLR: 0.00030000\t*\n",
      "Steps: 25000\tLoss: 40448.24609\tPPL: 3.80811\tbleu: 40.12918\tLR: 0.00030000\t*\n",
      "Steps: 30000\tLoss: 38351.96875\tPPL: 3.55315\tbleu: 41.78984\tLR: 0.00030000\t*\n"
     ]
    }
   ],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"$FILE/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66WhRE9lIhoD",
    "outputId": "b4297dba-67ae-48be-9068-4a0e92c4f98e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-08 19:06:38,237 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-10-08 19:06:38,238 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-10-08 19:06:38,528 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-10-08 19:06:38,541 - INFO - joeynmt.data - Loading test data...\n",
      "2021-10-08 19:06:38,568 - INFO - joeynmt.data - Data loaded.\n",
      "2021-10-08 19:06:38,613 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
      "2021-10-08 19:06:41,391 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-10-08 19:06:42,250 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-10-08 19:06:42,449 - INFO - joeynmt.prediction - Decoding on dev set (data/afen/dev.bpe.en)...\n",
      "2021-10-08 19:08:20,774 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 19:08:20,774 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 19:08:20,774 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 19:08:21,111 - INFO - joeynmt.prediction -  dev bleu[13a]:  42.74 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-10-08 19:08:21,112 - INFO - joeynmt.prediction - Decoding on test set (data/afen/test.bpe.en)...\n",
      "2021-10-08 19:10:33,601 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 19:10:33,602 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 19:10:33,602 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 19:10:34,379 - INFO - joeynmt.prediction - test bleu[13a]:  51.80 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Test our model\n",
    "! cd joeynmt; python3 -m joeynmt test ../\"$FILE/config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000.hyps  20000.hyps\t30000.ckpt  best.ckpt\t src_vocab.txt\ttrain.log\n",
      "15000.hyps  25000.ckpt\t30000.hyps  config.yaml  tensorboard\ttrg_vocab.txt\n",
      "20000.ckpt  25000.hyps\t5000.hyps   latest.ckpt  test.log\tvalidations.txt\n"
     ]
    }
   ],
   "source": [
    "!ls joeynmt/models/${tgt}${src}_reverse_transformer_big-model-end1008/30000.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Continue Training\n",
    "\n",
    "For the above, we stopped the training, but saved a checkpoint. Here we simply load the checkpoint again.\n",
    "\n",
    "We performed the actual training in a normal terminal instead of in the notebook, but the output was saved, and that will be shown now.\n",
    "\n",
    "The config shown below here is the same as the above one, but it contains the load_model option too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
    "# (You can of course play with all the parameters if you'd like!)\n",
    "\n",
    "name = '%s%s' % (target_language, source_language)\n",
    "# gdrive_path = os.environ[\"gdrive_path\"]\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{target_language}{source_language}_reverse_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{target_language}\"\n",
    "    trg: \"{source_language}\"\n",
    "    train: \"data/{name}/train.bpe\"\n",
    "    dev:   \"data/{name}/dev.bpe\"\n",
    "    test:  \"data/{name}/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"data/{name}/vocab.txt\"\n",
    "    trg_vocab: \"data/{name}/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    load_model: \"../nlp_things/masakhane/af-en-mcb_af_to_en_1007_v2_larger/models/enaf_bigger-model/30000.ckpt\"\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 3600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 20                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 5000          # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/{name}_reverse_transformer_big-model-end1008\"\n",
    "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 8             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 512   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 512         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 2048            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 8              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 512    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 512         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 2048            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_$tgt$src.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-08 19:23:32,135 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-10-08 19:23:32,188 - INFO - joeynmt.data - Loading training data...\n",
      "2021-10-08 19:23:51,543 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-10-08 19:23:51,802 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-10-08 19:23:51,911 - INFO - joeynmt.data - Loading test data...\n",
      "2021-10-08 19:23:51,939 - INFO - joeynmt.data - Data loaded.\n",
      "2021-10-08 19:23:51,939 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-10-08 19:23:52,736 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-10-08 19:23:52,749 - INFO - joeynmt.training - Total params: 46467072\n",
      "2021-10-08 19:23:55,350 - INFO - joeynmt.training - Loading model from ../nlp_things/masakhane/af-en-mcb_af_to_en_1007_v2_larger/models/enaf_bigger-model/30000.ckpt\n",
      "2021-10-08 19:23:55,866 - INFO - joeynmt.helpers - cfg.name                           : afen_reverse_transformer\n",
      "2021-10-08 19:23:55,866 - INFO - joeynmt.helpers - cfg.data.src                       : af\n",
      "2021-10-08 19:23:55,866 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-10-08 19:23:55,866 - INFO - joeynmt.helpers - cfg.data.train                     : data/afen/train.bpe\n",
      "2021-10-08 19:23:55,866 - INFO - joeynmt.helpers - cfg.data.dev                       : data/afen/dev.bpe\n",
      "2021-10-08 19:23:55,866 - INFO - joeynmt.helpers - cfg.data.test                      : data/afen/test.bpe\n",
      "2021-10-08 19:23:55,866 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-10-08 19:23:55,867 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-10-08 19:23:55,867 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-10-08 19:23:55,867 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : data/afen/vocab.txt\n",
      "2021-10-08 19:23:55,867 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : data/afen/vocab.txt\n",
      "2021-10-08 19:23:55,867 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-10-08 19:23:55,867 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-10-08 19:23:55,867 - INFO - joeynmt.helpers - cfg.training.load_model            : ../nlp_things/masakhane/af-en-mcb_af_to_en_1007_v2_larger/models/enaf_bigger-model/30000.ckpt\n",
      "2021-10-08 19:23:55,868 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-10-08 19:23:55,868 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-10-08 19:23:55,868 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-10-08 19:23:55,868 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-10-08 19:23:55,868 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-10-08 19:23:55,868 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-10-08 19:23:55,868 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-10-08 19:23:55,868 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-10-08 19:23:55,869 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-10-08 19:23:55,869 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-10-08 19:23:55,869 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-10-08 19:23:55,869 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-10-08 19:23:55,869 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-10-08 19:23:55,869 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-10-08 19:23:55,869 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-10-08 19:23:55,869 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-10-08 19:23:55,869 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
      "2021-10-08 19:23:55,870 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-10-08 19:23:55,870 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-10-08 19:23:55,870 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-10-08 19:23:55,870 - INFO - joeynmt.helpers - cfg.training.epochs                : 20\n",
      "2021-10-08 19:23:55,870 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
      "2021-10-08 19:23:55,870 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-10-08 19:23:55,870 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-10-08 19:23:55,870 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/afen_reverse_transformer_big-model-end1008\n",
      "2021-10-08 19:23:55,871 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-10-08 19:23:55,871 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-10-08 19:23:55,871 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-10-08 19:23:55,871 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-10-08 19:23:55,871 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-10-08 19:23:55,871 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-10-08 19:23:55,871 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-10-08 19:23:55,871 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-10-08 19:23:55,872 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-10-08 19:23:55,872 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-10-08 19:23:55,872 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-10-08 19:23:55,872 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-10-08 19:23:55,872 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-10-08 19:23:55,872 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-10-08 19:23:55,872 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-10-08 19:23:55,872 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 8\n",
      "2021-10-08 19:23:55,872 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 512\n",
      "2021-10-08 19:23:55,873 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-10-08 19:23:55,873 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-10-08 19:23:55,873 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 512\n",
      "2021-10-08 19:23:55,873 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 2048\n",
      "2021-10-08 19:23:55,873 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-10-08 19:23:55,873 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-10-08 19:23:55,873 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-10-08 19:23:55,873 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 8\n",
      "2021-10-08 19:23:55,874 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512\n",
      "2021-10-08 19:23:55,874 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-10-08 19:23:55,874 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-10-08 19:23:55,874 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 512\n",
      "2021-10-08 19:23:55,874 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 2048\n",
      "2021-10-08 19:23:55,874 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-10-08 19:23:55,874 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 959951,\n",
      "\tvalid 1000,\n",
      "\ttest 2682\n",
      "2021-10-08 19:23:55,874 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Mo@@ der@@ ne te@@ g@@ no@@ lo@@ gie stel leer@@ ders en studen@@ te in staat om ma@@ k@@ liker en op be@@ dre@@ wen@@ er wyse te k@@ ul .\n",
      "\t[TRG] Mo@@ der@@ n te@@ ch@@ no@@ lo@@ gy en@@ ab@@ les studen@@ ts to che@@ at with new le@@ vel@@ s of e@@ ase and s@@ op@@ hi@@ sti@@ cation .\n",
      "2021-10-08 19:23:55,875 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) die (7) the (8) in (9) ’\n",
      "2021-10-08 19:23:55,875 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) die (7) the (8) in (9) ’\n",
      "2021-10-08 19:23:55,875 - INFO - joeynmt.helpers - Number of Src words (types): 4544\n",
      "2021-10-08 19:23:55,875 - INFO - joeynmt.helpers - Number of Trg words (types): 4544\n",
      "2021-10-08 19:23:55,875 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=8),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=8),\n",
      "\tsrc_embed=Embeddings(embedding_dim=512, vocab_size=4544),\n",
      "\ttrg_embed=Embeddings(embedding_dim=512, vocab_size=4544))\n",
      "2021-10-08 19:23:55,882 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-10-08 19:23:55,882 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-10-08 19:24:42,617 - INFO - joeynmt.training - Epoch   1, Step:    30100, Batch Loss:     1.466970, Tokens per Sec:     4705, Lr: 0.000300\n",
      "2021-10-08 19:25:27,428 - INFO - joeynmt.training - Epoch   1, Step:    30200, Batch Loss:     1.490685, Tokens per Sec:     5121, Lr: 0.000300\n",
      "2021-10-08 19:26:12,576 - INFO - joeynmt.training - Epoch   1, Step:    30300, Batch Loss:     1.316594, Tokens per Sec:     5044, Lr: 0.000300\n",
      "2021-10-08 19:26:58,184 - INFO - joeynmt.training - Epoch   1, Step:    30400, Batch Loss:     1.369968, Tokens per Sec:     5010, Lr: 0.000300\n",
      "2021-10-08 19:27:45,363 - INFO - joeynmt.training - Epoch   1, Step:    30500, Batch Loss:     1.538924, Tokens per Sec:     5030, Lr: 0.000300\n",
      "2021-10-08 19:28:30,796 - INFO - joeynmt.training - Epoch   1, Step:    30600, Batch Loss:     1.374419, Tokens per Sec:     5018, Lr: 0.000300\n",
      "2021-10-08 19:29:16,633 - INFO - joeynmt.training - Epoch   1, Step:    30700, Batch Loss:     1.397756, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-08 19:30:02,573 - INFO - joeynmt.training - Epoch   1, Step:    30800, Batch Loss:     1.343390, Tokens per Sec:     4955, Lr: 0.000300\n",
      "2021-10-08 19:30:48,747 - INFO - joeynmt.training - Epoch   1, Step:    30900, Batch Loss:     1.262329, Tokens per Sec:     4844, Lr: 0.000300\n",
      "2021-10-08 19:31:34,217 - INFO - joeynmt.training - Epoch   1, Step:    31000, Batch Loss:     1.488990, Tokens per Sec:     4938, Lr: 0.000300\n",
      "2021-10-08 19:32:19,337 - INFO - joeynmt.training - Epoch   1, Step:    31100, Batch Loss:     1.441438, Tokens per Sec:     4961, Lr: 0.000300\n",
      "2021-10-08 19:33:04,630 - INFO - joeynmt.training - Epoch   1, Step:    31200, Batch Loss:     1.464375, Tokens per Sec:     4894, Lr: 0.000300\n",
      "2021-10-08 19:33:49,105 - INFO - joeynmt.training - Epoch   1, Step:    31300, Batch Loss:     1.262095, Tokens per Sec:     4920, Lr: 0.000300\n",
      "2021-10-08 19:34:34,333 - INFO - joeynmt.training - Epoch   1, Step:    31400, Batch Loss:     1.513981, Tokens per Sec:     5082, Lr: 0.000300\n",
      "2021-10-08 19:35:18,866 - INFO - joeynmt.training - Epoch   1, Step:    31500, Batch Loss:     1.399407, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-08 19:36:03,428 - INFO - joeynmt.training - Epoch   1, Step:    31600, Batch Loss:     1.325716, Tokens per Sec:     4953, Lr: 0.000300\n",
      "2021-10-08 19:36:48,346 - INFO - joeynmt.training - Epoch   1, Step:    31700, Batch Loss:     1.443719, Tokens per Sec:     5028, Lr: 0.000300\n",
      "2021-10-08 19:37:33,397 - INFO - joeynmt.training - Epoch   1, Step:    31800, Batch Loss:     1.305665, Tokens per Sec:     5037, Lr: 0.000300\n",
      "2021-10-08 19:38:18,593 - INFO - joeynmt.training - Epoch   1, Step:    31900, Batch Loss:     1.280466, Tokens per Sec:     5062, Lr: 0.000300\n",
      "2021-10-08 19:39:04,669 - INFO - joeynmt.training - Epoch   1, Step:    32000, Batch Loss:     1.560291, Tokens per Sec:     4912, Lr: 0.000300\n",
      "2021-10-08 19:39:50,098 - INFO - joeynmt.training - Epoch   1, Step:    32100, Batch Loss:     1.304163, Tokens per Sec:     4962, Lr: 0.000300\n",
      "2021-10-08 19:40:35,917 - INFO - joeynmt.training - Epoch   1, Step:    32200, Batch Loss:     1.400775, Tokens per Sec:     5010, Lr: 0.000300\n",
      "2021-10-08 19:41:21,167 - INFO - joeynmt.training - Epoch   1, Step:    32300, Batch Loss:     1.453561, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-08 19:42:07,133 - INFO - joeynmt.training - Epoch   1, Step:    32400, Batch Loss:     1.444018, Tokens per Sec:     5048, Lr: 0.000300\n",
      "2021-10-08 19:42:52,076 - INFO - joeynmt.training - Epoch   1, Step:    32500, Batch Loss:     1.528874, Tokens per Sec:     4957, Lr: 0.000300\n",
      "2021-10-08 19:43:37,024 - INFO - joeynmt.training - Epoch   1, Step:    32600, Batch Loss:     1.370717, Tokens per Sec:     4980, Lr: 0.000300\n",
      "2021-10-08 19:44:22,340 - INFO - joeynmt.training - Epoch   1, Step:    32700, Batch Loss:     1.274031, Tokens per Sec:     4921, Lr: 0.000300\n",
      "2021-10-08 19:45:07,925 - INFO - joeynmt.training - Epoch   1, Step:    32800, Batch Loss:     1.344823, Tokens per Sec:     5081, Lr: 0.000300\n",
      "2021-10-08 19:45:53,593 - INFO - joeynmt.training - Epoch   1, Step:    32900, Batch Loss:     1.498276, Tokens per Sec:     4972, Lr: 0.000300\n",
      "2021-10-08 19:46:39,120 - INFO - joeynmt.training - Epoch   1, Step:    33000, Batch Loss:     1.343430, Tokens per Sec:     4846, Lr: 0.000300\n",
      "2021-10-08 19:47:25,849 - INFO - joeynmt.training - Epoch   1, Step:    33100, Batch Loss:     1.470222, Tokens per Sec:     5098, Lr: 0.000300\n",
      "2021-10-08 19:48:11,558 - INFO - joeynmt.training - Epoch   1, Step:    33200, Batch Loss:     1.468008, Tokens per Sec:     4940, Lr: 0.000300\n",
      "2021-10-08 19:48:57,042 - INFO - joeynmt.training - Epoch   1, Step:    33300, Batch Loss:     1.473566, Tokens per Sec:     4920, Lr: 0.000300\n",
      "2021-10-08 19:49:42,484 - INFO - joeynmt.training - Epoch   1, Step:    33400, Batch Loss:     1.463460, Tokens per Sec:     4919, Lr: 0.000300\n",
      "2021-10-08 19:50:28,338 - INFO - joeynmt.training - Epoch   1, Step:    33500, Batch Loss:     1.346763, Tokens per Sec:     4988, Lr: 0.000300\n",
      "2021-10-08 19:51:14,671 - INFO - joeynmt.training - Epoch   1, Step:    33600, Batch Loss:     1.395306, Tokens per Sec:     5098, Lr: 0.000300\n",
      "2021-10-08 19:52:00,398 - INFO - joeynmt.training - Epoch   1, Step:    33700, Batch Loss:     1.421297, Tokens per Sec:     4970, Lr: 0.000300\n",
      "2021-10-08 19:52:45,954 - INFO - joeynmt.training - Epoch   1, Step:    33800, Batch Loss:     1.445895, Tokens per Sec:     4985, Lr: 0.000300\n",
      "2021-10-08 19:53:31,364 - INFO - joeynmt.training - Epoch   1, Step:    33900, Batch Loss:     1.510056, Tokens per Sec:     5097, Lr: 0.000300\n",
      "2021-10-08 19:54:16,473 - INFO - joeynmt.training - Epoch   1, Step:    34000, Batch Loss:     1.424287, Tokens per Sec:     4965, Lr: 0.000300\n",
      "2021-10-08 19:55:02,225 - INFO - joeynmt.training - Epoch   1, Step:    34100, Batch Loss:     1.261409, Tokens per Sec:     4969, Lr: 0.000300\n",
      "2021-10-08 19:55:48,415 - INFO - joeynmt.training - Epoch   1, Step:    34200, Batch Loss:     1.376808, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2021-10-08 19:56:33,866 - INFO - joeynmt.training - Epoch   1, Step:    34300, Batch Loss:     1.424635, Tokens per Sec:     4938, Lr: 0.000300\n",
      "2021-10-08 19:57:18,946 - INFO - joeynmt.training - Epoch   1, Step:    34400, Batch Loss:     1.230896, Tokens per Sec:     4971, Lr: 0.000300\n",
      "2021-10-08 19:58:04,698 - INFO - joeynmt.training - Epoch   1, Step:    34500, Batch Loss:     1.478927, Tokens per Sec:     4984, Lr: 0.000300\n",
      "2021-10-08 19:58:50,130 - INFO - joeynmt.training - Epoch   1, Step:    34600, Batch Loss:     1.301840, Tokens per Sec:     4895, Lr: 0.000300\n",
      "2021-10-08 19:59:35,461 - INFO - joeynmt.training - Epoch   1, Step:    34700, Batch Loss:     1.052731, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-08 20:00:21,146 - INFO - joeynmt.training - Epoch   1, Step:    34800, Batch Loss:     1.342621, Tokens per Sec:     4943, Lr: 0.000300\n",
      "2021-10-08 20:01:05,867 - INFO - joeynmt.training - Epoch   1, Step:    34900, Batch Loss:     1.423707, Tokens per Sec:     4882, Lr: 0.000300\n",
      "2021-10-08 20:01:51,794 - INFO - joeynmt.training - Epoch   1, Step:    35000, Batch Loss:     1.172577, Tokens per Sec:     4975, Lr: 0.000300\n",
      "2021-10-08 20:03:14,537 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 20:03:14,537 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 20:03:14,537 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 20:03:14,874 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 20:03:14,874 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 20:03:21,565 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 20:03:21,566 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 20:03:21,566 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 20:03:21,566 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at that time for the brother who took the lead in a congregation .\n",
      "2021-10-08 20:03:21,566 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 20:03:21,567 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 20:03:21,567 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 20:03:21,567 - INFO - joeynmt.training - \tHypothesis: The attraction is astounding , for adolescence is a time when you get to know yourself and express your feelings in a way that speaks to others and deprives them .\n",
      "2021-10-08 20:03:21,567 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 20:03:21,567 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 20:03:21,567 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 20:03:21,568 - INFO - joeynmt.training - \tHypothesis: He even used the words “ Your son David ” with reference to himself , perhaps to recognize that Nabal was older than he .\n",
      "2021-10-08 20:03:21,568 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 20:03:21,568 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 20:03:21,568 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 20:03:21,568 - INFO - joeynmt.training - \tHypothesis: HOW WHAT WILL WITH WITH WITH ?\n",
      "2021-10-08 20:03:21,568 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step    35000: bleu:  42.66, loss: 37326.5352, ppl:   3.4347, duration: 89.7740s\n",
      "2021-10-08 20:04:06,841 - INFO - joeynmt.training - Epoch   1, Step:    35100, Batch Loss:     1.389253, Tokens per Sec:     5069, Lr: 0.000300\n",
      "2021-10-08 20:04:52,773 - INFO - joeynmt.training - Epoch   1, Step:    35200, Batch Loss:     1.522402, Tokens per Sec:     5050, Lr: 0.000300\n",
      "2021-10-08 20:05:37,985 - INFO - joeynmt.training - Epoch   1, Step:    35300, Batch Loss:     1.429077, Tokens per Sec:     4781, Lr: 0.000300\n",
      "2021-10-08 20:06:23,324 - INFO - joeynmt.training - Epoch   1, Step:    35400, Batch Loss:     1.272936, Tokens per Sec:     4916, Lr: 0.000300\n",
      "2021-10-08 20:07:09,067 - INFO - joeynmt.training - Epoch   1, Step:    35500, Batch Loss:     1.385327, Tokens per Sec:     4967, Lr: 0.000300\n",
      "2021-10-08 20:07:55,215 - INFO - joeynmt.training - Epoch   1, Step:    35600, Batch Loss:     1.448113, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2021-10-08 20:08:40,287 - INFO - joeynmt.training - Epoch   1, Step:    35700, Batch Loss:     1.380433, Tokens per Sec:     4933, Lr: 0.000300\n",
      "2021-10-08 20:09:26,144 - INFO - joeynmt.training - Epoch   1, Step:    35800, Batch Loss:     1.377584, Tokens per Sec:     4962, Lr: 0.000300\n",
      "2021-10-08 20:10:12,272 - INFO - joeynmt.training - Epoch   1, Step:    35900, Batch Loss:     1.208754, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2021-10-08 20:10:57,104 - INFO - joeynmt.training - Epoch   1, Step:    36000, Batch Loss:     1.482700, Tokens per Sec:     4757, Lr: 0.000300\n",
      "2021-10-08 20:11:43,345 - INFO - joeynmt.training - Epoch   1, Step:    36100, Batch Loss:     1.246485, Tokens per Sec:     4994, Lr: 0.000300\n",
      "2021-10-08 20:12:29,921 - INFO - joeynmt.training - Epoch   1, Step:    36200, Batch Loss:     1.326422, Tokens per Sec:     4893, Lr: 0.000300\n",
      "2021-10-08 20:13:14,549 - INFO - joeynmt.training - Epoch   1, Step:    36300, Batch Loss:     1.337880, Tokens per Sec:     4912, Lr: 0.000300\n",
      "2021-10-08 20:14:00,178 - INFO - joeynmt.training - Epoch   1, Step:    36400, Batch Loss:     1.319748, Tokens per Sec:     5017, Lr: 0.000300\n",
      "2021-10-08 20:14:44,801 - INFO - joeynmt.training - Epoch   1, Step:    36500, Batch Loss:     1.112350, Tokens per Sec:     4882, Lr: 0.000300\n",
      "2021-10-08 20:15:30,193 - INFO - joeynmt.training - Epoch   1, Step:    36600, Batch Loss:     1.413118, Tokens per Sec:     4970, Lr: 0.000300\n",
      "2021-10-08 20:16:16,236 - INFO - joeynmt.training - Epoch   1, Step:    36700, Batch Loss:     1.405967, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-08 20:17:01,374 - INFO - joeynmt.training - Epoch   1, Step:    36800, Batch Loss:     1.293986, Tokens per Sec:     4966, Lr: 0.000300\n",
      "2021-10-08 20:17:46,873 - INFO - joeynmt.training - Epoch   1, Step:    36900, Batch Loss:     1.516633, Tokens per Sec:     4981, Lr: 0.000300\n",
      "2021-10-08 20:18:32,449 - INFO - joeynmt.training - Epoch   1, Step:    37000, Batch Loss:     1.427867, Tokens per Sec:     5036, Lr: 0.000300\n",
      "2021-10-08 20:19:18,789 - INFO - joeynmt.training - Epoch   1, Step:    37100, Batch Loss:     1.225869, Tokens per Sec:     4984, Lr: 0.000300\n",
      "2021-10-08 20:20:03,777 - INFO - joeynmt.training - Epoch   1, Step:    37200, Batch Loss:     1.486978, Tokens per Sec:     4881, Lr: 0.000300\n",
      "2021-10-08 20:20:49,031 - INFO - joeynmt.training - Epoch   1, Step:    37300, Batch Loss:     1.437968, Tokens per Sec:     4995, Lr: 0.000300\n",
      "2021-10-08 20:21:33,669 - INFO - joeynmt.training - Epoch   1, Step:    37400, Batch Loss:     1.337686, Tokens per Sec:     4960, Lr: 0.000300\n",
      "2021-10-08 20:22:18,892 - INFO - joeynmt.training - Epoch   1, Step:    37500, Batch Loss:     1.360254, Tokens per Sec:     5097, Lr: 0.000300\n",
      "2021-10-08 20:22:44,455 - INFO - joeynmt.training - Epoch   1: total training loss 10557.65\n",
      "2021-10-08 20:22:44,456 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-10-08 20:23:05,566 - INFO - joeynmt.training - Epoch   2, Step:    37600, Batch Loss:     1.371276, Tokens per Sec:     4572, Lr: 0.000300\n",
      "2021-10-08 20:23:50,694 - INFO - joeynmt.training - Epoch   2, Step:    37700, Batch Loss:     1.347418, Tokens per Sec:     5012, Lr: 0.000300\n",
      "2021-10-08 20:24:35,952 - INFO - joeynmt.training - Epoch   2, Step:    37800, Batch Loss:     1.309673, Tokens per Sec:     4965, Lr: 0.000300\n",
      "2021-10-08 20:25:21,427 - INFO - joeynmt.training - Epoch   2, Step:    37900, Batch Loss:     1.616558, Tokens per Sec:     4989, Lr: 0.000300\n",
      "2021-10-08 20:26:06,352 - INFO - joeynmt.training - Epoch   2, Step:    38000, Batch Loss:     1.382267, Tokens per Sec:     4976, Lr: 0.000300\n",
      "2021-10-08 20:26:51,384 - INFO - joeynmt.training - Epoch   2, Step:    38100, Batch Loss:     1.470390, Tokens per Sec:     5012, Lr: 0.000300\n",
      "2021-10-08 20:27:36,133 - INFO - joeynmt.training - Epoch   2, Step:    38200, Batch Loss:     1.256525, Tokens per Sec:     4960, Lr: 0.000300\n",
      "2021-10-08 20:28:22,494 - INFO - joeynmt.training - Epoch   2, Step:    38300, Batch Loss:     1.270555, Tokens per Sec:     4985, Lr: 0.000300\n",
      "2021-10-08 20:29:08,486 - INFO - joeynmt.training - Epoch   2, Step:    38400, Batch Loss:     1.361739, Tokens per Sec:     4813, Lr: 0.000300\n",
      "2021-10-08 20:29:54,193 - INFO - joeynmt.training - Epoch   2, Step:    38500, Batch Loss:     1.358231, Tokens per Sec:     4865, Lr: 0.000300\n",
      "2021-10-08 20:30:39,273 - INFO - joeynmt.training - Epoch   2, Step:    38600, Batch Loss:     1.441469, Tokens per Sec:     4935, Lr: 0.000300\n",
      "2021-10-08 20:31:24,818 - INFO - joeynmt.training - Epoch   2, Step:    38700, Batch Loss:     1.433437, Tokens per Sec:     4934, Lr: 0.000300\n",
      "2021-10-08 20:32:11,388 - INFO - joeynmt.training - Epoch   2, Step:    38800, Batch Loss:     1.277092, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2021-10-08 20:32:57,874 - INFO - joeynmt.training - Epoch   2, Step:    38900, Batch Loss:     1.213287, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2021-10-08 20:33:43,308 - INFO - joeynmt.training - Epoch   2, Step:    39000, Batch Loss:     1.362530, Tokens per Sec:     5018, Lr: 0.000300\n",
      "2021-10-08 20:34:27,918 - INFO - joeynmt.training - Epoch   2, Step:    39100, Batch Loss:     1.440078, Tokens per Sec:     4953, Lr: 0.000300\n",
      "2021-10-08 20:35:13,513 - INFO - joeynmt.training - Epoch   2, Step:    39200, Batch Loss:     1.262531, Tokens per Sec:     5038, Lr: 0.000300\n",
      "2021-10-08 20:36:00,755 - INFO - joeynmt.training - Epoch   2, Step:    39300, Batch Loss:     1.538787, Tokens per Sec:     4875, Lr: 0.000300\n",
      "2021-10-08 20:36:46,160 - INFO - joeynmt.training - Epoch   2, Step:    39400, Batch Loss:     1.430156, Tokens per Sec:     4801, Lr: 0.000300\n",
      "2021-10-08 20:37:32,069 - INFO - joeynmt.training - Epoch   2, Step:    39500, Batch Loss:     1.133668, Tokens per Sec:     4929, Lr: 0.000300\n",
      "2021-10-08 20:38:18,613 - INFO - joeynmt.training - Epoch   2, Step:    39600, Batch Loss:     1.430380, Tokens per Sec:     5041, Lr: 0.000300\n",
      "2021-10-08 20:39:04,569 - INFO - joeynmt.training - Epoch   2, Step:    39700, Batch Loss:     1.234164, Tokens per Sec:     4985, Lr: 0.000300\n",
      "2021-10-08 20:39:49,794 - INFO - joeynmt.training - Epoch   2, Step:    39800, Batch Loss:     1.278915, Tokens per Sec:     4912, Lr: 0.000300\n",
      "2021-10-08 20:40:34,899 - INFO - joeynmt.training - Epoch   2, Step:    39900, Batch Loss:     1.337057, Tokens per Sec:     4844, Lr: 0.000300\n",
      "2021-10-08 20:41:20,437 - INFO - joeynmt.training - Epoch   2, Step:    40000, Batch Loss:     1.312775, Tokens per Sec:     4957, Lr: 0.000300\n",
      "2021-10-08 20:42:44,400 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 20:42:44,401 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 20:42:44,401 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 20:42:44,739 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 20:42:44,739 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 20:42:50,725 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 20:42:50,726 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 20:42:50,726 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 20:42:50,726 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-08 20:42:50,726 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 20:42:50,726 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 20:42:50,727 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 20:42:50,727 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one knows yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-08 20:42:50,727 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 20:42:50,727 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 20:42:50,728 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 20:42:50,728 - INFO - joeynmt.training - \tHypothesis: He even used the words “ Your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-08 20:42:50,728 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 20:42:50,728 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 20:42:50,728 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 20:42:50,729 - INFO - joeynmt.training - \tHypothesis: HOW DO MOLD WE WITH WITH WITH ?\n",
      "2021-10-08 20:42:50,729 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    40000: bleu:  43.05, loss: 36330.9258, ppl:   3.3235, duration: 90.2910s\n",
      "2021-10-08 20:43:37,008 - INFO - joeynmt.training - Epoch   2, Step:    40100, Batch Loss:     1.404291, Tokens per Sec:     4965, Lr: 0.000300\n",
      "2021-10-08 20:44:22,076 - INFO - joeynmt.training - Epoch   2, Step:    40200, Batch Loss:     1.347305, Tokens per Sec:     5013, Lr: 0.000300\n",
      "2021-10-08 20:45:07,452 - INFO - joeynmt.training - Epoch   2, Step:    40300, Batch Loss:     1.212979, Tokens per Sec:     4974, Lr: 0.000300\n",
      "2021-10-08 20:45:52,420 - INFO - joeynmt.training - Epoch   2, Step:    40400, Batch Loss:     1.467825, Tokens per Sec:     4978, Lr: 0.000300\n",
      "2021-10-08 20:46:37,140 - INFO - joeynmt.training - Epoch   2, Step:    40500, Batch Loss:     1.307013, Tokens per Sec:     4956, Lr: 0.000300\n",
      "2021-10-08 20:47:21,444 - INFO - joeynmt.training - Epoch   2, Step:    40600, Batch Loss:     1.490525, Tokens per Sec:     5003, Lr: 0.000300\n",
      "2021-10-08 20:48:06,381 - INFO - joeynmt.training - Epoch   2, Step:    40700, Batch Loss:     1.354875, Tokens per Sec:     5033, Lr: 0.000300\n",
      "2021-10-08 20:48:51,745 - INFO - joeynmt.training - Epoch   2, Step:    40800, Batch Loss:     1.258359, Tokens per Sec:     4951, Lr: 0.000300\n",
      "2021-10-08 20:49:36,975 - INFO - joeynmt.training - Epoch   2, Step:    40900, Batch Loss:     1.522297, Tokens per Sec:     4878, Lr: 0.000300\n",
      "2021-10-08 20:50:22,282 - INFO - joeynmt.training - Epoch   2, Step:    41000, Batch Loss:     1.291144, Tokens per Sec:     4977, Lr: 0.000300\n",
      "2021-10-08 20:51:07,780 - INFO - joeynmt.training - Epoch   2, Step:    41100, Batch Loss:     1.188464, Tokens per Sec:     4944, Lr: 0.000300\n",
      "2021-10-08 20:51:52,116 - INFO - joeynmt.training - Epoch   2, Step:    41200, Batch Loss:     1.218613, Tokens per Sec:     4868, Lr: 0.000300\n",
      "2021-10-08 20:52:38,468 - INFO - joeynmt.training - Epoch   2, Step:    41300, Batch Loss:     1.449875, Tokens per Sec:     4995, Lr: 0.000300\n",
      "2021-10-08 20:53:23,345 - INFO - joeynmt.training - Epoch   2, Step:    41400, Batch Loss:     1.196534, Tokens per Sec:     4778, Lr: 0.000300\n",
      "2021-10-08 20:54:08,751 - INFO - joeynmt.training - Epoch   2, Step:    41500, Batch Loss:     1.351331, Tokens per Sec:     4862, Lr: 0.000300\n",
      "2021-10-08 20:54:55,420 - INFO - joeynmt.training - Epoch   2, Step:    41600, Batch Loss:     1.332330, Tokens per Sec:     5057, Lr: 0.000300\n",
      "2021-10-08 20:55:41,708 - INFO - joeynmt.training - Epoch   2, Step:    41700, Batch Loss:     1.330677, Tokens per Sec:     5099, Lr: 0.000300\n",
      "2021-10-08 20:56:27,876 - INFO - joeynmt.training - Epoch   2, Step:    41800, Batch Loss:     1.213836, Tokens per Sec:     5027, Lr: 0.000300\n",
      "2021-10-08 20:57:13,236 - INFO - joeynmt.training - Epoch   2, Step:    41900, Batch Loss:     1.288069, Tokens per Sec:     5003, Lr: 0.000300\n",
      "2021-10-08 20:57:58,056 - INFO - joeynmt.training - Epoch   2, Step:    42000, Batch Loss:     1.277958, Tokens per Sec:     4904, Lr: 0.000300\n",
      "2021-10-08 20:58:44,189 - INFO - joeynmt.training - Epoch   2, Step:    42100, Batch Loss:     1.197253, Tokens per Sec:     5029, Lr: 0.000300\n",
      "2021-10-08 20:59:30,516 - INFO - joeynmt.training - Epoch   2, Step:    42200, Batch Loss:     1.295705, Tokens per Sec:     4932, Lr: 0.000300\n",
      "2021-10-08 21:00:17,316 - INFO - joeynmt.training - Epoch   2, Step:    42300, Batch Loss:     1.154061, Tokens per Sec:     4967, Lr: 0.000300\n",
      "2021-10-08 21:01:03,356 - INFO - joeynmt.training - Epoch   2, Step:    42400, Batch Loss:     1.160715, Tokens per Sec:     4912, Lr: 0.000300\n",
      "2021-10-08 21:01:49,193 - INFO - joeynmt.training - Epoch   2, Step:    42500, Batch Loss:     1.251693, Tokens per Sec:     4864, Lr: 0.000300\n",
      "2021-10-08 21:02:34,445 - INFO - joeynmt.training - Epoch   2, Step:    42600, Batch Loss:     1.089589, Tokens per Sec:     4943, Lr: 0.000300\n",
      "2021-10-08 21:03:19,872 - INFO - joeynmt.training - Epoch   2, Step:    42700, Batch Loss:     1.389626, Tokens per Sec:     4903, Lr: 0.000300\n",
      "2021-10-08 21:04:05,625 - INFO - joeynmt.training - Epoch   2, Step:    42800, Batch Loss:     1.370683, Tokens per Sec:     4952, Lr: 0.000300\n",
      "2021-10-08 21:04:51,520 - INFO - joeynmt.training - Epoch   2, Step:    42900, Batch Loss:     1.136699, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2021-10-08 21:05:37,665 - INFO - joeynmt.training - Epoch   2, Step:    43000, Batch Loss:     1.362346, Tokens per Sec:     5003, Lr: 0.000300\n",
      "2021-10-08 21:06:24,089 - INFO - joeynmt.training - Epoch   2, Step:    43100, Batch Loss:     1.262431, Tokens per Sec:     4924, Lr: 0.000300\n",
      "2021-10-08 21:07:09,241 - INFO - joeynmt.training - Epoch   2, Step:    43200, Batch Loss:     1.534232, Tokens per Sec:     5062, Lr: 0.000300\n",
      "2021-10-08 21:07:54,794 - INFO - joeynmt.training - Epoch   2, Step:    43300, Batch Loss:     1.532001, Tokens per Sec:     4934, Lr: 0.000300\n",
      "2021-10-08 21:08:40,615 - INFO - joeynmt.training - Epoch   2, Step:    43400, Batch Loss:     1.353430, Tokens per Sec:     4915, Lr: 0.000300\n",
      "2021-10-08 21:09:27,424 - INFO - joeynmt.training - Epoch   2, Step:    43500, Batch Loss:     1.339206, Tokens per Sec:     5003, Lr: 0.000300\n",
      "2021-10-08 21:10:13,863 - INFO - joeynmt.training - Epoch   2, Step:    43600, Batch Loss:     1.169843, Tokens per Sec:     4983, Lr: 0.000300\n",
      "2021-10-08 21:10:59,480 - INFO - joeynmt.training - Epoch   2, Step:    43700, Batch Loss:     1.274230, Tokens per Sec:     4903, Lr: 0.000300\n",
      "2021-10-08 21:11:45,370 - INFO - joeynmt.training - Epoch   2, Step:    43800, Batch Loss:     1.346590, Tokens per Sec:     5057, Lr: 0.000300\n",
      "2021-10-08 21:12:30,861 - INFO - joeynmt.training - Epoch   2, Step:    43900, Batch Loss:     1.290504, Tokens per Sec:     4920, Lr: 0.000300\n",
      "2021-10-08 21:13:16,713 - INFO - joeynmt.training - Epoch   2, Step:    44000, Batch Loss:     1.277126, Tokens per Sec:     4974, Lr: 0.000300\n",
      "2021-10-08 21:14:01,749 - INFO - joeynmt.training - Epoch   2, Step:    44100, Batch Loss:     1.270904, Tokens per Sec:     4878, Lr: 0.000300\n",
      "2021-10-08 21:14:47,513 - INFO - joeynmt.training - Epoch   2, Step:    44200, Batch Loss:     1.375321, Tokens per Sec:     4903, Lr: 0.000300\n",
      "2021-10-08 21:15:32,383 - INFO - joeynmt.training - Epoch   2, Step:    44300, Batch Loss:     1.315983, Tokens per Sec:     4909, Lr: 0.000300\n",
      "2021-10-08 21:16:17,907 - INFO - joeynmt.training - Epoch   2, Step:    44400, Batch Loss:     1.337043, Tokens per Sec:     4884, Lr: 0.000300\n",
      "2021-10-08 21:17:03,642 - INFO - joeynmt.training - Epoch   2, Step:    44500, Batch Loss:     1.236257, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-08 21:17:48,808 - INFO - joeynmt.training - Epoch   2, Step:    44600, Batch Loss:     1.281476, Tokens per Sec:     4878, Lr: 0.000300\n",
      "2021-10-08 21:18:34,528 - INFO - joeynmt.training - Epoch   2, Step:    44700, Batch Loss:     1.447950, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-08 21:19:20,200 - INFO - joeynmt.training - Epoch   2, Step:    44800, Batch Loss:     1.313666, Tokens per Sec:     4976, Lr: 0.000300\n",
      "2021-10-08 21:20:05,561 - INFO - joeynmt.training - Epoch   2, Step:    44900, Batch Loss:     1.315519, Tokens per Sec:     4938, Lr: 0.000300\n",
      "2021-10-08 21:20:51,228 - INFO - joeynmt.training - Epoch   2, Step:    45000, Batch Loss:     1.645373, Tokens per Sec:     5075, Lr: 0.000300\n",
      "2021-10-08 21:22:14,488 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 21:22:14,488 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 21:22:14,489 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 21:22:14,826 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 21:22:14,827 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 21:22:21,527 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 21:22:21,528 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 21:22:21,528 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 21:22:21,528 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at that time for the brother who took the lead in a congregation .\n",
      "2021-10-08 21:22:21,528 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 21:22:21,529 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 21:22:21,529 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 21:22:21,529 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-08 21:22:21,529 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 21:22:21,530 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 21:22:21,530 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 21:22:21,530 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-08 21:22:21,530 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 21:22:21,531 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 21:22:21,531 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 21:22:21,531 - INFO - joeynmt.training - \tHypothesis: HOW DOES MOLD WE SEPLE DOELVITS STEL ?\n",
      "2021-10-08 21:22:21,531 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    45000: bleu:  44.01, loss: 35266.6680, ppl:   3.2086, duration: 90.3027s\n",
      "2021-10-08 21:23:06,463 - INFO - joeynmt.training - Epoch   2, Step:    45100, Batch Loss:     1.180983, Tokens per Sec:     4897, Lr: 0.000300\n",
      "2021-10-08 21:23:51,776 - INFO - joeynmt.training - Epoch   2, Step:    45200, Batch Loss:     1.224892, Tokens per Sec:     5018, Lr: 0.000300\n",
      "2021-10-08 21:24:37,298 - INFO - joeynmt.training - Epoch   2, Step:    45300, Batch Loss:     1.231806, Tokens per Sec:     4963, Lr: 0.000300\n",
      "2021-10-08 21:25:23,778 - INFO - joeynmt.training - Epoch   2, Step:    45400, Batch Loss:     1.392493, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-08 21:26:09,720 - INFO - joeynmt.training - Epoch   2, Step:    45500, Batch Loss:     1.336183, Tokens per Sec:     4982, Lr: 0.000300\n",
      "2021-10-08 21:26:54,955 - INFO - joeynmt.training - Epoch   2, Step:    45600, Batch Loss:     1.332822, Tokens per Sec:     4836, Lr: 0.000300\n",
      "2021-10-08 21:27:41,262 - INFO - joeynmt.training - Epoch   2, Step:    45700, Batch Loss:     1.352953, Tokens per Sec:     4984, Lr: 0.000300\n",
      "2021-10-08 21:28:27,208 - INFO - joeynmt.training - Epoch   2, Step:    45800, Batch Loss:     1.286097, Tokens per Sec:     4960, Lr: 0.000300\n",
      "2021-10-08 21:29:12,452 - INFO - joeynmt.training - Epoch   2, Step:    45900, Batch Loss:     1.383783, Tokens per Sec:     4964, Lr: 0.000300\n",
      "2021-10-08 21:29:58,548 - INFO - joeynmt.training - Epoch   2, Step:    46000, Batch Loss:     1.254003, Tokens per Sec:     4953, Lr: 0.000300\n",
      "2021-10-08 21:30:43,695 - INFO - joeynmt.training - Epoch   2, Step:    46100, Batch Loss:     1.395190, Tokens per Sec:     4917, Lr: 0.000300\n",
      "2021-10-08 21:31:29,648 - INFO - joeynmt.training - Epoch   2, Step:    46200, Batch Loss:     1.414837, Tokens per Sec:     4934, Lr: 0.000300\n",
      "2021-10-08 21:32:15,251 - INFO - joeynmt.training - Epoch   2, Step:    46300, Batch Loss:     1.333826, Tokens per Sec:     4887, Lr: 0.000300\n",
      "2021-10-08 21:33:00,450 - INFO - joeynmt.training - Epoch   2, Step:    46400, Batch Loss:     1.501282, Tokens per Sec:     4822, Lr: 0.000300\n",
      "2021-10-08 21:33:46,464 - INFO - joeynmt.training - Epoch   2, Step:    46500, Batch Loss:     1.213637, Tokens per Sec:     4881, Lr: 0.000300\n",
      "2021-10-08 21:34:32,387 - INFO - joeynmt.training - Epoch   2, Step:    46600, Batch Loss:     1.216486, Tokens per Sec:     4927, Lr: 0.000300\n",
      "2021-10-08 21:35:18,391 - INFO - joeynmt.training - Epoch   2, Step:    46700, Batch Loss:     1.380061, Tokens per Sec:     4897, Lr: 0.000300\n",
      "2021-10-08 21:36:03,981 - INFO - joeynmt.training - Epoch   2, Step:    46800, Batch Loss:     1.426717, Tokens per Sec:     4844, Lr: 0.000300\n",
      "2021-10-08 21:36:50,621 - INFO - joeynmt.training - Epoch   2, Step:    46900, Batch Loss:     1.171518, Tokens per Sec:     4918, Lr: 0.000300\n",
      "2021-10-08 21:37:35,832 - INFO - joeynmt.training - Epoch   2, Step:    47000, Batch Loss:     1.266876, Tokens per Sec:     4874, Lr: 0.000300\n",
      "2021-10-08 21:38:22,403 - INFO - joeynmt.training - Epoch   2, Step:    47100, Batch Loss:     1.136362, Tokens per Sec:     5007, Lr: 0.000300\n",
      "2021-10-08 21:39:08,227 - INFO - joeynmt.training - Epoch   2, Step:    47200, Batch Loss:     1.283229, Tokens per Sec:     4887, Lr: 0.000300\n",
      "2021-10-08 21:39:55,054 - INFO - joeynmt.training - Epoch   2, Step:    47300, Batch Loss:     1.083131, Tokens per Sec:     4948, Lr: 0.000300\n",
      "2021-10-08 21:40:40,743 - INFO - joeynmt.training - Epoch   2, Step:    47400, Batch Loss:     1.411378, Tokens per Sec:     5014, Lr: 0.000300\n",
      "2021-10-08 21:41:26,404 - INFO - joeynmt.training - Epoch   2, Step:    47500, Batch Loss:     1.206971, Tokens per Sec:     4970, Lr: 0.000300\n",
      "2021-10-08 21:42:12,032 - INFO - joeynmt.training - Epoch   2, Step:    47600, Batch Loss:     1.228774, Tokens per Sec:     4924, Lr: 0.000300\n",
      "2021-10-08 21:42:58,117 - INFO - joeynmt.training - Epoch   2, Step:    47700, Batch Loss:     1.110501, Tokens per Sec:     5065, Lr: 0.000300\n",
      "2021-10-08 21:43:43,022 - INFO - joeynmt.training - Epoch   2, Step:    47800, Batch Loss:     1.269832, Tokens per Sec:     4944, Lr: 0.000300\n",
      "2021-10-08 21:44:27,698 - INFO - joeynmt.training - Epoch   2, Step:    47900, Batch Loss:     1.322580, Tokens per Sec:     4947, Lr: 0.000300\n",
      "2021-10-08 21:45:12,972 - INFO - joeynmt.training - Epoch   2, Step:    48000, Batch Loss:     1.405444, Tokens per Sec:     5036, Lr: 0.000300\n",
      "2021-10-08 21:45:58,647 - INFO - joeynmt.training - Epoch   2, Step:    48100, Batch Loss:     1.351711, Tokens per Sec:     4928, Lr: 0.000300\n",
      "2021-10-08 21:46:43,953 - INFO - joeynmt.training - Epoch   2, Step:    48200, Batch Loss:     1.342542, Tokens per Sec:     4782, Lr: 0.000300\n",
      "2021-10-08 21:47:29,658 - INFO - joeynmt.training - Epoch   2, Step:    48300, Batch Loss:     1.311383, Tokens per Sec:     4993, Lr: 0.000300\n",
      "2021-10-08 21:48:15,797 - INFO - joeynmt.training - Epoch   2, Step:    48400, Batch Loss:     1.340374, Tokens per Sec:     4902, Lr: 0.000300\n",
      "2021-10-08 21:49:01,269 - INFO - joeynmt.training - Epoch   2, Step:    48500, Batch Loss:     1.303895, Tokens per Sec:     4954, Lr: 0.000300\n",
      "2021-10-08 21:49:46,897 - INFO - joeynmt.training - Epoch   2, Step:    48600, Batch Loss:     1.268765, Tokens per Sec:     5049, Lr: 0.000300\n",
      "2021-10-08 21:50:31,536 - INFO - joeynmt.training - Epoch   2, Step:    48700, Batch Loss:     1.318425, Tokens per Sec:     4911, Lr: 0.000300\n",
      "2021-10-08 21:51:17,348 - INFO - joeynmt.training - Epoch   2, Step:    48800, Batch Loss:     1.321832, Tokens per Sec:     5080, Lr: 0.000300\n",
      "2021-10-08 21:52:03,261 - INFO - joeynmt.training - Epoch   2, Step:    48900, Batch Loss:     1.295344, Tokens per Sec:     4976, Lr: 0.000300\n",
      "2021-10-08 21:52:48,489 - INFO - joeynmt.training - Epoch   2, Step:    49000, Batch Loss:     1.069892, Tokens per Sec:     4974, Lr: 0.000300\n",
      "2021-10-08 21:53:35,162 - INFO - joeynmt.training - Epoch   2, Step:    49100, Batch Loss:     1.242071, Tokens per Sec:     4969, Lr: 0.000300\n",
      "2021-10-08 21:54:20,313 - INFO - joeynmt.training - Epoch   2, Step:    49200, Batch Loss:     1.160764, Tokens per Sec:     4885, Lr: 0.000300\n",
      "2021-10-08 21:55:05,704 - INFO - joeynmt.training - Epoch   2, Step:    49300, Batch Loss:     1.271798, Tokens per Sec:     4974, Lr: 0.000300\n",
      "2021-10-08 21:55:50,828 - INFO - joeynmt.training - Epoch   2, Step:    49400, Batch Loss:     1.270938, Tokens per Sec:     4937, Lr: 0.000300\n",
      "2021-10-08 21:56:36,236 - INFO - joeynmt.training - Epoch   2, Step:    49500, Batch Loss:     1.350462, Tokens per Sec:     4986, Lr: 0.000300\n",
      "2021-10-08 21:57:22,015 - INFO - joeynmt.training - Epoch   2, Step:    49600, Batch Loss:     1.350277, Tokens per Sec:     5152, Lr: 0.000300\n",
      "2021-10-08 21:58:08,034 - INFO - joeynmt.training - Epoch   2, Step:    49700, Batch Loss:     1.154616, Tokens per Sec:     5062, Lr: 0.000300\n",
      "2021-10-08 21:58:53,971 - INFO - joeynmt.training - Epoch   2, Step:    49800, Batch Loss:     1.205188, Tokens per Sec:     4956, Lr: 0.000300\n",
      "2021-10-08 21:59:39,521 - INFO - joeynmt.training - Epoch   2, Step:    49900, Batch Loss:     1.117940, Tokens per Sec:     5037, Lr: 0.000300\n",
      "2021-10-08 22:00:24,843 - INFO - joeynmt.training - Epoch   2, Step:    50000, Batch Loss:     1.285395, Tokens per Sec:     4887, Lr: 0.000300\n",
      "2021-10-08 22:01:48,781 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 22:01:48,789 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 22:01:48,789 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 22:01:49,131 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 22:01:49,131 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 22:01:55,301 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 22:01:55,302 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 22:01:55,302 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 22:01:55,302 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-08 22:01:55,302 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 22:01:55,302 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 22:01:55,303 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 22:01:55,303 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-08 22:01:55,303 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 22:01:55,303 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 22:01:55,303 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 22:01:55,303 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize reverence that Nabal was older than he was .\n",
      "2021-10-08 22:01:55,304 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 22:01:55,304 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 22:01:55,304 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 22:01:55,304 - INFO - joeynmt.training - \tHypothesis: HOW DO MOLD MET WE WE DEEEEEEATS ?\n",
      "2021-10-08 22:01:55,304 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    50000: bleu:  44.15, loss: 34401.1797, ppl:   3.1181, duration: 90.4609s\n",
      "2021-10-08 22:02:31,749 - INFO - joeynmt.training - Epoch   2: total training loss 16494.03\n",
      "2021-10-08 22:02:31,750 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-10-08 22:02:42,596 - INFO - joeynmt.training - Epoch   3, Step:    50100, Batch Loss:     1.170830, Tokens per Sec:     4346, Lr: 0.000300\n",
      "2021-10-08 22:03:28,786 - INFO - joeynmt.training - Epoch   3, Step:    50200, Batch Loss:     1.303845, Tokens per Sec:     5121, Lr: 0.000300\n",
      "2021-10-08 22:04:14,413 - INFO - joeynmt.training - Epoch   3, Step:    50300, Batch Loss:     1.140116, Tokens per Sec:     4785, Lr: 0.000300\n",
      "2021-10-08 22:05:00,866 - INFO - joeynmt.training - Epoch   3, Step:    50400, Batch Loss:     1.163148, Tokens per Sec:     5058, Lr: 0.000300\n",
      "2021-10-08 22:05:45,967 - INFO - joeynmt.training - Epoch   3, Step:    50500, Batch Loss:     1.281261, Tokens per Sec:     5014, Lr: 0.000300\n",
      "2021-10-08 22:06:30,832 - INFO - joeynmt.training - Epoch   3, Step:    50600, Batch Loss:     1.382427, Tokens per Sec:     4940, Lr: 0.000300\n",
      "2021-10-08 22:07:16,007 - INFO - joeynmt.training - Epoch   3, Step:    50700, Batch Loss:     1.113110, Tokens per Sec:     5012, Lr: 0.000300\n",
      "2021-10-08 22:08:00,973 - INFO - joeynmt.training - Epoch   3, Step:    50800, Batch Loss:     1.232284, Tokens per Sec:     4971, Lr: 0.000300\n",
      "2021-10-08 22:08:45,434 - INFO - joeynmt.training - Epoch   3, Step:    50900, Batch Loss:     1.108187, Tokens per Sec:     4910, Lr: 0.000300\n",
      "2021-10-08 22:09:31,285 - INFO - joeynmt.training - Epoch   3, Step:    51000, Batch Loss:     1.200330, Tokens per Sec:     5084, Lr: 0.000300\n",
      "2021-10-08 22:10:16,354 - INFO - joeynmt.training - Epoch   3, Step:    51100, Batch Loss:     1.372860, Tokens per Sec:     4925, Lr: 0.000300\n",
      "2021-10-08 22:11:01,857 - INFO - joeynmt.training - Epoch   3, Step:    51200, Batch Loss:     1.192110, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-08 22:11:47,597 - INFO - joeynmt.training - Epoch   3, Step:    51300, Batch Loss:     1.300875, Tokens per Sec:     5070, Lr: 0.000300\n",
      "2021-10-08 22:12:32,480 - INFO - joeynmt.training - Epoch   3, Step:    51400, Batch Loss:     1.097225, Tokens per Sec:     4863, Lr: 0.000300\n",
      "2021-10-08 22:13:17,618 - INFO - joeynmt.training - Epoch   3, Step:    51500, Batch Loss:     1.348981, Tokens per Sec:     4904, Lr: 0.000300\n",
      "2021-10-08 22:14:02,847 - INFO - joeynmt.training - Epoch   3, Step:    51600, Batch Loss:     1.305085, Tokens per Sec:     5012, Lr: 0.000300\n",
      "2021-10-08 22:14:48,273 - INFO - joeynmt.training - Epoch   3, Step:    51700, Batch Loss:     1.351762, Tokens per Sec:     5065, Lr: 0.000300\n",
      "2021-10-08 22:15:33,829 - INFO - joeynmt.training - Epoch   3, Step:    51800, Batch Loss:     1.195921, Tokens per Sec:     4968, Lr: 0.000300\n",
      "2021-10-08 22:16:18,901 - INFO - joeynmt.training - Epoch   3, Step:    51900, Batch Loss:     1.231586, Tokens per Sec:     4926, Lr: 0.000300\n",
      "2021-10-08 22:17:04,809 - INFO - joeynmt.training - Epoch   3, Step:    52000, Batch Loss:     1.302770, Tokens per Sec:     4985, Lr: 0.000300\n",
      "2021-10-08 22:17:50,040 - INFO - joeynmt.training - Epoch   3, Step:    52100, Batch Loss:     1.480741, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2021-10-08 22:18:35,488 - INFO - joeynmt.training - Epoch   3, Step:    52200, Batch Loss:     1.211558, Tokens per Sec:     4846, Lr: 0.000300\n",
      "2021-10-08 22:19:20,819 - INFO - joeynmt.training - Epoch   3, Step:    52300, Batch Loss:     1.257313, Tokens per Sec:     4931, Lr: 0.000300\n",
      "2021-10-08 22:20:06,174 - INFO - joeynmt.training - Epoch   3, Step:    52400, Batch Loss:     1.213590, Tokens per Sec:     4913, Lr: 0.000300\n",
      "2021-10-08 22:20:52,397 - INFO - joeynmt.training - Epoch   3, Step:    52500, Batch Loss:     1.187244, Tokens per Sec:     4982, Lr: 0.000300\n",
      "2021-10-08 22:21:37,729 - INFO - joeynmt.training - Epoch   3, Step:    52600, Batch Loss:     1.161627, Tokens per Sec:     4889, Lr: 0.000300\n",
      "2021-10-08 22:22:23,619 - INFO - joeynmt.training - Epoch   3, Step:    52700, Batch Loss:     1.276959, Tokens per Sec:     4954, Lr: 0.000300\n",
      "2021-10-08 22:23:08,751 - INFO - joeynmt.training - Epoch   3, Step:    52800, Batch Loss:     1.290556, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-08 22:23:54,557 - INFO - joeynmt.training - Epoch   3, Step:    52900, Batch Loss:     1.085278, Tokens per Sec:     5092, Lr: 0.000300\n",
      "2021-10-08 22:24:40,041 - INFO - joeynmt.training - Epoch   3, Step:    53000, Batch Loss:     1.394663, Tokens per Sec:     4945, Lr: 0.000300\n",
      "2021-10-08 22:25:25,515 - INFO - joeynmt.training - Epoch   3, Step:    53100, Batch Loss:     1.294188, Tokens per Sec:     4939, Lr: 0.000300\n",
      "2021-10-08 22:26:11,532 - INFO - joeynmt.training - Epoch   3, Step:    53200, Batch Loss:     1.185030, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-08 22:26:56,240 - INFO - joeynmt.training - Epoch   3, Step:    53300, Batch Loss:     1.171738, Tokens per Sec:     4963, Lr: 0.000300\n",
      "2021-10-08 22:27:42,010 - INFO - joeynmt.training - Epoch   3, Step:    53400, Batch Loss:     1.606364, Tokens per Sec:     4981, Lr: 0.000300\n",
      "2021-10-08 22:28:28,149 - INFO - joeynmt.training - Epoch   3, Step:    53500, Batch Loss:     1.455824, Tokens per Sec:     5029, Lr: 0.000300\n",
      "2021-10-08 22:29:14,122 - INFO - joeynmt.training - Epoch   3, Step:    53600, Batch Loss:     1.289907, Tokens per Sec:     5010, Lr: 0.000300\n",
      "2021-10-08 22:29:58,976 - INFO - joeynmt.training - Epoch   3, Step:    53700, Batch Loss:     1.331865, Tokens per Sec:     4946, Lr: 0.000300\n",
      "2021-10-08 22:30:45,370 - INFO - joeynmt.training - Epoch   3, Step:    53800, Batch Loss:     1.226250, Tokens per Sec:     4976, Lr: 0.000300\n",
      "2021-10-08 22:31:30,398 - INFO - joeynmt.training - Epoch   3, Step:    53900, Batch Loss:     1.298069, Tokens per Sec:     4860, Lr: 0.000300\n",
      "2021-10-08 22:32:16,611 - INFO - joeynmt.training - Epoch   3, Step:    54000, Batch Loss:     1.243921, Tokens per Sec:     5051, Lr: 0.000300\n",
      "2021-10-08 22:33:01,489 - INFO - joeynmt.training - Epoch   3, Step:    54100, Batch Loss:     1.128477, Tokens per Sec:     4864, Lr: 0.000300\n",
      "2021-10-08 22:33:46,472 - INFO - joeynmt.training - Epoch   3, Step:    54200, Batch Loss:     1.255925, Tokens per Sec:     4903, Lr: 0.000300\n",
      "2021-10-08 22:34:32,274 - INFO - joeynmt.training - Epoch   3, Step:    54300, Batch Loss:     1.208956, Tokens per Sec:     4869, Lr: 0.000300\n",
      "2021-10-08 22:35:18,878 - INFO - joeynmt.training - Epoch   3, Step:    54400, Batch Loss:     1.224055, Tokens per Sec:     4994, Lr: 0.000300\n",
      "2021-10-08 22:36:04,256 - INFO - joeynmt.training - Epoch   3, Step:    54500, Batch Loss:     1.131929, Tokens per Sec:     4781, Lr: 0.000300\n",
      "2021-10-08 22:36:50,019 - INFO - joeynmt.training - Epoch   3, Step:    54600, Batch Loss:     1.219250, Tokens per Sec:     4980, Lr: 0.000300\n",
      "2021-10-08 22:37:35,005 - INFO - joeynmt.training - Epoch   3, Step:    54700, Batch Loss:     1.290154, Tokens per Sec:     4874, Lr: 0.000300\n",
      "2021-10-08 22:38:20,924 - INFO - joeynmt.training - Epoch   3, Step:    54800, Batch Loss:     1.217733, Tokens per Sec:     4979, Lr: 0.000300\n",
      "2021-10-08 22:39:07,068 - INFO - joeynmt.training - Epoch   3, Step:    54900, Batch Loss:     1.289170, Tokens per Sec:     5068, Lr: 0.000300\n",
      "2021-10-08 22:39:52,242 - INFO - joeynmt.training - Epoch   3, Step:    55000, Batch Loss:     1.387563, Tokens per Sec:     4842, Lr: 0.000300\n",
      "2021-10-08 22:41:18,709 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 22:41:18,709 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 22:41:18,710 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 22:41:19,050 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 22:41:19,050 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 22:41:25,066 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 22:41:25,067 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 22:41:25,067 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 22:41:25,067 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-08 22:41:25,067 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 22:41:25,067 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 22:41:25,067 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 22:41:25,067 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-08 22:41:25,067 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 22:41:25,068 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 22:41:25,068 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 22:41:25,068 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize reverential that Nabal was older than he was .\n",
      "2021-10-08 22:41:25,068 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 22:41:25,068 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 22:41:25,068 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 22:41:25,068 - INFO - joeynmt.training - \tHypothesis: HOW DO MOST WE DEEEEEEEVERS ?\n",
      "2021-10-08 22:41:25,068 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    55000: bleu:  44.80, loss: 33955.3164, ppl:   3.0725, duration: 92.8256s\n",
      "2021-10-08 22:42:10,627 - INFO - joeynmt.training - Epoch   3, Step:    55100, Batch Loss:     1.244992, Tokens per Sec:     4834, Lr: 0.000300\n",
      "2021-10-08 22:42:56,253 - INFO - joeynmt.training - Epoch   3, Step:    55200, Batch Loss:     1.197241, Tokens per Sec:     4908, Lr: 0.000300\n",
      "2021-10-08 22:43:42,501 - INFO - joeynmt.training - Epoch   3, Step:    55300, Batch Loss:     1.229068, Tokens per Sec:     4970, Lr: 0.000300\n",
      "2021-10-08 22:44:29,337 - INFO - joeynmt.training - Epoch   3, Step:    55400, Batch Loss:     1.183384, Tokens per Sec:     4861, Lr: 0.000300\n",
      "2021-10-08 22:45:15,055 - INFO - joeynmt.training - Epoch   3, Step:    55500, Batch Loss:     1.262655, Tokens per Sec:     4894, Lr: 0.000300\n",
      "2021-10-08 22:46:00,924 - INFO - joeynmt.training - Epoch   3, Step:    55600, Batch Loss:     1.070832, Tokens per Sec:     4800, Lr: 0.000300\n",
      "2021-10-08 22:46:47,179 - INFO - joeynmt.training - Epoch   3, Step:    55700, Batch Loss:     1.419613, Tokens per Sec:     4927, Lr: 0.000300\n",
      "2021-10-08 22:47:34,250 - INFO - joeynmt.training - Epoch   3, Step:    55800, Batch Loss:     1.235750, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-08 22:48:20,044 - INFO - joeynmt.training - Epoch   3, Step:    55900, Batch Loss:     1.250290, Tokens per Sec:     4862, Lr: 0.000300\n",
      "2021-10-08 22:49:06,537 - INFO - joeynmt.training - Epoch   3, Step:    56000, Batch Loss:     1.195462, Tokens per Sec:     4989, Lr: 0.000300\n",
      "2021-10-08 22:49:52,952 - INFO - joeynmt.training - Epoch   3, Step:    56100, Batch Loss:     1.241336, Tokens per Sec:     4887, Lr: 0.000300\n",
      "2021-10-08 22:50:39,545 - INFO - joeynmt.training - Epoch   3, Step:    56200, Batch Loss:     1.188584, Tokens per Sec:     4964, Lr: 0.000300\n",
      "2021-10-08 22:51:25,448 - INFO - joeynmt.training - Epoch   3, Step:    56300, Batch Loss:     1.272929, Tokens per Sec:     4978, Lr: 0.000300\n",
      "2021-10-08 22:52:11,432 - INFO - joeynmt.training - Epoch   3, Step:    56400, Batch Loss:     1.251863, Tokens per Sec:     4920, Lr: 0.000300\n",
      "2021-10-08 22:52:58,293 - INFO - joeynmt.training - Epoch   3, Step:    56500, Batch Loss:     1.183342, Tokens per Sec:     4929, Lr: 0.000300\n",
      "2021-10-08 22:53:44,140 - INFO - joeynmt.training - Epoch   3, Step:    56600, Batch Loss:     1.222592, Tokens per Sec:     4946, Lr: 0.000300\n",
      "2021-10-08 22:54:30,110 - INFO - joeynmt.training - Epoch   3, Step:    56700, Batch Loss:     1.131497, Tokens per Sec:     4990, Lr: 0.000300\n",
      "2021-10-08 22:55:15,890 - INFO - joeynmt.training - Epoch   3, Step:    56800, Batch Loss:     1.383764, Tokens per Sec:     4787, Lr: 0.000300\n",
      "2021-10-08 22:56:02,612 - INFO - joeynmt.training - Epoch   3, Step:    56900, Batch Loss:     1.058289, Tokens per Sec:     4972, Lr: 0.000300\n",
      "2021-10-08 22:56:48,689 - INFO - joeynmt.training - Epoch   3, Step:    57000, Batch Loss:     0.963591, Tokens per Sec:     4962, Lr: 0.000300\n",
      "2021-10-08 22:57:34,520 - INFO - joeynmt.training - Epoch   3, Step:    57100, Batch Loss:     1.330492, Tokens per Sec:     4880, Lr: 0.000300\n",
      "2021-10-08 22:58:20,972 - INFO - joeynmt.training - Epoch   3, Step:    57200, Batch Loss:     1.324613, Tokens per Sec:     4846, Lr: 0.000300\n",
      "2021-10-08 22:59:07,400 - INFO - joeynmt.training - Epoch   3, Step:    57300, Batch Loss:     1.343995, Tokens per Sec:     4952, Lr: 0.000300\n",
      "2021-10-08 22:59:54,164 - INFO - joeynmt.training - Epoch   3, Step:    57400, Batch Loss:     1.243128, Tokens per Sec:     4898, Lr: 0.000300\n",
      "2021-10-08 23:00:39,330 - INFO - joeynmt.training - Epoch   3, Step:    57500, Batch Loss:     1.106210, Tokens per Sec:     4838, Lr: 0.000300\n",
      "2021-10-08 23:01:25,624 - INFO - joeynmt.training - Epoch   3, Step:    57600, Batch Loss:     1.113697, Tokens per Sec:     4932, Lr: 0.000300\n",
      "2021-10-08 23:02:11,835 - INFO - joeynmt.training - Epoch   3, Step:    57700, Batch Loss:     1.164105, Tokens per Sec:     4905, Lr: 0.000300\n",
      "2021-10-08 23:02:57,342 - INFO - joeynmt.training - Epoch   3, Step:    57800, Batch Loss:     1.276757, Tokens per Sec:     4954, Lr: 0.000300\n",
      "2021-10-08 23:03:42,937 - INFO - joeynmt.training - Epoch   3, Step:    57900, Batch Loss:     1.200918, Tokens per Sec:     5039, Lr: 0.000300\n",
      "2021-10-08 23:04:28,397 - INFO - joeynmt.training - Epoch   3, Step:    58000, Batch Loss:     1.293834, Tokens per Sec:     5044, Lr: 0.000300\n",
      "2021-10-08 23:05:14,022 - INFO - joeynmt.training - Epoch   3, Step:    58100, Batch Loss:     1.132415, Tokens per Sec:     5066, Lr: 0.000300\n",
      "2021-10-08 23:05:59,960 - INFO - joeynmt.training - Epoch   3, Step:    58200, Batch Loss:     1.239880, Tokens per Sec:     5025, Lr: 0.000300\n",
      "2021-10-08 23:06:45,366 - INFO - joeynmt.training - Epoch   3, Step:    58300, Batch Loss:     1.099416, Tokens per Sec:     4975, Lr: 0.000300\n",
      "2021-10-08 23:07:31,954 - INFO - joeynmt.training - Epoch   3, Step:    58400, Batch Loss:     1.303077, Tokens per Sec:     4776, Lr: 0.000300\n",
      "2021-10-08 23:08:18,541 - INFO - joeynmt.training - Epoch   3, Step:    58500, Batch Loss:     1.321946, Tokens per Sec:     4881, Lr: 0.000300\n",
      "2021-10-08 23:09:04,209 - INFO - joeynmt.training - Epoch   3, Step:    58600, Batch Loss:     1.268736, Tokens per Sec:     4825, Lr: 0.000300\n",
      "2021-10-08 23:09:49,954 - INFO - joeynmt.training - Epoch   3, Step:    58700, Batch Loss:     1.298822, Tokens per Sec:     4834, Lr: 0.000300\n",
      "2021-10-08 23:10:35,994 - INFO - joeynmt.training - Epoch   3, Step:    58800, Batch Loss:     1.149480, Tokens per Sec:     4931, Lr: 0.000300\n",
      "2021-10-08 23:11:21,645 - INFO - joeynmt.training - Epoch   3, Step:    58900, Batch Loss:     1.108661, Tokens per Sec:     4836, Lr: 0.000300\n",
      "2021-10-08 23:12:08,029 - INFO - joeynmt.training - Epoch   3, Step:    59000, Batch Loss:     1.156278, Tokens per Sec:     4973, Lr: 0.000300\n",
      "2021-10-08 23:12:54,473 - INFO - joeynmt.training - Epoch   3, Step:    59100, Batch Loss:     1.325406, Tokens per Sec:     4924, Lr: 0.000300\n",
      "2021-10-08 23:13:40,322 - INFO - joeynmt.training - Epoch   3, Step:    59200, Batch Loss:     1.264583, Tokens per Sec:     4911, Lr: 0.000300\n",
      "2021-10-08 23:14:26,873 - INFO - joeynmt.training - Epoch   3, Step:    59300, Batch Loss:     1.274971, Tokens per Sec:     4955, Lr: 0.000300\n",
      "2021-10-08 23:15:12,890 - INFO - joeynmt.training - Epoch   3, Step:    59400, Batch Loss:     1.231135, Tokens per Sec:     4897, Lr: 0.000300\n",
      "2021-10-08 23:15:58,817 - INFO - joeynmt.training - Epoch   3, Step:    59500, Batch Loss:     1.189504, Tokens per Sec:     4984, Lr: 0.000300\n",
      "2021-10-08 23:16:44,796 - INFO - joeynmt.training - Epoch   3, Step:    59600, Batch Loss:     1.278088, Tokens per Sec:     5017, Lr: 0.000300\n",
      "2021-10-08 23:17:30,449 - INFO - joeynmt.training - Epoch   3, Step:    59700, Batch Loss:     1.199443, Tokens per Sec:     4835, Lr: 0.000300\n",
      "2021-10-08 23:18:16,797 - INFO - joeynmt.training - Epoch   3, Step:    59800, Batch Loss:     1.061616, Tokens per Sec:     4902, Lr: 0.000300\n",
      "2021-10-08 23:19:02,201 - INFO - joeynmt.training - Epoch   3, Step:    59900, Batch Loss:     1.271985, Tokens per Sec:     4817, Lr: 0.000300\n",
      "2021-10-08 23:19:47,924 - INFO - joeynmt.training - Epoch   3, Step:    60000, Batch Loss:     1.290603, Tokens per Sec:     4892, Lr: 0.000300\n",
      "2021-10-08 23:21:11,376 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-08 23:21:11,376 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-08 23:21:11,376 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-08 23:21:11,712 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-08 23:21:11,712 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-08 23:21:18,484 - INFO - joeynmt.training - Example #0\n",
      "2021-10-08 23:21:18,485 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-08 23:21:18,485 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-08 23:21:18,485 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-08 23:21:18,485 - INFO - joeynmt.training - Example #1\n",
      "2021-10-08 23:21:18,486 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-08 23:21:18,486 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-08 23:21:18,486 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-08 23:21:18,486 - INFO - joeynmt.training - Example #2\n",
      "2021-10-08 23:21:18,487 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-08 23:21:18,487 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-08 23:21:18,487 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-08 23:21:18,487 - INFO - joeynmt.training - Example #3\n",
      "2021-10-08 23:21:18,487 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-08 23:21:18,487 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-08 23:21:18,488 - INFO - joeynmt.training - \tHypothesis: HOW WILL MOLD WE WE WITH ?\n",
      "2021-10-08 23:21:18,488 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    60000: bleu:  45.70, loss: 33105.2969, ppl:   2.9874, duration: 90.5633s\n",
      "2021-10-08 23:22:05,036 - INFO - joeynmt.training - Epoch   3, Step:    60100, Batch Loss:     1.241823, Tokens per Sec:     4971, Lr: 0.000300\n",
      "2021-10-08 23:22:50,658 - INFO - joeynmt.training - Epoch   3, Step:    60200, Batch Loss:     1.342085, Tokens per Sec:     4937, Lr: 0.000300\n",
      "2021-10-08 23:23:36,925 - INFO - joeynmt.training - Epoch   3, Step:    60300, Batch Loss:     1.310011, Tokens per Sec:     4946, Lr: 0.000300\n",
      "2021-10-08 23:24:22,968 - INFO - joeynmt.training - Epoch   3, Step:    60400, Batch Loss:     1.181089, Tokens per Sec:     4960, Lr: 0.000300\n",
      "2021-10-08 23:25:08,154 - INFO - joeynmt.training - Epoch   3, Step:    60500, Batch Loss:     1.166600, Tokens per Sec:     4883, Lr: 0.000300\n",
      "2021-10-08 23:25:53,918 - INFO - joeynmt.training - Epoch   3, Step:    60600, Batch Loss:     1.377212, Tokens per Sec:     4847, Lr: 0.000300\n",
      "2021-10-08 23:26:40,320 - INFO - joeynmt.training - Epoch   3, Step:    60700, Batch Loss:     1.291031, Tokens per Sec:     4958, Lr: 0.000300\n",
      "2021-10-08 23:27:26,568 - INFO - joeynmt.training - Epoch   3, Step:    60800, Batch Loss:     1.279567, Tokens per Sec:     5015, Lr: 0.000300\n",
      "2021-10-08 23:28:12,670 - INFO - joeynmt.training - Epoch   3, Step:    60900, Batch Loss:     1.277548, Tokens per Sec:     5054, Lr: 0.000300\n",
      "2021-10-08 23:28:58,928 - INFO - joeynmt.training - Epoch   3, Step:    61000, Batch Loss:     1.237616, Tokens per Sec:     4934, Lr: 0.000300\n",
      "2021-10-08 23:29:45,043 - INFO - joeynmt.training - Epoch   3, Step:    61100, Batch Loss:     1.284841, Tokens per Sec:     4999, Lr: 0.000300\n",
      "2021-10-08 23:30:31,008 - INFO - joeynmt.training - Epoch   3, Step:    61200, Batch Loss:     1.255551, Tokens per Sec:     4870, Lr: 0.000300\n",
      "2021-10-08 23:31:17,025 - INFO - joeynmt.training - Epoch   3, Step:    61300, Batch Loss:     1.248193, Tokens per Sec:     4808, Lr: 0.000300\n",
      "2021-10-08 23:32:02,497 - INFO - joeynmt.training - Epoch   3, Step:    61400, Batch Loss:     1.415961, Tokens per Sec:     4922, Lr: 0.000300\n",
      "2021-10-08 23:32:48,667 - INFO - joeynmt.training - Epoch   3, Step:    61500, Batch Loss:     1.388146, Tokens per Sec:     4933, Lr: 0.000300\n",
      "2021-10-08 23:33:35,033 - INFO - joeynmt.training - Epoch   3, Step:    61600, Batch Loss:     1.333660, Tokens per Sec:     4991, Lr: 0.000300\n",
      "2021-10-08 23:34:20,133 - INFO - joeynmt.training - Epoch   3, Step:    61700, Batch Loss:     1.237466, Tokens per Sec:     4812, Lr: 0.000300\n",
      "2021-10-08 23:35:05,356 - INFO - joeynmt.training - Epoch   3, Step:    61800, Batch Loss:     1.135919, Tokens per Sec:     4842, Lr: 0.000300\n",
      "2021-10-08 23:35:51,245 - INFO - joeynmt.training - Epoch   3, Step:    61900, Batch Loss:     1.282602, Tokens per Sec:     5014, Lr: 0.000300\n",
      "2021-10-08 23:36:36,921 - INFO - joeynmt.training - Epoch   3, Step:    62000, Batch Loss:     1.299203, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2021-10-08 23:37:23,480 - INFO - joeynmt.training - Epoch   3, Step:    62100, Batch Loss:     1.238259, Tokens per Sec:     5109, Lr: 0.000300\n",
      "2021-10-08 23:38:08,684 - INFO - joeynmt.training - Epoch   3, Step:    62200, Batch Loss:     1.357343, Tokens per Sec:     4944, Lr: 0.000300\n",
      "2021-10-08 23:38:54,332 - INFO - joeynmt.training - Epoch   3, Step:    62300, Batch Loss:     1.113189, Tokens per Sec:     5113, Lr: 0.000300\n",
      "2021-10-08 23:39:39,644 - INFO - joeynmt.training - Epoch   3, Step:    62400, Batch Loss:     1.142131, Tokens per Sec:     4941, Lr: 0.000300\n",
      "2021-10-08 23:40:25,561 - INFO - joeynmt.training - Epoch   3, Step:    62500, Batch Loss:     1.294337, Tokens per Sec:     4860, Lr: 0.000300\n",
      "2021-10-08 23:41:06,705 - INFO - joeynmt.training - Epoch   3: total training loss 15613.11\n",
      "2021-10-08 23:41:06,706 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-10-08 23:41:13,684 - INFO - joeynmt.training - Epoch   4, Step:    62600, Batch Loss:     1.245264, Tokens per Sec:     4090, Lr: 0.000300\n",
      "2021-10-08 23:41:59,656 - INFO - joeynmt.training - Epoch   4, Step:    62700, Batch Loss:     1.070283, Tokens per Sec:     4848, Lr: 0.000300\n",
      "2021-10-08 23:42:45,473 - INFO - joeynmt.training - Epoch   4, Step:    62800, Batch Loss:     1.336910, Tokens per Sec:     4795, Lr: 0.000300\n",
      "2021-10-08 23:43:31,633 - INFO - joeynmt.training - Epoch   4, Step:    62900, Batch Loss:     1.185727, Tokens per Sec:     4903, Lr: 0.000300\n",
      "2021-10-08 23:44:18,046 - INFO - joeynmt.training - Epoch   4, Step:    63000, Batch Loss:     0.890528, Tokens per Sec:     4921, Lr: 0.000300\n",
      "2021-10-08 23:45:03,460 - INFO - joeynmt.training - Epoch   4, Step:    63100, Batch Loss:     1.209481, Tokens per Sec:     4910, Lr: 0.000300\n",
      "2021-10-08 23:45:49,921 - INFO - joeynmt.training - Epoch   4, Step:    63200, Batch Loss:     1.151674, Tokens per Sec:     4919, Lr: 0.000300\n",
      "2021-10-08 23:46:35,520 - INFO - joeynmt.training - Epoch   4, Step:    63300, Batch Loss:     1.260550, Tokens per Sec:     4931, Lr: 0.000300\n",
      "2021-10-08 23:47:21,439 - INFO - joeynmt.training - Epoch   4, Step:    63400, Batch Loss:     1.188768, Tokens per Sec:     4805, Lr: 0.000300\n",
      "2021-10-08 23:48:06,729 - INFO - joeynmt.training - Epoch   4, Step:    63500, Batch Loss:     1.318216, Tokens per Sec:     4856, Lr: 0.000300\n",
      "2021-10-08 23:48:52,421 - INFO - joeynmt.training - Epoch   4, Step:    63600, Batch Loss:     1.166832, Tokens per Sec:     4948, Lr: 0.000300\n",
      "2021-10-08 23:49:38,497 - INFO - joeynmt.training - Epoch   4, Step:    63700, Batch Loss:     1.335260, Tokens per Sec:     4982, Lr: 0.000300\n",
      "2021-10-08 23:50:24,003 - INFO - joeynmt.training - Epoch   4, Step:    63800, Batch Loss:     1.114231, Tokens per Sec:     4928, Lr: 0.000300\n",
      "2021-10-08 23:51:10,047 - INFO - joeynmt.training - Epoch   4, Step:    63900, Batch Loss:     1.179348, Tokens per Sec:     5014, Lr: 0.000300\n",
      "2021-10-08 23:51:55,655 - INFO - joeynmt.training - Epoch   4, Step:    64000, Batch Loss:     1.302517, Tokens per Sec:     4882, Lr: 0.000300\n",
      "2021-10-08 23:52:41,845 - INFO - joeynmt.training - Epoch   4, Step:    64100, Batch Loss:     1.298108, Tokens per Sec:     5019, Lr: 0.000300\n",
      "2021-10-08 23:53:27,566 - INFO - joeynmt.training - Epoch   4, Step:    64200, Batch Loss:     1.198897, Tokens per Sec:     4939, Lr: 0.000300\n",
      "2021-10-08 23:54:13,314 - INFO - joeynmt.training - Epoch   4, Step:    64300, Batch Loss:     1.122193, Tokens per Sec:     4953, Lr: 0.000300\n",
      "2021-10-08 23:54:59,320 - INFO - joeynmt.training - Epoch   4, Step:    64400, Batch Loss:     1.140735, Tokens per Sec:     4958, Lr: 0.000300\n",
      "2021-10-08 23:55:45,592 - INFO - joeynmt.training - Epoch   4, Step:    64500, Batch Loss:     1.243472, Tokens per Sec:     4785, Lr: 0.000300\n",
      "2021-10-08 23:56:31,320 - INFO - joeynmt.training - Epoch   4, Step:    64600, Batch Loss:     1.286736, Tokens per Sec:     4848, Lr: 0.000300\n",
      "2021-10-08 23:57:17,742 - INFO - joeynmt.training - Epoch   4, Step:    64700, Batch Loss:     1.177331, Tokens per Sec:     5001, Lr: 0.000300\n",
      "2021-10-08 23:58:02,939 - INFO - joeynmt.training - Epoch   4, Step:    64800, Batch Loss:     1.219969, Tokens per Sec:     4901, Lr: 0.000300\n",
      "2021-10-08 23:58:48,238 - INFO - joeynmt.training - Epoch   4, Step:    64900, Batch Loss:     1.242147, Tokens per Sec:     4924, Lr: 0.000300\n",
      "2021-10-08 23:59:34,812 - INFO - joeynmt.training - Epoch   4, Step:    65000, Batch Loss:     1.300578, Tokens per Sec:     4914, Lr: 0.000300\n",
      "2021-10-09 00:01:02,275 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 00:01:02,275 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 00:01:02,276 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 00:01:02,612 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 00:01:02,612 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 00:01:08,612 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 00:01:08,613 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 00:01:08,613 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 00:01:08,613 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at that time for the brother who took the lead in a congregation .\n",
      "2021-10-09 00:01:08,613 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 00:01:08,613 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 00:01:08,614 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 00:01:08,614 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 00:01:08,614 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 00:01:08,614 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 00:01:08,614 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 00:01:08,614 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 00:01:08,614 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 00:01:08,615 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 00:01:08,615 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 00:01:08,615 - INFO - joeynmt.training - \tHypothesis: HOW DO MOST WE WE WE TO BEEEECTLY ?\n",
      "2021-10-09 00:01:08,615 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    65000: bleu:  46.11, loss: 32428.8203, ppl:   2.9213, duration: 93.8020s\n",
      "2021-10-09 00:01:54,892 - INFO - joeynmt.training - Epoch   4, Step:    65100, Batch Loss:     1.286001, Tokens per Sec:     4803, Lr: 0.000300\n",
      "2021-10-09 00:02:41,181 - INFO - joeynmt.training - Epoch   4, Step:    65200, Batch Loss:     1.180414, Tokens per Sec:     4910, Lr: 0.000300\n",
      "2021-10-09 00:03:27,516 - INFO - joeynmt.training - Epoch   4, Step:    65300, Batch Loss:     1.272397, Tokens per Sec:     4982, Lr: 0.000300\n",
      "2021-10-09 00:04:13,838 - INFO - joeynmt.training - Epoch   4, Step:    65400, Batch Loss:     1.204906, Tokens per Sec:     4948, Lr: 0.000300\n",
      "2021-10-09 00:04:59,880 - INFO - joeynmt.training - Epoch   4, Step:    65500, Batch Loss:     1.220480, Tokens per Sec:     4974, Lr: 0.000300\n",
      "2021-10-09 00:05:46,368 - INFO - joeynmt.training - Epoch   4, Step:    65600, Batch Loss:     1.288991, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-09 00:06:32,747 - INFO - joeynmt.training - Epoch   4, Step:    65700, Batch Loss:     0.997625, Tokens per Sec:     5024, Lr: 0.000300\n",
      "2021-10-09 00:07:18,130 - INFO - joeynmt.training - Epoch   4, Step:    65800, Batch Loss:     1.284235, Tokens per Sec:     4969, Lr: 0.000300\n",
      "2021-10-09 00:08:03,319 - INFO - joeynmt.training - Epoch   4, Step:    65900, Batch Loss:     1.187923, Tokens per Sec:     4859, Lr: 0.000300\n",
      "2021-10-09 00:08:49,865 - INFO - joeynmt.training - Epoch   4, Step:    66000, Batch Loss:     1.274742, Tokens per Sec:     5064, Lr: 0.000300\n",
      "2021-10-09 00:09:36,091 - INFO - joeynmt.training - Epoch   4, Step:    66100, Batch Loss:     1.153257, Tokens per Sec:     5023, Lr: 0.000300\n",
      "2021-10-09 00:10:21,507 - INFO - joeynmt.training - Epoch   4, Step:    66200, Batch Loss:     1.127686, Tokens per Sec:     4907, Lr: 0.000300\n",
      "2021-10-09 00:11:07,195 - INFO - joeynmt.training - Epoch   4, Step:    66300, Batch Loss:     1.205083, Tokens per Sec:     4937, Lr: 0.000300\n",
      "2021-10-09 00:11:53,246 - INFO - joeynmt.training - Epoch   4, Step:    66400, Batch Loss:     1.244674, Tokens per Sec:     4970, Lr: 0.000300\n",
      "2021-10-09 00:12:38,887 - INFO - joeynmt.training - Epoch   4, Step:    66500, Batch Loss:     1.247583, Tokens per Sec:     4956, Lr: 0.000300\n",
      "2021-10-09 00:13:23,886 - INFO - joeynmt.training - Epoch   4, Step:    66600, Batch Loss:     1.305849, Tokens per Sec:     4839, Lr: 0.000300\n",
      "2021-10-09 00:14:09,140 - INFO - joeynmt.training - Epoch   4, Step:    66700, Batch Loss:     1.230479, Tokens per Sec:     4935, Lr: 0.000300\n",
      "2021-10-09 00:14:54,373 - INFO - joeynmt.training - Epoch   4, Step:    66800, Batch Loss:     1.194920, Tokens per Sec:     5024, Lr: 0.000300\n",
      "2021-10-09 00:15:39,853 - INFO - joeynmt.training - Epoch   4, Step:    66900, Batch Loss:     1.154963, Tokens per Sec:     4883, Lr: 0.000300\n",
      "2021-10-09 00:16:26,060 - INFO - joeynmt.training - Epoch   4, Step:    67000, Batch Loss:     1.191887, Tokens per Sec:     5062, Lr: 0.000300\n",
      "2021-10-09 00:17:10,432 - INFO - joeynmt.training - Epoch   4, Step:    67100, Batch Loss:     1.226721, Tokens per Sec:     4862, Lr: 0.000300\n",
      "2021-10-09 00:17:55,365 - INFO - joeynmt.training - Epoch   4, Step:    67200, Batch Loss:     1.199170, Tokens per Sec:     4964, Lr: 0.000300\n",
      "2021-10-09 00:18:40,480 - INFO - joeynmt.training - Epoch   4, Step:    67300, Batch Loss:     1.115364, Tokens per Sec:     4944, Lr: 0.000300\n",
      "2021-10-09 00:19:26,884 - INFO - joeynmt.training - Epoch   4, Step:    67400, Batch Loss:     1.143335, Tokens per Sec:     4952, Lr: 0.000300\n",
      "2021-10-09 00:20:13,216 - INFO - joeynmt.training - Epoch   4, Step:    67500, Batch Loss:     1.237888, Tokens per Sec:     5032, Lr: 0.000300\n",
      "2021-10-09 00:20:58,501 - INFO - joeynmt.training - Epoch   4, Step:    67600, Batch Loss:     1.278146, Tokens per Sec:     4916, Lr: 0.000300\n",
      "2021-10-09 00:21:44,901 - INFO - joeynmt.training - Epoch   4, Step:    67700, Batch Loss:     1.278522, Tokens per Sec:     5039, Lr: 0.000300\n",
      "2021-10-09 00:22:30,635 - INFO - joeynmt.training - Epoch   4, Step:    67800, Batch Loss:     1.192973, Tokens per Sec:     4862, Lr: 0.000300\n",
      "2021-10-09 00:23:16,008 - INFO - joeynmt.training - Epoch   4, Step:    67900, Batch Loss:     1.145149, Tokens per Sec:     4871, Lr: 0.000300\n",
      "2021-10-09 00:24:00,940 - INFO - joeynmt.training - Epoch   4, Step:    68000, Batch Loss:     1.385660, Tokens per Sec:     4965, Lr: 0.000300\n",
      "2021-10-09 00:24:47,227 - INFO - joeynmt.training - Epoch   4, Step:    68100, Batch Loss:     1.113960, Tokens per Sec:     5007, Lr: 0.000300\n",
      "2021-10-09 00:25:33,013 - INFO - joeynmt.training - Epoch   4, Step:    68200, Batch Loss:     1.102209, Tokens per Sec:     4920, Lr: 0.000300\n",
      "2021-10-09 00:26:18,008 - INFO - joeynmt.training - Epoch   4, Step:    68300, Batch Loss:     1.149164, Tokens per Sec:     4883, Lr: 0.000300\n",
      "2021-10-09 00:27:04,074 - INFO - joeynmt.training - Epoch   4, Step:    68400, Batch Loss:     1.152207, Tokens per Sec:     5066, Lr: 0.000300\n",
      "2021-10-09 00:27:50,076 - INFO - joeynmt.training - Epoch   4, Step:    68500, Batch Loss:     1.196062, Tokens per Sec:     5054, Lr: 0.000300\n",
      "2021-10-09 00:28:35,728 - INFO - joeynmt.training - Epoch   4, Step:    68600, Batch Loss:     0.970465, Tokens per Sec:     4932, Lr: 0.000300\n",
      "2021-10-09 00:29:21,212 - INFO - joeynmt.training - Epoch   4, Step:    68700, Batch Loss:     1.187959, Tokens per Sec:     4904, Lr: 0.000300\n",
      "2021-10-09 00:30:07,164 - INFO - joeynmt.training - Epoch   4, Step:    68800, Batch Loss:     1.264211, Tokens per Sec:     4990, Lr: 0.000300\n",
      "2021-10-09 00:30:52,991 - INFO - joeynmt.training - Epoch   4, Step:    68900, Batch Loss:     1.235193, Tokens per Sec:     5028, Lr: 0.000300\n",
      "2021-10-09 00:31:39,048 - INFO - joeynmt.training - Epoch   4, Step:    69000, Batch Loss:     1.244005, Tokens per Sec:     4866, Lr: 0.000300\n",
      "2021-10-09 00:32:24,784 - INFO - joeynmt.training - Epoch   4, Step:    69100, Batch Loss:     1.249200, Tokens per Sec:     4948, Lr: 0.000300\n",
      "2021-10-09 00:33:10,072 - INFO - joeynmt.training - Epoch   4, Step:    69200, Batch Loss:     1.268590, Tokens per Sec:     4875, Lr: 0.000300\n",
      "2021-10-09 00:33:55,328 - INFO - joeynmt.training - Epoch   4, Step:    69300, Batch Loss:     1.183669, Tokens per Sec:     5028, Lr: 0.000300\n",
      "2021-10-09 00:34:40,932 - INFO - joeynmt.training - Epoch   4, Step:    69400, Batch Loss:     1.021888, Tokens per Sec:     4997, Lr: 0.000300\n",
      "2021-10-09 00:35:27,228 - INFO - joeynmt.training - Epoch   4, Step:    69500, Batch Loss:     1.112119, Tokens per Sec:     4904, Lr: 0.000300\n",
      "2021-10-09 00:36:13,460 - INFO - joeynmt.training - Epoch   4, Step:    69600, Batch Loss:     1.349661, Tokens per Sec:     4908, Lr: 0.000300\n",
      "2021-10-09 00:36:59,060 - INFO - joeynmt.training - Epoch   4, Step:    69700, Batch Loss:     1.291766, Tokens per Sec:     4900, Lr: 0.000300\n",
      "2021-10-09 00:37:44,133 - INFO - joeynmt.training - Epoch   4, Step:    69800, Batch Loss:     1.070341, Tokens per Sec:     4866, Lr: 0.000300\n",
      "2021-10-09 00:38:29,865 - INFO - joeynmt.training - Epoch   4, Step:    69900, Batch Loss:     1.045021, Tokens per Sec:     4954, Lr: 0.000300\n",
      "2021-10-09 00:39:16,144 - INFO - joeynmt.training - Epoch   4, Step:    70000, Batch Loss:     1.122983, Tokens per Sec:     5069, Lr: 0.000300\n",
      "2021-10-09 00:40:40,321 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 00:40:40,322 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 00:40:40,322 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 00:40:40,662 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 00:40:40,662 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 00:40:47,352 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 00:40:47,352 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 00:40:47,353 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 00:40:47,353 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term then used for the brother who took the lead in a congregation .\n",
      "2021-10-09 00:40:47,353 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 00:40:47,353 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 00:40:47,353 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 00:40:47,354 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 00:40:47,354 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 00:40:47,354 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 00:40:47,354 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 00:40:47,354 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 00:40:47,355 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 00:40:47,355 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 00:40:47,355 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 00:40:47,355 - INFO - joeynmt.training - \tHypothesis: HOW DO MOST WE WE WE PROVERSITED DOLVITS ?\n",
      "2021-10-09 00:40:47,355 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    70000: bleu:  46.56, loss: 32225.1758, ppl:   2.9017, duration: 91.2106s\n",
      "2021-10-09 00:41:32,538 - INFO - joeynmt.training - Epoch   4, Step:    70100, Batch Loss:     1.205925, Tokens per Sec:     4933, Lr: 0.000300\n",
      "2021-10-09 00:42:17,992 - INFO - joeynmt.training - Epoch   4, Step:    70200, Batch Loss:     1.098374, Tokens per Sec:     4880, Lr: 0.000300\n",
      "2021-10-09 00:43:03,584 - INFO - joeynmt.training - Epoch   4, Step:    70300, Batch Loss:     1.273422, Tokens per Sec:     4947, Lr: 0.000300\n",
      "2021-10-09 00:43:49,036 - INFO - joeynmt.training - Epoch   4, Step:    70400, Batch Loss:     1.283187, Tokens per Sec:     4904, Lr: 0.000300\n",
      "2021-10-09 00:44:35,092 - INFO - joeynmt.training - Epoch   4, Step:    70500, Batch Loss:     1.112722, Tokens per Sec:     4913, Lr: 0.000300\n",
      "2021-10-09 00:45:20,317 - INFO - joeynmt.training - Epoch   4, Step:    70600, Batch Loss:     1.094523, Tokens per Sec:     4807, Lr: 0.000300\n",
      "2021-10-09 00:46:06,741 - INFO - joeynmt.training - Epoch   4, Step:    70700, Batch Loss:     1.099266, Tokens per Sec:     4899, Lr: 0.000300\n",
      "2021-10-09 00:46:53,063 - INFO - joeynmt.training - Epoch   4, Step:    70800, Batch Loss:     1.357963, Tokens per Sec:     4993, Lr: 0.000300\n",
      "2021-10-09 00:47:38,354 - INFO - joeynmt.training - Epoch   4, Step:    70900, Batch Loss:     1.154932, Tokens per Sec:     4872, Lr: 0.000300\n",
      "2021-10-09 00:48:24,724 - INFO - joeynmt.training - Epoch   4, Step:    71000, Batch Loss:     1.376852, Tokens per Sec:     4951, Lr: 0.000300\n",
      "2021-10-09 00:49:09,887 - INFO - joeynmt.training - Epoch   4, Step:    71100, Batch Loss:     1.231948, Tokens per Sec:     4961, Lr: 0.000300\n",
      "2021-10-09 00:49:55,652 - INFO - joeynmt.training - Epoch   4, Step:    71200, Batch Loss:     1.113487, Tokens per Sec:     4874, Lr: 0.000300\n",
      "2021-10-09 00:50:41,570 - INFO - joeynmt.training - Epoch   4, Step:    71300, Batch Loss:     1.103653, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-09 00:51:27,178 - INFO - joeynmt.training - Epoch   4, Step:    71400, Batch Loss:     1.247973, Tokens per Sec:     4879, Lr: 0.000300\n",
      "2021-10-09 00:52:13,000 - INFO - joeynmt.training - Epoch   4, Step:    71500, Batch Loss:     1.266623, Tokens per Sec:     4934, Lr: 0.000300\n",
      "2021-10-09 00:52:58,852 - INFO - joeynmt.training - Epoch   4, Step:    71600, Batch Loss:     1.197126, Tokens per Sec:     4958, Lr: 0.000300\n",
      "2021-10-09 00:53:44,894 - INFO - joeynmt.training - Epoch   4, Step:    71700, Batch Loss:     1.115976, Tokens per Sec:     4912, Lr: 0.000300\n",
      "2021-10-09 00:54:30,620 - INFO - joeynmt.training - Epoch   4, Step:    71800, Batch Loss:     1.130862, Tokens per Sec:     4893, Lr: 0.000300\n",
      "2021-10-09 00:55:16,692 - INFO - joeynmt.training - Epoch   4, Step:    71900, Batch Loss:     1.334429, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-09 00:56:02,645 - INFO - joeynmt.training - Epoch   4, Step:    72000, Batch Loss:     1.136710, Tokens per Sec:     4960, Lr: 0.000300\n",
      "2021-10-09 00:56:48,268 - INFO - joeynmt.training - Epoch   4, Step:    72100, Batch Loss:     1.082139, Tokens per Sec:     4994, Lr: 0.000300\n",
      "2021-10-09 00:57:33,608 - INFO - joeynmt.training - Epoch   4, Step:    72200, Batch Loss:     1.106012, Tokens per Sec:     4917, Lr: 0.000300\n",
      "2021-10-09 00:58:18,572 - INFO - joeynmt.training - Epoch   4, Step:    72300, Batch Loss:     1.075387, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-09 00:59:03,801 - INFO - joeynmt.training - Epoch   4, Step:    72400, Batch Loss:     1.208631, Tokens per Sec:     5018, Lr: 0.000300\n",
      "2021-10-09 00:59:50,792 - INFO - joeynmt.training - Epoch   4, Step:    72500, Batch Loss:     1.198832, Tokens per Sec:     5036, Lr: 0.000300\n",
      "2021-10-09 01:00:36,871 - INFO - joeynmt.training - Epoch   4, Step:    72600, Batch Loss:     1.198098, Tokens per Sec:     4875, Lr: 0.000300\n",
      "2021-10-09 01:01:22,627 - INFO - joeynmt.training - Epoch   4, Step:    72700, Batch Loss:     1.086132, Tokens per Sec:     4892, Lr: 0.000300\n",
      "2021-10-09 01:02:09,024 - INFO - joeynmt.training - Epoch   4, Step:    72800, Batch Loss:     1.261505, Tokens per Sec:     5011, Lr: 0.000300\n",
      "2021-10-09 01:02:54,993 - INFO - joeynmt.training - Epoch   4, Step:    72900, Batch Loss:     1.291057, Tokens per Sec:     4881, Lr: 0.000300\n",
      "2021-10-09 01:03:41,487 - INFO - joeynmt.training - Epoch   4, Step:    73000, Batch Loss:     1.175204, Tokens per Sec:     4890, Lr: 0.000300\n",
      "2021-10-09 01:04:27,232 - INFO - joeynmt.training - Epoch   4, Step:    73100, Batch Loss:     1.078415, Tokens per Sec:     4859, Lr: 0.000300\n",
      "2021-10-09 01:05:12,421 - INFO - joeynmt.training - Epoch   4, Step:    73200, Batch Loss:     1.134567, Tokens per Sec:     4849, Lr: 0.000300\n",
      "2021-10-09 01:05:57,993 - INFO - joeynmt.training - Epoch   4, Step:    73300, Batch Loss:     1.178937, Tokens per Sec:     4868, Lr: 0.000300\n",
      "2021-10-09 01:06:44,735 - INFO - joeynmt.training - Epoch   4, Step:    73400, Batch Loss:     1.087796, Tokens per Sec:     4927, Lr: 0.000300\n",
      "2021-10-09 01:07:31,301 - INFO - joeynmt.training - Epoch   4, Step:    73500, Batch Loss:     1.064947, Tokens per Sec:     4939, Lr: 0.000300\n",
      "2021-10-09 01:08:17,381 - INFO - joeynmt.training - Epoch   4, Step:    73600, Batch Loss:     1.189819, Tokens per Sec:     4826, Lr: 0.000300\n",
      "2021-10-09 01:09:03,361 - INFO - joeynmt.training - Epoch   4, Step:    73700, Batch Loss:     1.145899, Tokens per Sec:     4970, Lr: 0.000300\n",
      "2021-10-09 01:09:49,207 - INFO - joeynmt.training - Epoch   4, Step:    73800, Batch Loss:     1.154184, Tokens per Sec:     4854, Lr: 0.000300\n",
      "2021-10-09 01:10:35,677 - INFO - joeynmt.training - Epoch   4, Step:    73900, Batch Loss:     1.242075, Tokens per Sec:     4933, Lr: 0.000300\n",
      "2021-10-09 01:11:21,630 - INFO - joeynmt.training - Epoch   4, Step:    74000, Batch Loss:     1.112236, Tokens per Sec:     4965, Lr: 0.000300\n",
      "2021-10-09 01:12:07,109 - INFO - joeynmt.training - Epoch   4, Step:    74100, Batch Loss:     1.111076, Tokens per Sec:     4985, Lr: 0.000300\n",
      "2021-10-09 01:12:53,764 - INFO - joeynmt.training - Epoch   4, Step:    74200, Batch Loss:     1.049655, Tokens per Sec:     5029, Lr: 0.000300\n",
      "2021-10-09 01:13:39,524 - INFO - joeynmt.training - Epoch   4, Step:    74300, Batch Loss:     1.233780, Tokens per Sec:     4826, Lr: 0.000300\n",
      "2021-10-09 01:14:25,656 - INFO - joeynmt.training - Epoch   4, Step:    74400, Batch Loss:     1.181416, Tokens per Sec:     4985, Lr: 0.000300\n",
      "2021-10-09 01:15:10,702 - INFO - joeynmt.training - Epoch   4, Step:    74500, Batch Loss:     1.154563, Tokens per Sec:     4921, Lr: 0.000300\n",
      "2021-10-09 01:15:56,905 - INFO - joeynmt.training - Epoch   4, Step:    74600, Batch Loss:     1.265254, Tokens per Sec:     4903, Lr: 0.000300\n",
      "2021-10-09 01:16:41,912 - INFO - joeynmt.training - Epoch   4, Step:    74700, Batch Loss:     1.074192, Tokens per Sec:     4772, Lr: 0.000300\n",
      "2021-10-09 01:17:27,813 - INFO - joeynmt.training - Epoch   4, Step:    74800, Batch Loss:     1.288419, Tokens per Sec:     4936, Lr: 0.000300\n",
      "2021-10-09 01:18:13,268 - INFO - joeynmt.training - Epoch   4, Step:    74900, Batch Loss:     1.183350, Tokens per Sec:     4891, Lr: 0.000300\n",
      "2021-10-09 01:18:59,001 - INFO - joeynmt.training - Epoch   4, Step:    75000, Batch Loss:     1.161434, Tokens per Sec:     4853, Lr: 0.000300\n",
      "2021-10-09 01:20:22,747 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 01:20:22,748 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 01:20:22,748 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 01:20:23,085 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 01:20:23,085 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 01:20:29,026 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 01:20:29,027 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 01:20:29,027 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 01:20:29,027 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at that time for the brother who took the lead in a congregation .\n",
      "2021-10-09 01:20:29,027 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 01:20:29,027 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 01:20:29,027 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 01:20:29,028 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 01:20:29,028 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 01:20:29,028 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 01:20:29,028 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 01:20:29,028 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 01:20:29,028 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 01:20:29,028 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 01:20:29,028 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 01:20:29,029 - INFO - joeynmt.training - \tHypothesis: HOW DO MOLD WE WE WE TRUTH DOLVITS ?\n",
      "2021-10-09 01:20:29,029 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    75000: bleu:  46.66, loss: 31921.3496, ppl:   2.8727, duration: 90.0273s\n",
      "2021-10-09 01:21:14,488 - INFO - joeynmt.training - Epoch   4, Step:    75100, Batch Loss:     1.122494, Tokens per Sec:     4970, Lr: 0.000300\n",
      "2021-10-09 01:21:23,698 - INFO - joeynmt.training - Epoch   4: total training loss 15033.54\n",
      "2021-10-09 01:21:23,699 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-10-09 01:22:02,501 - INFO - joeynmt.training - Epoch   5, Step:    75200, Batch Loss:     1.220553, Tokens per Sec:     4784, Lr: 0.000300\n",
      "2021-10-09 01:22:47,915 - INFO - joeynmt.training - Epoch   5, Step:    75300, Batch Loss:     1.217083, Tokens per Sec:     4945, Lr: 0.000300\n",
      "2021-10-09 01:23:32,616 - INFO - joeynmt.training - Epoch   5, Step:    75400, Batch Loss:     1.068271, Tokens per Sec:     4929, Lr: 0.000300\n",
      "2021-10-09 01:24:18,823 - INFO - joeynmt.training - Epoch   5, Step:    75500, Batch Loss:     1.338969, Tokens per Sec:     5087, Lr: 0.000300\n",
      "2021-10-09 01:25:05,157 - INFO - joeynmt.training - Epoch   5, Step:    75600, Batch Loss:     0.999845, Tokens per Sec:     4904, Lr: 0.000300\n",
      "2021-10-09 01:25:50,585 - INFO - joeynmt.training - Epoch   5, Step:    75700, Batch Loss:     1.199983, Tokens per Sec:     4882, Lr: 0.000300\n",
      "2021-10-09 01:26:36,386 - INFO - joeynmt.training - Epoch   5, Step:    75800, Batch Loss:     0.936415, Tokens per Sec:     4930, Lr: 0.000300\n",
      "2021-10-09 01:27:22,298 - INFO - joeynmt.training - Epoch   5, Step:    75900, Batch Loss:     1.080101, Tokens per Sec:     4960, Lr: 0.000300\n",
      "2021-10-09 01:28:07,645 - INFO - joeynmt.training - Epoch   5, Step:    76000, Batch Loss:     1.195081, Tokens per Sec:     4849, Lr: 0.000300\n",
      "2021-10-09 01:28:53,859 - INFO - joeynmt.training - Epoch   5, Step:    76100, Batch Loss:     1.104896, Tokens per Sec:     4912, Lr: 0.000300\n",
      "2021-10-09 01:29:39,164 - INFO - joeynmt.training - Epoch   5, Step:    76200, Batch Loss:     1.143417, Tokens per Sec:     4926, Lr: 0.000300\n",
      "2021-10-09 01:30:25,164 - INFO - joeynmt.training - Epoch   5, Step:    76300, Batch Loss:     1.142328, Tokens per Sec:     4993, Lr: 0.000300\n",
      "2021-10-09 01:31:10,939 - INFO - joeynmt.training - Epoch   5, Step:    76400, Batch Loss:     1.278967, Tokens per Sec:     4965, Lr: 0.000300\n",
      "2021-10-09 01:31:55,747 - INFO - joeynmt.training - Epoch   5, Step:    76500, Batch Loss:     1.076108, Tokens per Sec:     4959, Lr: 0.000300\n",
      "2021-10-09 01:32:41,506 - INFO - joeynmt.training - Epoch   5, Step:    76600, Batch Loss:     1.095257, Tokens per Sec:     4962, Lr: 0.000300\n",
      "2021-10-09 01:33:27,629 - INFO - joeynmt.training - Epoch   5, Step:    76700, Batch Loss:     1.168491, Tokens per Sec:     4869, Lr: 0.000300\n",
      "2021-10-09 01:34:13,564 - INFO - joeynmt.training - Epoch   5, Step:    76800, Batch Loss:     1.019083, Tokens per Sec:     5011, Lr: 0.000300\n",
      "2021-10-09 01:34:59,696 - INFO - joeynmt.training - Epoch   5, Step:    76900, Batch Loss:     1.210266, Tokens per Sec:     4945, Lr: 0.000300\n",
      "2021-10-09 01:35:45,860 - INFO - joeynmt.training - Epoch   5, Step:    77000, Batch Loss:     1.343781, Tokens per Sec:     4941, Lr: 0.000300\n",
      "2021-10-09 01:36:31,401 - INFO - joeynmt.training - Epoch   5, Step:    77100, Batch Loss:     1.282714, Tokens per Sec:     4955, Lr: 0.000300\n",
      "2021-10-09 01:37:16,862 - INFO - joeynmt.training - Epoch   5, Step:    77200, Batch Loss:     1.238171, Tokens per Sec:     4936, Lr: 0.000300\n",
      "2021-10-09 01:38:02,307 - INFO - joeynmt.training - Epoch   5, Step:    77300, Batch Loss:     1.192985, Tokens per Sec:     4860, Lr: 0.000300\n",
      "2021-10-09 01:38:48,495 - INFO - joeynmt.training - Epoch   5, Step:    77400, Batch Loss:     1.187190, Tokens per Sec:     5036, Lr: 0.000300\n",
      "2021-10-09 01:39:34,626 - INFO - joeynmt.training - Epoch   5, Step:    77500, Batch Loss:     1.227773, Tokens per Sec:     4940, Lr: 0.000300\n",
      "2021-10-09 01:40:20,078 - INFO - joeynmt.training - Epoch   5, Step:    77600, Batch Loss:     1.198547, Tokens per Sec:     4916, Lr: 0.000300\n",
      "2021-10-09 01:41:06,472 - INFO - joeynmt.training - Epoch   5, Step:    77700, Batch Loss:     1.078304, Tokens per Sec:     4955, Lr: 0.000300\n",
      "2021-10-09 01:41:51,469 - INFO - joeynmt.training - Epoch   5, Step:    77800, Batch Loss:     1.186518, Tokens per Sec:     4951, Lr: 0.000300\n",
      "2021-10-09 01:42:38,017 - INFO - joeynmt.training - Epoch   5, Step:    77900, Batch Loss:     1.058604, Tokens per Sec:     5013, Lr: 0.000300\n",
      "2021-10-09 01:43:23,492 - INFO - joeynmt.training - Epoch   5, Step:    78000, Batch Loss:     1.220762, Tokens per Sec:     4871, Lr: 0.000300\n",
      "2021-10-09 01:44:09,244 - INFO - joeynmt.training - Epoch   5, Step:    78100, Batch Loss:     1.079137, Tokens per Sec:     5002, Lr: 0.000300\n",
      "2021-10-09 01:44:55,141 - INFO - joeynmt.training - Epoch   5, Step:    78200, Batch Loss:     1.063912, Tokens per Sec:     4885, Lr: 0.000300\n",
      "2021-10-09 01:45:40,700 - INFO - joeynmt.training - Epoch   5, Step:    78300, Batch Loss:     1.237091, Tokens per Sec:     4882, Lr: 0.000300\n",
      "2021-10-09 01:46:26,805 - INFO - joeynmt.training - Epoch   5, Step:    78400, Batch Loss:     1.216738, Tokens per Sec:     5009, Lr: 0.000300\n",
      "2021-10-09 01:47:13,210 - INFO - joeynmt.training - Epoch   5, Step:    78500, Batch Loss:     1.124204, Tokens per Sec:     4930, Lr: 0.000300\n",
      "2021-10-09 01:47:59,282 - INFO - joeynmt.training - Epoch   5, Step:    78600, Batch Loss:     1.183392, Tokens per Sec:     4900, Lr: 0.000300\n",
      "2021-10-09 01:48:45,843 - INFO - joeynmt.training - Epoch   5, Step:    78700, Batch Loss:     1.197890, Tokens per Sec:     4816, Lr: 0.000300\n",
      "2021-10-09 01:49:31,756 - INFO - joeynmt.training - Epoch   5, Step:    78800, Batch Loss:     1.186925, Tokens per Sec:     4966, Lr: 0.000300\n",
      "2021-10-09 01:50:17,293 - INFO - joeynmt.training - Epoch   5, Step:    78900, Batch Loss:     1.235983, Tokens per Sec:     4941, Lr: 0.000300\n",
      "2021-10-09 01:51:02,889 - INFO - joeynmt.training - Epoch   5, Step:    79000, Batch Loss:     1.260541, Tokens per Sec:     4939, Lr: 0.000300\n",
      "2021-10-09 01:51:48,475 - INFO - joeynmt.training - Epoch   5, Step:    79100, Batch Loss:     1.082229, Tokens per Sec:     5009, Lr: 0.000300\n",
      "2021-10-09 01:52:33,790 - INFO - joeynmt.training - Epoch   5, Step:    79200, Batch Loss:     1.182711, Tokens per Sec:     4931, Lr: 0.000300\n",
      "2021-10-09 01:53:18,989 - INFO - joeynmt.training - Epoch   5, Step:    79300, Batch Loss:     1.223974, Tokens per Sec:     4994, Lr: 0.000300\n",
      "2021-10-09 01:54:05,102 - INFO - joeynmt.training - Epoch   5, Step:    79400, Batch Loss:     1.107985, Tokens per Sec:     4879, Lr: 0.000300\n",
      "2021-10-09 01:54:50,399 - INFO - joeynmt.training - Epoch   5, Step:    79500, Batch Loss:     0.954408, Tokens per Sec:     4888, Lr: 0.000300\n",
      "2021-10-09 01:55:35,900 - INFO - joeynmt.training - Epoch   5, Step:    79600, Batch Loss:     1.170783, Tokens per Sec:     4995, Lr: 0.000300\n",
      "2021-10-09 01:56:22,233 - INFO - joeynmt.training - Epoch   5, Step:    79700, Batch Loss:     1.263256, Tokens per Sec:     4911, Lr: 0.000300\n",
      "2021-10-09 01:57:07,992 - INFO - joeynmt.training - Epoch   5, Step:    79800, Batch Loss:     1.349581, Tokens per Sec:     5075, Lr: 0.000300\n",
      "2021-10-09 01:57:53,730 - INFO - joeynmt.training - Epoch   5, Step:    79900, Batch Loss:     1.102217, Tokens per Sec:     4949, Lr: 0.000300\n",
      "2021-10-09 01:58:38,798 - INFO - joeynmt.training - Epoch   5, Step:    80000, Batch Loss:     1.216480, Tokens per Sec:     4998, Lr: 0.000300\n",
      "2021-10-09 02:00:05,358 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 02:00:05,359 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 02:00:05,359 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 02:00:05,701 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 02:00:05,701 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 02:00:11,681 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 02:00:11,681 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 02:00:11,681 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 02:00:11,682 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at that time for the brother who took the lead in a congregation .\n",
      "2021-10-09 02:00:11,682 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 02:00:11,682 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 02:00:11,682 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 02:00:11,682 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 02:00:11,682 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 02:00:11,682 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 02:00:11,682 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 02:00:11,683 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 02:00:11,683 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 02:00:11,683 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 02:00:11,683 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 02:00:11,683 - INFO - joeynmt.training - \tHypothesis: HOW WE MOLD WE WE GEESTELY DOELWITTS STEL ?\n",
      "2021-10-09 02:00:11,683 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    80000: bleu:  47.06, loss: 31358.1074, ppl:   2.8197, duration: 92.8850s\n",
      "2021-10-09 02:00:57,817 - INFO - joeynmt.training - Epoch   5, Step:    80100, Batch Loss:     1.124750, Tokens per Sec:     4866, Lr: 0.000300\n",
      "2021-10-09 02:01:43,960 - INFO - joeynmt.training - Epoch   5, Step:    80200, Batch Loss:     1.169594, Tokens per Sec:     4979, Lr: 0.000300\n",
      "2021-10-09 02:02:28,752 - INFO - joeynmt.training - Epoch   5, Step:    80300, Batch Loss:     1.123323, Tokens per Sec:     4928, Lr: 0.000300\n",
      "2021-10-09 02:03:14,762 - INFO - joeynmt.training - Epoch   5, Step:    80400, Batch Loss:     1.279395, Tokens per Sec:     4961, Lr: 0.000300\n",
      "2021-10-09 02:04:00,172 - INFO - joeynmt.training - Epoch   5, Step:    80500, Batch Loss:     1.093182, Tokens per Sec:     4995, Lr: 0.000300\n",
      "2021-10-09 02:04:46,180 - INFO - joeynmt.training - Epoch   5, Step:    80600, Batch Loss:     1.126977, Tokens per Sec:     4900, Lr: 0.000300\n",
      "2021-10-09 02:05:32,512 - INFO - joeynmt.training - Epoch   5, Step:    80700, Batch Loss:     1.133423, Tokens per Sec:     4892, Lr: 0.000300\n",
      "2021-10-09 02:06:18,433 - INFO - joeynmt.training - Epoch   5, Step:    80800, Batch Loss:     1.436292, Tokens per Sec:     4909, Lr: 0.000300\n",
      "2021-10-09 02:07:04,411 - INFO - joeynmt.training - Epoch   5, Step:    80900, Batch Loss:     1.228202, Tokens per Sec:     4920, Lr: 0.000300\n",
      "2021-10-09 02:07:50,084 - INFO - joeynmt.training - Epoch   5, Step:    81000, Batch Loss:     1.217169, Tokens per Sec:     4864, Lr: 0.000300\n",
      "2021-10-09 02:08:35,057 - INFO - joeynmt.training - Epoch   5, Step:    81100, Batch Loss:     1.154489, Tokens per Sec:     4826, Lr: 0.000300\n",
      "2021-10-09 02:09:21,320 - INFO - joeynmt.training - Epoch   5, Step:    81200, Batch Loss:     1.336841, Tokens per Sec:     4961, Lr: 0.000300\n",
      "2021-10-09 02:10:07,791 - INFO - joeynmt.training - Epoch   5, Step:    81300, Batch Loss:     1.134585, Tokens per Sec:     4874, Lr: 0.000300\n",
      "2021-10-09 02:10:53,360 - INFO - joeynmt.training - Epoch   5, Step:    81400, Batch Loss:     1.184559, Tokens per Sec:     4931, Lr: 0.000300\n",
      "2021-10-09 02:11:39,196 - INFO - joeynmt.training - Epoch   5, Step:    81500, Batch Loss:     1.041814, Tokens per Sec:     4794, Lr: 0.000300\n",
      "2021-10-09 02:12:26,041 - INFO - joeynmt.training - Epoch   5, Step:    81600, Batch Loss:     1.254626, Tokens per Sec:     4953, Lr: 0.000300\n",
      "2021-10-09 02:13:11,452 - INFO - joeynmt.training - Epoch   5, Step:    81700, Batch Loss:     1.225965, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2021-10-09 02:13:57,689 - INFO - joeynmt.training - Epoch   5, Step:    81800, Batch Loss:     1.215844, Tokens per Sec:     5003, Lr: 0.000300\n",
      "2021-10-09 02:14:43,377 - INFO - joeynmt.training - Epoch   5, Step:    81900, Batch Loss:     1.129866, Tokens per Sec:     4971, Lr: 0.000300\n",
      "2021-10-09 02:15:28,298 - INFO - joeynmt.training - Epoch   5, Step:    82000, Batch Loss:     1.157711, Tokens per Sec:     4765, Lr: 0.000300\n",
      "2021-10-09 02:16:13,911 - INFO - joeynmt.training - Epoch   5, Step:    82100, Batch Loss:     1.245732, Tokens per Sec:     5004, Lr: 0.000300\n",
      "2021-10-09 02:16:59,304 - INFO - joeynmt.training - Epoch   5, Step:    82200, Batch Loss:     1.275038, Tokens per Sec:     4999, Lr: 0.000300\n",
      "2021-10-09 02:17:45,257 - INFO - joeynmt.training - Epoch   5, Step:    82300, Batch Loss:     1.035283, Tokens per Sec:     5070, Lr: 0.000300\n",
      "2021-10-09 02:18:31,097 - INFO - joeynmt.training - Epoch   5, Step:    82400, Batch Loss:     1.241376, Tokens per Sec:     4961, Lr: 0.000300\n",
      "2021-10-09 02:19:16,485 - INFO - joeynmt.training - Epoch   5, Step:    82500, Batch Loss:     1.118378, Tokens per Sec:     4847, Lr: 0.000300\n",
      "2021-10-09 02:20:02,044 - INFO - joeynmt.training - Epoch   5, Step:    82600, Batch Loss:     0.999012, Tokens per Sec:     4891, Lr: 0.000300\n",
      "2021-10-09 02:20:47,665 - INFO - joeynmt.training - Epoch   5, Step:    82700, Batch Loss:     1.220115, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-09 02:21:32,829 - INFO - joeynmt.training - Epoch   5, Step:    82800, Batch Loss:     1.178225, Tokens per Sec:     4889, Lr: 0.000300\n",
      "2021-10-09 02:22:18,700 - INFO - joeynmt.training - Epoch   5, Step:    82900, Batch Loss:     1.152731, Tokens per Sec:     5014, Lr: 0.000300\n",
      "2021-10-09 02:23:04,861 - INFO - joeynmt.training - Epoch   5, Step:    83000, Batch Loss:     1.194807, Tokens per Sec:     4997, Lr: 0.000300\n",
      "2021-10-09 02:23:50,183 - INFO - joeynmt.training - Epoch   5, Step:    83100, Batch Loss:     1.302882, Tokens per Sec:     4917, Lr: 0.000300\n",
      "2021-10-09 02:24:35,405 - INFO - joeynmt.training - Epoch   5, Step:    83200, Batch Loss:     1.167460, Tokens per Sec:     4857, Lr: 0.000300\n",
      "2021-10-09 02:25:22,105 - INFO - joeynmt.training - Epoch   5, Step:    83300, Batch Loss:     1.166374, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2021-10-09 02:26:08,424 - INFO - joeynmt.training - Epoch   5, Step:    83400, Batch Loss:     1.153664, Tokens per Sec:     5003, Lr: 0.000300\n",
      "2021-10-09 02:26:54,325 - INFO - joeynmt.training - Epoch   5, Step:    83500, Batch Loss:     1.126189, Tokens per Sec:     4948, Lr: 0.000300\n",
      "2021-10-09 02:27:39,705 - INFO - joeynmt.training - Epoch   5, Step:    83600, Batch Loss:     1.092226, Tokens per Sec:     4978, Lr: 0.000300\n",
      "2021-10-09 02:28:25,574 - INFO - joeynmt.training - Epoch   5, Step:    83700, Batch Loss:     1.211118, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2021-10-09 02:29:12,012 - INFO - joeynmt.training - Epoch   5, Step:    83800, Batch Loss:     1.144111, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-09 02:29:57,436 - INFO - joeynmt.training - Epoch   5, Step:    83900, Batch Loss:     1.193290, Tokens per Sec:     4921, Lr: 0.000300\n",
      "2021-10-09 02:30:42,961 - INFO - joeynmt.training - Epoch   5, Step:    84000, Batch Loss:     1.134850, Tokens per Sec:     4892, Lr: 0.000300\n",
      "2021-10-09 02:31:29,202 - INFO - joeynmt.training - Epoch   5, Step:    84100, Batch Loss:     1.174102, Tokens per Sec:     4989, Lr: 0.000300\n",
      "2021-10-09 02:32:15,260 - INFO - joeynmt.training - Epoch   5, Step:    84200, Batch Loss:     1.128470, Tokens per Sec:     4982, Lr: 0.000300\n",
      "2021-10-09 02:33:01,095 - INFO - joeynmt.training - Epoch   5, Step:    84300, Batch Loss:     1.094369, Tokens per Sec:     4969, Lr: 0.000300\n",
      "2021-10-09 02:33:47,213 - INFO - joeynmt.training - Epoch   5, Step:    84400, Batch Loss:     1.080269, Tokens per Sec:     5010, Lr: 0.000300\n",
      "2021-10-09 02:34:33,122 - INFO - joeynmt.training - Epoch   5, Step:    84500, Batch Loss:     1.299209, Tokens per Sec:     4993, Lr: 0.000300\n",
      "2021-10-09 02:35:19,113 - INFO - joeynmt.training - Epoch   5, Step:    84600, Batch Loss:     1.157814, Tokens per Sec:     5014, Lr: 0.000300\n",
      "2021-10-09 02:36:04,608 - INFO - joeynmt.training - Epoch   5, Step:    84700, Batch Loss:     0.990134, Tokens per Sec:     4976, Lr: 0.000300\n",
      "2021-10-09 02:36:50,225 - INFO - joeynmt.training - Epoch   5, Step:    84800, Batch Loss:     1.156356, Tokens per Sec:     4892, Lr: 0.000300\n",
      "2021-10-09 02:37:36,437 - INFO - joeynmt.training - Epoch   5, Step:    84900, Batch Loss:     0.981130, Tokens per Sec:     5007, Lr: 0.000300\n",
      "2021-10-09 02:38:22,525 - INFO - joeynmt.training - Epoch   5, Step:    85000, Batch Loss:     1.188211, Tokens per Sec:     4881, Lr: 0.000300\n",
      "2021-10-09 02:39:50,103 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 02:39:50,104 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 02:39:50,104 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 02:39:50,443 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 02:39:50,444 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 02:39:56,992 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 02:39:56,992 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 02:39:56,993 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 02:39:56,993 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-09 02:39:56,993 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 02:39:56,993 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 02:39:56,993 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 02:39:56,993 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 02:39:56,993 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 02:39:56,994 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 02:39:56,994 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 02:39:56,994 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 02:39:56,994 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 02:39:56,994 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 02:39:56,994 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 02:39:56,994 - INFO - joeynmt.training - \tHypothesis: HOW WE MOST WE WE GEESTELY DOELVITS ?\n",
      "2021-10-09 02:39:56,994 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    85000: bleu:  47.66, loss: 31065.8125, ppl:   2.7926, duration: 94.4687s\n",
      "2021-10-09 02:40:42,748 - INFO - joeynmt.training - Epoch   5, Step:    85100, Batch Loss:     1.172668, Tokens per Sec:     4894, Lr: 0.000300\n",
      "2021-10-09 02:41:28,171 - INFO - joeynmt.training - Epoch   5, Step:    85200, Batch Loss:     1.143888, Tokens per Sec:     4951, Lr: 0.000300\n",
      "2021-10-09 02:42:14,590 - INFO - joeynmt.training - Epoch   5, Step:    85300, Batch Loss:     1.187707, Tokens per Sec:     4969, Lr: 0.000300\n",
      "2021-10-09 02:43:00,896 - INFO - joeynmt.training - Epoch   5, Step:    85400, Batch Loss:     1.323538, Tokens per Sec:     4960, Lr: 0.000300\n",
      "2021-10-09 02:43:46,364 - INFO - joeynmt.training - Epoch   5, Step:    85500, Batch Loss:     1.214175, Tokens per Sec:     4831, Lr: 0.000300\n",
      "2021-10-09 02:44:32,414 - INFO - joeynmt.training - Epoch   5, Step:    85600, Batch Loss:     1.339509, Tokens per Sec:     4985, Lr: 0.000300\n",
      "2021-10-09 02:45:18,666 - INFO - joeynmt.training - Epoch   5, Step:    85700, Batch Loss:     1.132320, Tokens per Sec:     4986, Lr: 0.000300\n",
      "2021-10-09 02:46:03,842 - INFO - joeynmt.training - Epoch   5, Step:    85800, Batch Loss:     1.010685, Tokens per Sec:     4912, Lr: 0.000300\n",
      "2021-10-09 02:46:49,210 - INFO - joeynmt.training - Epoch   5, Step:    85900, Batch Loss:     1.281373, Tokens per Sec:     4823, Lr: 0.000300\n",
      "2021-10-09 02:47:34,636 - INFO - joeynmt.training - Epoch   5, Step:    86000, Batch Loss:     1.146498, Tokens per Sec:     4907, Lr: 0.000300\n",
      "2021-10-09 02:48:19,764 - INFO - joeynmt.training - Epoch   5, Step:    86100, Batch Loss:     1.241567, Tokens per Sec:     4864, Lr: 0.000300\n",
      "2021-10-09 02:49:05,059 - INFO - joeynmt.training - Epoch   5, Step:    86200, Batch Loss:     1.053947, Tokens per Sec:     4989, Lr: 0.000300\n",
      "2021-10-09 02:49:50,677 - INFO - joeynmt.training - Epoch   5, Step:    86300, Batch Loss:     1.101344, Tokens per Sec:     4863, Lr: 0.000300\n",
      "2021-10-09 02:50:36,489 - INFO - joeynmt.training - Epoch   5, Step:    86400, Batch Loss:     1.278554, Tokens per Sec:     5002, Lr: 0.000300\n",
      "2021-10-09 02:51:21,684 - INFO - joeynmt.training - Epoch   5, Step:    86500, Batch Loss:     1.220118, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-09 02:52:07,795 - INFO - joeynmt.training - Epoch   5, Step:    86600, Batch Loss:     1.078244, Tokens per Sec:     4893, Lr: 0.000300\n",
      "2021-10-09 02:52:53,298 - INFO - joeynmt.training - Epoch   5, Step:    86700, Batch Loss:     1.312665, Tokens per Sec:     4968, Lr: 0.000300\n",
      "2021-10-09 02:53:39,001 - INFO - joeynmt.training - Epoch   5, Step:    86800, Batch Loss:     1.237380, Tokens per Sec:     4960, Lr: 0.000300\n",
      "2021-10-09 02:54:25,097 - INFO - joeynmt.training - Epoch   5, Step:    86900, Batch Loss:     1.362828, Tokens per Sec:     5063, Lr: 0.000300\n",
      "2021-10-09 02:55:10,890 - INFO - joeynmt.training - Epoch   5, Step:    87000, Batch Loss:     1.173274, Tokens per Sec:     4923, Lr: 0.000300\n",
      "2021-10-09 02:55:57,481 - INFO - joeynmt.training - Epoch   5, Step:    87100, Batch Loss:     1.148708, Tokens per Sec:     5092, Lr: 0.000300\n",
      "2021-10-09 02:56:43,556 - INFO - joeynmt.training - Epoch   5, Step:    87200, Batch Loss:     1.228104, Tokens per Sec:     5006, Lr: 0.000300\n",
      "2021-10-09 02:57:29,612 - INFO - joeynmt.training - Epoch   5, Step:    87300, Batch Loss:     1.102421, Tokens per Sec:     4966, Lr: 0.000300\n",
      "2021-10-09 02:58:15,427 - INFO - joeynmt.training - Epoch   5, Step:    87400, Batch Loss:     1.127636, Tokens per Sec:     4945, Lr: 0.000300\n",
      "2021-10-09 02:59:00,594 - INFO - joeynmt.training - Epoch   5, Step:    87500, Batch Loss:     1.039013, Tokens per Sec:     4858, Lr: 0.000300\n",
      "2021-10-09 02:59:46,913 - INFO - joeynmt.training - Epoch   5, Step:    87600, Batch Loss:     1.147297, Tokens per Sec:     4926, Lr: 0.000300\n",
      "2021-10-09 02:59:59,350 - INFO - joeynmt.training - Epoch   5: total training loss 14534.77\n",
      "2021-10-09 02:59:59,351 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-10-09 03:00:34,992 - INFO - joeynmt.training - Epoch   6, Step:    87700, Batch Loss:     1.205309, Tokens per Sec:     4694, Lr: 0.000300\n",
      "2021-10-09 03:01:20,781 - INFO - joeynmt.training - Epoch   6, Step:    87800, Batch Loss:     1.109646, Tokens per Sec:     4873, Lr: 0.000300\n",
      "2021-10-09 03:02:06,992 - INFO - joeynmt.training - Epoch   6, Step:    87900, Batch Loss:     1.240770, Tokens per Sec:     5021, Lr: 0.000300\n",
      "2021-10-09 03:02:52,574 - INFO - joeynmt.training - Epoch   6, Step:    88000, Batch Loss:     1.152254, Tokens per Sec:     4946, Lr: 0.000300\n",
      "2021-10-09 03:03:38,349 - INFO - joeynmt.training - Epoch   6, Step:    88100, Batch Loss:     1.086307, Tokens per Sec:     4911, Lr: 0.000300\n",
      "2021-10-09 03:04:23,529 - INFO - joeynmt.training - Epoch   6, Step:    88200, Batch Loss:     1.223934, Tokens per Sec:     4827, Lr: 0.000300\n",
      "2021-10-09 03:05:09,174 - INFO - joeynmt.training - Epoch   6, Step:    88300, Batch Loss:     1.100486, Tokens per Sec:     4969, Lr: 0.000300\n",
      "2021-10-09 03:05:55,117 - INFO - joeynmt.training - Epoch   6, Step:    88400, Batch Loss:     1.206999, Tokens per Sec:     4972, Lr: 0.000300\n",
      "2021-10-09 03:06:40,897 - INFO - joeynmt.training - Epoch   6, Step:    88500, Batch Loss:     1.084265, Tokens per Sec:     4962, Lr: 0.000300\n",
      "2021-10-09 03:07:25,624 - INFO - joeynmt.training - Epoch   6, Step:    88600, Batch Loss:     1.078864, Tokens per Sec:     4729, Lr: 0.000300\n",
      "2021-10-09 03:08:11,634 - INFO - joeynmt.training - Epoch   6, Step:    88700, Batch Loss:     1.130038, Tokens per Sec:     4994, Lr: 0.000300\n",
      "2021-10-09 03:08:58,236 - INFO - joeynmt.training - Epoch   6, Step:    88800, Batch Loss:     1.109762, Tokens per Sec:     4989, Lr: 0.000300\n",
      "2021-10-09 03:09:44,365 - INFO - joeynmt.training - Epoch   6, Step:    88900, Batch Loss:     1.257139, Tokens per Sec:     4868, Lr: 0.000300\n",
      "2021-10-09 03:10:30,857 - INFO - joeynmt.training - Epoch   6, Step:    89000, Batch Loss:     1.152016, Tokens per Sec:     4937, Lr: 0.000300\n",
      "2021-10-09 03:11:17,080 - INFO - joeynmt.training - Epoch   6, Step:    89100, Batch Loss:     1.216012, Tokens per Sec:     4964, Lr: 0.000300\n",
      "2021-10-09 03:12:03,748 - INFO - joeynmt.training - Epoch   6, Step:    89200, Batch Loss:     1.114574, Tokens per Sec:     4872, Lr: 0.000300\n",
      "2021-10-09 03:12:49,776 - INFO - joeynmt.training - Epoch   6, Step:    89300, Batch Loss:     1.055069, Tokens per Sec:     4943, Lr: 0.000300\n",
      "2021-10-09 03:13:35,312 - INFO - joeynmt.training - Epoch   6, Step:    89400, Batch Loss:     1.009242, Tokens per Sec:     4877, Lr: 0.000300\n",
      "2021-10-09 03:14:21,518 - INFO - joeynmt.training - Epoch   6, Step:    89500, Batch Loss:     1.082186, Tokens per Sec:     5046, Lr: 0.000300\n",
      "2021-10-09 03:15:07,256 - INFO - joeynmt.training - Epoch   6, Step:    89600, Batch Loss:     1.101287, Tokens per Sec:     4986, Lr: 0.000300\n",
      "2021-10-09 03:15:54,037 - INFO - joeynmt.training - Epoch   6, Step:    89700, Batch Loss:     1.116336, Tokens per Sec:     5007, Lr: 0.000300\n",
      "2021-10-09 03:16:40,266 - INFO - joeynmt.training - Epoch   6, Step:    89800, Batch Loss:     1.187193, Tokens per Sec:     4957, Lr: 0.000300\n",
      "2021-10-09 03:17:25,050 - INFO - joeynmt.training - Epoch   6, Step:    89900, Batch Loss:     1.176888, Tokens per Sec:     4904, Lr: 0.000300\n",
      "2021-10-09 03:18:11,061 - INFO - joeynmt.training - Epoch   6, Step:    90000, Batch Loss:     1.117669, Tokens per Sec:     5004, Lr: 0.000300\n",
      "2021-10-09 03:19:39,051 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 03:19:39,051 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 03:19:39,052 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 03:19:39,391 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 03:19:39,392 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 03:19:45,444 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 03:19:45,445 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 03:19:45,445 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 03:19:45,445 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term used at that time for the brother who took the lead in a congregation .\n",
      "2021-10-09 03:19:45,445 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 03:19:45,446 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 03:19:45,446 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 03:19:45,446 - INFO - joeynmt.training - \tHypothesis: The appeal is understandable , for adolescence is a time when one comes to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 03:19:45,446 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 03:19:45,446 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 03:19:45,446 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 03:19:45,446 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 03:19:45,446 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 03:19:45,447 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 03:19:45,447 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 03:19:45,447 - INFO - joeynmt.training - \tHypothesis: HOW WE WE WE TO BE US GEESTELICE DOELVITE ?\n",
      "2021-10-09 03:19:45,447 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    90000: bleu:  47.42, loss: 30789.0254, ppl:   2.7672, duration: 94.3851s\n",
      "2021-10-09 03:20:31,659 - INFO - joeynmt.training - Epoch   6, Step:    90100, Batch Loss:     1.057200, Tokens per Sec:     5024, Lr: 0.000300\n",
      "2021-10-09 03:21:17,902 - INFO - joeynmt.training - Epoch   6, Step:    90200, Batch Loss:     1.208484, Tokens per Sec:     4930, Lr: 0.000300\n",
      "2021-10-09 03:22:03,792 - INFO - joeynmt.training - Epoch   6, Step:    90300, Batch Loss:     1.273343, Tokens per Sec:     4929, Lr: 0.000300\n",
      "2021-10-09 03:22:49,366 - INFO - joeynmt.training - Epoch   6, Step:    90400, Batch Loss:     1.258374, Tokens per Sec:     4941, Lr: 0.000300\n",
      "2021-10-09 03:23:35,575 - INFO - joeynmt.training - Epoch   6, Step:    90500, Batch Loss:     1.210439, Tokens per Sec:     4848, Lr: 0.000300\n",
      "2021-10-09 03:24:21,098 - INFO - joeynmt.training - Epoch   6, Step:    90600, Batch Loss:     1.167240, Tokens per Sec:     4934, Lr: 0.000300\n",
      "2021-10-09 03:25:06,501 - INFO - joeynmt.training - Epoch   6, Step:    90700, Batch Loss:     1.186792, Tokens per Sec:     4857, Lr: 0.000300\n",
      "2021-10-09 03:25:52,688 - INFO - joeynmt.training - Epoch   6, Step:    90800, Batch Loss:     1.175975, Tokens per Sec:     4988, Lr: 0.000300\n",
      "2021-10-09 03:26:39,246 - INFO - joeynmt.training - Epoch   6, Step:    90900, Batch Loss:     1.185227, Tokens per Sec:     4885, Lr: 0.000300\n",
      "2021-10-09 03:27:25,293 - INFO - joeynmt.training - Epoch   6, Step:    91000, Batch Loss:     1.181995, Tokens per Sec:     4968, Lr: 0.000300\n",
      "2021-10-09 03:28:10,470 - INFO - joeynmt.training - Epoch   6, Step:    91100, Batch Loss:     1.099008, Tokens per Sec:     4854, Lr: 0.000300\n",
      "2021-10-09 03:28:57,361 - INFO - joeynmt.training - Epoch   6, Step:    91200, Batch Loss:     1.076603, Tokens per Sec:     5042, Lr: 0.000300\n",
      "2021-10-09 03:29:43,227 - INFO - joeynmt.training - Epoch   6, Step:    91300, Batch Loss:     0.976160, Tokens per Sec:     4921, Lr: 0.000300\n",
      "2021-10-09 03:30:29,062 - INFO - joeynmt.training - Epoch   6, Step:    91400, Batch Loss:     1.330024, Tokens per Sec:     4977, Lr: 0.000300\n",
      "2021-10-09 03:31:14,969 - INFO - joeynmt.training - Epoch   6, Step:    91500, Batch Loss:     1.202605, Tokens per Sec:     4944, Lr: 0.000300\n",
      "2021-10-09 03:31:59,656 - INFO - joeynmt.training - Epoch   6, Step:    91600, Batch Loss:     1.095359, Tokens per Sec:     4852, Lr: 0.000300\n",
      "2021-10-09 03:32:45,413 - INFO - joeynmt.training - Epoch   6, Step:    91700, Batch Loss:     1.198730, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-09 03:33:30,168 - INFO - joeynmt.training - Epoch   6, Step:    91800, Batch Loss:     1.107820, Tokens per Sec:     4897, Lr: 0.000300\n",
      "2021-10-09 03:34:16,362 - INFO - joeynmt.training - Epoch   6, Step:    91900, Batch Loss:     1.222734, Tokens per Sec:     4944, Lr: 0.000300\n",
      "2021-10-09 03:35:01,601 - INFO - joeynmt.training - Epoch   6, Step:    92000, Batch Loss:     1.147182, Tokens per Sec:     4991, Lr: 0.000300\n",
      "2021-10-09 03:35:45,912 - INFO - joeynmt.training - Epoch   6, Step:    92100, Batch Loss:     1.182549, Tokens per Sec:     4889, Lr: 0.000300\n",
      "2021-10-09 03:36:31,134 - INFO - joeynmt.training - Epoch   6, Step:    92200, Batch Loss:     0.896374, Tokens per Sec:     4932, Lr: 0.000300\n",
      "2021-10-09 03:37:17,152 - INFO - joeynmt.training - Epoch   6, Step:    92300, Batch Loss:     1.117319, Tokens per Sec:     5007, Lr: 0.000300\n",
      "2021-10-09 03:38:02,582 - INFO - joeynmt.training - Epoch   6, Step:    92400, Batch Loss:     1.078169, Tokens per Sec:     4949, Lr: 0.000300\n",
      "2021-10-09 03:38:48,942 - INFO - joeynmt.training - Epoch   6, Step:    92500, Batch Loss:     0.758565, Tokens per Sec:     4904, Lr: 0.000300\n",
      "2021-10-09 03:39:35,050 - INFO - joeynmt.training - Epoch   6, Step:    92600, Batch Loss:     1.063852, Tokens per Sec:     4926, Lr: 0.000300\n",
      "2021-10-09 03:40:20,321 - INFO - joeynmt.training - Epoch   6, Step:    92700, Batch Loss:     0.985840, Tokens per Sec:     4902, Lr: 0.000300\n",
      "2021-10-09 03:41:05,978 - INFO - joeynmt.training - Epoch   6, Step:    92800, Batch Loss:     1.178730, Tokens per Sec:     4948, Lr: 0.000300\n",
      "2021-10-09 03:41:51,700 - INFO - joeynmt.training - Epoch   6, Step:    92900, Batch Loss:     1.254874, Tokens per Sec:     4925, Lr: 0.000300\n",
      "2021-10-09 03:42:37,532 - INFO - joeynmt.training - Epoch   6, Step:    93000, Batch Loss:     1.082181, Tokens per Sec:     4872, Lr: 0.000300\n",
      "2021-10-09 03:43:23,205 - INFO - joeynmt.training - Epoch   6, Step:    93100, Batch Loss:     1.286982, Tokens per Sec:     4958, Lr: 0.000300\n",
      "2021-10-09 03:44:08,669 - INFO - joeynmt.training - Epoch   6, Step:    93200, Batch Loss:     1.088442, Tokens per Sec:     4984, Lr: 0.000300\n",
      "2021-10-09 03:44:53,794 - INFO - joeynmt.training - Epoch   6, Step:    93300, Batch Loss:     1.100557, Tokens per Sec:     5039, Lr: 0.000300\n",
      "2021-10-09 03:45:39,774 - INFO - joeynmt.training - Epoch   6, Step:    93400, Batch Loss:     0.952245, Tokens per Sec:     4872, Lr: 0.000300\n",
      "2021-10-09 03:46:25,625 - INFO - joeynmt.training - Epoch   6, Step:    93500, Batch Loss:     1.001158, Tokens per Sec:     5029, Lr: 0.000300\n",
      "2021-10-09 03:47:11,558 - INFO - joeynmt.training - Epoch   6, Step:    93600, Batch Loss:     1.206409, Tokens per Sec:     4904, Lr: 0.000300\n",
      "2021-10-09 03:47:57,859 - INFO - joeynmt.training - Epoch   6, Step:    93700, Batch Loss:     1.079022, Tokens per Sec:     4904, Lr: 0.000300\n",
      "2021-10-09 03:48:43,233 - INFO - joeynmt.training - Epoch   6, Step:    93800, Batch Loss:     1.189669, Tokens per Sec:     4904, Lr: 0.000300\n",
      "2021-10-09 03:49:28,496 - INFO - joeynmt.training - Epoch   6, Step:    93900, Batch Loss:     1.096856, Tokens per Sec:     5002, Lr: 0.000300\n",
      "2021-10-09 03:50:14,233 - INFO - joeynmt.training - Epoch   6, Step:    94000, Batch Loss:     1.042292, Tokens per Sec:     5037, Lr: 0.000300\n",
      "2021-10-09 03:50:59,553 - INFO - joeynmt.training - Epoch   6, Step:    94100, Batch Loss:     1.112730, Tokens per Sec:     4914, Lr: 0.000300\n",
      "2021-10-09 03:51:45,251 - INFO - joeynmt.training - Epoch   6, Step:    94200, Batch Loss:     1.007554, Tokens per Sec:     5043, Lr: 0.000300\n",
      "2021-10-09 03:52:30,825 - INFO - joeynmt.training - Epoch   6, Step:    94300, Batch Loss:     1.099665, Tokens per Sec:     5004, Lr: 0.000300\n",
      "2021-10-09 03:53:15,968 - INFO - joeynmt.training - Epoch   6, Step:    94400, Batch Loss:     1.100865, Tokens per Sec:     4967, Lr: 0.000300\n",
      "2021-10-09 03:54:01,254 - INFO - joeynmt.training - Epoch   6, Step:    94500, Batch Loss:     1.002680, Tokens per Sec:     4825, Lr: 0.000300\n",
      "2021-10-09 03:54:46,786 - INFO - joeynmt.training - Epoch   6, Step:    94600, Batch Loss:     1.324500, Tokens per Sec:     4932, Lr: 0.000300\n",
      "2021-10-09 03:55:32,658 - INFO - joeynmt.training - Epoch   6, Step:    94700, Batch Loss:     1.151351, Tokens per Sec:     4953, Lr: 0.000300\n",
      "2021-10-09 03:56:18,573 - INFO - joeynmt.training - Epoch   6, Step:    94800, Batch Loss:     1.229745, Tokens per Sec:     5062, Lr: 0.000300\n",
      "2021-10-09 03:57:03,352 - INFO - joeynmt.training - Epoch   6, Step:    94900, Batch Loss:     1.334277, Tokens per Sec:     4907, Lr: 0.000300\n",
      "2021-10-09 03:57:48,267 - INFO - joeynmt.training - Epoch   6, Step:    95000, Batch Loss:     1.052887, Tokens per Sec:     5024, Lr: 0.000300\n",
      "2021-10-09 03:59:14,373 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 03:59:14,374 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 03:59:14,374 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 03:59:14,713 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 03:59:14,714 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 03:59:21,489 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 03:59:21,490 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 03:59:21,490 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 03:59:21,490 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term then used for the brother who took the lead in a congregation .\n",
      "2021-10-09 03:59:21,490 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 03:59:21,490 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 03:59:21,490 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 03:59:21,490 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 03:59:21,490 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 03:59:21,491 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 03:59:21,491 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 03:59:21,491 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 03:59:21,491 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 03:59:21,491 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 03:59:21,491 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 03:59:21,491 - INFO - joeynmt.training - \tHypothesis: HOW DO MOST US WE FAITH DOLVITS ?\n",
      "2021-10-09 03:59:21,491 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    95000: bleu:  47.45, loss: 30523.2285, ppl:   2.7429, duration: 93.2238s\n",
      "2021-10-09 04:00:08,073 - INFO - joeynmt.training - Epoch   6, Step:    95100, Batch Loss:     1.312703, Tokens per Sec:     5012, Lr: 0.000300\n",
      "2021-10-09 04:00:53,203 - INFO - joeynmt.training - Epoch   6, Step:    95200, Batch Loss:     1.100159, Tokens per Sec:     4960, Lr: 0.000300\n",
      "2021-10-09 04:01:38,578 - INFO - joeynmt.training - Epoch   6, Step:    95300, Batch Loss:     1.158116, Tokens per Sec:     4983, Lr: 0.000300\n",
      "2021-10-09 04:02:24,186 - INFO - joeynmt.training - Epoch   6, Step:    95400, Batch Loss:     1.112704, Tokens per Sec:     4999, Lr: 0.000300\n",
      "2021-10-09 04:03:08,790 - INFO - joeynmt.training - Epoch   6, Step:    95500, Batch Loss:     1.115525, Tokens per Sec:     4814, Lr: 0.000300\n",
      "2021-10-09 04:03:54,718 - INFO - joeynmt.training - Epoch   6, Step:    95600, Batch Loss:     1.153167, Tokens per Sec:     5086, Lr: 0.000300\n",
      "2021-10-09 04:04:40,305 - INFO - joeynmt.training - Epoch   6, Step:    95700, Batch Loss:     1.020788, Tokens per Sec:     4811, Lr: 0.000300\n",
      "2021-10-09 04:05:25,985 - INFO - joeynmt.training - Epoch   6, Step:    95800, Batch Loss:     1.303365, Tokens per Sec:     5023, Lr: 0.000300\n",
      "2021-10-09 04:06:11,155 - INFO - joeynmt.training - Epoch   6, Step:    95900, Batch Loss:     1.036415, Tokens per Sec:     4952, Lr: 0.000300\n",
      "2021-10-09 04:06:56,869 - INFO - joeynmt.training - Epoch   6, Step:    96000, Batch Loss:     0.946628, Tokens per Sec:     5007, Lr: 0.000300\n",
      "2021-10-09 04:07:42,502 - INFO - joeynmt.training - Epoch   6, Step:    96100, Batch Loss:     1.213770, Tokens per Sec:     4961, Lr: 0.000300\n",
      "2021-10-09 04:08:28,101 - INFO - joeynmt.training - Epoch   6, Step:    96200, Batch Loss:     1.136994, Tokens per Sec:     4957, Lr: 0.000300\n",
      "2021-10-09 04:09:13,926 - INFO - joeynmt.training - Epoch   6, Step:    96300, Batch Loss:     1.140886, Tokens per Sec:     4941, Lr: 0.000300\n",
      "2021-10-09 04:09:59,256 - INFO - joeynmt.training - Epoch   6, Step:    96400, Batch Loss:     1.088745, Tokens per Sec:     4939, Lr: 0.000300\n",
      "2021-10-09 04:10:44,900 - INFO - joeynmt.training - Epoch   6, Step:    96500, Batch Loss:     1.083359, Tokens per Sec:     5033, Lr: 0.000300\n",
      "2021-10-09 04:11:31,066 - INFO - joeynmt.training - Epoch   6, Step:    96600, Batch Loss:     1.258643, Tokens per Sec:     4985, Lr: 0.000300\n",
      "2021-10-09 04:12:16,473 - INFO - joeynmt.training - Epoch   6, Step:    96700, Batch Loss:     1.166518, Tokens per Sec:     4832, Lr: 0.000300\n",
      "2021-10-09 04:13:02,866 - INFO - joeynmt.training - Epoch   6, Step:    96800, Batch Loss:     1.142755, Tokens per Sec:     4922, Lr: 0.000300\n",
      "2021-10-09 04:13:48,674 - INFO - joeynmt.training - Epoch   6, Step:    96900, Batch Loss:     1.073080, Tokens per Sec:     4995, Lr: 0.000300\n",
      "2021-10-09 04:14:34,709 - INFO - joeynmt.training - Epoch   6, Step:    97000, Batch Loss:     1.234728, Tokens per Sec:     4977, Lr: 0.000300\n",
      "2021-10-09 04:15:20,003 - INFO - joeynmt.training - Epoch   6, Step:    97100, Batch Loss:     0.996424, Tokens per Sec:     4936, Lr: 0.000300\n",
      "2021-10-09 04:16:05,773 - INFO - joeynmt.training - Epoch   6, Step:    97200, Batch Loss:     1.205698, Tokens per Sec:     4910, Lr: 0.000300\n",
      "2021-10-09 04:16:50,868 - INFO - joeynmt.training - Epoch   6, Step:    97300, Batch Loss:     1.095734, Tokens per Sec:     4905, Lr: 0.000300\n",
      "2021-10-09 04:17:36,012 - INFO - joeynmt.training - Epoch   6, Step:    97400, Batch Loss:     1.315786, Tokens per Sec:     4994, Lr: 0.000300\n",
      "2021-10-09 04:18:21,841 - INFO - joeynmt.training - Epoch   6, Step:    97500, Batch Loss:     1.039932, Tokens per Sec:     4896, Lr: 0.000300\n",
      "2021-10-09 04:19:08,024 - INFO - joeynmt.training - Epoch   6, Step:    97600, Batch Loss:     1.125567, Tokens per Sec:     4859, Lr: 0.000300\n",
      "2021-10-09 04:19:54,960 - INFO - joeynmt.training - Epoch   6, Step:    97700, Batch Loss:     1.289252, Tokens per Sec:     5048, Lr: 0.000300\n",
      "2021-10-09 04:20:40,434 - INFO - joeynmt.training - Epoch   6, Step:    97800, Batch Loss:     1.246735, Tokens per Sec:     4911, Lr: 0.000300\n",
      "2021-10-09 04:21:26,333 - INFO - joeynmt.training - Epoch   6, Step:    97900, Batch Loss:     1.188868, Tokens per Sec:     4998, Lr: 0.000300\n",
      "2021-10-09 04:22:12,086 - INFO - joeynmt.training - Epoch   6, Step:    98000, Batch Loss:     1.048496, Tokens per Sec:     5026, Lr: 0.000300\n",
      "2021-10-09 04:22:57,445 - INFO - joeynmt.training - Epoch   6, Step:    98100, Batch Loss:     1.022517, Tokens per Sec:     4974, Lr: 0.000300\n",
      "2021-10-09 04:23:42,180 - INFO - joeynmt.training - Epoch   6, Step:    98200, Batch Loss:     0.963878, Tokens per Sec:     4952, Lr: 0.000300\n",
      "2021-10-09 04:24:27,643 - INFO - joeynmt.training - Epoch   6, Step:    98300, Batch Loss:     1.073425, Tokens per Sec:     4977, Lr: 0.000300\n",
      "2021-10-09 04:25:12,568 - INFO - joeynmt.training - Epoch   6, Step:    98400, Batch Loss:     1.121665, Tokens per Sec:     4832, Lr: 0.000300\n",
      "2021-10-09 04:25:57,950 - INFO - joeynmt.training - Epoch   6, Step:    98500, Batch Loss:     1.078749, Tokens per Sec:     4957, Lr: 0.000300\n",
      "2021-10-09 04:26:43,461 - INFO - joeynmt.training - Epoch   6, Step:    98600, Batch Loss:     1.046745, Tokens per Sec:     5115, Lr: 0.000300\n",
      "2021-10-09 04:27:28,844 - INFO - joeynmt.training - Epoch   6, Step:    98700, Batch Loss:     1.322213, Tokens per Sec:     4765, Lr: 0.000300\n",
      "2021-10-09 04:28:13,596 - INFO - joeynmt.training - Epoch   6, Step:    98800, Batch Loss:     1.100217, Tokens per Sec:     4820, Lr: 0.000300\n",
      "2021-10-09 04:28:59,653 - INFO - joeynmt.training - Epoch   6, Step:    98900, Batch Loss:     1.242208, Tokens per Sec:     4920, Lr: 0.000300\n",
      "2021-10-09 04:29:45,780 - INFO - joeynmt.training - Epoch   6, Step:    99000, Batch Loss:     1.075189, Tokens per Sec:     4977, Lr: 0.000300\n",
      "2021-10-09 04:30:30,880 - INFO - joeynmt.training - Epoch   6, Step:    99100, Batch Loss:     1.156597, Tokens per Sec:     4961, Lr: 0.000300\n",
      "2021-10-09 04:31:16,268 - INFO - joeynmt.training - Epoch   6, Step:    99200, Batch Loss:     0.917379, Tokens per Sec:     4973, Lr: 0.000300\n",
      "2021-10-09 04:32:02,660 - INFO - joeynmt.training - Epoch   6, Step:    99300, Batch Loss:     1.063832, Tokens per Sec:     4991, Lr: 0.000300\n",
      "2021-10-09 04:32:47,992 - INFO - joeynmt.training - Epoch   6, Step:    99400, Batch Loss:     1.054111, Tokens per Sec:     4993, Lr: 0.000300\n",
      "2021-10-09 04:33:33,180 - INFO - joeynmt.training - Epoch   6, Step:    99500, Batch Loss:     1.284080, Tokens per Sec:     4841, Lr: 0.000300\n",
      "2021-10-09 04:34:19,237 - INFO - joeynmt.training - Epoch   6, Step:    99600, Batch Loss:     1.118474, Tokens per Sec:     4998, Lr: 0.000300\n",
      "2021-10-09 04:35:06,081 - INFO - joeynmt.training - Epoch   6, Step:    99700, Batch Loss:     1.193291, Tokens per Sec:     4976, Lr: 0.000300\n",
      "2021-10-09 04:35:52,185 - INFO - joeynmt.training - Epoch   6, Step:    99800, Batch Loss:     1.162418, Tokens per Sec:     4910, Lr: 0.000300\n",
      "2021-10-09 04:36:37,417 - INFO - joeynmt.training - Epoch   6, Step:    99900, Batch Loss:     1.456448, Tokens per Sec:     4910, Lr: 0.000300\n",
      "2021-10-09 04:37:23,741 - INFO - joeynmt.training - Epoch   6, Step:   100000, Batch Loss:     1.010031, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-09 04:38:50,285 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 04:38:50,286 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 04:38:50,286 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 04:38:50,622 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 04:38:50,622 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 04:39:00,004 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 04:39:00,005 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 04:39:00,005 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 04:39:00,005 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-09 04:39:00,005 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 04:39:00,005 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 04:39:00,005 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 04:39:00,005 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one gets to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 04:39:00,005 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 04:39:00,006 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 04:39:00,006 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 04:39:00,006 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” referring to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 04:39:00,006 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 04:39:00,006 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 04:39:00,006 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 04:39:00,006 - INFO - joeynmt.training - \tHypothesis: HOW DO WE WE WE WE DECTED DEAR DEAR ?\n",
      "2021-10-09 04:39:00,006 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   100000: bleu:  47.86, loss: 30466.5977, ppl:   2.7378, duration: 96.2641s\n",
      "2021-10-09 04:39:45,304 - INFO - joeynmt.training - Epoch   6, Step:   100100, Batch Loss:     0.987223, Tokens per Sec:     4733, Lr: 0.000300\n",
      "2021-10-09 04:40:12,363 - INFO - joeynmt.training - Epoch   6: total training loss 14195.09\n",
      "2021-10-09 04:40:12,364 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-10-09 04:40:33,073 - INFO - joeynmt.training - Epoch   7, Step:   100200, Batch Loss:     0.924415, Tokens per Sec:     4762, Lr: 0.000300\n",
      "2021-10-09 04:41:18,484 - INFO - joeynmt.training - Epoch   7, Step:   100300, Batch Loss:     1.121751, Tokens per Sec:     4986, Lr: 0.000300\n",
      "2021-10-09 04:42:03,509 - INFO - joeynmt.training - Epoch   7, Step:   100400, Batch Loss:     1.198916, Tokens per Sec:     4886, Lr: 0.000300\n",
      "2021-10-09 04:42:48,210 - INFO - joeynmt.training - Epoch   7, Step:   100500, Batch Loss:     1.265667, Tokens per Sec:     4932, Lr: 0.000300\n",
      "2021-10-09 04:43:33,529 - INFO - joeynmt.training - Epoch   7, Step:   100600, Batch Loss:     1.182222, Tokens per Sec:     4888, Lr: 0.000300\n",
      "2021-10-09 04:44:19,162 - INFO - joeynmt.training - Epoch   7, Step:   100700, Batch Loss:     1.080622, Tokens per Sec:     4920, Lr: 0.000300\n",
      "2021-10-09 04:45:04,319 - INFO - joeynmt.training - Epoch   7, Step:   100800, Batch Loss:     1.048680, Tokens per Sec:     4878, Lr: 0.000300\n",
      "2021-10-09 04:45:49,561 - INFO - joeynmt.training - Epoch   7, Step:   100900, Batch Loss:     1.128905, Tokens per Sec:     4970, Lr: 0.000300\n",
      "2021-10-09 04:46:35,580 - INFO - joeynmt.training - Epoch   7, Step:   101000, Batch Loss:     1.178708, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-09 04:47:21,745 - INFO - joeynmt.training - Epoch   7, Step:   101100, Batch Loss:     1.216672, Tokens per Sec:     5059, Lr: 0.000300\n",
      "2021-10-09 04:48:07,722 - INFO - joeynmt.training - Epoch   7, Step:   101200, Batch Loss:     0.998738, Tokens per Sec:     5055, Lr: 0.000300\n",
      "2021-10-09 04:48:53,861 - INFO - joeynmt.training - Epoch   7, Step:   101300, Batch Loss:     1.143925, Tokens per Sec:     4974, Lr: 0.000300\n",
      "2021-10-09 04:49:39,284 - INFO - joeynmt.training - Epoch   7, Step:   101400, Batch Loss:     1.356153, Tokens per Sec:     5053, Lr: 0.000300\n",
      "2021-10-09 04:50:25,039 - INFO - joeynmt.training - Epoch   7, Step:   101500, Batch Loss:     1.101191, Tokens per Sec:     4905, Lr: 0.000300\n",
      "2021-10-09 04:51:10,801 - INFO - joeynmt.training - Epoch   7, Step:   101600, Batch Loss:     1.105717, Tokens per Sec:     4955, Lr: 0.000300\n",
      "2021-10-09 04:51:56,017 - INFO - joeynmt.training - Epoch   7, Step:   101700, Batch Loss:     1.113977, Tokens per Sec:     4849, Lr: 0.000300\n",
      "2021-10-09 04:52:42,123 - INFO - joeynmt.training - Epoch   7, Step:   101800, Batch Loss:     1.125576, Tokens per Sec:     4994, Lr: 0.000300\n",
      "2021-10-09 04:53:27,437 - INFO - joeynmt.training - Epoch   7, Step:   101900, Batch Loss:     1.134628, Tokens per Sec:     4979, Lr: 0.000300\n",
      "2021-10-09 04:54:12,818 - INFO - joeynmt.training - Epoch   7, Step:   102000, Batch Loss:     1.148745, Tokens per Sec:     4914, Lr: 0.000300\n",
      "2021-10-09 04:54:57,626 - INFO - joeynmt.training - Epoch   7, Step:   102100, Batch Loss:     1.213322, Tokens per Sec:     4839, Lr: 0.000300\n",
      "2021-10-09 04:55:42,614 - INFO - joeynmt.training - Epoch   7, Step:   102200, Batch Loss:     1.177063, Tokens per Sec:     5018, Lr: 0.000300\n",
      "2021-10-09 04:56:28,493 - INFO - joeynmt.training - Epoch   7, Step:   102300, Batch Loss:     1.083704, Tokens per Sec:     5019, Lr: 0.000300\n",
      "2021-10-09 04:57:13,938 - INFO - joeynmt.training - Epoch   7, Step:   102400, Batch Loss:     1.205841, Tokens per Sec:     4892, Lr: 0.000300\n",
      "2021-10-09 04:57:59,577 - INFO - joeynmt.training - Epoch   7, Step:   102500, Batch Loss:     1.158542, Tokens per Sec:     4958, Lr: 0.000300\n",
      "2021-10-09 04:58:45,160 - INFO - joeynmt.training - Epoch   7, Step:   102600, Batch Loss:     1.123863, Tokens per Sec:     4911, Lr: 0.000300\n",
      "2021-10-09 04:59:31,032 - INFO - joeynmt.training - Epoch   7, Step:   102700, Batch Loss:     1.031509, Tokens per Sec:     4920, Lr: 0.000300\n",
      "2021-10-09 05:00:16,255 - INFO - joeynmt.training - Epoch   7, Step:   102800, Batch Loss:     1.267905, Tokens per Sec:     4830, Lr: 0.000300\n",
      "2021-10-09 05:01:02,433 - INFO - joeynmt.training - Epoch   7, Step:   102900, Batch Loss:     1.075944, Tokens per Sec:     4928, Lr: 0.000300\n",
      "2021-10-09 05:01:47,769 - INFO - joeynmt.training - Epoch   7, Step:   103000, Batch Loss:     1.003980, Tokens per Sec:     4932, Lr: 0.000300\n",
      "2021-10-09 05:02:34,218 - INFO - joeynmt.training - Epoch   7, Step:   103100, Batch Loss:     1.233638, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-09 05:03:20,484 - INFO - joeynmt.training - Epoch   7, Step:   103200, Batch Loss:     1.144298, Tokens per Sec:     4889, Lr: 0.000300\n",
      "2021-10-09 05:04:06,102 - INFO - joeynmt.training - Epoch   7, Step:   103300, Batch Loss:     1.019801, Tokens per Sec:     4945, Lr: 0.000300\n",
      "2021-10-09 05:04:51,669 - INFO - joeynmt.training - Epoch   7, Step:   103400, Batch Loss:     1.214752, Tokens per Sec:     4860, Lr: 0.000300\n",
      "2021-10-09 05:05:37,984 - INFO - joeynmt.training - Epoch   7, Step:   103500, Batch Loss:     1.128239, Tokens per Sec:     5004, Lr: 0.000300\n",
      "2021-10-09 05:06:23,990 - INFO - joeynmt.training - Epoch   7, Step:   103600, Batch Loss:     0.955515, Tokens per Sec:     4869, Lr: 0.000300\n",
      "2021-10-09 05:07:09,832 - INFO - joeynmt.training - Epoch   7, Step:   103700, Batch Loss:     1.077268, Tokens per Sec:     4842, Lr: 0.000300\n",
      "2021-10-09 05:07:56,079 - INFO - joeynmt.training - Epoch   7, Step:   103800, Batch Loss:     1.135520, Tokens per Sec:     4877, Lr: 0.000300\n",
      "2021-10-09 05:08:41,948 - INFO - joeynmt.training - Epoch   7, Step:   103900, Batch Loss:     1.063251, Tokens per Sec:     4867, Lr: 0.000300\n",
      "2021-10-09 05:09:27,944 - INFO - joeynmt.training - Epoch   7, Step:   104000, Batch Loss:     1.105618, Tokens per Sec:     5006, Lr: 0.000300\n",
      "2021-10-09 05:10:13,850 - INFO - joeynmt.training - Epoch   7, Step:   104100, Batch Loss:     1.147692, Tokens per Sec:     4915, Lr: 0.000300\n",
      "2021-10-09 05:10:59,537 - INFO - joeynmt.training - Epoch   7, Step:   104200, Batch Loss:     1.078279, Tokens per Sec:     4945, Lr: 0.000300\n",
      "2021-10-09 05:11:45,221 - INFO - joeynmt.training - Epoch   7, Step:   104300, Batch Loss:     1.008396, Tokens per Sec:     4938, Lr: 0.000300\n",
      "2021-10-09 05:12:31,329 - INFO - joeynmt.training - Epoch   7, Step:   104400, Batch Loss:     1.066500, Tokens per Sec:     5011, Lr: 0.000300\n",
      "2021-10-09 05:13:17,541 - INFO - joeynmt.training - Epoch   7, Step:   104500, Batch Loss:     1.127053, Tokens per Sec:     4976, Lr: 0.000300\n",
      "2021-10-09 05:14:02,410 - INFO - joeynmt.training - Epoch   7, Step:   104600, Batch Loss:     1.001329, Tokens per Sec:     4803, Lr: 0.000300\n",
      "2021-10-09 05:14:48,160 - INFO - joeynmt.training - Epoch   7, Step:   104700, Batch Loss:     1.177861, Tokens per Sec:     4946, Lr: 0.000300\n",
      "2021-10-09 05:15:34,397 - INFO - joeynmt.training - Epoch   7, Step:   104800, Batch Loss:     1.173779, Tokens per Sec:     4927, Lr: 0.000300\n",
      "2021-10-09 05:16:20,553 - INFO - joeynmt.training - Epoch   7, Step:   104900, Batch Loss:     1.052028, Tokens per Sec:     4928, Lr: 0.000300\n",
      "2021-10-09 05:17:06,780 - INFO - joeynmt.training - Epoch   7, Step:   105000, Batch Loss:     1.006917, Tokens per Sec:     4895, Lr: 0.000300\n",
      "2021-10-09 05:18:32,707 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 05:18:32,708 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 05:18:32,708 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 05:18:33,045 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 05:18:33,045 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 05:18:39,145 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 05:18:39,146 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 05:18:39,146 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 05:18:39,146 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at that time for the brother who took the lead in a congregation .\n",
      "2021-10-09 05:18:39,146 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 05:18:39,146 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 05:18:39,146 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 05:18:39,146 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 05:18:39,146 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 05:18:39,147 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 05:18:39,147 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 05:18:39,147 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 05:18:39,147 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 05:18:39,147 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 05:18:39,147 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 05:18:39,147 - INFO - joeynmt.training - \tHypothesis: HOW DO MOST WE HAVE GECTELY THEY ?\n",
      "2021-10-09 05:18:39,147 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   105000: bleu:  48.23, loss: 30169.7422, ppl:   2.7111, duration: 92.3663s\n",
      "2021-10-09 05:19:25,214 - INFO - joeynmt.training - Epoch   7, Step:   105100, Batch Loss:     1.049891, Tokens per Sec:     4988, Lr: 0.000300\n",
      "2021-10-09 05:20:10,472 - INFO - joeynmt.training - Epoch   7, Step:   105200, Batch Loss:     1.028397, Tokens per Sec:     4998, Lr: 0.000300\n",
      "2021-10-09 05:20:56,415 - INFO - joeynmt.training - Epoch   7, Step:   105300, Batch Loss:     1.077552, Tokens per Sec:     5022, Lr: 0.000300\n",
      "2021-10-09 05:21:42,120 - INFO - joeynmt.training - Epoch   7, Step:   105400, Batch Loss:     0.985692, Tokens per Sec:     5003, Lr: 0.000300\n",
      "2021-10-09 05:22:28,017 - INFO - joeynmt.training - Epoch   7, Step:   105500, Batch Loss:     1.114603, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-09 05:23:14,069 - INFO - joeynmt.training - Epoch   7, Step:   105600, Batch Loss:     1.125123, Tokens per Sec:     5063, Lr: 0.000300\n",
      "2021-10-09 05:23:59,578 - INFO - joeynmt.training - Epoch   7, Step:   105700, Batch Loss:     1.064914, Tokens per Sec:     4809, Lr: 0.000300\n",
      "2021-10-09 05:24:44,788 - INFO - joeynmt.training - Epoch   7, Step:   105800, Batch Loss:     1.061490, Tokens per Sec:     4984, Lr: 0.000300\n",
      "2021-10-09 05:25:30,235 - INFO - joeynmt.training - Epoch   7, Step:   105900, Batch Loss:     1.185241, Tokens per Sec:     4975, Lr: 0.000300\n",
      "2021-10-09 05:26:15,272 - INFO - joeynmt.training - Epoch   7, Step:   106000, Batch Loss:     1.044293, Tokens per Sec:     5091, Lr: 0.000300\n",
      "2021-10-09 05:27:00,772 - INFO - joeynmt.training - Epoch   7, Step:   106100, Batch Loss:     1.103352, Tokens per Sec:     5009, Lr: 0.000300\n",
      "2021-10-09 05:27:45,878 - INFO - joeynmt.training - Epoch   7, Step:   106200, Batch Loss:     1.089473, Tokens per Sec:     4964, Lr: 0.000300\n",
      "2021-10-09 05:28:31,233 - INFO - joeynmt.training - Epoch   7, Step:   106300, Batch Loss:     0.994417, Tokens per Sec:     4945, Lr: 0.000300\n",
      "2021-10-09 05:29:16,745 - INFO - joeynmt.training - Epoch   7, Step:   106400, Batch Loss:     1.112048, Tokens per Sec:     4934, Lr: 0.000300\n",
      "2021-10-09 05:30:02,333 - INFO - joeynmt.training - Epoch   7, Step:   106500, Batch Loss:     1.259949, Tokens per Sec:     4883, Lr: 0.000300\n",
      "2021-10-09 05:30:47,697 - INFO - joeynmt.training - Epoch   7, Step:   106600, Batch Loss:     1.116617, Tokens per Sec:     4954, Lr: 0.000300\n",
      "2021-10-09 05:31:33,696 - INFO - joeynmt.training - Epoch   7, Step:   106700, Batch Loss:     1.178637, Tokens per Sec:     4960, Lr: 0.000300\n",
      "2021-10-09 05:32:19,072 - INFO - joeynmt.training - Epoch   7, Step:   106800, Batch Loss:     1.021454, Tokens per Sec:     4863, Lr: 0.000300\n",
      "2021-10-09 05:33:04,621 - INFO - joeynmt.training - Epoch   7, Step:   106900, Batch Loss:     1.065129, Tokens per Sec:     4877, Lr: 0.000300\n",
      "2021-10-09 05:33:49,977 - INFO - joeynmt.training - Epoch   7, Step:   107000, Batch Loss:     0.903189, Tokens per Sec:     4961, Lr: 0.000300\n",
      "2021-10-09 05:34:35,760 - INFO - joeynmt.training - Epoch   7, Step:   107100, Batch Loss:     1.201846, Tokens per Sec:     4977, Lr: 0.000300\n",
      "2021-10-09 05:35:20,753 - INFO - joeynmt.training - Epoch   7, Step:   107200, Batch Loss:     1.073212, Tokens per Sec:     4840, Lr: 0.000300\n",
      "2021-10-09 05:36:06,790 - INFO - joeynmt.training - Epoch   7, Step:   107300, Batch Loss:     1.076440, Tokens per Sec:     5060, Lr: 0.000300\n",
      "2021-10-09 05:36:52,613 - INFO - joeynmt.training - Epoch   7, Step:   107400, Batch Loss:     1.037102, Tokens per Sec:     4922, Lr: 0.000300\n",
      "2021-10-09 05:37:39,071 - INFO - joeynmt.training - Epoch   7, Step:   107500, Batch Loss:     1.225392, Tokens per Sec:     4943, Lr: 0.000300\n",
      "2021-10-09 05:38:24,609 - INFO - joeynmt.training - Epoch   7, Step:   107600, Batch Loss:     1.184797, Tokens per Sec:     4840, Lr: 0.000300\n",
      "2021-10-09 05:39:11,349 - INFO - joeynmt.training - Epoch   7, Step:   107700, Batch Loss:     1.090881, Tokens per Sec:     4876, Lr: 0.000300\n",
      "2021-10-09 05:39:57,325 - INFO - joeynmt.training - Epoch   7, Step:   107800, Batch Loss:     1.187554, Tokens per Sec:     5029, Lr: 0.000300\n",
      "2021-10-09 05:40:43,261 - INFO - joeynmt.training - Epoch   7, Step:   107900, Batch Loss:     1.189226, Tokens per Sec:     4890, Lr: 0.000300\n",
      "2021-10-09 05:41:29,219 - INFO - joeynmt.training - Epoch   7, Step:   108000, Batch Loss:     1.162689, Tokens per Sec:     4911, Lr: 0.000300\n",
      "2021-10-09 05:42:15,357 - INFO - joeynmt.training - Epoch   7, Step:   108100, Batch Loss:     1.130772, Tokens per Sec:     4933, Lr: 0.000300\n",
      "2021-10-09 05:43:01,257 - INFO - joeynmt.training - Epoch   7, Step:   108200, Batch Loss:     1.009141, Tokens per Sec:     4983, Lr: 0.000300\n",
      "2021-10-09 05:43:47,273 - INFO - joeynmt.training - Epoch   7, Step:   108300, Batch Loss:     1.143924, Tokens per Sec:     5024, Lr: 0.000300\n",
      "2021-10-09 05:44:33,365 - INFO - joeynmt.training - Epoch   7, Step:   108400, Batch Loss:     1.196785, Tokens per Sec:     4915, Lr: 0.000300\n",
      "2021-10-09 05:45:19,256 - INFO - joeynmt.training - Epoch   7, Step:   108500, Batch Loss:     0.969858, Tokens per Sec:     4984, Lr: 0.000300\n",
      "2021-10-09 05:46:05,508 - INFO - joeynmt.training - Epoch   7, Step:   108600, Batch Loss:     1.082356, Tokens per Sec:     4956, Lr: 0.000300\n",
      "2021-10-09 05:46:50,172 - INFO - joeynmt.training - Epoch   7, Step:   108700, Batch Loss:     1.116289, Tokens per Sec:     4830, Lr: 0.000300\n",
      "2021-10-09 05:47:35,741 - INFO - joeynmt.training - Epoch   7, Step:   108800, Batch Loss:     1.024441, Tokens per Sec:     4906, Lr: 0.000300\n",
      "2021-10-09 05:48:21,488 - INFO - joeynmt.training - Epoch   7, Step:   108900, Batch Loss:     0.979151, Tokens per Sec:     4958, Lr: 0.000300\n",
      "2021-10-09 05:49:07,645 - INFO - joeynmt.training - Epoch   7, Step:   109000, Batch Loss:     1.174211, Tokens per Sec:     4929, Lr: 0.000300\n",
      "2021-10-09 05:49:53,988 - INFO - joeynmt.training - Epoch   7, Step:   109100, Batch Loss:     1.104979, Tokens per Sec:     5025, Lr: 0.000300\n",
      "2021-10-09 05:50:39,376 - INFO - joeynmt.training - Epoch   7, Step:   109200, Batch Loss:     1.182810, Tokens per Sec:     4823, Lr: 0.000300\n",
      "2021-10-09 05:51:25,020 - INFO - joeynmt.training - Epoch   7, Step:   109300, Batch Loss:     0.920454, Tokens per Sec:     4890, Lr: 0.000300\n",
      "2021-10-09 05:52:09,964 - INFO - joeynmt.training - Epoch   7, Step:   109400, Batch Loss:     1.105670, Tokens per Sec:     4954, Lr: 0.000300\n",
      "2021-10-09 05:52:55,363 - INFO - joeynmt.training - Epoch   7, Step:   109500, Batch Loss:     1.111195, Tokens per Sec:     4926, Lr: 0.000300\n",
      "2021-10-09 05:53:41,656 - INFO - joeynmt.training - Epoch   7, Step:   109600, Batch Loss:     0.998536, Tokens per Sec:     4999, Lr: 0.000300\n",
      "2021-10-09 05:54:27,092 - INFO - joeynmt.training - Epoch   7, Step:   109700, Batch Loss:     1.211206, Tokens per Sec:     4933, Lr: 0.000300\n",
      "2021-10-09 05:55:12,377 - INFO - joeynmt.training - Epoch   7, Step:   109800, Batch Loss:     1.099079, Tokens per Sec:     5023, Lr: 0.000300\n",
      "2021-10-09 05:55:58,148 - INFO - joeynmt.training - Epoch   7, Step:   109900, Batch Loss:     1.200645, Tokens per Sec:     5032, Lr: 0.000300\n",
      "2021-10-09 05:56:44,253 - INFO - joeynmt.training - Epoch   7, Step:   110000, Batch Loss:     1.149517, Tokens per Sec:     4926, Lr: 0.000300\n",
      "2021-10-09 05:58:12,582 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 05:58:12,583 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 05:58:12,583 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 05:58:12,921 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 05:58:12,922 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 05:58:18,962 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 05:58:18,962 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 05:58:18,962 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 05:58:18,963 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at that time for the brother who took the lead in a congregation .\n",
      "2021-10-09 05:58:18,963 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 05:58:18,963 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 05:58:18,963 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 05:58:18,963 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 05:58:18,963 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 05:58:18,963 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 05:58:18,963 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 05:58:18,964 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 05:58:18,964 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 05:58:18,964 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 05:58:18,964 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 05:58:18,964 - INFO - joeynmt.training - \tHypothesis: HOW DO MOST WE WE WE WE GEESTELICE DOELVITE STEL ?\n",
      "2021-10-09 05:58:18,964 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   110000: bleu:  48.61, loss: 29764.1445, ppl:   2.6750, duration: 94.7100s\n",
      "2021-10-09 05:59:05,125 - INFO - joeynmt.training - Epoch   7, Step:   110100, Batch Loss:     1.159336, Tokens per Sec:     5081, Lr: 0.000300\n",
      "2021-10-09 05:59:50,421 - INFO - joeynmt.training - Epoch   7, Step:   110200, Batch Loss:     1.023418, Tokens per Sec:     4928, Lr: 0.000300\n",
      "2021-10-09 06:00:36,195 - INFO - joeynmt.training - Epoch   7, Step:   110300, Batch Loss:     1.047231, Tokens per Sec:     4964, Lr: 0.000300\n",
      "2021-10-09 06:01:22,417 - INFO - joeynmt.training - Epoch   7, Step:   110400, Batch Loss:     1.138936, Tokens per Sec:     4951, Lr: 0.000300\n",
      "2021-10-09 06:02:08,460 - INFO - joeynmt.training - Epoch   7, Step:   110500, Batch Loss:     1.157824, Tokens per Sec:     4906, Lr: 0.000300\n",
      "2021-10-09 06:02:55,081 - INFO - joeynmt.training - Epoch   7, Step:   110600, Batch Loss:     0.961629, Tokens per Sec:     5028, Lr: 0.000300\n",
      "2021-10-09 06:03:42,521 - INFO - joeynmt.training - Epoch   7, Step:   110700, Batch Loss:     1.100639, Tokens per Sec:     4802, Lr: 0.000300\n",
      "2021-10-09 06:04:28,000 - INFO - joeynmt.training - Epoch   7, Step:   110800, Batch Loss:     1.178975, Tokens per Sec:     4885, Lr: 0.000300\n",
      "2021-10-09 06:05:13,716 - INFO - joeynmt.training - Epoch   7, Step:   110900, Batch Loss:     1.202354, Tokens per Sec:     4883, Lr: 0.000300\n",
      "2021-10-09 06:06:00,049 - INFO - joeynmt.training - Epoch   7, Step:   111000, Batch Loss:     1.044060, Tokens per Sec:     4933, Lr: 0.000300\n",
      "2021-10-09 06:06:45,073 - INFO - joeynmt.training - Epoch   7, Step:   111100, Batch Loss:     1.117667, Tokens per Sec:     4909, Lr: 0.000300\n",
      "2021-10-09 06:07:31,855 - INFO - joeynmt.training - Epoch   7, Step:   111200, Batch Loss:     1.023219, Tokens per Sec:     5022, Lr: 0.000300\n",
      "2021-10-09 06:08:17,389 - INFO - joeynmt.training - Epoch   7, Step:   111300, Batch Loss:     1.036100, Tokens per Sec:     4872, Lr: 0.000300\n",
      "2021-10-09 06:09:03,162 - INFO - joeynmt.training - Epoch   7, Step:   111400, Batch Loss:     1.153230, Tokens per Sec:     4867, Lr: 0.000300\n",
      "2021-10-09 06:09:49,189 - INFO - joeynmt.training - Epoch   7, Step:   111500, Batch Loss:     1.094613, Tokens per Sec:     5019, Lr: 0.000300\n",
      "2021-10-09 06:10:35,200 - INFO - joeynmt.training - Epoch   7, Step:   111600, Batch Loss:     0.989197, Tokens per Sec:     4949, Lr: 0.000300\n",
      "2021-10-09 06:11:20,769 - INFO - joeynmt.training - Epoch   7, Step:   111700, Batch Loss:     1.034545, Tokens per Sec:     4940, Lr: 0.000300\n",
      "2021-10-09 06:12:06,705 - INFO - joeynmt.training - Epoch   7, Step:   111800, Batch Loss:     1.213020, Tokens per Sec:     5017, Lr: 0.000300\n",
      "2021-10-09 06:12:52,157 - INFO - joeynmt.training - Epoch   7, Step:   111900, Batch Loss:     0.990903, Tokens per Sec:     4921, Lr: 0.000300\n",
      "2021-10-09 06:13:37,974 - INFO - joeynmt.training - Epoch   7, Step:   112000, Batch Loss:     1.057797, Tokens per Sec:     4921, Lr: 0.000300\n",
      "2021-10-09 06:14:23,947 - INFO - joeynmt.training - Epoch   7, Step:   112100, Batch Loss:     1.130139, Tokens per Sec:     5034, Lr: 0.000300\n",
      "2021-10-09 06:15:09,166 - INFO - joeynmt.training - Epoch   7, Step:   112200, Batch Loss:     1.090436, Tokens per Sec:     4973, Lr: 0.000300\n",
      "2021-10-09 06:15:55,308 - INFO - joeynmt.training - Epoch   7, Step:   112300, Batch Loss:     1.167678, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-09 06:16:40,397 - INFO - joeynmt.training - Epoch   7, Step:   112400, Batch Loss:     1.315639, Tokens per Sec:     4963, Lr: 0.000300\n",
      "2021-10-09 06:17:26,301 - INFO - joeynmt.training - Epoch   7, Step:   112500, Batch Loss:     1.038222, Tokens per Sec:     4897, Lr: 0.000300\n",
      "2021-10-09 06:18:12,651 - INFO - joeynmt.training - Epoch   7, Step:   112600, Batch Loss:     1.144199, Tokens per Sec:     4889, Lr: 0.000300\n",
      "2021-10-09 06:18:48,030 - INFO - joeynmt.training - Epoch   7: total training loss 13867.92\n",
      "2021-10-09 06:18:48,030 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-10-09 06:19:01,057 - INFO - joeynmt.training - Epoch   8, Step:   112700, Batch Loss:     1.054883, Tokens per Sec:     4616, Lr: 0.000300\n",
      "2021-10-09 06:19:46,720 - INFO - joeynmt.training - Epoch   8, Step:   112800, Batch Loss:     1.025446, Tokens per Sec:     4864, Lr: 0.000300\n",
      "2021-10-09 06:20:33,175 - INFO - joeynmt.training - Epoch   8, Step:   112900, Batch Loss:     1.178497, Tokens per Sec:     4942, Lr: 0.000300\n",
      "2021-10-09 06:21:17,985 - INFO - joeynmt.training - Epoch   8, Step:   113000, Batch Loss:     0.984157, Tokens per Sec:     4838, Lr: 0.000300\n",
      "2021-10-09 06:22:03,164 - INFO - joeynmt.training - Epoch   8, Step:   113100, Batch Loss:     1.029826, Tokens per Sec:     5071, Lr: 0.000300\n",
      "2021-10-09 06:22:48,433 - INFO - joeynmt.training - Epoch   8, Step:   113200, Batch Loss:     1.046833, Tokens per Sec:     4949, Lr: 0.000300\n",
      "2021-10-09 06:23:33,308 - INFO - joeynmt.training - Epoch   8, Step:   113300, Batch Loss:     1.060706, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-09 06:24:18,280 - INFO - joeynmt.training - Epoch   8, Step:   113400, Batch Loss:     1.003490, Tokens per Sec:     4893, Lr: 0.000300\n",
      "2021-10-09 06:25:03,904 - INFO - joeynmt.training - Epoch   8, Step:   113500, Batch Loss:     0.942209, Tokens per Sec:     4977, Lr: 0.000300\n",
      "2021-10-09 06:25:49,538 - INFO - joeynmt.training - Epoch   8, Step:   113600, Batch Loss:     1.041305, Tokens per Sec:     5012, Lr: 0.000300\n",
      "2021-10-09 06:26:34,923 - INFO - joeynmt.training - Epoch   8, Step:   113700, Batch Loss:     1.179023, Tokens per Sec:     5025, Lr: 0.000300\n",
      "2021-10-09 06:27:20,866 - INFO - joeynmt.training - Epoch   8, Step:   113800, Batch Loss:     1.122577, Tokens per Sec:     4892, Lr: 0.000300\n",
      "2021-10-09 06:28:06,109 - INFO - joeynmt.training - Epoch   8, Step:   113900, Batch Loss:     1.094645, Tokens per Sec:     4927, Lr: 0.000300\n",
      "2021-10-09 06:28:51,716 - INFO - joeynmt.training - Epoch   8, Step:   114000, Batch Loss:     1.127020, Tokens per Sec:     4963, Lr: 0.000300\n",
      "2021-10-09 06:29:37,201 - INFO - joeynmt.training - Epoch   8, Step:   114100, Batch Loss:     1.023587, Tokens per Sec:     4906, Lr: 0.000300\n",
      "2021-10-09 06:30:22,729 - INFO - joeynmt.training - Epoch   8, Step:   114200, Batch Loss:     1.134648, Tokens per Sec:     4948, Lr: 0.000300\n",
      "2021-10-09 06:31:08,382 - INFO - joeynmt.training - Epoch   8, Step:   114300, Batch Loss:     1.090902, Tokens per Sec:     5077, Lr: 0.000300\n",
      "2021-10-09 06:31:53,828 - INFO - joeynmt.training - Epoch   8, Step:   114400, Batch Loss:     1.005251, Tokens per Sec:     5065, Lr: 0.000300\n",
      "2021-10-09 06:32:39,860 - INFO - joeynmt.training - Epoch   8, Step:   114500, Batch Loss:     1.021719, Tokens per Sec:     5039, Lr: 0.000300\n",
      "2021-10-09 06:33:25,605 - INFO - joeynmt.training - Epoch   8, Step:   114600, Batch Loss:     0.980473, Tokens per Sec:     4947, Lr: 0.000300\n",
      "2021-10-09 06:34:11,470 - INFO - joeynmt.training - Epoch   8, Step:   114700, Batch Loss:     1.235598, Tokens per Sec:     5049, Lr: 0.000300\n",
      "2021-10-09 06:34:56,677 - INFO - joeynmt.training - Epoch   8, Step:   114800, Batch Loss:     1.084416, Tokens per Sec:     4878, Lr: 0.000300\n",
      "2021-10-09 06:35:42,644 - INFO - joeynmt.training - Epoch   8, Step:   114900, Batch Loss:     0.933565, Tokens per Sec:     4955, Lr: 0.000300\n",
      "2021-10-09 06:36:28,626 - INFO - joeynmt.training - Epoch   8, Step:   115000, Batch Loss:     0.911051, Tokens per Sec:     4994, Lr: 0.000300\n",
      "2021-10-09 06:37:54,615 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 06:37:54,616 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 06:37:54,616 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 06:37:54,955 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 06:37:54,955 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 06:38:01,001 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 06:38:01,002 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 06:38:01,002 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 06:38:01,002 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at that time for the brother who took the lead in a congregation .\n",
      "2021-10-09 06:38:01,002 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 06:38:01,002 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 06:38:01,002 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 06:38:01,002 - INFO - joeynmt.training - \tHypothesis: The appeal is understandable , for adolescence is a time when one comes to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 06:38:01,002 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 06:38:01,003 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 06:38:01,003 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 06:38:01,003 - INFO - joeynmt.training - \tHypothesis: He even used the words “ Your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 06:38:01,003 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 06:38:01,003 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 06:38:01,003 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 06:38:01,003 - INFO - joeynmt.training - \tHypothesis: HOW WE MOST WE WE WE SECTLESSICE DOLVITS ?\n",
      "2021-10-09 06:38:01,003 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   115000: bleu:  48.51, loss: 29571.8418, ppl:   2.6580, duration: 92.3769s\n",
      "2021-10-09 06:38:46,168 - INFO - joeynmt.training - Epoch   8, Step:   115100, Batch Loss:     1.094426, Tokens per Sec:     4945, Lr: 0.000300\n",
      "2021-10-09 06:39:32,287 - INFO - joeynmt.training - Epoch   8, Step:   115200, Batch Loss:     1.117532, Tokens per Sec:     4997, Lr: 0.000300\n",
      "2021-10-09 06:40:17,805 - INFO - joeynmt.training - Epoch   8, Step:   115300, Batch Loss:     1.159968, Tokens per Sec:     4940, Lr: 0.000300\n",
      "2021-10-09 06:41:03,981 - INFO - joeynmt.training - Epoch   8, Step:   115400, Batch Loss:     1.182072, Tokens per Sec:     5004, Lr: 0.000300\n",
      "2021-10-09 06:41:49,028 - INFO - joeynmt.training - Epoch   8, Step:   115500, Batch Loss:     0.929975, Tokens per Sec:     4937, Lr: 0.000300\n",
      "2021-10-09 06:42:34,890 - INFO - joeynmt.training - Epoch   8, Step:   115600, Batch Loss:     0.925522, Tokens per Sec:     4946, Lr: 0.000300\n",
      "2021-10-09 06:43:20,752 - INFO - joeynmt.training - Epoch   8, Step:   115700, Batch Loss:     1.131030, Tokens per Sec:     4939, Lr: 0.000300\n",
      "2021-10-09 06:44:05,464 - INFO - joeynmt.training - Epoch   8, Step:   115800, Batch Loss:     1.073881, Tokens per Sec:     4900, Lr: 0.000300\n",
      "2021-10-09 06:44:51,305 - INFO - joeynmt.training - Epoch   8, Step:   115900, Batch Loss:     1.216998, Tokens per Sec:     4942, Lr: 0.000300\n",
      "2021-10-09 06:45:37,072 - INFO - joeynmt.training - Epoch   8, Step:   116000, Batch Loss:     1.165744, Tokens per Sec:     4905, Lr: 0.000300\n",
      "2021-10-09 06:46:22,360 - INFO - joeynmt.training - Epoch   8, Step:   116100, Batch Loss:     0.998340, Tokens per Sec:     4891, Lr: 0.000300\n",
      "2021-10-09 06:47:07,275 - INFO - joeynmt.training - Epoch   8, Step:   116200, Batch Loss:     1.065004, Tokens per Sec:     4893, Lr: 0.000300\n",
      "2021-10-09 06:47:53,025 - INFO - joeynmt.training - Epoch   8, Step:   116300, Batch Loss:     1.195606, Tokens per Sec:     5000, Lr: 0.000300\n",
      "2021-10-09 06:48:38,573 - INFO - joeynmt.training - Epoch   8, Step:   116400, Batch Loss:     1.072688, Tokens per Sec:     4951, Lr: 0.000300\n",
      "2021-10-09 06:49:24,808 - INFO - joeynmt.training - Epoch   8, Step:   116500, Batch Loss:     1.038887, Tokens per Sec:     4965, Lr: 0.000300\n",
      "2021-10-09 06:50:11,099 - INFO - joeynmt.training - Epoch   8, Step:   116600, Batch Loss:     1.183091, Tokens per Sec:     4988, Lr: 0.000300\n",
      "2021-10-09 06:50:56,985 - INFO - joeynmt.training - Epoch   8, Step:   116700, Batch Loss:     1.190310, Tokens per Sec:     4971, Lr: 0.000300\n",
      "2021-10-09 06:51:43,000 - INFO - joeynmt.training - Epoch   8, Step:   116800, Batch Loss:     1.147022, Tokens per Sec:     4989, Lr: 0.000300\n",
      "2021-10-09 06:52:28,673 - INFO - joeynmt.training - Epoch   8, Step:   116900, Batch Loss:     0.994815, Tokens per Sec:     4845, Lr: 0.000300\n",
      "2021-10-09 06:53:14,632 - INFO - joeynmt.training - Epoch   8, Step:   117000, Batch Loss:     1.096869, Tokens per Sec:     4925, Lr: 0.000300\n",
      "2021-10-09 06:54:00,872 - INFO - joeynmt.training - Epoch   8, Step:   117100, Batch Loss:     0.981541, Tokens per Sec:     4960, Lr: 0.000300\n",
      "2021-10-09 06:54:47,183 - INFO - joeynmt.training - Epoch   8, Step:   117200, Batch Loss:     1.233345, Tokens per Sec:     4972, Lr: 0.000300\n",
      "2021-10-09 06:55:32,614 - INFO - joeynmt.training - Epoch   8, Step:   117300, Batch Loss:     1.185686, Tokens per Sec:     4821, Lr: 0.000300\n",
      "2021-10-09 06:56:18,044 - INFO - joeynmt.training - Epoch   8, Step:   117400, Batch Loss:     0.923204, Tokens per Sec:     4906, Lr: 0.000300\n",
      "2021-10-09 06:57:04,336 - INFO - joeynmt.training - Epoch   8, Step:   117500, Batch Loss:     1.151787, Tokens per Sec:     5009, Lr: 0.000300\n",
      "2021-10-09 06:57:50,364 - INFO - joeynmt.training - Epoch   8, Step:   117600, Batch Loss:     1.169687, Tokens per Sec:     5017, Lr: 0.000300\n",
      "2021-10-09 06:58:36,252 - INFO - joeynmt.training - Epoch   8, Step:   117700, Batch Loss:     1.134046, Tokens per Sec:     4884, Lr: 0.000300\n",
      "2021-10-09 06:59:22,796 - INFO - joeynmt.training - Epoch   8, Step:   117800, Batch Loss:     1.104756, Tokens per Sec:     4942, Lr: 0.000300\n",
      "2021-10-09 07:00:08,452 - INFO - joeynmt.training - Epoch   8, Step:   117900, Batch Loss:     1.039075, Tokens per Sec:     4807, Lr: 0.000300\n",
      "2021-10-09 07:00:55,812 - INFO - joeynmt.training - Epoch   8, Step:   118000, Batch Loss:     1.172493, Tokens per Sec:     4873, Lr: 0.000300\n",
      "2021-10-09 07:01:41,087 - INFO - joeynmt.training - Epoch   8, Step:   118100, Batch Loss:     1.035116, Tokens per Sec:     4897, Lr: 0.000300\n",
      "2021-10-09 07:02:26,881 - INFO - joeynmt.training - Epoch   8, Step:   118200, Batch Loss:     1.164271, Tokens per Sec:     4884, Lr: 0.000300\n",
      "2021-10-09 07:03:12,603 - INFO - joeynmt.training - Epoch   8, Step:   118300, Batch Loss:     1.162750, Tokens per Sec:     4892, Lr: 0.000300\n",
      "2021-10-09 07:03:58,021 - INFO - joeynmt.training - Epoch   8, Step:   118400, Batch Loss:     0.965166, Tokens per Sec:     4906, Lr: 0.000300\n",
      "2021-10-09 07:04:45,001 - INFO - joeynmt.training - Epoch   8, Step:   118500, Batch Loss:     1.147471, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-09 07:05:31,148 - INFO - joeynmt.training - Epoch   8, Step:   118600, Batch Loss:     1.242109, Tokens per Sec:     4918, Lr: 0.000300\n",
      "2021-10-09 07:06:17,117 - INFO - joeynmt.training - Epoch   8, Step:   118700, Batch Loss:     0.874627, Tokens per Sec:     4906, Lr: 0.000300\n",
      "2021-10-09 07:07:03,293 - INFO - joeynmt.training - Epoch   8, Step:   118800, Batch Loss:     1.049947, Tokens per Sec:     5000, Lr: 0.000300\n",
      "2021-10-09 07:07:48,201 - INFO - joeynmt.training - Epoch   8, Step:   118900, Batch Loss:     1.162858, Tokens per Sec:     4991, Lr: 0.000300\n",
      "2021-10-09 07:08:34,679 - INFO - joeynmt.training - Epoch   8, Step:   119000, Batch Loss:     1.016656, Tokens per Sec:     4980, Lr: 0.000300\n",
      "2021-10-09 07:09:20,546 - INFO - joeynmt.training - Epoch   8, Step:   119100, Batch Loss:     1.028122, Tokens per Sec:     4911, Lr: 0.000300\n",
      "2021-10-09 07:10:06,360 - INFO - joeynmt.training - Epoch   8, Step:   119200, Batch Loss:     0.993761, Tokens per Sec:     4891, Lr: 0.000300\n",
      "2021-10-09 07:10:52,229 - INFO - joeynmt.training - Epoch   8, Step:   119300, Batch Loss:     1.285773, Tokens per Sec:     4914, Lr: 0.000300\n",
      "2021-10-09 07:11:38,306 - INFO - joeynmt.training - Epoch   8, Step:   119400, Batch Loss:     0.944803, Tokens per Sec:     5013, Lr: 0.000300\n",
      "2021-10-09 07:12:24,403 - INFO - joeynmt.training - Epoch   8, Step:   119500, Batch Loss:     1.050930, Tokens per Sec:     4878, Lr: 0.000300\n",
      "2021-10-09 07:13:11,029 - INFO - joeynmt.training - Epoch   8, Step:   119600, Batch Loss:     1.099911, Tokens per Sec:     5026, Lr: 0.000300\n",
      "2021-10-09 07:13:57,037 - INFO - joeynmt.training - Epoch   8, Step:   119700, Batch Loss:     1.019902, Tokens per Sec:     4924, Lr: 0.000300\n",
      "2021-10-09 07:14:42,761 - INFO - joeynmt.training - Epoch   8, Step:   119800, Batch Loss:     1.241654, Tokens per Sec:     5021, Lr: 0.000300\n",
      "2021-10-09 07:15:28,769 - INFO - joeynmt.training - Epoch   8, Step:   119900, Batch Loss:     1.097858, Tokens per Sec:     4912, Lr: 0.000300\n",
      "2021-10-09 07:16:14,683 - INFO - joeynmt.training - Epoch   8, Step:   120000, Batch Loss:     1.050060, Tokens per Sec:     4912, Lr: 0.000300\n",
      "2021-10-09 07:17:40,923 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 07:17:40,932 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 07:17:40,932 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 07:17:41,272 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 07:17:41,272 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 07:17:48,108 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 07:17:48,109 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 07:17:48,109 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 07:17:48,109 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-09 07:17:48,109 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 07:17:48,110 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 07:17:48,110 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 07:17:48,110 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 07:17:48,110 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 07:17:48,110 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 07:17:48,110 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 07:17:48,110 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to respectfully acknowledge that Nabal was older than he was .\n",
      "2021-10-09 07:17:48,111 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 07:17:48,111 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 07:17:48,111 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 07:17:48,111 - INFO - joeynmt.training - \tHypothesis: HOW WE MOST WE WE GEECTLY DOELVITS STEL ?\n",
      "2021-10-09 07:17:48,111 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   120000: bleu:  48.48, loss: 29475.8398, ppl:   2.6496, duration: 93.4275s\n",
      "2021-10-09 07:18:34,206 - INFO - joeynmt.training - Epoch   8, Step:   120100, Batch Loss:     1.113425, Tokens per Sec:     4921, Lr: 0.000300\n",
      "2021-10-09 07:19:19,605 - INFO - joeynmt.training - Epoch   8, Step:   120200, Batch Loss:     1.032412, Tokens per Sec:     4813, Lr: 0.000300\n",
      "2021-10-09 07:20:05,937 - INFO - joeynmt.training - Epoch   8, Step:   120300, Batch Loss:     1.057231, Tokens per Sec:     5054, Lr: 0.000300\n",
      "2021-10-09 07:20:52,269 - INFO - joeynmt.training - Epoch   8, Step:   120400, Batch Loss:     1.208040, Tokens per Sec:     4983, Lr: 0.000300\n",
      "2021-10-09 07:21:38,497 - INFO - joeynmt.training - Epoch   8, Step:   120500, Batch Loss:     0.908773, Tokens per Sec:     4968, Lr: 0.000300\n",
      "2021-10-09 07:22:24,781 - INFO - joeynmt.training - Epoch   8, Step:   120600, Batch Loss:     1.093854, Tokens per Sec:     4910, Lr: 0.000300\n",
      "2021-10-09 07:23:10,334 - INFO - joeynmt.training - Epoch   8, Step:   120700, Batch Loss:     1.042852, Tokens per Sec:     4945, Lr: 0.000300\n",
      "2021-10-09 07:23:56,221 - INFO - joeynmt.training - Epoch   8, Step:   120800, Batch Loss:     1.106145, Tokens per Sec:     4876, Lr: 0.000300\n",
      "2021-10-09 07:24:41,057 - INFO - joeynmt.training - Epoch   8, Step:   120900, Batch Loss:     1.084234, Tokens per Sec:     4860, Lr: 0.000300\n",
      "2021-10-09 07:25:26,661 - INFO - joeynmt.training - Epoch   8, Step:   121000, Batch Loss:     1.106176, Tokens per Sec:     5037, Lr: 0.000300\n",
      "2021-10-09 07:26:12,052 - INFO - joeynmt.training - Epoch   8, Step:   121100, Batch Loss:     1.032440, Tokens per Sec:     5027, Lr: 0.000300\n",
      "2021-10-09 07:26:56,942 - INFO - joeynmt.training - Epoch   8, Step:   121200, Batch Loss:     1.132082, Tokens per Sec:     4962, Lr: 0.000300\n",
      "2021-10-09 07:27:42,732 - INFO - joeynmt.training - Epoch   8, Step:   121300, Batch Loss:     0.935979, Tokens per Sec:     4944, Lr: 0.000300\n",
      "2021-10-09 07:28:27,754 - INFO - joeynmt.training - Epoch   8, Step:   121400, Batch Loss:     0.894136, Tokens per Sec:     4947, Lr: 0.000300\n",
      "2021-10-09 07:29:13,705 - INFO - joeynmt.training - Epoch   8, Step:   121500, Batch Loss:     1.122282, Tokens per Sec:     5084, Lr: 0.000300\n",
      "2021-10-09 07:29:59,269 - INFO - joeynmt.training - Epoch   8, Step:   121600, Batch Loss:     1.084491, Tokens per Sec:     4896, Lr: 0.000300\n",
      "2021-10-09 07:30:44,414 - INFO - joeynmt.training - Epoch   8, Step:   121700, Batch Loss:     1.096863, Tokens per Sec:     4871, Lr: 0.000300\n",
      "2021-10-09 07:31:30,320 - INFO - joeynmt.training - Epoch   8, Step:   121800, Batch Loss:     1.062456, Tokens per Sec:     5117, Lr: 0.000300\n",
      "2021-10-09 07:32:16,224 - INFO - joeynmt.training - Epoch   8, Step:   121900, Batch Loss:     1.174696, Tokens per Sec:     4964, Lr: 0.000300\n",
      "2021-10-09 07:33:02,460 - INFO - joeynmt.training - Epoch   8, Step:   122000, Batch Loss:     1.066447, Tokens per Sec:     4995, Lr: 0.000300\n",
      "2021-10-09 07:33:47,488 - INFO - joeynmt.training - Epoch   8, Step:   122100, Batch Loss:     1.014305, Tokens per Sec:     4889, Lr: 0.000300\n",
      "2021-10-09 07:34:32,555 - INFO - joeynmt.training - Epoch   8, Step:   122200, Batch Loss:     1.147275, Tokens per Sec:     4961, Lr: 0.000300\n",
      "2021-10-09 07:35:17,517 - INFO - joeynmt.training - Epoch   8, Step:   122300, Batch Loss:     1.024132, Tokens per Sec:     4894, Lr: 0.000300\n",
      "2021-10-09 07:36:03,452 - INFO - joeynmt.training - Epoch   8, Step:   122400, Batch Loss:     1.056534, Tokens per Sec:     5028, Lr: 0.000300\n",
      "2021-10-09 07:36:48,796 - INFO - joeynmt.training - Epoch   8, Step:   122500, Batch Loss:     1.096905, Tokens per Sec:     4954, Lr: 0.000300\n",
      "2021-10-09 07:37:34,228 - INFO - joeynmt.training - Epoch   8, Step:   122600, Batch Loss:     1.082832, Tokens per Sec:     4849, Lr: 0.000300\n",
      "2021-10-09 07:38:20,777 - INFO - joeynmt.training - Epoch   8, Step:   122700, Batch Loss:     1.052689, Tokens per Sec:     5032, Lr: 0.000300\n",
      "2021-10-09 07:39:07,125 - INFO - joeynmt.training - Epoch   8, Step:   122800, Batch Loss:     1.010952, Tokens per Sec:     4942, Lr: 0.000300\n",
      "2021-10-09 07:39:52,816 - INFO - joeynmt.training - Epoch   8, Step:   122900, Batch Loss:     1.113036, Tokens per Sec:     4915, Lr: 0.000300\n",
      "2021-10-09 07:40:38,554 - INFO - joeynmt.training - Epoch   8, Step:   123000, Batch Loss:     1.174964, Tokens per Sec:     4911, Lr: 0.000300\n",
      "2021-10-09 07:41:24,215 - INFO - joeynmt.training - Epoch   8, Step:   123100, Batch Loss:     0.962499, Tokens per Sec:     4870, Lr: 0.000300\n",
      "2021-10-09 07:42:10,372 - INFO - joeynmt.training - Epoch   8, Step:   123200, Batch Loss:     1.150095, Tokens per Sec:     4986, Lr: 0.000300\n",
      "2021-10-09 07:42:56,153 - INFO - joeynmt.training - Epoch   8, Step:   123300, Batch Loss:     1.031002, Tokens per Sec:     4930, Lr: 0.000300\n",
      "2021-10-09 07:43:41,505 - INFO - joeynmt.training - Epoch   8, Step:   123400, Batch Loss:     1.067041, Tokens per Sec:     4942, Lr: 0.000300\n",
      "2021-10-09 07:44:27,536 - INFO - joeynmt.training - Epoch   8, Step:   123500, Batch Loss:     1.202081, Tokens per Sec:     5014, Lr: 0.000300\n",
      "2021-10-09 07:45:12,882 - INFO - joeynmt.training - Epoch   8, Step:   123600, Batch Loss:     1.093139, Tokens per Sec:     4936, Lr: 0.000300\n",
      "2021-10-09 07:45:59,024 - INFO - joeynmt.training - Epoch   8, Step:   123700, Batch Loss:     1.055127, Tokens per Sec:     4946, Lr: 0.000300\n",
      "2021-10-09 07:46:45,255 - INFO - joeynmt.training - Epoch   8, Step:   123800, Batch Loss:     1.189280, Tokens per Sec:     4951, Lr: 0.000300\n",
      "2021-10-09 07:47:31,153 - INFO - joeynmt.training - Epoch   8, Step:   123900, Batch Loss:     1.050379, Tokens per Sec:     4989, Lr: 0.000300\n",
      "2021-10-09 07:48:16,622 - INFO - joeynmt.training - Epoch   8, Step:   124000, Batch Loss:     1.156137, Tokens per Sec:     4937, Lr: 0.000300\n",
      "2021-10-09 07:49:02,648 - INFO - joeynmt.training - Epoch   8, Step:   124100, Batch Loss:     1.014272, Tokens per Sec:     5012, Lr: 0.000300\n",
      "2021-10-09 07:49:48,461 - INFO - joeynmt.training - Epoch   8, Step:   124200, Batch Loss:     1.098944, Tokens per Sec:     4930, Lr: 0.000300\n",
      "2021-10-09 07:50:34,152 - INFO - joeynmt.training - Epoch   8, Step:   124300, Batch Loss:     1.484388, Tokens per Sec:     4918, Lr: 0.000300\n",
      "2021-10-09 07:51:19,636 - INFO - joeynmt.training - Epoch   8, Step:   124400, Batch Loss:     1.387810, Tokens per Sec:     4967, Lr: 0.000300\n",
      "2021-10-09 07:52:05,837 - INFO - joeynmt.training - Epoch   8, Step:   124500, Batch Loss:     1.121571, Tokens per Sec:     4947, Lr: 0.000300\n",
      "2021-10-09 07:52:51,124 - INFO - joeynmt.training - Epoch   8, Step:   124600, Batch Loss:     1.147037, Tokens per Sec:     4859, Lr: 0.000300\n",
      "2021-10-09 07:53:36,829 - INFO - joeynmt.training - Epoch   8, Step:   124700, Batch Loss:     1.266741, Tokens per Sec:     4936, Lr: 0.000300\n",
      "2021-10-09 07:54:22,557 - INFO - joeynmt.training - Epoch   8, Step:   124800, Batch Loss:     1.096313, Tokens per Sec:     4955, Lr: 0.000300\n",
      "2021-10-09 07:55:08,857 - INFO - joeynmt.training - Epoch   8, Step:   124900, Batch Loss:     1.029842, Tokens per Sec:     4927, Lr: 0.000300\n",
      "2021-10-09 07:55:54,589 - INFO - joeynmt.training - Epoch   8, Step:   125000, Batch Loss:     0.920563, Tokens per Sec:     4849, Lr: 0.000300\n",
      "2021-10-09 07:57:21,381 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 07:57:21,381 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 07:57:21,381 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 07:57:21,718 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 07:57:21,719 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 07:57:29,627 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 07:57:29,631 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 07:57:29,631 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 07:57:29,632 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-09 07:57:29,632 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 07:57:29,632 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 07:57:29,632 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 07:57:29,633 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when you get to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 07:57:29,633 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 07:57:29,633 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 07:57:29,634 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 07:57:29,634 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” referring to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 07:57:29,634 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 07:57:29,634 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 07:57:29,634 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 07:57:29,635 - INFO - joeynmt.training - \tHypothesis: HOW WE MOTHING US SECTELICE DOELVITTE ?\n",
      "2021-10-09 07:57:29,635 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   125000: bleu:  48.57, loss: 29280.6035, ppl:   2.6326, duration: 95.0449s\n",
      "2021-10-09 07:58:15,772 - INFO - joeynmt.training - Epoch   8, Step:   125100, Batch Loss:     1.065349, Tokens per Sec:     4993, Lr: 0.000300\n",
      "2021-10-09 07:58:53,398 - INFO - joeynmt.training - Epoch   8: total training loss 13587.20\n",
      "2021-10-09 07:58:53,400 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-10-09 07:59:03,845 - INFO - joeynmt.training - Epoch   9, Step:   125200, Batch Loss:     1.064692, Tokens per Sec:     4156, Lr: 0.000300\n",
      "2021-10-09 07:59:48,716 - INFO - joeynmt.training - Epoch   9, Step:   125300, Batch Loss:     1.030171, Tokens per Sec:     4887, Lr: 0.000300\n",
      "2021-10-09 08:00:35,558 - INFO - joeynmt.training - Epoch   9, Step:   125400, Batch Loss:     1.094269, Tokens per Sec:     4977, Lr: 0.000300\n",
      "2021-10-09 08:01:21,226 - INFO - joeynmt.training - Epoch   9, Step:   125500, Batch Loss:     1.040815, Tokens per Sec:     4943, Lr: 0.000300\n",
      "2021-10-09 08:02:07,655 - INFO - joeynmt.training - Epoch   9, Step:   125600, Batch Loss:     1.164544, Tokens per Sec:     4950, Lr: 0.000300\n",
      "2021-10-09 08:02:53,208 - INFO - joeynmt.training - Epoch   9, Step:   125700, Batch Loss:     1.005071, Tokens per Sec:     4910, Lr: 0.000300\n",
      "2021-10-09 08:03:39,598 - INFO - joeynmt.training - Epoch   9, Step:   125800, Batch Loss:     1.032340, Tokens per Sec:     4977, Lr: 0.000300\n",
      "2021-10-09 08:04:25,657 - INFO - joeynmt.training - Epoch   9, Step:   125900, Batch Loss:     1.115275, Tokens per Sec:     4948, Lr: 0.000300\n",
      "2021-10-09 08:05:11,258 - INFO - joeynmt.training - Epoch   9, Step:   126000, Batch Loss:     0.941994, Tokens per Sec:     4966, Lr: 0.000300\n",
      "2021-10-09 08:05:57,169 - INFO - joeynmt.training - Epoch   9, Step:   126100, Batch Loss:     1.022064, Tokens per Sec:     4998, Lr: 0.000300\n",
      "2021-10-09 08:06:43,167 - INFO - joeynmt.training - Epoch   9, Step:   126200, Batch Loss:     1.037046, Tokens per Sec:     4899, Lr: 0.000300\n",
      "2021-10-09 08:07:28,321 - INFO - joeynmt.training - Epoch   9, Step:   126300, Batch Loss:     1.138911, Tokens per Sec:     4843, Lr: 0.000300\n",
      "2021-10-09 08:08:14,111 - INFO - joeynmt.training - Epoch   9, Step:   126400, Batch Loss:     1.076782, Tokens per Sec:     4936, Lr: 0.000300\n",
      "2021-10-09 08:08:59,573 - INFO - joeynmt.training - Epoch   9, Step:   126500, Batch Loss:     1.090559, Tokens per Sec:     4955, Lr: 0.000300\n",
      "2021-10-09 08:09:45,564 - INFO - joeynmt.training - Epoch   9, Step:   126600, Batch Loss:     1.113310, Tokens per Sec:     4969, Lr: 0.000300\n",
      "2021-10-09 08:10:31,144 - INFO - joeynmt.training - Epoch   9, Step:   126700, Batch Loss:     1.105162, Tokens per Sec:     4898, Lr: 0.000300\n",
      "2021-10-09 08:11:16,281 - INFO - joeynmt.training - Epoch   9, Step:   126800, Batch Loss:     1.005125, Tokens per Sec:     4743, Lr: 0.000300\n",
      "2021-10-09 08:12:02,948 - INFO - joeynmt.training - Epoch   9, Step:   126900, Batch Loss:     1.069901, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-09 08:12:48,477 - INFO - joeynmt.training - Epoch   9, Step:   127000, Batch Loss:     1.045503, Tokens per Sec:     4911, Lr: 0.000300\n",
      "2021-10-09 08:13:34,613 - INFO - joeynmt.training - Epoch   9, Step:   127100, Batch Loss:     1.051803, Tokens per Sec:     4879, Lr: 0.000300\n",
      "2021-10-09 08:14:20,179 - INFO - joeynmt.training - Epoch   9, Step:   127200, Batch Loss:     1.102748, Tokens per Sec:     4916, Lr: 0.000300\n",
      "2021-10-09 08:15:06,497 - INFO - joeynmt.training - Epoch   9, Step:   127300, Batch Loss:     0.942476, Tokens per Sec:     4976, Lr: 0.000300\n",
      "2021-10-09 08:15:52,308 - INFO - joeynmt.training - Epoch   9, Step:   127400, Batch Loss:     1.107704, Tokens per Sec:     5044, Lr: 0.000300\n",
      "2021-10-09 08:16:38,770 - INFO - joeynmt.training - Epoch   9, Step:   127500, Batch Loss:     0.972219, Tokens per Sec:     5041, Lr: 0.000300\n",
      "2021-10-09 08:17:24,541 - INFO - joeynmt.training - Epoch   9, Step:   127600, Batch Loss:     1.098648, Tokens per Sec:     5019, Lr: 0.000300\n",
      "2021-10-09 08:18:10,275 - INFO - joeynmt.training - Epoch   9, Step:   127700, Batch Loss:     1.119702, Tokens per Sec:     4914, Lr: 0.000300\n",
      "2021-10-09 08:18:55,970 - INFO - joeynmt.training - Epoch   9, Step:   127800, Batch Loss:     1.103771, Tokens per Sec:     4947, Lr: 0.000300\n",
      "2021-10-09 08:19:41,626 - INFO - joeynmt.training - Epoch   9, Step:   127900, Batch Loss:     1.179070, Tokens per Sec:     4957, Lr: 0.000300\n",
      "2021-10-09 08:20:26,869 - INFO - joeynmt.training - Epoch   9, Step:   128000, Batch Loss:     1.063512, Tokens per Sec:     4820, Lr: 0.000300\n",
      "2021-10-09 08:21:12,897 - INFO - joeynmt.training - Epoch   9, Step:   128100, Batch Loss:     1.093316, Tokens per Sec:     4970, Lr: 0.000300\n",
      "2021-10-09 08:21:58,975 - INFO - joeynmt.training - Epoch   9, Step:   128200, Batch Loss:     1.135027, Tokens per Sec:     4923, Lr: 0.000300\n",
      "2021-10-09 08:22:44,444 - INFO - joeynmt.training - Epoch   9, Step:   128300, Batch Loss:     1.155915, Tokens per Sec:     4892, Lr: 0.000300\n",
      "2021-10-09 08:23:29,930 - INFO - joeynmt.training - Epoch   9, Step:   128400, Batch Loss:     0.968658, Tokens per Sec:     4935, Lr: 0.000300\n",
      "2021-10-09 08:24:15,892 - INFO - joeynmt.training - Epoch   9, Step:   128500, Batch Loss:     1.018179, Tokens per Sec:     5031, Lr: 0.000300\n",
      "2021-10-09 08:25:01,653 - INFO - joeynmt.training - Epoch   9, Step:   128600, Batch Loss:     1.134556, Tokens per Sec:     4967, Lr: 0.000300\n",
      "2021-10-09 08:25:47,599 - INFO - joeynmt.training - Epoch   9, Step:   128700, Batch Loss:     0.963539, Tokens per Sec:     4902, Lr: 0.000300\n",
      "2021-10-09 08:26:33,382 - INFO - joeynmt.training - Epoch   9, Step:   128800, Batch Loss:     1.077319, Tokens per Sec:     4982, Lr: 0.000300\n",
      "2021-10-09 08:27:18,424 - INFO - joeynmt.training - Epoch   9, Step:   128900, Batch Loss:     1.247792, Tokens per Sec:     4971, Lr: 0.000300\n",
      "2021-10-09 08:28:04,417 - INFO - joeynmt.training - Epoch   9, Step:   129000, Batch Loss:     1.211226, Tokens per Sec:     4784, Lr: 0.000300\n",
      "2021-10-09 08:28:49,827 - INFO - joeynmt.training - Epoch   9, Step:   129100, Batch Loss:     1.135702, Tokens per Sec:     4953, Lr: 0.000300\n",
      "2021-10-09 08:29:35,460 - INFO - joeynmt.training - Epoch   9, Step:   129200, Batch Loss:     1.205963, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-09 08:30:20,581 - INFO - joeynmt.training - Epoch   9, Step:   129300, Batch Loss:     1.070417, Tokens per Sec:     4873, Lr: 0.000300\n",
      "2021-10-09 08:31:07,169 - INFO - joeynmt.training - Epoch   9, Step:   129400, Batch Loss:     1.069847, Tokens per Sec:     4965, Lr: 0.000300\n",
      "2021-10-09 08:31:52,845 - INFO - joeynmt.training - Epoch   9, Step:   129500, Batch Loss:     1.186437, Tokens per Sec:     4965, Lr: 0.000300\n",
      "2021-10-09 08:32:38,776 - INFO - joeynmt.training - Epoch   9, Step:   129600, Batch Loss:     1.066316, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-09 08:33:23,959 - INFO - joeynmt.training - Epoch   9, Step:   129700, Batch Loss:     0.910167, Tokens per Sec:     4954, Lr: 0.000300\n",
      "2021-10-09 08:34:10,269 - INFO - joeynmt.training - Epoch   9, Step:   129800, Batch Loss:     0.982454, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-09 08:34:56,160 - INFO - joeynmt.training - Epoch   9, Step:   129900, Batch Loss:     1.176826, Tokens per Sec:     4957, Lr: 0.000300\n",
      "2021-10-09 08:35:42,173 - INFO - joeynmt.training - Epoch   9, Step:   130000, Batch Loss:     1.088018, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-09 08:37:09,488 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 08:37:09,489 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 08:37:09,489 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 08:37:09,826 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 08:37:09,826 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 08:37:15,946 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 08:37:15,946 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 08:37:15,946 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 08:37:15,947 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used then for the brother who took the lead in a congregation .\n",
      "2021-10-09 08:37:15,947 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 08:37:15,947 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 08:37:15,947 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 08:37:15,947 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know himself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 08:37:15,947 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 08:37:15,947 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 08:37:15,948 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 08:37:15,948 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 08:37:15,948 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 08:37:15,948 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 08:37:15,948 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 08:37:15,948 - INFO - joeynmt.training - \tHypothesis: HOW DO MODE WE SECTLE OUR DOELVITS ?\n",
      "2021-10-09 08:37:15,948 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   130000: bleu:  48.90, loss: 28920.0156, ppl:   2.6014, duration: 93.7746s\n",
      "2021-10-09 08:38:01,733 - INFO - joeynmt.training - Epoch   9, Step:   130100, Batch Loss:     1.181965, Tokens per Sec:     4954, Lr: 0.000300\n",
      "2021-10-09 08:38:47,842 - INFO - joeynmt.training - Epoch   9, Step:   130200, Batch Loss:     1.017375, Tokens per Sec:     4919, Lr: 0.000300\n",
      "2021-10-09 08:39:32,862 - INFO - joeynmt.training - Epoch   9, Step:   130300, Batch Loss:     1.205422, Tokens per Sec:     4867, Lr: 0.000300\n",
      "2021-10-09 08:40:18,172 - INFO - joeynmt.training - Epoch   9, Step:   130400, Batch Loss:     1.021304, Tokens per Sec:     4805, Lr: 0.000300\n",
      "2021-10-09 08:41:04,293 - INFO - joeynmt.training - Epoch   9, Step:   130500, Batch Loss:     1.143800, Tokens per Sec:     4908, Lr: 0.000300\n",
      "2021-10-09 08:41:50,461 - INFO - joeynmt.training - Epoch   9, Step:   130600, Batch Loss:     1.094673, Tokens per Sec:     4875, Lr: 0.000300\n",
      "2021-10-09 08:42:35,829 - INFO - joeynmt.training - Epoch   9, Step:   130700, Batch Loss:     1.204346, Tokens per Sec:     5010, Lr: 0.000300\n",
      "2021-10-09 08:43:20,984 - INFO - joeynmt.training - Epoch   9, Step:   130800, Batch Loss:     1.315288, Tokens per Sec:     5088, Lr: 0.000300\n",
      "2021-10-09 08:44:06,881 - INFO - joeynmt.training - Epoch   9, Step:   130900, Batch Loss:     1.083968, Tokens per Sec:     4958, Lr: 0.000300\n",
      "2021-10-09 08:44:53,049 - INFO - joeynmt.training - Epoch   9, Step:   131000, Batch Loss:     1.160800, Tokens per Sec:     4893, Lr: 0.000300\n",
      "2021-10-09 08:45:38,385 - INFO - joeynmt.training - Epoch   9, Step:   131100, Batch Loss:     1.051399, Tokens per Sec:     4871, Lr: 0.000300\n",
      "2021-10-09 08:46:24,080 - INFO - joeynmt.training - Epoch   9, Step:   131200, Batch Loss:     1.190540, Tokens per Sec:     5011, Lr: 0.000300\n",
      "2021-10-09 08:47:09,297 - INFO - joeynmt.training - Epoch   9, Step:   131300, Batch Loss:     0.991513, Tokens per Sec:     4972, Lr: 0.000300\n",
      "2021-10-09 08:47:54,930 - INFO - joeynmt.training - Epoch   9, Step:   131400, Batch Loss:     1.188598, Tokens per Sec:     4957, Lr: 0.000300\n",
      "2021-10-09 08:48:41,000 - INFO - joeynmt.training - Epoch   9, Step:   131500, Batch Loss:     1.061043, Tokens per Sec:     5037, Lr: 0.000300\n",
      "2021-10-09 08:49:26,809 - INFO - joeynmt.training - Epoch   9, Step:   131600, Batch Loss:     1.038769, Tokens per Sec:     4940, Lr: 0.000300\n",
      "2021-10-09 08:50:12,365 - INFO - joeynmt.training - Epoch   9, Step:   131700, Batch Loss:     1.128739, Tokens per Sec:     4892, Lr: 0.000300\n",
      "2021-10-09 08:50:58,296 - INFO - joeynmt.training - Epoch   9, Step:   131800, Batch Loss:     1.096524, Tokens per Sec:     4970, Lr: 0.000300\n",
      "2021-10-09 08:51:43,890 - INFO - joeynmt.training - Epoch   9, Step:   131900, Batch Loss:     1.154325, Tokens per Sec:     4870, Lr: 0.000300\n",
      "2021-10-09 08:52:30,128 - INFO - joeynmt.training - Epoch   9, Step:   132000, Batch Loss:     1.041724, Tokens per Sec:     4942, Lr: 0.000300\n",
      "2021-10-09 08:53:15,340 - INFO - joeynmt.training - Epoch   9, Step:   132100, Batch Loss:     1.059552, Tokens per Sec:     4970, Lr: 0.000300\n",
      "2021-10-09 08:54:00,079 - INFO - joeynmt.training - Epoch   9, Step:   132200, Batch Loss:     0.857289, Tokens per Sec:     4990, Lr: 0.000300\n",
      "2021-10-09 08:54:45,452 - INFO - joeynmt.training - Epoch   9, Step:   132300, Batch Loss:     1.028250, Tokens per Sec:     5129, Lr: 0.000300\n",
      "2021-10-09 08:55:31,320 - INFO - joeynmt.training - Epoch   9, Step:   132400, Batch Loss:     1.011721, Tokens per Sec:     4955, Lr: 0.000300\n",
      "2021-10-09 08:56:16,463 - INFO - joeynmt.training - Epoch   9, Step:   132500, Batch Loss:     1.032127, Tokens per Sec:     5047, Lr: 0.000300\n",
      "2021-10-09 08:57:02,512 - INFO - joeynmt.training - Epoch   9, Step:   132600, Batch Loss:     1.060371, Tokens per Sec:     4937, Lr: 0.000300\n",
      "2021-10-09 08:57:47,917 - INFO - joeynmt.training - Epoch   9, Step:   132700, Batch Loss:     1.020947, Tokens per Sec:     5003, Lr: 0.000300\n",
      "2021-10-09 08:58:34,965 - INFO - joeynmt.training - Epoch   9, Step:   132800, Batch Loss:     1.196463, Tokens per Sec:     4920, Lr: 0.000300\n",
      "2021-10-09 08:59:20,940 - INFO - joeynmt.training - Epoch   9, Step:   132900, Batch Loss:     0.900758, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2021-10-09 09:00:06,832 - INFO - joeynmt.training - Epoch   9, Step:   133000, Batch Loss:     1.007610, Tokens per Sec:     4881, Lr: 0.000300\n",
      "2021-10-09 09:00:52,918 - INFO - joeynmt.training - Epoch   9, Step:   133100, Batch Loss:     1.186769, Tokens per Sec:     4998, Lr: 0.000300\n",
      "2021-10-09 09:01:39,076 - INFO - joeynmt.training - Epoch   9, Step:   133200, Batch Loss:     1.094453, Tokens per Sec:     4907, Lr: 0.000300\n",
      "2021-10-09 09:02:24,369 - INFO - joeynmt.training - Epoch   9, Step:   133300, Batch Loss:     1.089380, Tokens per Sec:     4881, Lr: 0.000300\n",
      "2021-10-09 09:03:09,578 - INFO - joeynmt.training - Epoch   9, Step:   133400, Batch Loss:     0.857212, Tokens per Sec:     4903, Lr: 0.000300\n",
      "2021-10-09 09:03:55,347 - INFO - joeynmt.training - Epoch   9, Step:   133500, Batch Loss:     1.033137, Tokens per Sec:     4933, Lr: 0.000300\n",
      "2021-10-09 09:04:40,718 - INFO - joeynmt.training - Epoch   9, Step:   133600, Batch Loss:     1.120506, Tokens per Sec:     4897, Lr: 0.000300\n",
      "2021-10-09 09:05:26,265 - INFO - joeynmt.training - Epoch   9, Step:   133700, Batch Loss:     1.167862, Tokens per Sec:     4852, Lr: 0.000300\n",
      "2021-10-09 09:06:12,293 - INFO - joeynmt.training - Epoch   9, Step:   133800, Batch Loss:     1.171566, Tokens per Sec:     4930, Lr: 0.000300\n",
      "2021-10-09 09:06:57,755 - INFO - joeynmt.training - Epoch   9, Step:   133900, Batch Loss:     1.105766, Tokens per Sec:     4993, Lr: 0.000300\n",
      "2021-10-09 09:07:43,621 - INFO - joeynmt.training - Epoch   9, Step:   134000, Batch Loss:     1.117590, Tokens per Sec:     4867, Lr: 0.000300\n",
      "2021-10-09 09:08:29,986 - INFO - joeynmt.training - Epoch   9, Step:   134100, Batch Loss:     0.937975, Tokens per Sec:     4953, Lr: 0.000300\n",
      "2021-10-09 09:09:15,407 - INFO - joeynmt.training - Epoch   9, Step:   134200, Batch Loss:     0.985361, Tokens per Sec:     4909, Lr: 0.000300\n",
      "2021-10-09 09:10:00,410 - INFO - joeynmt.training - Epoch   9, Step:   134300, Batch Loss:     1.031940, Tokens per Sec:     4907, Lr: 0.000300\n",
      "2021-10-09 09:10:45,746 - INFO - joeynmt.training - Epoch   9, Step:   134400, Batch Loss:     0.852944, Tokens per Sec:     4869, Lr: 0.000300\n",
      "2021-10-09 09:11:31,941 - INFO - joeynmt.training - Epoch   9, Step:   134500, Batch Loss:     1.003684, Tokens per Sec:     4994, Lr: 0.000300\n",
      "2021-10-09 09:12:17,556 - INFO - joeynmt.training - Epoch   9, Step:   134600, Batch Loss:     1.216853, Tokens per Sec:     4983, Lr: 0.000300\n",
      "2021-10-09 09:13:03,513 - INFO - joeynmt.training - Epoch   9, Step:   134700, Batch Loss:     1.144289, Tokens per Sec:     4943, Lr: 0.000300\n",
      "2021-10-09 09:13:49,476 - INFO - joeynmt.training - Epoch   9, Step:   134800, Batch Loss:     1.104302, Tokens per Sec:     4882, Lr: 0.000300\n",
      "2021-10-09 09:14:34,950 - INFO - joeynmt.training - Epoch   9, Step:   134900, Batch Loss:     1.123004, Tokens per Sec:     4949, Lr: 0.000300\n",
      "2021-10-09 09:15:21,036 - INFO - joeynmt.training - Epoch   9, Step:   135000, Batch Loss:     1.106062, Tokens per Sec:     4978, Lr: 0.000300\n",
      "2021-10-09 09:16:50,582 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 09:16:50,583 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 09:16:50,583 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 09:16:50,929 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 09:16:50,929 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 09:16:56,979 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 09:16:56,980 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 09:16:56,980 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 09:16:56,980 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at that time for the brother who took the lead in a congregation .\n",
      "2021-10-09 09:16:56,980 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 09:16:56,980 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 09:16:56,980 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 09:16:56,981 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 09:16:56,981 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 09:16:56,981 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 09:16:56,981 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 09:16:56,981 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 09:16:56,981 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 09:16:56,981 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 09:16:56,981 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 09:16:56,982 - INFO - joeynmt.training - \tHypothesis: HOW DO MOST WE SECTEACH DOELVITS ?\n",
      "2021-10-09 09:16:56,982 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   135000: bleu:  49.09, loss: 28881.5547, ppl:   2.5981, duration: 95.9446s\n",
      "2021-10-09 09:17:42,661 - INFO - joeynmt.training - Epoch   9, Step:   135100, Batch Loss:     1.041851, Tokens per Sec:     4867, Lr: 0.000300\n",
      "2021-10-09 09:18:27,831 - INFO - joeynmt.training - Epoch   9, Step:   135200, Batch Loss:     0.999509, Tokens per Sec:     4881, Lr: 0.000300\n",
      "2021-10-09 09:19:13,654 - INFO - joeynmt.training - Epoch   9, Step:   135300, Batch Loss:     0.918577, Tokens per Sec:     4878, Lr: 0.000300\n",
      "2021-10-09 09:19:59,223 - INFO - joeynmt.training - Epoch   9, Step:   135400, Batch Loss:     1.073019, Tokens per Sec:     4927, Lr: 0.000300\n",
      "2021-10-09 09:20:44,831 - INFO - joeynmt.training - Epoch   9, Step:   135500, Batch Loss:     1.080880, Tokens per Sec:     4891, Lr: 0.000300\n",
      "2021-10-09 09:21:30,676 - INFO - joeynmt.training - Epoch   9, Step:   135600, Batch Loss:     1.185634, Tokens per Sec:     4927, Lr: 0.000300\n",
      "2021-10-09 09:22:16,878 - INFO - joeynmt.training - Epoch   9, Step:   135700, Batch Loss:     1.059238, Tokens per Sec:     4968, Lr: 0.000300\n",
      "2021-10-09 09:23:03,538 - INFO - joeynmt.training - Epoch   9, Step:   135800, Batch Loss:     0.987291, Tokens per Sec:     5106, Lr: 0.000300\n",
      "2021-10-09 09:23:49,955 - INFO - joeynmt.training - Epoch   9, Step:   135900, Batch Loss:     1.096811, Tokens per Sec:     4902, Lr: 0.000300\n",
      "2021-10-09 09:24:36,136 - INFO - joeynmt.training - Epoch   9, Step:   136000, Batch Loss:     1.106392, Tokens per Sec:     4941, Lr: 0.000300\n",
      "2021-10-09 09:25:22,033 - INFO - joeynmt.training - Epoch   9, Step:   136100, Batch Loss:     1.025089, Tokens per Sec:     4908, Lr: 0.000300\n",
      "2021-10-09 09:26:07,164 - INFO - joeynmt.training - Epoch   9, Step:   136200, Batch Loss:     1.073942, Tokens per Sec:     4864, Lr: 0.000300\n",
      "2021-10-09 09:26:53,139 - INFO - joeynmt.training - Epoch   9, Step:   136300, Batch Loss:     1.169697, Tokens per Sec:     5034, Lr: 0.000300\n",
      "2021-10-09 09:27:38,622 - INFO - joeynmt.training - Epoch   9, Step:   136400, Batch Loss:     1.019238, Tokens per Sec:     4958, Lr: 0.000300\n",
      "2021-10-09 09:28:24,776 - INFO - joeynmt.training - Epoch   9, Step:   136500, Batch Loss:     1.077400, Tokens per Sec:     4970, Lr: 0.000300\n",
      "2021-10-09 09:29:11,142 - INFO - joeynmt.training - Epoch   9, Step:   136600, Batch Loss:     1.186891, Tokens per Sec:     5004, Lr: 0.000300\n",
      "2021-10-09 09:29:57,365 - INFO - joeynmt.training - Epoch   9, Step:   136700, Batch Loss:     1.174365, Tokens per Sec:     4983, Lr: 0.000300\n",
      "2021-10-09 09:30:43,299 - INFO - joeynmt.training - Epoch   9, Step:   136800, Batch Loss:     0.942404, Tokens per Sec:     4937, Lr: 0.000300\n",
      "2021-10-09 09:31:29,832 - INFO - joeynmt.training - Epoch   9, Step:   136900, Batch Loss:     0.933688, Tokens per Sec:     5082, Lr: 0.000300\n",
      "2021-10-09 09:32:15,022 - INFO - joeynmt.training - Epoch   9, Step:   137000, Batch Loss:     1.074742, Tokens per Sec:     4942, Lr: 0.000300\n",
      "2021-10-09 09:33:00,897 - INFO - joeynmt.training - Epoch   9, Step:   137100, Batch Loss:     1.118217, Tokens per Sec:     4938, Lr: 0.000300\n",
      "2021-10-09 09:33:46,209 - INFO - joeynmt.training - Epoch   9, Step:   137200, Batch Loss:     1.069287, Tokens per Sec:     5004, Lr: 0.000300\n",
      "2021-10-09 09:34:32,281 - INFO - joeynmt.training - Epoch   9, Step:   137300, Batch Loss:     1.073590, Tokens per Sec:     4881, Lr: 0.000300\n",
      "2021-10-09 09:35:18,067 - INFO - joeynmt.training - Epoch   9, Step:   137400, Batch Loss:     1.074501, Tokens per Sec:     5042, Lr: 0.000300\n",
      "2021-10-09 09:36:04,049 - INFO - joeynmt.training - Epoch   9, Step:   137500, Batch Loss:     1.064243, Tokens per Sec:     5019, Lr: 0.000300\n",
      "2021-10-09 09:36:49,220 - INFO - joeynmt.training - Epoch   9, Step:   137600, Batch Loss:     1.117975, Tokens per Sec:     5048, Lr: 0.000300\n",
      "2021-10-09 09:37:30,259 - INFO - joeynmt.training - Epoch   9: total training loss 13363.10\n",
      "2021-10-09 09:37:30,260 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-10-09 09:37:36,958 - INFO - joeynmt.training - Epoch  10, Step:   137700, Batch Loss:     1.222101, Tokens per Sec:     3915, Lr: 0.000300\n",
      "2021-10-09 09:38:21,893 - INFO - joeynmt.training - Epoch  10, Step:   137800, Batch Loss:     1.008414, Tokens per Sec:     4882, Lr: 0.000300\n",
      "2021-10-09 09:39:07,679 - INFO - joeynmt.training - Epoch  10, Step:   137900, Batch Loss:     0.932797, Tokens per Sec:     4947, Lr: 0.000300\n",
      "2021-10-09 09:39:53,800 - INFO - joeynmt.training - Epoch  10, Step:   138000, Batch Loss:     1.126692, Tokens per Sec:     4947, Lr: 0.000300\n",
      "2021-10-09 09:40:39,490 - INFO - joeynmt.training - Epoch  10, Step:   138100, Batch Loss:     0.936117, Tokens per Sec:     4893, Lr: 0.000300\n",
      "2021-10-09 09:41:24,928 - INFO - joeynmt.training - Epoch  10, Step:   138200, Batch Loss:     1.056387, Tokens per Sec:     4877, Lr: 0.000300\n",
      "2021-10-09 09:42:11,444 - INFO - joeynmt.training - Epoch  10, Step:   138300, Batch Loss:     1.150558, Tokens per Sec:     5040, Lr: 0.000300\n",
      "2021-10-09 09:42:57,381 - INFO - joeynmt.training - Epoch  10, Step:   138400, Batch Loss:     1.081818, Tokens per Sec:     4902, Lr: 0.000300\n",
      "2021-10-09 09:43:43,013 - INFO - joeynmt.training - Epoch  10, Step:   138500, Batch Loss:     1.038708, Tokens per Sec:     4982, Lr: 0.000300\n",
      "2021-10-09 09:44:28,773 - INFO - joeynmt.training - Epoch  10, Step:   138600, Batch Loss:     1.019044, Tokens per Sec:     4997, Lr: 0.000300\n",
      "2021-10-09 09:45:14,816 - INFO - joeynmt.training - Epoch  10, Step:   138700, Batch Loss:     1.058804, Tokens per Sec:     4962, Lr: 0.000300\n",
      "2021-10-09 09:46:00,514 - INFO - joeynmt.training - Epoch  10, Step:   138800, Batch Loss:     1.010697, Tokens per Sec:     4883, Lr: 0.000300\n",
      "2021-10-09 09:46:46,173 - INFO - joeynmt.training - Epoch  10, Step:   138900, Batch Loss:     1.079481, Tokens per Sec:     5048, Lr: 0.000300\n",
      "2021-10-09 09:47:31,652 - INFO - joeynmt.training - Epoch  10, Step:   139000, Batch Loss:     0.959301, Tokens per Sec:     4933, Lr: 0.000300\n",
      "2021-10-09 09:48:16,082 - INFO - joeynmt.training - Epoch  10, Step:   139100, Batch Loss:     1.022068, Tokens per Sec:     4916, Lr: 0.000300\n",
      "2021-10-09 09:49:01,538 - INFO - joeynmt.training - Epoch  10, Step:   139200, Batch Loss:     1.094496, Tokens per Sec:     5100, Lr: 0.000300\n",
      "2021-10-09 09:49:46,857 - INFO - joeynmt.training - Epoch  10, Step:   139300, Batch Loss:     1.082006, Tokens per Sec:     5055, Lr: 0.000300\n",
      "2021-10-09 09:50:31,796 - INFO - joeynmt.training - Epoch  10, Step:   139400, Batch Loss:     1.024567, Tokens per Sec:     4951, Lr: 0.000300\n",
      "2021-10-09 09:51:16,433 - INFO - joeynmt.training - Epoch  10, Step:   139500, Batch Loss:     0.965147, Tokens per Sec:     5035, Lr: 0.000300\n",
      "2021-10-09 09:52:01,683 - INFO - joeynmt.training - Epoch  10, Step:   139600, Batch Loss:     1.092091, Tokens per Sec:     5030, Lr: 0.000300\n",
      "2021-10-09 09:52:46,298 - INFO - joeynmt.training - Epoch  10, Step:   139700, Batch Loss:     1.005272, Tokens per Sec:     4984, Lr: 0.000300\n",
      "2021-10-09 09:53:31,984 - INFO - joeynmt.training - Epoch  10, Step:   139800, Batch Loss:     1.142704, Tokens per Sec:     5002, Lr: 0.000300\n",
      "2021-10-09 09:54:16,340 - INFO - joeynmt.training - Epoch  10, Step:   139900, Batch Loss:     1.047117, Tokens per Sec:     4931, Lr: 0.000300\n",
      "2021-10-09 09:55:01,824 - INFO - joeynmt.training - Epoch  10, Step:   140000, Batch Loss:     1.003671, Tokens per Sec:     5132, Lr: 0.000300\n",
      "2021-10-09 09:56:30,160 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 09:56:30,161 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 09:56:30,161 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 09:56:30,499 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 09:56:30,499 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 09:56:36,548 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 09:56:36,549 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 09:56:36,549 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 09:56:36,549 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-09 09:56:36,549 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 09:56:36,549 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 09:56:36,549 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 09:56:36,549 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 09:56:36,550 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 09:56:36,550 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 09:56:36,550 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 09:56:36,550 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 09:56:36,550 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 09:56:36,550 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 09:56:36,550 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 09:56:36,550 - INFO - joeynmt.training - \tHypothesis: HOW DO MAKE WE HAVE HISTORY ?\n",
      "2021-10-09 09:56:36,551 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   140000: bleu:  48.98, loss: 28604.0723, ppl:   2.5743, duration: 94.7260s\n",
      "2021-10-09 09:57:21,308 - INFO - joeynmt.training - Epoch  10, Step:   140100, Batch Loss:     1.052711, Tokens per Sec:     4954, Lr: 0.000300\n",
      "2021-10-09 09:58:06,402 - INFO - joeynmt.training - Epoch  10, Step:   140200, Batch Loss:     0.896444, Tokens per Sec:     4986, Lr: 0.000300\n",
      "2021-10-09 09:58:51,712 - INFO - joeynmt.training - Epoch  10, Step:   140300, Batch Loss:     1.005440, Tokens per Sec:     5017, Lr: 0.000300\n",
      "2021-10-09 09:59:36,651 - INFO - joeynmt.training - Epoch  10, Step:   140400, Batch Loss:     1.164645, Tokens per Sec:     5060, Lr: 0.000300\n",
      "2021-10-09 10:00:21,496 - INFO - joeynmt.training - Epoch  10, Step:   140500, Batch Loss:     0.990784, Tokens per Sec:     5056, Lr: 0.000300\n",
      "2021-10-09 10:01:06,629 - INFO - joeynmt.training - Epoch  10, Step:   140600, Batch Loss:     0.946892, Tokens per Sec:     5014, Lr: 0.000300\n",
      "2021-10-09 10:01:51,793 - INFO - joeynmt.training - Epoch  10, Step:   140700, Batch Loss:     1.148929, Tokens per Sec:     5045, Lr: 0.000300\n",
      "2021-10-09 10:02:37,091 - INFO - joeynmt.training - Epoch  10, Step:   140800, Batch Loss:     1.089501, Tokens per Sec:     5019, Lr: 0.000300\n",
      "2021-10-09 10:03:22,004 - INFO - joeynmt.training - Epoch  10, Step:   140900, Batch Loss:     1.109683, Tokens per Sec:     5046, Lr: 0.000300\n",
      "2021-10-09 10:04:07,620 - INFO - joeynmt.training - Epoch  10, Step:   141000, Batch Loss:     0.973148, Tokens per Sec:     5077, Lr: 0.000300\n",
      "2021-10-09 10:04:52,865 - INFO - joeynmt.training - Epoch  10, Step:   141100, Batch Loss:     1.065973, Tokens per Sec:     5060, Lr: 0.000300\n",
      "2021-10-09 10:05:37,665 - INFO - joeynmt.training - Epoch  10, Step:   141200, Batch Loss:     1.198675, Tokens per Sec:     4997, Lr: 0.000300\n",
      "2021-10-09 10:06:21,775 - INFO - joeynmt.training - Epoch  10, Step:   141300, Batch Loss:     1.060359, Tokens per Sec:     4892, Lr: 0.000300\n",
      "2021-10-09 10:07:06,985 - INFO - joeynmt.training - Epoch  10, Step:   141400, Batch Loss:     1.069890, Tokens per Sec:     5103, Lr: 0.000300\n",
      "2021-10-09 10:07:52,859 - INFO - joeynmt.training - Epoch  10, Step:   141500, Batch Loss:     1.052076, Tokens per Sec:     5159, Lr: 0.000300\n",
      "2021-10-09 10:08:38,056 - INFO - joeynmt.training - Epoch  10, Step:   141600, Batch Loss:     1.103797, Tokens per Sec:     5114, Lr: 0.000300\n",
      "2021-10-09 10:09:23,031 - INFO - joeynmt.training - Epoch  10, Step:   141700, Batch Loss:     0.972713, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-09 10:10:07,972 - INFO - joeynmt.training - Epoch  10, Step:   141800, Batch Loss:     1.089895, Tokens per Sec:     5053, Lr: 0.000300\n",
      "2021-10-09 10:10:53,070 - INFO - joeynmt.training - Epoch  10, Step:   141900, Batch Loss:     1.049059, Tokens per Sec:     4975, Lr: 0.000300\n",
      "2021-10-09 10:11:37,546 - INFO - joeynmt.training - Epoch  10, Step:   142000, Batch Loss:     1.156221, Tokens per Sec:     4940, Lr: 0.000300\n",
      "2021-10-09 10:12:22,173 - INFO - joeynmt.training - Epoch  10, Step:   142100, Batch Loss:     1.001909, Tokens per Sec:     5013, Lr: 0.000300\n",
      "2021-10-09 10:13:06,672 - INFO - joeynmt.training - Epoch  10, Step:   142200, Batch Loss:     1.025830, Tokens per Sec:     5027, Lr: 0.000300\n",
      "2021-10-09 10:13:51,828 - INFO - joeynmt.training - Epoch  10, Step:   142300, Batch Loss:     1.068703, Tokens per Sec:     5030, Lr: 0.000300\n",
      "2021-10-09 10:14:36,515 - INFO - joeynmt.training - Epoch  10, Step:   142400, Batch Loss:     1.027347, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-09 10:15:21,752 - INFO - joeynmt.training - Epoch  10, Step:   142500, Batch Loss:     1.086408, Tokens per Sec:     5100, Lr: 0.000300\n",
      "2021-10-09 10:16:06,884 - INFO - joeynmt.training - Epoch  10, Step:   142600, Batch Loss:     1.055031, Tokens per Sec:     5130, Lr: 0.000300\n",
      "2021-10-09 10:16:51,429 - INFO - joeynmt.training - Epoch  10, Step:   142700, Batch Loss:     1.107317, Tokens per Sec:     4975, Lr: 0.000300\n",
      "2021-10-09 10:17:35,925 - INFO - joeynmt.training - Epoch  10, Step:   142800, Batch Loss:     1.007728, Tokens per Sec:     5026, Lr: 0.000300\n",
      "2021-10-09 10:18:21,455 - INFO - joeynmt.training - Epoch  10, Step:   142900, Batch Loss:     0.986007, Tokens per Sec:     5136, Lr: 0.000300\n",
      "2021-10-09 10:19:06,571 - INFO - joeynmt.training - Epoch  10, Step:   143000, Batch Loss:     1.133227, Tokens per Sec:     5070, Lr: 0.000300\n",
      "2021-10-09 10:19:51,547 - INFO - joeynmt.training - Epoch  10, Step:   143100, Batch Loss:     1.066139, Tokens per Sec:     4978, Lr: 0.000300\n",
      "2021-10-09 10:20:36,706 - INFO - joeynmt.training - Epoch  10, Step:   143200, Batch Loss:     1.044263, Tokens per Sec:     5074, Lr: 0.000300\n",
      "2021-10-09 10:21:21,625 - INFO - joeynmt.training - Epoch  10, Step:   143300, Batch Loss:     1.113922, Tokens per Sec:     5060, Lr: 0.000300\n",
      "2021-10-09 10:22:06,194 - INFO - joeynmt.training - Epoch  10, Step:   143400, Batch Loss:     1.255757, Tokens per Sec:     5023, Lr: 0.000300\n",
      "2021-10-09 10:22:51,079 - INFO - joeynmt.training - Epoch  10, Step:   143500, Batch Loss:     1.083297, Tokens per Sec:     5021, Lr: 0.000300\n",
      "2021-10-09 10:23:35,762 - INFO - joeynmt.training - Epoch  10, Step:   143600, Batch Loss:     0.980275, Tokens per Sec:     5004, Lr: 0.000300\n",
      "2021-10-09 10:24:20,969 - INFO - joeynmt.training - Epoch  10, Step:   143700, Batch Loss:     1.058936, Tokens per Sec:     5058, Lr: 0.000300\n",
      "2021-10-09 10:25:05,539 - INFO - joeynmt.training - Epoch  10, Step:   143800, Batch Loss:     0.996196, Tokens per Sec:     4940, Lr: 0.000300\n",
      "2021-10-09 10:25:50,703 - INFO - joeynmt.training - Epoch  10, Step:   143900, Batch Loss:     1.115835, Tokens per Sec:     5080, Lr: 0.000300\n",
      "2021-10-09 10:26:35,273 - INFO - joeynmt.training - Epoch  10, Step:   144000, Batch Loss:     0.970140, Tokens per Sec:     5004, Lr: 0.000300\n",
      "2021-10-09 10:27:20,574 - INFO - joeynmt.training - Epoch  10, Step:   144100, Batch Loss:     1.122889, Tokens per Sec:     5071, Lr: 0.000300\n",
      "2021-10-09 10:28:05,812 - INFO - joeynmt.training - Epoch  10, Step:   144200, Batch Loss:     0.996027, Tokens per Sec:     5015, Lr: 0.000300\n",
      "2021-10-09 10:28:50,871 - INFO - joeynmt.training - Epoch  10, Step:   144300, Batch Loss:     1.092647, Tokens per Sec:     5026, Lr: 0.000300\n",
      "2021-10-09 10:29:35,740 - INFO - joeynmt.training - Epoch  10, Step:   144400, Batch Loss:     0.978334, Tokens per Sec:     5026, Lr: 0.000300\n",
      "2021-10-09 10:30:20,517 - INFO - joeynmt.training - Epoch  10, Step:   144500, Batch Loss:     1.029156, Tokens per Sec:     5024, Lr: 0.000300\n",
      "2021-10-09 10:31:04,762 - INFO - joeynmt.training - Epoch  10, Step:   144600, Batch Loss:     1.116268, Tokens per Sec:     5004, Lr: 0.000300\n",
      "2021-10-09 10:31:49,368 - INFO - joeynmt.training - Epoch  10, Step:   144700, Batch Loss:     1.042773, Tokens per Sec:     4997, Lr: 0.000300\n",
      "2021-10-09 10:32:34,741 - INFO - joeynmt.training - Epoch  10, Step:   144800, Batch Loss:     1.015298, Tokens per Sec:     5073, Lr: 0.000300\n",
      "2021-10-09 10:33:19,562 - INFO - joeynmt.training - Epoch  10, Step:   144900, Batch Loss:     1.055230, Tokens per Sec:     4993, Lr: 0.000300\n",
      "2021-10-09 10:34:03,587 - INFO - joeynmt.training - Epoch  10, Step:   145000, Batch Loss:     1.030295, Tokens per Sec:     4892, Lr: 0.000300\n",
      "2021-10-09 10:35:32,700 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 10:35:32,701 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 10:35:32,701 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 10:35:33,039 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 10:35:33,040 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 10:35:39,071 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 10:35:39,072 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 10:35:39,072 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 10:35:39,072 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-09 10:35:39,072 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 10:35:39,072 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 10:35:39,072 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 10:35:39,073 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know oneself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 10:35:39,073 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 10:35:39,073 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 10:35:39,073 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 10:35:39,073 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 10:35:39,073 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 10:35:39,073 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 10:35:39,073 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 10:35:39,074 - INFO - joeynmt.training - \tHypothesis: HOW DO WE MAY WE WE SECTLEST DOELVITE ?\n",
      "2021-10-09 10:35:39,074 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   145000: bleu:  49.52, loss: 28601.5215, ppl:   2.5741, duration: 95.4860s\n",
      "2021-10-09 10:36:23,860 - INFO - joeynmt.training - Epoch  10, Step:   145100, Batch Loss:     1.121253, Tokens per Sec:     5027, Lr: 0.000300\n",
      "2021-10-09 10:37:09,731 - INFO - joeynmt.training - Epoch  10, Step:   145200, Batch Loss:     1.054200, Tokens per Sec:     5094, Lr: 0.000300\n",
      "2021-10-09 10:37:54,985 - INFO - joeynmt.training - Epoch  10, Step:   145300, Batch Loss:     1.058320, Tokens per Sec:     5012, Lr: 0.000300\n",
      "2021-10-09 10:38:39,965 - INFO - joeynmt.training - Epoch  10, Step:   145400, Batch Loss:     0.956284, Tokens per Sec:     4999, Lr: 0.000300\n",
      "2021-10-09 10:39:24,763 - INFO - joeynmt.training - Epoch  10, Step:   145500, Batch Loss:     0.953725, Tokens per Sec:     4995, Lr: 0.000300\n",
      "2021-10-09 10:40:09,635 - INFO - joeynmt.training - Epoch  10, Step:   145600, Batch Loss:     0.968665, Tokens per Sec:     5040, Lr: 0.000300\n",
      "2021-10-09 10:40:54,625 - INFO - joeynmt.training - Epoch  10, Step:   145700, Batch Loss:     0.971699, Tokens per Sec:     5019, Lr: 0.000300\n",
      "2021-10-09 10:41:39,617 - INFO - joeynmt.training - Epoch  10, Step:   145800, Batch Loss:     1.030853, Tokens per Sec:     4931, Lr: 0.000300\n",
      "2021-10-09 10:42:24,876 - INFO - joeynmt.training - Epoch  10, Step:   145900, Batch Loss:     1.003205, Tokens per Sec:     5026, Lr: 0.000300\n",
      "2021-10-09 10:43:09,945 - INFO - joeynmt.training - Epoch  10, Step:   146000, Batch Loss:     1.137399, Tokens per Sec:     4952, Lr: 0.000300\n",
      "2021-10-09 10:43:55,171 - INFO - joeynmt.training - Epoch  10, Step:   146100, Batch Loss:     0.962054, Tokens per Sec:     5028, Lr: 0.000300\n",
      "2021-10-09 10:44:39,930 - INFO - joeynmt.training - Epoch  10, Step:   146200, Batch Loss:     1.011967, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-09 10:45:24,800 - INFO - joeynmt.training - Epoch  10, Step:   146300, Batch Loss:     1.111734, Tokens per Sec:     5026, Lr: 0.000300\n",
      "2021-10-09 10:46:10,272 - INFO - joeynmt.training - Epoch  10, Step:   146400, Batch Loss:     1.117139, Tokens per Sec:     4969, Lr: 0.000300\n",
      "2021-10-09 10:46:54,792 - INFO - joeynmt.training - Epoch  10, Step:   146500, Batch Loss:     0.902208, Tokens per Sec:     4923, Lr: 0.000300\n",
      "2021-10-09 10:47:39,711 - INFO - joeynmt.training - Epoch  10, Step:   146600, Batch Loss:     1.042650, Tokens per Sec:     5048, Lr: 0.000300\n",
      "2021-10-09 10:48:24,734 - INFO - joeynmt.training - Epoch  10, Step:   146700, Batch Loss:     0.972627, Tokens per Sec:     5031, Lr: 0.000300\n",
      "2021-10-09 10:49:08,986 - INFO - joeynmt.training - Epoch  10, Step:   146800, Batch Loss:     1.125188, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-09 10:49:54,215 - INFO - joeynmt.training - Epoch  10, Step:   146900, Batch Loss:     1.099196, Tokens per Sec:     5006, Lr: 0.000300\n",
      "2021-10-09 10:50:38,821 - INFO - joeynmt.training - Epoch  10, Step:   147000, Batch Loss:     1.163718, Tokens per Sec:     4881, Lr: 0.000300\n",
      "2021-10-09 10:51:23,817 - INFO - joeynmt.training - Epoch  10, Step:   147100, Batch Loss:     0.934447, Tokens per Sec:     4964, Lr: 0.000300\n",
      "2021-10-09 10:52:08,354 - INFO - joeynmt.training - Epoch  10, Step:   147200, Batch Loss:     1.099149, Tokens per Sec:     5053, Lr: 0.000300\n",
      "2021-10-09 10:52:53,427 - INFO - joeynmt.training - Epoch  10, Step:   147300, Batch Loss:     1.051000, Tokens per Sec:     5078, Lr: 0.000300\n",
      "2021-10-09 10:53:38,036 - INFO - joeynmt.training - Epoch  10, Step:   147400, Batch Loss:     1.028507, Tokens per Sec:     5018, Lr: 0.000300\n",
      "2021-10-09 10:54:22,908 - INFO - joeynmt.training - Epoch  10, Step:   147500, Batch Loss:     0.987706, Tokens per Sec:     5064, Lr: 0.000300\n",
      "2021-10-09 10:55:07,711 - INFO - joeynmt.training - Epoch  10, Step:   147600, Batch Loss:     1.088523, Tokens per Sec:     5050, Lr: 0.000300\n",
      "2021-10-09 10:55:51,772 - INFO - joeynmt.training - Epoch  10, Step:   147700, Batch Loss:     0.927975, Tokens per Sec:     5015, Lr: 0.000300\n",
      "2021-10-09 10:56:36,533 - INFO - joeynmt.training - Epoch  10, Step:   147800, Batch Loss:     0.932946, Tokens per Sec:     5049, Lr: 0.000300\n",
      "2021-10-09 10:57:21,482 - INFO - joeynmt.training - Epoch  10, Step:   147900, Batch Loss:     1.041249, Tokens per Sec:     5042, Lr: 0.000300\n",
      "2021-10-09 10:58:06,341 - INFO - joeynmt.training - Epoch  10, Step:   148000, Batch Loss:     0.991439, Tokens per Sec:     5027, Lr: 0.000300\n",
      "2021-10-09 10:58:51,620 - INFO - joeynmt.training - Epoch  10, Step:   148100, Batch Loss:     0.957513, Tokens per Sec:     5108, Lr: 0.000300\n",
      "2021-10-09 10:59:35,675 - INFO - joeynmt.training - Epoch  10, Step:   148200, Batch Loss:     0.965143, Tokens per Sec:     4879, Lr: 0.000300\n",
      "2021-10-09 11:00:20,828 - INFO - joeynmt.training - Epoch  10, Step:   148300, Batch Loss:     1.117999, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-09 11:01:05,996 - INFO - joeynmt.training - Epoch  10, Step:   148400, Batch Loss:     1.136401, Tokens per Sec:     5056, Lr: 0.000300\n",
      "2021-10-09 11:01:51,228 - INFO - joeynmt.training - Epoch  10, Step:   148500, Batch Loss:     1.080428, Tokens per Sec:     5033, Lr: 0.000300\n",
      "2021-10-09 11:02:35,948 - INFO - joeynmt.training - Epoch  10, Step:   148600, Batch Loss:     0.956148, Tokens per Sec:     5034, Lr: 0.000300\n",
      "2021-10-09 11:03:20,867 - INFO - joeynmt.training - Epoch  10, Step:   148700, Batch Loss:     1.057407, Tokens per Sec:     5113, Lr: 0.000300\n",
      "2021-10-09 11:04:06,240 - INFO - joeynmt.training - Epoch  10, Step:   148800, Batch Loss:     1.113852, Tokens per Sec:     5038, Lr: 0.000300\n",
      "2021-10-09 11:04:50,669 - INFO - joeynmt.training - Epoch  10, Step:   148900, Batch Loss:     1.021195, Tokens per Sec:     4854, Lr: 0.000300\n",
      "2021-10-09 11:05:36,052 - INFO - joeynmt.training - Epoch  10, Step:   149000, Batch Loss:     1.090182, Tokens per Sec:     5115, Lr: 0.000300\n",
      "2021-10-09 11:06:20,630 - INFO - joeynmt.training - Epoch  10, Step:   149100, Batch Loss:     1.093949, Tokens per Sec:     5013, Lr: 0.000300\n",
      "2021-10-09 11:07:05,203 - INFO - joeynmt.training - Epoch  10, Step:   149200, Batch Loss:     1.077751, Tokens per Sec:     5052, Lr: 0.000300\n",
      "2021-10-09 11:07:49,675 - INFO - joeynmt.training - Epoch  10, Step:   149300, Batch Loss:     1.257534, Tokens per Sec:     5046, Lr: 0.000300\n",
      "2021-10-09 11:08:35,343 - INFO - joeynmt.training - Epoch  10, Step:   149400, Batch Loss:     1.020375, Tokens per Sec:     5057, Lr: 0.000300\n",
      "2021-10-09 11:09:20,432 - INFO - joeynmt.training - Epoch  10, Step:   149500, Batch Loss:     1.040655, Tokens per Sec:     5011, Lr: 0.000300\n",
      "2021-10-09 11:10:05,749 - INFO - joeynmt.training - Epoch  10, Step:   149600, Batch Loss:     1.063444, Tokens per Sec:     5143, Lr: 0.000300\n",
      "2021-10-09 11:10:50,783 - INFO - joeynmt.training - Epoch  10, Step:   149700, Batch Loss:     0.971198, Tokens per Sec:     5046, Lr: 0.000300\n",
      "2021-10-09 11:11:35,522 - INFO - joeynmt.training - Epoch  10, Step:   149800, Batch Loss:     1.184987, Tokens per Sec:     5053, Lr: 0.000300\n",
      "2021-10-09 11:12:20,354 - INFO - joeynmt.training - Epoch  10, Step:   149900, Batch Loss:     1.008024, Tokens per Sec:     5135, Lr: 0.000300\n",
      "2021-10-09 11:13:05,241 - INFO - joeynmt.training - Epoch  10, Step:   150000, Batch Loss:     0.906754, Tokens per Sec:     4984, Lr: 0.000300\n",
      "2021-10-09 11:14:32,563 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 11:14:32,564 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 11:14:32,564 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 11:14:32,902 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 11:14:32,902 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 11:14:38,960 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 11:14:38,961 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 11:14:38,961 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 11:14:38,961 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-09 11:14:38,961 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 11:14:38,961 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 11:14:38,961 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 11:14:38,962 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one gets to know oneself and express feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 11:14:38,962 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 11:14:38,962 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 11:14:38,962 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 11:14:38,962 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 11:14:38,962 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 11:14:38,962 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 11:14:38,962 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 11:14:38,963 - INFO - joeynmt.training - \tHypothesis: HOW DO WE WE HAVE WE DEACTS STEL US ?\n",
      "2021-10-09 11:14:38,963 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   150000: bleu:  49.51, loss: 28462.1113, ppl:   2.5623, duration: 93.7209s\n",
      "2021-10-09 11:15:23,444 - INFO - joeynmt.training - Epoch  10, Step:   150100, Batch Loss:     1.202305, Tokens per Sec:     4922, Lr: 0.000300\n",
      "2021-10-09 11:16:07,667 - INFO - joeynmt.training - Epoch  10, Step:   150200, Batch Loss:     1.148694, Tokens per Sec:     4844, Lr: 0.000300\n",
      "2021-10-09 11:16:21,984 - INFO - joeynmt.training - Epoch  10: total training loss 13205.60\n",
      "2021-10-09 11:16:21,993 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-10-09 11:16:54,283 - INFO - joeynmt.training - Epoch  11, Step:   150300, Batch Loss:     1.024989, Tokens per Sec:     4681, Lr: 0.000300\n",
      "2021-10-09 11:17:39,725 - INFO - joeynmt.training - Epoch  11, Step:   150400, Batch Loss:     1.023967, Tokens per Sec:     4903, Lr: 0.000300\n",
      "2021-10-09 11:18:24,555 - INFO - joeynmt.training - Epoch  11, Step:   150500, Batch Loss:     0.932263, Tokens per Sec:     4904, Lr: 0.000300\n",
      "2021-10-09 11:19:09,528 - INFO - joeynmt.training - Epoch  11, Step:   150600, Batch Loss:     1.012145, Tokens per Sec:     4917, Lr: 0.000300\n",
      "2021-10-09 11:19:54,867 - INFO - joeynmt.training - Epoch  11, Step:   150700, Batch Loss:     0.973914, Tokens per Sec:     5015, Lr: 0.000300\n",
      "2021-10-09 11:20:39,835 - INFO - joeynmt.training - Epoch  11, Step:   150800, Batch Loss:     1.098915, Tokens per Sec:     5019, Lr: 0.000300\n",
      "2021-10-09 11:21:24,693 - INFO - joeynmt.training - Epoch  11, Step:   150900, Batch Loss:     1.086564, Tokens per Sec:     4953, Lr: 0.000300\n",
      "2021-10-09 11:22:10,564 - INFO - joeynmt.training - Epoch  11, Step:   151000, Batch Loss:     1.157908, Tokens per Sec:     5111, Lr: 0.000300\n",
      "2021-10-09 11:22:56,160 - INFO - joeynmt.training - Epoch  11, Step:   151100, Batch Loss:     1.162228, Tokens per Sec:     5077, Lr: 0.000300\n",
      "2021-10-09 11:23:41,738 - INFO - joeynmt.training - Epoch  11, Step:   151200, Batch Loss:     0.948361, Tokens per Sec:     5082, Lr: 0.000300\n",
      "2021-10-09 11:24:26,141 - INFO - joeynmt.training - Epoch  11, Step:   151300, Batch Loss:     1.046227, Tokens per Sec:     5031, Lr: 0.000300\n",
      "2021-10-09 11:25:10,861 - INFO - joeynmt.training - Epoch  11, Step:   151400, Batch Loss:     1.034129, Tokens per Sec:     5054, Lr: 0.000300\n",
      "2021-10-09 11:25:56,077 - INFO - joeynmt.training - Epoch  11, Step:   151500, Batch Loss:     1.031730, Tokens per Sec:     5071, Lr: 0.000300\n",
      "2021-10-09 11:26:40,879 - INFO - joeynmt.training - Epoch  11, Step:   151600, Batch Loss:     1.132188, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-09 11:27:25,578 - INFO - joeynmt.training - Epoch  11, Step:   151700, Batch Loss:     1.064335, Tokens per Sec:     5039, Lr: 0.000300\n",
      "2021-10-09 11:28:10,649 - INFO - joeynmt.training - Epoch  11, Step:   151800, Batch Loss:     1.106071, Tokens per Sec:     5123, Lr: 0.000300\n",
      "2021-10-09 11:28:55,962 - INFO - joeynmt.training - Epoch  11, Step:   151900, Batch Loss:     1.020058, Tokens per Sec:     5041, Lr: 0.000300\n",
      "2021-10-09 11:29:40,455 - INFO - joeynmt.training - Epoch  11, Step:   152000, Batch Loss:     1.223687, Tokens per Sec:     5028, Lr: 0.000300\n",
      "2021-10-09 11:30:24,805 - INFO - joeynmt.training - Epoch  11, Step:   152100, Batch Loss:     1.122810, Tokens per Sec:     4866, Lr: 0.000300\n",
      "2021-10-09 11:31:09,637 - INFO - joeynmt.training - Epoch  11, Step:   152200, Batch Loss:     0.996392, Tokens per Sec:     5038, Lr: 0.000300\n",
      "2021-10-09 11:31:54,889 - INFO - joeynmt.training - Epoch  11, Step:   152300, Batch Loss:     1.195779, Tokens per Sec:     5119, Lr: 0.000300\n",
      "2021-10-09 11:32:39,575 - INFO - joeynmt.training - Epoch  11, Step:   152400, Batch Loss:     1.134767, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-09 11:33:24,394 - INFO - joeynmt.training - Epoch  11, Step:   152500, Batch Loss:     1.020587, Tokens per Sec:     5047, Lr: 0.000300\n",
      "2021-10-09 11:34:09,816 - INFO - joeynmt.training - Epoch  11, Step:   152600, Batch Loss:     0.914134, Tokens per Sec:     5158, Lr: 0.000300\n",
      "2021-10-09 11:34:54,079 - INFO - joeynmt.training - Epoch  11, Step:   152700, Batch Loss:     1.052503, Tokens per Sec:     4956, Lr: 0.000300\n",
      "2021-10-09 11:35:38,562 - INFO - joeynmt.training - Epoch  11, Step:   152800, Batch Loss:     1.161528, Tokens per Sec:     4929, Lr: 0.000300\n",
      "2021-10-09 11:36:23,401 - INFO - joeynmt.training - Epoch  11, Step:   152900, Batch Loss:     0.977694, Tokens per Sec:     5031, Lr: 0.000300\n",
      "2021-10-09 11:37:08,491 - INFO - joeynmt.training - Epoch  11, Step:   153000, Batch Loss:     1.199573, Tokens per Sec:     5077, Lr: 0.000300\n",
      "2021-10-09 11:37:53,602 - INFO - joeynmt.training - Epoch  11, Step:   153100, Batch Loss:     1.144221, Tokens per Sec:     5137, Lr: 0.000300\n",
      "2021-10-09 11:38:38,809 - INFO - joeynmt.training - Epoch  11, Step:   153200, Batch Loss:     1.048392, Tokens per Sec:     5083, Lr: 0.000300\n",
      "2021-10-09 11:39:23,126 - INFO - joeynmt.training - Epoch  11, Step:   153300, Batch Loss:     1.118248, Tokens per Sec:     5037, Lr: 0.000300\n",
      "2021-10-09 11:40:08,068 - INFO - joeynmt.training - Epoch  11, Step:   153400, Batch Loss:     1.078395, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2021-10-09 11:40:52,400 - INFO - joeynmt.training - Epoch  11, Step:   153500, Batch Loss:     1.026072, Tokens per Sec:     5045, Lr: 0.000300\n",
      "2021-10-09 11:41:37,357 - INFO - joeynmt.training - Epoch  11, Step:   153600, Batch Loss:     1.122809, Tokens per Sec:     5037, Lr: 0.000300\n",
      "2021-10-09 11:42:22,094 - INFO - joeynmt.training - Epoch  11, Step:   153700, Batch Loss:     0.892832, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-09 11:43:07,098 - INFO - joeynmt.training - Epoch  11, Step:   153800, Batch Loss:     0.947648, Tokens per Sec:     5082, Lr: 0.000300\n",
      "2021-10-09 11:43:52,036 - INFO - joeynmt.training - Epoch  11, Step:   153900, Batch Loss:     1.124879, Tokens per Sec:     5069, Lr: 0.000300\n",
      "2021-10-09 11:44:37,045 - INFO - joeynmt.training - Epoch  11, Step:   154000, Batch Loss:     1.089733, Tokens per Sec:     5045, Lr: 0.000300\n",
      "2021-10-09 11:45:21,753 - INFO - joeynmt.training - Epoch  11, Step:   154100, Batch Loss:     0.889072, Tokens per Sec:     5055, Lr: 0.000300\n",
      "2021-10-09 11:46:06,544 - INFO - joeynmt.training - Epoch  11, Step:   154200, Batch Loss:     0.946564, Tokens per Sec:     5083, Lr: 0.000300\n",
      "2021-10-09 11:46:51,847 - INFO - joeynmt.training - Epoch  11, Step:   154300, Batch Loss:     1.144839, Tokens per Sec:     5102, Lr: 0.000300\n",
      "2021-10-09 11:47:36,402 - INFO - joeynmt.training - Epoch  11, Step:   154400, Batch Loss:     1.051504, Tokens per Sec:     4976, Lr: 0.000300\n",
      "2021-10-09 11:48:21,471 - INFO - joeynmt.training - Epoch  11, Step:   154500, Batch Loss:     0.943430, Tokens per Sec:     5057, Lr: 0.000300\n",
      "2021-10-09 11:49:05,803 - INFO - joeynmt.training - Epoch  11, Step:   154600, Batch Loss:     1.115414, Tokens per Sec:     4979, Lr: 0.000300\n",
      "2021-10-09 11:49:50,933 - INFO - joeynmt.training - Epoch  11, Step:   154700, Batch Loss:     1.043530, Tokens per Sec:     5003, Lr: 0.000300\n",
      "2021-10-09 11:50:35,359 - INFO - joeynmt.training - Epoch  11, Step:   154800, Batch Loss:     0.920821, Tokens per Sec:     4985, Lr: 0.000300\n",
      "2021-10-09 11:51:20,113 - INFO - joeynmt.training - Epoch  11, Step:   154900, Batch Loss:     1.010469, Tokens per Sec:     5035, Lr: 0.000300\n",
      "2021-10-09 11:52:05,229 - INFO - joeynmt.training - Epoch  11, Step:   155000, Batch Loss:     1.052890, Tokens per Sec:     5098, Lr: 0.000300\n",
      "2021-10-09 11:53:31,785 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 11:53:31,786 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 11:53:31,786 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 11:53:32,123 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 11:53:32,123 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 11:53:38,249 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 11:53:38,249 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 11:53:38,249 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 11:53:38,250 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term used at that time for the brother who took the lead in a congregation .\n",
      "2021-10-09 11:53:38,250 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 11:53:38,250 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 11:53:38,250 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 11:53:38,250 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know oneself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 11:53:38,250 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 11:53:38,250 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 11:53:38,250 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 11:53:38,251 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” referring to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 11:53:38,251 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 11:53:38,251 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 11:53:38,251 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 11:53:38,251 - INFO - joeynmt.training - \tHypothesis: HOW DO WE SHOULD WE SEECTLY ?\n",
      "2021-10-09 11:53:38,251 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   155000: bleu:  49.19, loss: 28400.6348, ppl:   2.5571, duration: 93.0210s\n",
      "2021-10-09 11:54:23,003 - INFO - joeynmt.training - Epoch  11, Step:   155100, Batch Loss:     0.974773, Tokens per Sec:     5045, Lr: 0.000300\n",
      "2021-10-09 11:55:08,376 - INFO - joeynmt.training - Epoch  11, Step:   155200, Batch Loss:     1.135659, Tokens per Sec:     5118, Lr: 0.000300\n",
      "2021-10-09 11:55:53,203 - INFO - joeynmt.training - Epoch  11, Step:   155300, Batch Loss:     0.972351, Tokens per Sec:     5043, Lr: 0.000300\n",
      "2021-10-09 11:56:38,033 - INFO - joeynmt.training - Epoch  11, Step:   155400, Batch Loss:     1.100014, Tokens per Sec:     5123, Lr: 0.000300\n",
      "2021-10-09 11:57:23,172 - INFO - joeynmt.training - Epoch  11, Step:   155500, Batch Loss:     0.995363, Tokens per Sec:     5119, Lr: 0.000300\n",
      "2021-10-09 11:58:07,934 - INFO - joeynmt.training - Epoch  11, Step:   155600, Batch Loss:     1.147309, Tokens per Sec:     5043, Lr: 0.000300\n",
      "2021-10-09 11:58:53,254 - INFO - joeynmt.training - Epoch  11, Step:   155700, Batch Loss:     0.991359, Tokens per Sec:     5031, Lr: 0.000300\n",
      "2021-10-09 11:59:38,165 - INFO - joeynmt.training - Epoch  11, Step:   155800, Batch Loss:     1.108440, Tokens per Sec:     4942, Lr: 0.000300\n",
      "2021-10-09 12:00:23,070 - INFO - joeynmt.training - Epoch  11, Step:   155900, Batch Loss:     1.094754, Tokens per Sec:     5025, Lr: 0.000300\n",
      "2021-10-09 12:01:07,324 - INFO - joeynmt.training - Epoch  11, Step:   156000, Batch Loss:     1.026577, Tokens per Sec:     4964, Lr: 0.000300\n",
      "2021-10-09 12:01:51,701 - INFO - joeynmt.training - Epoch  11, Step:   156100, Batch Loss:     0.890674, Tokens per Sec:     5050, Lr: 0.000300\n",
      "2021-10-09 12:02:36,504 - INFO - joeynmt.training - Epoch  11, Step:   156200, Batch Loss:     1.057142, Tokens per Sec:     5099, Lr: 0.000300\n",
      "2021-10-09 12:03:21,100 - INFO - joeynmt.training - Epoch  11, Step:   156300, Batch Loss:     0.919723, Tokens per Sec:     5021, Lr: 0.000300\n",
      "2021-10-09 12:04:05,881 - INFO - joeynmt.training - Epoch  11, Step:   156400, Batch Loss:     1.040830, Tokens per Sec:     5004, Lr: 0.000300\n",
      "2021-10-09 12:04:50,190 - INFO - joeynmt.training - Epoch  11, Step:   156500, Batch Loss:     0.859314, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-09 12:05:34,465 - INFO - joeynmt.training - Epoch  11, Step:   156600, Batch Loss:     1.146405, Tokens per Sec:     4936, Lr: 0.000300\n",
      "2021-10-09 12:06:19,465 - INFO - joeynmt.training - Epoch  11, Step:   156700, Batch Loss:     1.047182, Tokens per Sec:     5098, Lr: 0.000300\n",
      "2021-10-09 12:07:04,046 - INFO - joeynmt.training - Epoch  11, Step:   156800, Batch Loss:     1.060885, Tokens per Sec:     4950, Lr: 0.000300\n",
      "2021-10-09 12:07:49,271 - INFO - joeynmt.training - Epoch  11, Step:   156900, Batch Loss:     1.074417, Tokens per Sec:     5083, Lr: 0.000300\n",
      "2021-10-09 12:08:34,296 - INFO - joeynmt.training - Epoch  11, Step:   157000, Batch Loss:     1.013730, Tokens per Sec:     5091, Lr: 0.000300\n",
      "2021-10-09 12:09:19,041 - INFO - joeynmt.training - Epoch  11, Step:   157100, Batch Loss:     0.944645, Tokens per Sec:     5062, Lr: 0.000300\n",
      "2021-10-09 12:10:04,158 - INFO - joeynmt.training - Epoch  11, Step:   157200, Batch Loss:     0.997821, Tokens per Sec:     5118, Lr: 0.000300\n",
      "2021-10-09 12:10:48,695 - INFO - joeynmt.training - Epoch  11, Step:   157300, Batch Loss:     0.944160, Tokens per Sec:     4979, Lr: 0.000300\n",
      "2021-10-09 12:11:32,899 - INFO - joeynmt.training - Epoch  11, Step:   157400, Batch Loss:     1.050757, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-09 12:12:17,676 - INFO - joeynmt.training - Epoch  11, Step:   157500, Batch Loss:     1.187485, Tokens per Sec:     5060, Lr: 0.000300\n",
      "2021-10-09 12:13:02,124 - INFO - joeynmt.training - Epoch  11, Step:   157600, Batch Loss:     1.008176, Tokens per Sec:     4978, Lr: 0.000300\n",
      "2021-10-09 12:13:46,639 - INFO - joeynmt.training - Epoch  11, Step:   157700, Batch Loss:     1.001775, Tokens per Sec:     4995, Lr: 0.000300\n",
      "2021-10-09 12:14:31,270 - INFO - joeynmt.training - Epoch  11, Step:   157800, Batch Loss:     0.999329, Tokens per Sec:     5090, Lr: 0.000300\n",
      "2021-10-09 12:15:15,945 - INFO - joeynmt.training - Epoch  11, Step:   157900, Batch Loss:     1.102795, Tokens per Sec:     5026, Lr: 0.000300\n",
      "2021-10-09 12:16:01,267 - INFO - joeynmt.training - Epoch  11, Step:   158000, Batch Loss:     0.996178, Tokens per Sec:     5153, Lr: 0.000300\n",
      "2021-10-09 12:16:46,534 - INFO - joeynmt.training - Epoch  11, Step:   158100, Batch Loss:     0.941591, Tokens per Sec:     5093, Lr: 0.000300\n",
      "2021-10-09 12:17:31,566 - INFO - joeynmt.training - Epoch  11, Step:   158200, Batch Loss:     0.941068, Tokens per Sec:     5057, Lr: 0.000300\n",
      "2021-10-09 12:18:16,647 - INFO - joeynmt.training - Epoch  11, Step:   158300, Batch Loss:     1.194392, Tokens per Sec:     5078, Lr: 0.000300\n",
      "2021-10-09 12:19:00,677 - INFO - joeynmt.training - Epoch  11, Step:   158400, Batch Loss:     1.043434, Tokens per Sec:     4930, Lr: 0.000300\n",
      "2021-10-09 12:19:46,166 - INFO - joeynmt.training - Epoch  11, Step:   158500, Batch Loss:     1.091640, Tokens per Sec:     5121, Lr: 0.000300\n",
      "2021-10-09 12:20:30,664 - INFO - joeynmt.training - Epoch  11, Step:   158600, Batch Loss:     1.170631, Tokens per Sec:     5004, Lr: 0.000300\n",
      "2021-10-09 12:21:14,815 - INFO - joeynmt.training - Epoch  11, Step:   158700, Batch Loss:     1.028181, Tokens per Sec:     5003, Lr: 0.000300\n",
      "2021-10-09 12:21:59,998 - INFO - joeynmt.training - Epoch  11, Step:   158800, Batch Loss:     0.930563, Tokens per Sec:     5062, Lr: 0.000300\n",
      "2021-10-09 12:22:44,107 - INFO - joeynmt.training - Epoch  11, Step:   158900, Batch Loss:     1.086976, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-09 12:23:29,351 - INFO - joeynmt.training - Epoch  11, Step:   159000, Batch Loss:     1.038202, Tokens per Sec:     5023, Lr: 0.000300\n",
      "2021-10-09 12:24:13,814 - INFO - joeynmt.training - Epoch  11, Step:   159100, Batch Loss:     1.073030, Tokens per Sec:     5078, Lr: 0.000300\n",
      "2021-10-09 12:24:59,250 - INFO - joeynmt.training - Epoch  11, Step:   159200, Batch Loss:     0.984750, Tokens per Sec:     5069, Lr: 0.000300\n",
      "2021-10-09 12:25:44,015 - INFO - joeynmt.training - Epoch  11, Step:   159300, Batch Loss:     1.107544, Tokens per Sec:     5076, Lr: 0.000300\n",
      "2021-10-09 12:26:28,953 - INFO - joeynmt.training - Epoch  11, Step:   159400, Batch Loss:     0.951561, Tokens per Sec:     5021, Lr: 0.000300\n",
      "2021-10-09 12:27:13,926 - INFO - joeynmt.training - Epoch  11, Step:   159500, Batch Loss:     1.118571, Tokens per Sec:     5033, Lr: 0.000300\n",
      "2021-10-09 12:27:59,185 - INFO - joeynmt.training - Epoch  11, Step:   159600, Batch Loss:     1.180254, Tokens per Sec:     5073, Lr: 0.000300\n",
      "2021-10-09 12:28:43,924 - INFO - joeynmt.training - Epoch  11, Step:   159700, Batch Loss:     1.084821, Tokens per Sec:     5053, Lr: 0.000300\n",
      "2021-10-09 12:29:28,702 - INFO - joeynmt.training - Epoch  11, Step:   159800, Batch Loss:     1.035550, Tokens per Sec:     4950, Lr: 0.000300\n",
      "2021-10-09 12:30:13,906 - INFO - joeynmt.training - Epoch  11, Step:   159900, Batch Loss:     1.135499, Tokens per Sec:     5097, Lr: 0.000300\n",
      "2021-10-09 12:30:58,573 - INFO - joeynmt.training - Epoch  11, Step:   160000, Batch Loss:     1.033480, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2021-10-09 12:32:26,048 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 12:32:26,049 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 12:32:26,049 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 12:32:26,385 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 12:32:26,385 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 12:32:32,440 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 12:32:32,441 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 12:32:32,441 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 12:32:32,441 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother taking the lead in a congregation .\n",
      "2021-10-09 12:32:32,442 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 12:32:32,442 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 12:32:32,442 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 12:32:32,442 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know himself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 12:32:32,442 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 12:32:32,443 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 12:32:32,443 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 12:32:32,443 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 12:32:32,443 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 12:32:32,443 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 12:32:32,444 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 12:32:32,444 - INFO - joeynmt.training - \tHypothesis: HOW DO WE SHOULD WE SEECTLY DOELVITS ?\n",
      "2021-10-09 12:32:32,444 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   160000: bleu:  49.66, loss: 28113.3613, ppl:   2.5329, duration: 93.8702s\n",
      "2021-10-09 12:33:17,600 - INFO - joeynmt.training - Epoch  11, Step:   160100, Batch Loss:     0.975342, Tokens per Sec:     5097, Lr: 0.000300\n",
      "2021-10-09 12:34:02,642 - INFO - joeynmt.training - Epoch  11, Step:   160200, Batch Loss:     0.914521, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-09 12:34:47,545 - INFO - joeynmt.training - Epoch  11, Step:   160300, Batch Loss:     1.057175, Tokens per Sec:     4969, Lr: 0.000300\n",
      "2021-10-09 12:35:32,893 - INFO - joeynmt.training - Epoch  11, Step:   160400, Batch Loss:     1.191406, Tokens per Sec:     5050, Lr: 0.000300\n",
      "2021-10-09 12:36:17,305 - INFO - joeynmt.training - Epoch  11, Step:   160500, Batch Loss:     0.981393, Tokens per Sec:     4912, Lr: 0.000300\n",
      "2021-10-09 12:37:02,713 - INFO - joeynmt.training - Epoch  11, Step:   160600, Batch Loss:     1.043383, Tokens per Sec:     5084, Lr: 0.000300\n",
      "2021-10-09 12:37:47,473 - INFO - joeynmt.training - Epoch  11, Step:   160700, Batch Loss:     1.036011, Tokens per Sec:     5032, Lr: 0.000300\n",
      "2021-10-09 12:38:32,936 - INFO - joeynmt.training - Epoch  11, Step:   160800, Batch Loss:     0.859969, Tokens per Sec:     5074, Lr: 0.000300\n",
      "2021-10-09 12:39:18,110 - INFO - joeynmt.training - Epoch  11, Step:   160900, Batch Loss:     1.101115, Tokens per Sec:     5046, Lr: 0.000300\n",
      "2021-10-09 12:40:03,525 - INFO - joeynmt.training - Epoch  11, Step:   161000, Batch Loss:     0.980660, Tokens per Sec:     5098, Lr: 0.000300\n",
      "2021-10-09 12:40:48,539 - INFO - joeynmt.training - Epoch  11, Step:   161100, Batch Loss:     1.163407, Tokens per Sec:     5058, Lr: 0.000300\n",
      "2021-10-09 12:41:33,448 - INFO - joeynmt.training - Epoch  11, Step:   161200, Batch Loss:     1.241812, Tokens per Sec:     4972, Lr: 0.000300\n",
      "2021-10-09 12:42:17,834 - INFO - joeynmt.training - Epoch  11, Step:   161300, Batch Loss:     1.061311, Tokens per Sec:     4887, Lr: 0.000300\n",
      "2021-10-09 12:43:02,855 - INFO - joeynmt.training - Epoch  11, Step:   161400, Batch Loss:     1.174810, Tokens per Sec:     5015, Lr: 0.000300\n",
      "2021-10-09 12:43:48,013 - INFO - joeynmt.training - Epoch  11, Step:   161500, Batch Loss:     1.104962, Tokens per Sec:     5093, Lr: 0.000300\n",
      "2021-10-09 12:44:32,753 - INFO - joeynmt.training - Epoch  11, Step:   161600, Batch Loss:     1.077851, Tokens per Sec:     4985, Lr: 0.000300\n",
      "2021-10-09 12:45:17,796 - INFO - joeynmt.training - Epoch  11, Step:   161700, Batch Loss:     0.853261, Tokens per Sec:     5105, Lr: 0.000300\n",
      "2021-10-09 12:46:02,455 - INFO - joeynmt.training - Epoch  11, Step:   161800, Batch Loss:     1.238893, Tokens per Sec:     5015, Lr: 0.000300\n",
      "2021-10-09 12:46:47,782 - INFO - joeynmt.training - Epoch  11, Step:   161900, Batch Loss:     0.991899, Tokens per Sec:     5084, Lr: 0.000300\n",
      "2021-10-09 12:47:31,694 - INFO - joeynmt.training - Epoch  11, Step:   162000, Batch Loss:     1.117624, Tokens per Sec:     4980, Lr: 0.000300\n",
      "2021-10-09 12:48:17,274 - INFO - joeynmt.training - Epoch  11, Step:   162100, Batch Loss:     1.000374, Tokens per Sec:     5127, Lr: 0.000300\n",
      "2021-10-09 12:49:02,331 - INFO - joeynmt.training - Epoch  11, Step:   162200, Batch Loss:     1.010515, Tokens per Sec:     5068, Lr: 0.000300\n",
      "2021-10-09 12:49:47,561 - INFO - joeynmt.training - Epoch  11, Step:   162300, Batch Loss:     1.022040, Tokens per Sec:     5033, Lr: 0.000300\n",
      "2021-10-09 12:50:33,407 - INFO - joeynmt.training - Epoch  11, Step:   162400, Batch Loss:     0.968218, Tokens per Sec:     5137, Lr: 0.000300\n",
      "2021-10-09 12:51:18,502 - INFO - joeynmt.training - Epoch  11, Step:   162500, Batch Loss:     0.969212, Tokens per Sec:     5137, Lr: 0.000300\n",
      "2021-10-09 12:52:03,988 - INFO - joeynmt.training - Epoch  11, Step:   162600, Batch Loss:     1.016428, Tokens per Sec:     5102, Lr: 0.000300\n",
      "2021-10-09 12:52:48,488 - INFO - joeynmt.training - Epoch  11, Step:   162700, Batch Loss:     1.118011, Tokens per Sec:     4940, Lr: 0.000300\n",
      "2021-10-09 12:53:10,728 - INFO - joeynmt.training - Epoch  11: total training loss 13018.04\n",
      "2021-10-09 12:53:10,728 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-10-09 12:53:35,479 - INFO - joeynmt.training - Epoch  12, Step:   162800, Batch Loss:     1.068092, Tokens per Sec:     4736, Lr: 0.000300\n",
      "2021-10-09 12:54:20,526 - INFO - joeynmt.training - Epoch  12, Step:   162900, Batch Loss:     0.983646, Tokens per Sec:     4977, Lr: 0.000300\n",
      "2021-10-09 12:55:05,474 - INFO - joeynmt.training - Epoch  12, Step:   163000, Batch Loss:     1.049976, Tokens per Sec:     4980, Lr: 0.000300\n",
      "2021-10-09 12:55:50,360 - INFO - joeynmt.training - Epoch  12, Step:   163100, Batch Loss:     0.968316, Tokens per Sec:     4999, Lr: 0.000300\n",
      "2021-10-09 12:56:35,340 - INFO - joeynmt.training - Epoch  12, Step:   163200, Batch Loss:     0.937850, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-09 12:57:20,536 - INFO - joeynmt.training - Epoch  12, Step:   163300, Batch Loss:     0.969104, Tokens per Sec:     5076, Lr: 0.000300\n",
      "2021-10-09 12:58:04,965 - INFO - joeynmt.training - Epoch  12, Step:   163400, Batch Loss:     0.938436, Tokens per Sec:     4946, Lr: 0.000300\n",
      "2021-10-09 12:58:50,105 - INFO - joeynmt.training - Epoch  12, Step:   163500, Batch Loss:     1.021168, Tokens per Sec:     5097, Lr: 0.000300\n",
      "2021-10-09 12:59:35,036 - INFO - joeynmt.training - Epoch  12, Step:   163600, Batch Loss:     1.017490, Tokens per Sec:     4958, Lr: 0.000300\n",
      "2021-10-09 13:00:19,623 - INFO - joeynmt.training - Epoch  12, Step:   163700, Batch Loss:     1.061949, Tokens per Sec:     5040, Lr: 0.000300\n",
      "2021-10-09 13:01:04,242 - INFO - joeynmt.training - Epoch  12, Step:   163800, Batch Loss:     0.945338, Tokens per Sec:     4949, Lr: 0.000300\n",
      "2021-10-09 13:01:48,761 - INFO - joeynmt.training - Epoch  12, Step:   163900, Batch Loss:     1.134772, Tokens per Sec:     4895, Lr: 0.000300\n",
      "2021-10-09 13:02:34,393 - INFO - joeynmt.training - Epoch  12, Step:   164000, Batch Loss:     1.295858, Tokens per Sec:     5070, Lr: 0.000300\n",
      "2021-10-09 13:03:19,146 - INFO - joeynmt.training - Epoch  12, Step:   164100, Batch Loss:     0.956318, Tokens per Sec:     5046, Lr: 0.000300\n",
      "2021-10-09 13:04:04,821 - INFO - joeynmt.training - Epoch  12, Step:   164200, Batch Loss:     1.053310, Tokens per Sec:     5176, Lr: 0.000300\n",
      "2021-10-09 13:04:50,396 - INFO - joeynmt.training - Epoch  12, Step:   164300, Batch Loss:     1.022992, Tokens per Sec:     5125, Lr: 0.000300\n",
      "2021-10-09 13:05:35,456 - INFO - joeynmt.training - Epoch  12, Step:   164400, Batch Loss:     1.153323, Tokens per Sec:     5047, Lr: 0.000300\n",
      "2021-10-09 13:06:20,661 - INFO - joeynmt.training - Epoch  12, Step:   164500, Batch Loss:     0.959568, Tokens per Sec:     5048, Lr: 0.000300\n",
      "2021-10-09 13:07:05,931 - INFO - joeynmt.training - Epoch  12, Step:   164600, Batch Loss:     0.996686, Tokens per Sec:     5023, Lr: 0.000300\n",
      "2021-10-09 13:07:51,135 - INFO - joeynmt.training - Epoch  12, Step:   164700, Batch Loss:     1.064782, Tokens per Sec:     5062, Lr: 0.000300\n",
      "2021-10-09 13:08:35,890 - INFO - joeynmt.training - Epoch  12, Step:   164800, Batch Loss:     1.213110, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-09 13:09:20,998 - INFO - joeynmt.training - Epoch  12, Step:   164900, Batch Loss:     1.061880, Tokens per Sec:     5086, Lr: 0.000300\n",
      "2021-10-09 13:10:06,121 - INFO - joeynmt.training - Epoch  12, Step:   165000, Batch Loss:     0.826491, Tokens per Sec:     5010, Lr: 0.000300\n",
      "2021-10-09 13:11:35,341 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 13:11:35,342 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 13:11:35,342 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 13:11:41,659 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 13:11:41,660 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 13:11:41,660 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 13:11:41,660 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term used at the time for the brother taking the lead in a congregation .\n",
      "2021-10-09 13:11:41,660 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 13:11:41,661 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 13:11:41,661 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 13:11:41,661 - INFO - joeynmt.training - \tHypothesis: The appeal is understandable , for adolescence is a time when you get to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 13:11:41,661 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 13:11:41,661 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 13:11:41,661 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 13:11:41,662 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 13:11:41,662 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 13:11:41,662 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 13:11:41,662 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 13:11:41,662 - INFO - joeynmt.training - \tHypothesis: HOW DO WE MAY WE SEEARLY THEIR DOELVITE ?\n",
      "2021-10-09 13:11:41,662 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   165000: bleu:  49.65, loss: 28122.8672, ppl:   2.5337, duration: 95.5403s\n",
      "2021-10-09 13:12:26,340 - INFO - joeynmt.training - Epoch  12, Step:   165100, Batch Loss:     0.984999, Tokens per Sec:     4920, Lr: 0.000300\n",
      "2021-10-09 13:13:12,173 - INFO - joeynmt.training - Epoch  12, Step:   165200, Batch Loss:     0.970255, Tokens per Sec:     5126, Lr: 0.000300\n",
      "2021-10-09 13:13:57,961 - INFO - joeynmt.training - Epoch  12, Step:   165300, Batch Loss:     1.064281, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-09 13:14:43,567 - INFO - joeynmt.training - Epoch  12, Step:   165400, Batch Loss:     0.929832, Tokens per Sec:     5070, Lr: 0.000300\n",
      "2021-10-09 13:15:28,223 - INFO - joeynmt.training - Epoch  12, Step:   165500, Batch Loss:     0.941588, Tokens per Sec:     5001, Lr: 0.000300\n",
      "2021-10-09 13:16:14,020 - INFO - joeynmt.training - Epoch  12, Step:   165600, Batch Loss:     1.016178, Tokens per Sec:     5058, Lr: 0.000300\n",
      "2021-10-09 13:16:58,957 - INFO - joeynmt.training - Epoch  12, Step:   165700, Batch Loss:     1.132204, Tokens per Sec:     4883, Lr: 0.000300\n",
      "2021-10-09 13:17:43,739 - INFO - joeynmt.training - Epoch  12, Step:   165800, Batch Loss:     0.947154, Tokens per Sec:     4954, Lr: 0.000300\n",
      "2021-10-09 13:18:28,922 - INFO - joeynmt.training - Epoch  12, Step:   165900, Batch Loss:     0.894830, Tokens per Sec:     5058, Lr: 0.000300\n",
      "2021-10-09 13:19:14,282 - INFO - joeynmt.training - Epoch  12, Step:   166000, Batch Loss:     1.074490, Tokens per Sec:     5044, Lr: 0.000300\n",
      "2021-10-09 13:19:59,481 - INFO - joeynmt.training - Epoch  12, Step:   166100, Batch Loss:     0.987818, Tokens per Sec:     5058, Lr: 0.000300\n",
      "2021-10-09 13:20:43,985 - INFO - joeynmt.training - Epoch  12, Step:   166200, Batch Loss:     1.042016, Tokens per Sec:     5032, Lr: 0.000300\n",
      "2021-10-09 13:21:28,425 - INFO - joeynmt.training - Epoch  12, Step:   166300, Batch Loss:     0.988041, Tokens per Sec:     5036, Lr: 0.000300\n",
      "2021-10-09 13:22:13,151 - INFO - joeynmt.training - Epoch  12, Step:   166400, Batch Loss:     1.089883, Tokens per Sec:     4899, Lr: 0.000300\n",
      "2021-10-09 13:22:58,559 - INFO - joeynmt.training - Epoch  12, Step:   166500, Batch Loss:     1.002321, Tokens per Sec:     5058, Lr: 0.000300\n",
      "2021-10-09 13:23:44,232 - INFO - joeynmt.training - Epoch  12, Step:   166600, Batch Loss:     1.064822, Tokens per Sec:     5146, Lr: 0.000300\n",
      "2021-10-09 13:24:30,060 - INFO - joeynmt.training - Epoch  12, Step:   166700, Batch Loss:     1.155810, Tokens per Sec:     5019, Lr: 0.000300\n",
      "2021-10-09 13:25:15,870 - INFO - joeynmt.training - Epoch  12, Step:   166800, Batch Loss:     1.017923, Tokens per Sec:     5056, Lr: 0.000300\n",
      "2021-10-09 13:26:00,651 - INFO - joeynmt.training - Epoch  12, Step:   166900, Batch Loss:     0.939120, Tokens per Sec:     5041, Lr: 0.000300\n",
      "2021-10-09 13:26:45,501 - INFO - joeynmt.training - Epoch  12, Step:   167000, Batch Loss:     1.039536, Tokens per Sec:     5025, Lr: 0.000300\n",
      "2021-10-09 13:27:30,568 - INFO - joeynmt.training - Epoch  12, Step:   167100, Batch Loss:     0.957418, Tokens per Sec:     5062, Lr: 0.000300\n",
      "2021-10-09 13:28:16,459 - INFO - joeynmt.training - Epoch  12, Step:   167200, Batch Loss:     0.937055, Tokens per Sec:     5098, Lr: 0.000300\n",
      "2021-10-09 13:29:01,212 - INFO - joeynmt.training - Epoch  12, Step:   167300, Batch Loss:     1.103409, Tokens per Sec:     5023, Lr: 0.000300\n",
      "2021-10-09 13:29:46,663 - INFO - joeynmt.training - Epoch  12, Step:   167400, Batch Loss:     0.950668, Tokens per Sec:     5023, Lr: 0.000300\n",
      "2021-10-09 13:30:32,114 - INFO - joeynmt.training - Epoch  12, Step:   167500, Batch Loss:     1.047596, Tokens per Sec:     5072, Lr: 0.000300\n",
      "2021-10-09 13:31:17,305 - INFO - joeynmt.training - Epoch  12, Step:   167600, Batch Loss:     0.976162, Tokens per Sec:     4998, Lr: 0.000300\n",
      "2021-10-09 13:32:02,592 - INFO - joeynmt.training - Epoch  12, Step:   167700, Batch Loss:     1.034353, Tokens per Sec:     5059, Lr: 0.000300\n",
      "2021-10-09 13:32:47,365 - INFO - joeynmt.training - Epoch  12, Step:   167800, Batch Loss:     1.316575, Tokens per Sec:     5038, Lr: 0.000300\n",
      "2021-10-09 13:33:32,180 - INFO - joeynmt.training - Epoch  12, Step:   167900, Batch Loss:     0.987486, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-09 13:34:17,671 - INFO - joeynmt.training - Epoch  12, Step:   168000, Batch Loss:     0.952306, Tokens per Sec:     4971, Lr: 0.000300\n",
      "2021-10-09 13:35:02,910 - INFO - joeynmt.training - Epoch  12, Step:   168100, Batch Loss:     1.131292, Tokens per Sec:     4998, Lr: 0.000300\n",
      "2021-10-09 13:35:47,720 - INFO - joeynmt.training - Epoch  12, Step:   168200, Batch Loss:     0.958238, Tokens per Sec:     4908, Lr: 0.000300\n",
      "2021-10-09 13:36:32,435 - INFO - joeynmt.training - Epoch  12, Step:   168300, Batch Loss:     0.998638, Tokens per Sec:     5004, Lr: 0.000300\n",
      "2021-10-09 13:37:17,525 - INFO - joeynmt.training - Epoch  12, Step:   168400, Batch Loss:     0.999511, Tokens per Sec:     5066, Lr: 0.000300\n",
      "2021-10-09 13:38:02,779 - INFO - joeynmt.training - Epoch  12, Step:   168500, Batch Loss:     1.112535, Tokens per Sec:     5086, Lr: 0.000300\n",
      "2021-10-09 13:38:47,675 - INFO - joeynmt.training - Epoch  12, Step:   168600, Batch Loss:     0.963294, Tokens per Sec:     5056, Lr: 0.000300\n",
      "2021-10-09 13:39:32,931 - INFO - joeynmt.training - Epoch  12, Step:   168700, Batch Loss:     0.952077, Tokens per Sec:     5049, Lr: 0.000300\n",
      "2021-10-09 13:40:18,527 - INFO - joeynmt.training - Epoch  12, Step:   168800, Batch Loss:     1.017979, Tokens per Sec:     5163, Lr: 0.000300\n",
      "2021-10-09 13:41:03,235 - INFO - joeynmt.training - Epoch  12, Step:   168900, Batch Loss:     1.104339, Tokens per Sec:     4944, Lr: 0.000300\n",
      "2021-10-09 13:41:48,101 - INFO - joeynmt.training - Epoch  12, Step:   169000, Batch Loss:     0.984698, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-09 13:42:33,429 - INFO - joeynmt.training - Epoch  12, Step:   169100, Batch Loss:     0.996829, Tokens per Sec:     5040, Lr: 0.000300\n",
      "2021-10-09 13:43:18,952 - INFO - joeynmt.training - Epoch  12, Step:   169200, Batch Loss:     1.090981, Tokens per Sec:     5114, Lr: 0.000300\n",
      "2021-10-09 13:44:03,859 - INFO - joeynmt.training - Epoch  12, Step:   169300, Batch Loss:     0.990353, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-09 13:44:48,746 - INFO - joeynmt.training - Epoch  12, Step:   169400, Batch Loss:     0.899599, Tokens per Sec:     4998, Lr: 0.000300\n",
      "2021-10-09 13:45:34,308 - INFO - joeynmt.training - Epoch  12, Step:   169500, Batch Loss:     0.968732, Tokens per Sec:     5116, Lr: 0.000300\n",
      "2021-10-09 13:46:19,449 - INFO - joeynmt.training - Epoch  12, Step:   169600, Batch Loss:     0.924476, Tokens per Sec:     5047, Lr: 0.000300\n",
      "2021-10-09 13:47:04,758 - INFO - joeynmt.training - Epoch  12, Step:   169700, Batch Loss:     1.109831, Tokens per Sec:     5097, Lr: 0.000300\n",
      "2021-10-09 13:47:49,145 - INFO - joeynmt.training - Epoch  12, Step:   169800, Batch Loss:     1.138394, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2021-10-09 13:48:34,178 - INFO - joeynmt.training - Epoch  12, Step:   169900, Batch Loss:     1.010874, Tokens per Sec:     4970, Lr: 0.000300\n",
      "2021-10-09 13:49:18,746 - INFO - joeynmt.training - Epoch  12, Step:   170000, Batch Loss:     1.087739, Tokens per Sec:     4964, Lr: 0.000300\n",
      "2021-10-09 13:50:47,404 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 13:50:47,405 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 13:50:47,405 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 13:50:47,746 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 13:50:47,746 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 13:50:53,886 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 13:50:53,887 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 13:50:53,887 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 13:50:53,887 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-09 13:50:53,887 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 13:50:53,887 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 13:50:53,887 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 13:50:53,887 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 13:50:53,887 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 13:50:53,888 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 13:50:53,888 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 13:50:53,888 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 13:50:53,888 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 13:50:53,888 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 13:50:53,888 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 13:50:53,888 - INFO - joeynmt.training - \tHypothesis: HOW DO WE STEL US FAMILY ?\n",
      "2021-10-09 13:50:53,888 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   170000: bleu:  50.06, loss: 27802.9316, ppl:   2.5070, duration: 95.1414s\n",
      "2021-10-09 13:51:39,147 - INFO - joeynmt.training - Epoch  12, Step:   170100, Batch Loss:     0.938141, Tokens per Sec:     5119, Lr: 0.000300\n",
      "2021-10-09 13:52:23,881 - INFO - joeynmt.training - Epoch  12, Step:   170200, Batch Loss:     0.963822, Tokens per Sec:     4981, Lr: 0.000300\n",
      "2021-10-09 13:53:09,728 - INFO - joeynmt.training - Epoch  12, Step:   170300, Batch Loss:     1.004990, Tokens per Sec:     5162, Lr: 0.000300\n",
      "2021-10-09 13:53:54,823 - INFO - joeynmt.training - Epoch  12, Step:   170400, Batch Loss:     0.982261, Tokens per Sec:     4926, Lr: 0.000300\n",
      "2021-10-09 13:54:39,543 - INFO - joeynmt.training - Epoch  12, Step:   170500, Batch Loss:     1.006950, Tokens per Sec:     4962, Lr: 0.000300\n",
      "2021-10-09 13:55:24,833 - INFO - joeynmt.training - Epoch  12, Step:   170600, Batch Loss:     1.074420, Tokens per Sec:     5072, Lr: 0.000300\n",
      "2021-10-09 13:56:09,600 - INFO - joeynmt.training - Epoch  12, Step:   170700, Batch Loss:     1.011326, Tokens per Sec:     4874, Lr: 0.000300\n",
      "2021-10-09 13:56:55,164 - INFO - joeynmt.training - Epoch  12, Step:   170800, Batch Loss:     0.938460, Tokens per Sec:     5099, Lr: 0.000300\n",
      "2021-10-09 13:57:40,421 - INFO - joeynmt.training - Epoch  12, Step:   170900, Batch Loss:     1.021121, Tokens per Sec:     4979, Lr: 0.000300\n",
      "2021-10-09 13:58:25,528 - INFO - joeynmt.training - Epoch  12, Step:   171000, Batch Loss:     0.968951, Tokens per Sec:     5081, Lr: 0.000300\n",
      "2021-10-09 13:59:10,817 - INFO - joeynmt.training - Epoch  12, Step:   171100, Batch Loss:     0.991560, Tokens per Sec:     5025, Lr: 0.000300\n",
      "2021-10-09 13:59:55,208 - INFO - joeynmt.training - Epoch  12, Step:   171200, Batch Loss:     1.051688, Tokens per Sec:     4939, Lr: 0.000300\n",
      "2021-10-09 14:00:40,365 - INFO - joeynmt.training - Epoch  12, Step:   171300, Batch Loss:     0.980942, Tokens per Sec:     5128, Lr: 0.000300\n",
      "2021-10-09 14:01:25,075 - INFO - joeynmt.training - Epoch  12, Step:   171400, Batch Loss:     1.339561, Tokens per Sec:     5032, Lr: 0.000300\n",
      "2021-10-09 14:02:09,797 - INFO - joeynmt.training - Epoch  12, Step:   171500, Batch Loss:     0.902631, Tokens per Sec:     5068, Lr: 0.000300\n",
      "2021-10-09 14:02:55,230 - INFO - joeynmt.training - Epoch  12, Step:   171600, Batch Loss:     1.065596, Tokens per Sec:     5097, Lr: 0.000300\n",
      "2021-10-09 14:03:39,645 - INFO - joeynmt.training - Epoch  12, Step:   171700, Batch Loss:     0.928234, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-09 14:04:24,683 - INFO - joeynmt.training - Epoch  12, Step:   171800, Batch Loss:     0.939943, Tokens per Sec:     5041, Lr: 0.000300\n",
      "2021-10-09 14:05:09,573 - INFO - joeynmt.training - Epoch  12, Step:   171900, Batch Loss:     0.875534, Tokens per Sec:     5117, Lr: 0.000300\n",
      "2021-10-09 14:05:54,005 - INFO - joeynmt.training - Epoch  12, Step:   172000, Batch Loss:     1.268959, Tokens per Sec:     4972, Lr: 0.000300\n",
      "2021-10-09 14:06:38,790 - INFO - joeynmt.training - Epoch  12, Step:   172100, Batch Loss:     1.086638, Tokens per Sec:     5007, Lr: 0.000300\n",
      "2021-10-09 14:07:23,292 - INFO - joeynmt.training - Epoch  12, Step:   172200, Batch Loss:     1.088614, Tokens per Sec:     5027, Lr: 0.000300\n",
      "2021-10-09 14:08:08,029 - INFO - joeynmt.training - Epoch  12, Step:   172300, Batch Loss:     0.951942, Tokens per Sec:     5012, Lr: 0.000300\n",
      "2021-10-09 14:08:52,818 - INFO - joeynmt.training - Epoch  12, Step:   172400, Batch Loss:     1.040374, Tokens per Sec:     5106, Lr: 0.000300\n",
      "2021-10-09 14:09:37,804 - INFO - joeynmt.training - Epoch  12, Step:   172500, Batch Loss:     0.983801, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-09 14:10:23,248 - INFO - joeynmt.training - Epoch  12, Step:   172600, Batch Loss:     1.007118, Tokens per Sec:     5045, Lr: 0.000300\n",
      "2021-10-09 14:11:08,301 - INFO - joeynmt.training - Epoch  12, Step:   172700, Batch Loss:     1.065103, Tokens per Sec:     4997, Lr: 0.000300\n",
      "2021-10-09 14:11:53,043 - INFO - joeynmt.training - Epoch  12, Step:   172800, Batch Loss:     0.970958, Tokens per Sec:     4986, Lr: 0.000300\n",
      "2021-10-09 14:12:38,251 - INFO - joeynmt.training - Epoch  12, Step:   172900, Batch Loss:     0.959745, Tokens per Sec:     5057, Lr: 0.000300\n",
      "2021-10-09 14:13:23,413 - INFO - joeynmt.training - Epoch  12, Step:   173000, Batch Loss:     1.042451, Tokens per Sec:     5058, Lr: 0.000300\n",
      "2021-10-09 14:14:08,501 - INFO - joeynmt.training - Epoch  12, Step:   173100, Batch Loss:     1.025869, Tokens per Sec:     5077, Lr: 0.000300\n",
      "2021-10-09 14:14:53,013 - INFO - joeynmt.training - Epoch  12, Step:   173200, Batch Loss:     0.869730, Tokens per Sec:     4967, Lr: 0.000300\n",
      "2021-10-09 14:15:37,833 - INFO - joeynmt.training - Epoch  12, Step:   173300, Batch Loss:     1.200418, Tokens per Sec:     5097, Lr: 0.000300\n",
      "2021-10-09 14:16:21,915 - INFO - joeynmt.training - Epoch  12, Step:   173400, Batch Loss:     1.013154, Tokens per Sec:     4939, Lr: 0.000300\n",
      "2021-10-09 14:17:06,614 - INFO - joeynmt.training - Epoch  12, Step:   173500, Batch Loss:     1.170564, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-09 14:17:51,616 - INFO - joeynmt.training - Epoch  12, Step:   173600, Batch Loss:     1.073055, Tokens per Sec:     5004, Lr: 0.000300\n",
      "2021-10-09 14:18:36,294 - INFO - joeynmt.training - Epoch  12, Step:   173700, Batch Loss:     1.043014, Tokens per Sec:     4974, Lr: 0.000300\n",
      "2021-10-09 14:19:21,934 - INFO - joeynmt.training - Epoch  12, Step:   173800, Batch Loss:     1.032982, Tokens per Sec:     5073, Lr: 0.000300\n",
      "2021-10-09 14:20:06,179 - INFO - joeynmt.training - Epoch  12, Step:   173900, Batch Loss:     1.005505, Tokens per Sec:     5010, Lr: 0.000300\n",
      "2021-10-09 14:20:50,700 - INFO - joeynmt.training - Epoch  12, Step:   174000, Batch Loss:     0.974013, Tokens per Sec:     5013, Lr: 0.000300\n",
      "2021-10-09 14:21:35,539 - INFO - joeynmt.training - Epoch  12, Step:   174100, Batch Loss:     0.956309, Tokens per Sec:     4989, Lr: 0.000300\n",
      "2021-10-09 14:22:20,874 - INFO - joeynmt.training - Epoch  12, Step:   174200, Batch Loss:     0.935967, Tokens per Sec:     5021, Lr: 0.000300\n",
      "2021-10-09 14:23:05,916 - INFO - joeynmt.training - Epoch  12, Step:   174300, Batch Loss:     0.924804, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-09 14:23:50,682 - INFO - joeynmt.training - Epoch  12, Step:   174400, Batch Loss:     0.940492, Tokens per Sec:     5018, Lr: 0.000300\n",
      "2021-10-09 14:24:35,571 - INFO - joeynmt.training - Epoch  12, Step:   174500, Batch Loss:     1.115222, Tokens per Sec:     5003, Lr: 0.000300\n",
      "2021-10-09 14:25:20,725 - INFO - joeynmt.training - Epoch  12, Step:   174600, Batch Loss:     1.122450, Tokens per Sec:     5038, Lr: 0.000300\n",
      "2021-10-09 14:26:05,116 - INFO - joeynmt.training - Epoch  12, Step:   174700, Batch Loss:     0.997044, Tokens per Sec:     4909, Lr: 0.000300\n",
      "2021-10-09 14:26:50,686 - INFO - joeynmt.training - Epoch  12, Step:   174800, Batch Loss:     1.090969, Tokens per Sec:     5046, Lr: 0.000300\n",
      "2021-10-09 14:27:35,883 - INFO - joeynmt.training - Epoch  12, Step:   174900, Batch Loss:     1.159329, Tokens per Sec:     5021, Lr: 0.000300\n",
      "2021-10-09 14:28:20,753 - INFO - joeynmt.training - Epoch  12, Step:   175000, Batch Loss:     0.984262, Tokens per Sec:     4960, Lr: 0.000300\n",
      "2021-10-09 14:29:46,684 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 14:29:46,685 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 14:29:46,685 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 14:29:47,025 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 14:29:47,025 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 14:29:53,017 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 14:29:53,018 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 14:29:53,018 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 14:29:53,018 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term then used for the brother who took the lead in a congregation .\n",
      "2021-10-09 14:29:53,019 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 14:29:53,019 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 14:29:53,019 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 14:29:53,019 - INFO - joeynmt.training - \tHypothesis: The appeal is understandable , for adolescence is a time when one comes to know himself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 14:29:53,019 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 14:29:53,019 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 14:29:53,019 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 14:29:53,019 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 14:29:53,020 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 14:29:53,020 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 14:29:53,020 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 14:29:53,020 - INFO - joeynmt.training - \tHypothesis: HOW DO WE STEED OUR FAMILY DEAR ?\n",
      "2021-10-09 14:29:53,020 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   175000: bleu:  49.59, loss: 27779.2637, ppl:   2.5051, duration: 92.2668s\n",
      "2021-10-09 14:30:37,694 - INFO - joeynmt.training - Epoch  12, Step:   175100, Batch Loss:     0.946823, Tokens per Sec:     4948, Lr: 0.000300\n",
      "2021-10-09 14:31:22,974 - INFO - joeynmt.training - Epoch  12, Step:   175200, Batch Loss:     0.873474, Tokens per Sec:     4977, Lr: 0.000300\n",
      "2021-10-09 14:31:49,472 - INFO - joeynmt.training - Epoch  12: total training loss 12841.05\n",
      "2021-10-09 14:31:49,472 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-10-09 14:32:10,726 - INFO - joeynmt.training - Epoch  13, Step:   175300, Batch Loss:     0.976222, Tokens per Sec:     4771, Lr: 0.000300\n",
      "2021-10-09 14:32:55,569 - INFO - joeynmt.training - Epoch  13, Step:   175400, Batch Loss:     0.945158, Tokens per Sec:     4929, Lr: 0.000300\n",
      "2021-10-09 14:33:40,472 - INFO - joeynmt.training - Epoch  13, Step:   175500, Batch Loss:     1.098085, Tokens per Sec:     4999, Lr: 0.000300\n",
      "2021-10-09 14:34:25,824 - INFO - joeynmt.training - Epoch  13, Step:   175600, Batch Loss:     0.985982, Tokens per Sec:     4958, Lr: 0.000300\n",
      "2021-10-09 14:35:10,040 - INFO - joeynmt.training - Epoch  13, Step:   175700, Batch Loss:     0.975702, Tokens per Sec:     4927, Lr: 0.000300\n",
      "2021-10-09 14:35:55,385 - INFO - joeynmt.training - Epoch  13, Step:   175800, Batch Loss:     1.100959, Tokens per Sec:     5036, Lr: 0.000300\n",
      "2021-10-09 14:36:40,297 - INFO - joeynmt.training - Epoch  13, Step:   175900, Batch Loss:     1.028123, Tokens per Sec:     5027, Lr: 0.000300\n",
      "2021-10-09 14:37:25,301 - INFO - joeynmt.training - Epoch  13, Step:   176000, Batch Loss:     1.088522, Tokens per Sec:     4963, Lr: 0.000300\n",
      "2021-10-09 14:38:10,204 - INFO - joeynmt.training - Epoch  13, Step:   176100, Batch Loss:     0.933171, Tokens per Sec:     5022, Lr: 0.000300\n",
      "2021-10-09 14:38:54,745 - INFO - joeynmt.training - Epoch  13, Step:   176200, Batch Loss:     1.002494, Tokens per Sec:     4859, Lr: 0.000300\n",
      "2021-10-09 14:39:39,088 - INFO - joeynmt.training - Epoch  13, Step:   176300, Batch Loss:     1.009176, Tokens per Sec:     4968, Lr: 0.000300\n",
      "2021-10-09 14:40:24,776 - INFO - joeynmt.training - Epoch  13, Step:   176400, Batch Loss:     1.110755, Tokens per Sec:     5042, Lr: 0.000300\n",
      "2021-10-09 14:41:09,750 - INFO - joeynmt.training - Epoch  13, Step:   176500, Batch Loss:     1.148096, Tokens per Sec:     4974, Lr: 0.000300\n",
      "2021-10-09 14:41:55,231 - INFO - joeynmt.training - Epoch  13, Step:   176600, Batch Loss:     0.816667, Tokens per Sec:     5031, Lr: 0.000300\n",
      "2021-10-09 14:42:40,194 - INFO - joeynmt.training - Epoch  13, Step:   176700, Batch Loss:     1.114420, Tokens per Sec:     4959, Lr: 0.000300\n",
      "2021-10-09 14:43:25,418 - INFO - joeynmt.training - Epoch  13, Step:   176800, Batch Loss:     0.995800, Tokens per Sec:     5041, Lr: 0.000300\n",
      "2021-10-09 14:44:10,000 - INFO - joeynmt.training - Epoch  13, Step:   176900, Batch Loss:     0.968172, Tokens per Sec:     4911, Lr: 0.000300\n",
      "2021-10-09 14:44:55,428 - INFO - joeynmt.training - Epoch  13, Step:   177000, Batch Loss:     0.981181, Tokens per Sec:     5003, Lr: 0.000300\n",
      "2021-10-09 14:45:41,042 - INFO - joeynmt.training - Epoch  13, Step:   177100, Batch Loss:     1.004360, Tokens per Sec:     5044, Lr: 0.000300\n",
      "2021-10-09 14:46:25,757 - INFO - joeynmt.training - Epoch  13, Step:   177200, Batch Loss:     1.088877, Tokens per Sec:     5039, Lr: 0.000300\n",
      "2021-10-09 14:47:10,489 - INFO - joeynmt.training - Epoch  13, Step:   177300, Batch Loss:     0.991496, Tokens per Sec:     4940, Lr: 0.000300\n",
      "2021-10-09 14:47:55,422 - INFO - joeynmt.training - Epoch  13, Step:   177400, Batch Loss:     0.863131, Tokens per Sec:     5069, Lr: 0.000300\n",
      "2021-10-09 14:48:40,313 - INFO - joeynmt.training - Epoch  13, Step:   177500, Batch Loss:     0.876920, Tokens per Sec:     4977, Lr: 0.000300\n",
      "2021-10-09 14:49:24,941 - INFO - joeynmt.training - Epoch  13, Step:   177600, Batch Loss:     1.154694, Tokens per Sec:     4942, Lr: 0.000300\n",
      "2021-10-09 14:50:09,856 - INFO - joeynmt.training - Epoch  13, Step:   177700, Batch Loss:     0.927744, Tokens per Sec:     5054, Lr: 0.000300\n",
      "2021-10-09 14:50:55,853 - INFO - joeynmt.training - Epoch  13, Step:   177800, Batch Loss:     0.987524, Tokens per Sec:     5110, Lr: 0.000300\n",
      "2021-10-09 14:51:40,851 - INFO - joeynmt.training - Epoch  13, Step:   177900, Batch Loss:     1.058597, Tokens per Sec:     4935, Lr: 0.000300\n",
      "2021-10-09 14:52:25,343 - INFO - joeynmt.training - Epoch  13, Step:   178000, Batch Loss:     0.976970, Tokens per Sec:     4978, Lr: 0.000300\n",
      "2021-10-09 14:53:10,403 - INFO - joeynmt.training - Epoch  13, Step:   178100, Batch Loss:     0.951203, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2021-10-09 14:53:55,037 - INFO - joeynmt.training - Epoch  13, Step:   178200, Batch Loss:     0.993596, Tokens per Sec:     4989, Lr: 0.000300\n",
      "2021-10-09 14:54:39,855 - INFO - joeynmt.training - Epoch  13, Step:   178300, Batch Loss:     0.888444, Tokens per Sec:     4984, Lr: 0.000300\n",
      "2021-10-09 14:55:25,681 - INFO - joeynmt.training - Epoch  13, Step:   178400, Batch Loss:     0.912350, Tokens per Sec:     5088, Lr: 0.000300\n",
      "2021-10-09 14:56:10,732 - INFO - joeynmt.training - Epoch  13, Step:   178500, Batch Loss:     0.950637, Tokens per Sec:     4998, Lr: 0.000300\n",
      "2021-10-09 14:56:55,742 - INFO - joeynmt.training - Epoch  13, Step:   178600, Batch Loss:     0.888607, Tokens per Sec:     5003, Lr: 0.000300\n",
      "2021-10-09 14:57:41,003 - INFO - joeynmt.training - Epoch  13, Step:   178700, Batch Loss:     1.086972, Tokens per Sec:     5136, Lr: 0.000300\n",
      "2021-10-09 14:58:26,309 - INFO - joeynmt.training - Epoch  13, Step:   178800, Batch Loss:     0.980619, Tokens per Sec:     5058, Lr: 0.000300\n",
      "2021-10-09 14:59:10,898 - INFO - joeynmt.training - Epoch  13, Step:   178900, Batch Loss:     1.086375, Tokens per Sec:     4997, Lr: 0.000300\n",
      "2021-10-09 14:59:56,503 - INFO - joeynmt.training - Epoch  13, Step:   179000, Batch Loss:     1.057246, Tokens per Sec:     5055, Lr: 0.000300\n",
      "2021-10-09 15:00:41,377 - INFO - joeynmt.training - Epoch  13, Step:   179100, Batch Loss:     1.068764, Tokens per Sec:     4959, Lr: 0.000300\n",
      "2021-10-09 15:01:26,393 - INFO - joeynmt.training - Epoch  13, Step:   179200, Batch Loss:     1.004123, Tokens per Sec:     5027, Lr: 0.000300\n",
      "2021-10-09 15:02:11,742 - INFO - joeynmt.training - Epoch  13, Step:   179300, Batch Loss:     1.138187, Tokens per Sec:     5033, Lr: 0.000300\n",
      "2021-10-09 15:02:57,339 - INFO - joeynmt.training - Epoch  13, Step:   179400, Batch Loss:     0.975970, Tokens per Sec:     5049, Lr: 0.000300\n",
      "2021-10-09 15:03:42,723 - INFO - joeynmt.training - Epoch  13, Step:   179500, Batch Loss:     0.894593, Tokens per Sec:     5072, Lr: 0.000300\n",
      "2021-10-09 15:04:27,569 - INFO - joeynmt.training - Epoch  13, Step:   179600, Batch Loss:     1.026827, Tokens per Sec:     5054, Lr: 0.000300\n",
      "2021-10-09 15:05:12,436 - INFO - joeynmt.training - Epoch  13, Step:   179700, Batch Loss:     1.108036, Tokens per Sec:     5021, Lr: 0.000300\n",
      "2021-10-09 15:05:57,194 - INFO - joeynmt.training - Epoch  13, Step:   179800, Batch Loss:     1.068588, Tokens per Sec:     5010, Lr: 0.000300\n",
      "2021-10-09 15:06:42,111 - INFO - joeynmt.training - Epoch  13, Step:   179900, Batch Loss:     1.086776, Tokens per Sec:     5033, Lr: 0.000300\n",
      "2021-10-09 15:07:26,846 - INFO - joeynmt.training - Epoch  13, Step:   180000, Batch Loss:     0.951623, Tokens per Sec:     5042, Lr: 0.000300\n",
      "2021-10-09 15:08:53,718 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 15:08:53,719 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 15:08:53,719 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 15:09:00,031 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 15:09:00,033 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 15:09:00,033 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 15:09:00,033 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-09 15:09:00,033 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 15:09:00,033 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 15:09:00,034 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 15:09:00,034 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 15:09:00,034 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 15:09:00,034 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 15:09:00,035 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 15:09:00,035 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 15:09:00,035 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 15:09:00,035 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 15:09:00,035 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 15:09:00,036 - INFO - joeynmt.training - \tHypothesis: HOW DO WE WE SHOW WE HAVE WE SECTED DEELVITS ?\n",
      "2021-10-09 15:09:00,036 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step   180000: bleu:  49.39, loss: 27813.2109, ppl:   2.5079, duration: 93.1888s\n",
      "2021-10-09 15:09:44,955 - INFO - joeynmt.training - Epoch  13, Step:   180100, Batch Loss:     0.988488, Tokens per Sec:     5053, Lr: 0.000300\n",
      "2021-10-09 15:10:29,959 - INFO - joeynmt.training - Epoch  13, Step:   180200, Batch Loss:     1.051627, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2021-10-09 15:11:14,474 - INFO - joeynmt.training - Epoch  13, Step:   180300, Batch Loss:     1.062700, Tokens per Sec:     5022, Lr: 0.000300\n",
      "2021-10-09 15:11:59,627 - INFO - joeynmt.training - Epoch  13, Step:   180400, Batch Loss:     0.983432, Tokens per Sec:     5040, Lr: 0.000300\n",
      "2021-10-09 15:12:44,090 - INFO - joeynmt.training - Epoch  13, Step:   180500, Batch Loss:     0.913877, Tokens per Sec:     4976, Lr: 0.000300\n",
      "2021-10-09 15:13:28,926 - INFO - joeynmt.training - Epoch  13, Step:   180600, Batch Loss:     1.073254, Tokens per Sec:     5027, Lr: 0.000300\n",
      "2021-10-09 15:14:14,155 - INFO - joeynmt.training - Epoch  13, Step:   180700, Batch Loss:     1.093018, Tokens per Sec:     5047, Lr: 0.000300\n",
      "2021-10-09 15:14:59,160 - INFO - joeynmt.training - Epoch  13, Step:   180800, Batch Loss:     0.928174, Tokens per Sec:     5073, Lr: 0.000300\n",
      "2021-10-09 15:15:43,815 - INFO - joeynmt.training - Epoch  13, Step:   180900, Batch Loss:     1.199813, Tokens per Sec:     5011, Lr: 0.000300\n",
      "2021-10-09 15:16:29,602 - INFO - joeynmt.training - Epoch  13, Step:   181000, Batch Loss:     1.009243, Tokens per Sec:     5191, Lr: 0.000300\n",
      "2021-10-09 15:17:14,681 - INFO - joeynmt.training - Epoch  13, Step:   181100, Batch Loss:     1.049598, Tokens per Sec:     5137, Lr: 0.000300\n",
      "2021-10-09 15:17:59,138 - INFO - joeynmt.training - Epoch  13, Step:   181200, Batch Loss:     1.028822, Tokens per Sec:     5010, Lr: 0.000300\n",
      "2021-10-09 15:18:43,408 - INFO - joeynmt.training - Epoch  13, Step:   181300, Batch Loss:     1.117104, Tokens per Sec:     5011, Lr: 0.000300\n",
      "2021-10-09 15:19:28,129 - INFO - joeynmt.training - Epoch  13, Step:   181400, Batch Loss:     1.147282, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-09 15:20:12,982 - INFO - joeynmt.training - Epoch  13, Step:   181500, Batch Loss:     1.072619, Tokens per Sec:     5060, Lr: 0.000300\n",
      "2021-10-09 15:20:57,483 - INFO - joeynmt.training - Epoch  13, Step:   181600, Batch Loss:     1.079293, Tokens per Sec:     5052, Lr: 0.000300\n",
      "2021-10-09 15:21:42,453 - INFO - joeynmt.training - Epoch  13, Step:   181700, Batch Loss:     1.133125, Tokens per Sec:     5045, Lr: 0.000300\n",
      "2021-10-09 15:22:27,752 - INFO - joeynmt.training - Epoch  13, Step:   181800, Batch Loss:     0.996033, Tokens per Sec:     5068, Lr: 0.000300\n",
      "2021-10-09 15:23:12,391 - INFO - joeynmt.training - Epoch  13, Step:   181900, Batch Loss:     0.983755, Tokens per Sec:     4984, Lr: 0.000300\n",
      "2021-10-09 15:23:57,225 - INFO - joeynmt.training - Epoch  13, Step:   182000, Batch Loss:     0.934502, Tokens per Sec:     5013, Lr: 0.000300\n",
      "2021-10-09 15:24:42,597 - INFO - joeynmt.training - Epoch  13, Step:   182100, Batch Loss:     1.043441, Tokens per Sec:     5086, Lr: 0.000300\n",
      "2021-10-09 15:25:27,503 - INFO - joeynmt.training - Epoch  13, Step:   182200, Batch Loss:     0.968691, Tokens per Sec:     5081, Lr: 0.000300\n",
      "2021-10-09 15:26:12,313 - INFO - joeynmt.training - Epoch  13, Step:   182300, Batch Loss:     1.015072, Tokens per Sec:     5051, Lr: 0.000300\n",
      "2021-10-09 15:26:57,484 - INFO - joeynmt.training - Epoch  13, Step:   182400, Batch Loss:     0.938592, Tokens per Sec:     4999, Lr: 0.000300\n",
      "2021-10-09 15:27:42,385 - INFO - joeynmt.training - Epoch  13, Step:   182500, Batch Loss:     0.975448, Tokens per Sec:     5006, Lr: 0.000300\n",
      "2021-10-09 15:28:26,685 - INFO - joeynmt.training - Epoch  13, Step:   182600, Batch Loss:     0.947541, Tokens per Sec:     5067, Lr: 0.000300\n",
      "2021-10-09 15:29:11,675 - INFO - joeynmt.training - Epoch  13, Step:   182700, Batch Loss:     0.814473, Tokens per Sec:     5114, Lr: 0.000300\n",
      "2021-10-09 15:29:56,178 - INFO - joeynmt.training - Epoch  13, Step:   182800, Batch Loss:     1.036275, Tokens per Sec:     4962, Lr: 0.000300\n",
      "2021-10-09 15:30:41,048 - INFO - joeynmt.training - Epoch  13, Step:   182900, Batch Loss:     1.061417, Tokens per Sec:     5048, Lr: 0.000300\n",
      "2021-10-09 15:31:25,512 - INFO - joeynmt.training - Epoch  13, Step:   183000, Batch Loss:     0.987736, Tokens per Sec:     5058, Lr: 0.000300\n",
      "2021-10-09 15:32:10,780 - INFO - joeynmt.training - Epoch  13, Step:   183100, Batch Loss:     1.123960, Tokens per Sec:     5045, Lr: 0.000300\n",
      "2021-10-09 15:32:56,111 - INFO - joeynmt.training - Epoch  13, Step:   183200, Batch Loss:     1.067659, Tokens per Sec:     5129, Lr: 0.000300\n",
      "2021-10-09 15:33:41,115 - INFO - joeynmt.training - Epoch  13, Step:   183300, Batch Loss:     1.153478, Tokens per Sec:     5138, Lr: 0.000300\n",
      "2021-10-09 15:34:26,471 - INFO - joeynmt.training - Epoch  13, Step:   183400, Batch Loss:     0.995191, Tokens per Sec:     5048, Lr: 0.000300\n",
      "2021-10-09 15:35:11,725 - INFO - joeynmt.training - Epoch  13, Step:   183500, Batch Loss:     0.927501, Tokens per Sec:     5080, Lr: 0.000300\n",
      "2021-10-09 15:35:56,963 - INFO - joeynmt.training - Epoch  13, Step:   183600, Batch Loss:     0.952983, Tokens per Sec:     5093, Lr: 0.000300\n",
      "2021-10-09 15:36:42,236 - INFO - joeynmt.training - Epoch  13, Step:   183700, Batch Loss:     0.891103, Tokens per Sec:     5134, Lr: 0.000300\n",
      "2021-10-09 15:37:27,702 - INFO - joeynmt.training - Epoch  13, Step:   183800, Batch Loss:     1.017095, Tokens per Sec:     5088, Lr: 0.000300\n",
      "2021-10-09 15:38:12,469 - INFO - joeynmt.training - Epoch  13, Step:   183900, Batch Loss:     1.001550, Tokens per Sec:     5041, Lr: 0.000300\n",
      "2021-10-09 15:38:56,745 - INFO - joeynmt.training - Epoch  13, Step:   184000, Batch Loss:     1.108660, Tokens per Sec:     4925, Lr: 0.000300\n",
      "2021-10-09 15:39:41,091 - INFO - joeynmt.training - Epoch  13, Step:   184100, Batch Loss:     0.884022, Tokens per Sec:     4949, Lr: 0.000300\n",
      "2021-10-09 15:40:26,538 - INFO - joeynmt.training - Epoch  13, Step:   184200, Batch Loss:     1.038019, Tokens per Sec:     5050, Lr: 0.000300\n",
      "2021-10-09 15:41:11,087 - INFO - joeynmt.training - Epoch  13, Step:   184300, Batch Loss:     0.921272, Tokens per Sec:     4955, Lr: 0.000300\n",
      "2021-10-09 15:41:55,738 - INFO - joeynmt.training - Epoch  13, Step:   184400, Batch Loss:     1.091403, Tokens per Sec:     5036, Lr: 0.000300\n",
      "2021-10-09 15:42:40,027 - INFO - joeynmt.training - Epoch  13, Step:   184500, Batch Loss:     0.981311, Tokens per Sec:     4971, Lr: 0.000300\n",
      "2021-10-09 15:43:24,645 - INFO - joeynmt.training - Epoch  13, Step:   184600, Batch Loss:     0.918705, Tokens per Sec:     5011, Lr: 0.000300\n",
      "2021-10-09 15:44:09,875 - INFO - joeynmt.training - Epoch  13, Step:   184700, Batch Loss:     0.987801, Tokens per Sec:     5158, Lr: 0.000300\n",
      "2021-10-09 15:44:54,485 - INFO - joeynmt.training - Epoch  13, Step:   184800, Batch Loss:     0.967598, Tokens per Sec:     4926, Lr: 0.000300\n",
      "2021-10-09 15:45:40,232 - INFO - joeynmt.training - Epoch  13, Step:   184900, Batch Loss:     1.045615, Tokens per Sec:     5149, Lr: 0.000300\n",
      "2021-10-09 15:46:25,221 - INFO - joeynmt.training - Epoch  13, Step:   185000, Batch Loss:     1.031861, Tokens per Sec:     5061, Lr: 0.000300\n",
      "2021-10-09 15:47:54,089 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 15:47:54,090 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 15:47:54,090 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 15:47:54,428 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 15:47:54,428 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 15:48:00,534 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 15:48:00,535 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 15:48:00,535 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 15:48:00,535 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-09 15:48:00,535 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 15:48:00,535 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 15:48:00,535 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 15:48:00,535 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know himself and expresses your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 15:48:00,535 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 15:48:00,536 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 15:48:00,536 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 15:48:00,536 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 15:48:00,536 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 15:48:00,536 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 15:48:00,536 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 15:48:00,536 - INFO - joeynmt.training - \tHypothesis: HOW DO WE SHOULD WE FAITH DEACTS ?\n",
      "2021-10-09 15:48:00,536 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step   185000: bleu:  50.02, loss: 27587.9590, ppl:   2.4893, duration: 95.3152s\n",
      "2021-10-09 15:48:46,185 - INFO - joeynmt.training - Epoch  13, Step:   185100, Batch Loss:     1.077735, Tokens per Sec:     5081, Lr: 0.000300\n",
      "2021-10-09 15:49:31,509 - INFO - joeynmt.training - Epoch  13, Step:   185200, Batch Loss:     0.927839, Tokens per Sec:     5068, Lr: 0.000300\n",
      "2021-10-09 15:50:16,420 - INFO - joeynmt.training - Epoch  13, Step:   185300, Batch Loss:     0.823536, Tokens per Sec:     5081, Lr: 0.000300\n",
      "2021-10-09 15:51:01,494 - INFO - joeynmt.training - Epoch  13, Step:   185400, Batch Loss:     0.999974, Tokens per Sec:     5007, Lr: 0.000300\n",
      "2021-10-09 15:51:46,620 - INFO - joeynmt.training - Epoch  13, Step:   185500, Batch Loss:     1.183207, Tokens per Sec:     5039, Lr: 0.000300\n",
      "2021-10-09 15:52:31,005 - INFO - joeynmt.training - Epoch  13, Step:   185600, Batch Loss:     1.017040, Tokens per Sec:     4935, Lr: 0.000300\n",
      "2021-10-09 15:53:16,164 - INFO - joeynmt.training - Epoch  13, Step:   185700, Batch Loss:     0.890461, Tokens per Sec:     5088, Lr: 0.000300\n",
      "2021-10-09 15:54:01,148 - INFO - joeynmt.training - Epoch  13, Step:   185800, Batch Loss:     1.089321, Tokens per Sec:     5063, Lr: 0.000300\n",
      "2021-10-09 15:54:46,260 - INFO - joeynmt.training - Epoch  13, Step:   185900, Batch Loss:     1.021216, Tokens per Sec:     4993, Lr: 0.000300\n",
      "2021-10-09 15:55:31,453 - INFO - joeynmt.training - Epoch  13, Step:   186000, Batch Loss:     1.002760, Tokens per Sec:     5084, Lr: 0.000300\n",
      "2021-10-09 15:56:16,442 - INFO - joeynmt.training - Epoch  13, Step:   186100, Batch Loss:     1.031193, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2021-10-09 15:57:01,061 - INFO - joeynmt.training - Epoch  13, Step:   186200, Batch Loss:     0.997396, Tokens per Sec:     5115, Lr: 0.000300\n",
      "2021-10-09 15:57:46,200 - INFO - joeynmt.training - Epoch  13, Step:   186300, Batch Loss:     0.956818, Tokens per Sec:     5092, Lr: 0.000300\n",
      "2021-10-09 15:58:30,883 - INFO - joeynmt.training - Epoch  13, Step:   186400, Batch Loss:     1.063744, Tokens per Sec:     4990, Lr: 0.000300\n",
      "2021-10-09 15:59:16,452 - INFO - joeynmt.training - Epoch  13, Step:   186500, Batch Loss:     0.980897, Tokens per Sec:     4994, Lr: 0.000300\n",
      "2021-10-09 16:00:01,471 - INFO - joeynmt.training - Epoch  13, Step:   186600, Batch Loss:     0.970292, Tokens per Sec:     5079, Lr: 0.000300\n",
      "2021-10-09 16:00:46,655 - INFO - joeynmt.training - Epoch  13, Step:   186700, Batch Loss:     1.083742, Tokens per Sec:     5119, Lr: 0.000300\n",
      "2021-10-09 16:01:31,304 - INFO - joeynmt.training - Epoch  13, Step:   186800, Batch Loss:     1.039983, Tokens per Sec:     4999, Lr: 0.000300\n",
      "2021-10-09 16:02:16,952 - INFO - joeynmt.training - Epoch  13, Step:   186900, Batch Loss:     1.036341, Tokens per Sec:     5151, Lr: 0.000300\n",
      "2021-10-09 16:03:02,009 - INFO - joeynmt.training - Epoch  13, Step:   187000, Batch Loss:     0.995586, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2021-10-09 16:03:47,091 - INFO - joeynmt.training - Epoch  13, Step:   187100, Batch Loss:     1.092721, Tokens per Sec:     5097, Lr: 0.000300\n",
      "2021-10-09 16:04:31,813 - INFO - joeynmt.training - Epoch  13, Step:   187200, Batch Loss:     1.087200, Tokens per Sec:     4928, Lr: 0.000300\n",
      "2021-10-09 16:05:16,014 - INFO - joeynmt.training - Epoch  13, Step:   187300, Batch Loss:     0.869567, Tokens per Sec:     4984, Lr: 0.000300\n",
      "2021-10-09 16:06:01,186 - INFO - joeynmt.training - Epoch  13, Step:   187400, Batch Loss:     0.889144, Tokens per Sec:     5037, Lr: 0.000300\n",
      "2021-10-09 16:06:46,663 - INFO - joeynmt.training - Epoch  13, Step:   187500, Batch Loss:     1.081737, Tokens per Sec:     5041, Lr: 0.000300\n",
      "2021-10-09 16:07:30,704 - INFO - joeynmt.training - Epoch  13, Step:   187600, Batch Loss:     0.976942, Tokens per Sec:     4933, Lr: 0.000300\n",
      "2021-10-09 16:08:16,086 - INFO - joeynmt.training - Epoch  13, Step:   187700, Batch Loss:     0.984272, Tokens per Sec:     5101, Lr: 0.000300\n",
      "2021-10-09 16:08:46,755 - INFO - joeynmt.training - Epoch  13: total training loss 12702.86\n",
      "2021-10-09 16:08:46,755 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-10-09 16:09:03,180 - INFO - joeynmt.training - Epoch  14, Step:   187800, Batch Loss:     0.921845, Tokens per Sec:     4624, Lr: 0.000300\n",
      "2021-10-09 16:09:48,308 - INFO - joeynmt.training - Epoch  14, Step:   187900, Batch Loss:     1.161314, Tokens per Sec:     4974, Lr: 0.000300\n",
      "2021-10-09 16:10:33,530 - INFO - joeynmt.training - Epoch  14, Step:   188000, Batch Loss:     1.000064, Tokens per Sec:     5084, Lr: 0.000300\n",
      "2021-10-09 16:11:18,841 - INFO - joeynmt.training - Epoch  14, Step:   188100, Batch Loss:     0.990663, Tokens per Sec:     5095, Lr: 0.000300\n",
      "2021-10-09 16:12:03,856 - INFO - joeynmt.training - Epoch  14, Step:   188200, Batch Loss:     1.187712, Tokens per Sec:     5012, Lr: 0.000300\n",
      "2021-10-09 16:12:47,996 - INFO - joeynmt.training - Epoch  14, Step:   188300, Batch Loss:     1.125594, Tokens per Sec:     4973, Lr: 0.000300\n",
      "2021-10-09 16:13:32,680 - INFO - joeynmt.training - Epoch  14, Step:   188400, Batch Loss:     1.093032, Tokens per Sec:     4983, Lr: 0.000300\n",
      "2021-10-09 16:14:17,356 - INFO - joeynmt.training - Epoch  14, Step:   188500, Batch Loss:     1.022534, Tokens per Sec:     5011, Lr: 0.000300\n",
      "2021-10-09 16:15:02,825 - INFO - joeynmt.training - Epoch  14, Step:   188600, Batch Loss:     1.025859, Tokens per Sec:     5096, Lr: 0.000300\n",
      "2021-10-09 16:15:47,932 - INFO - joeynmt.training - Epoch  14, Step:   188700, Batch Loss:     0.931468, Tokens per Sec:     5050, Lr: 0.000300\n",
      "2021-10-09 16:16:32,763 - INFO - joeynmt.training - Epoch  14, Step:   188800, Batch Loss:     0.982956, Tokens per Sec:     4985, Lr: 0.000300\n",
      "2021-10-09 16:17:17,350 - INFO - joeynmt.training - Epoch  14, Step:   188900, Batch Loss:     0.990706, Tokens per Sec:     4967, Lr: 0.000300\n",
      "2021-10-09 16:18:02,958 - INFO - joeynmt.training - Epoch  14, Step:   189000, Batch Loss:     0.831258, Tokens per Sec:     5071, Lr: 0.000300\n",
      "2021-10-09 16:18:48,050 - INFO - joeynmt.training - Epoch  14, Step:   189100, Batch Loss:     0.969699, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2021-10-09 16:19:33,421 - INFO - joeynmt.training - Epoch  14, Step:   189200, Batch Loss:     0.935442, Tokens per Sec:     5128, Lr: 0.000300\n",
      "2021-10-09 16:20:18,688 - INFO - joeynmt.training - Epoch  14, Step:   189300, Batch Loss:     1.047593, Tokens per Sec:     4999, Lr: 0.000300\n",
      "2021-10-09 16:21:04,018 - INFO - joeynmt.training - Epoch  14, Step:   189400, Batch Loss:     1.048431, Tokens per Sec:     5074, Lr: 0.000300\n",
      "2021-10-09 16:21:47,855 - INFO - joeynmt.training - Epoch  14, Step:   189500, Batch Loss:     0.821297, Tokens per Sec:     4918, Lr: 0.000300\n",
      "2021-10-09 16:22:33,202 - INFO - joeynmt.training - Epoch  14, Step:   189600, Batch Loss:     0.924962, Tokens per Sec:     5103, Lr: 0.000300\n",
      "2021-10-09 16:23:18,377 - INFO - joeynmt.training - Epoch  14, Step:   189700, Batch Loss:     0.907995, Tokens per Sec:     5073, Lr: 0.000300\n",
      "2021-10-09 16:24:03,401 - INFO - joeynmt.training - Epoch  14, Step:   189800, Batch Loss:     1.090273, Tokens per Sec:     5045, Lr: 0.000300\n",
      "2021-10-09 16:24:48,015 - INFO - joeynmt.training - Epoch  14, Step:   189900, Batch Loss:     1.100678, Tokens per Sec:     5026, Lr: 0.000300\n",
      "2021-10-09 16:25:32,812 - INFO - joeynmt.training - Epoch  14, Step:   190000, Batch Loss:     0.961740, Tokens per Sec:     4999, Lr: 0.000300\n",
      "2021-10-09 16:26:59,368 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 16:26:59,369 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 16:26:59,369 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 16:26:59,708 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 16:26:59,708 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 16:27:05,740 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 16:27:05,740 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 16:27:05,740 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 16:27:05,741 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term then used for the brother who took the lead in a congregation .\n",
      "2021-10-09 16:27:05,741 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 16:27:05,741 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 16:27:05,741 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 16:27:05,741 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 16:27:05,741 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 16:27:05,741 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 16:27:05,742 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 16:27:05,742 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 16:27:05,742 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 16:27:05,742 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 16:27:05,742 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 16:27:05,742 - INFO - joeynmt.training - \tHypothesis: HOW DO WE SEECTLY WE FECTLY DEAR ?\n",
      "2021-10-09 16:27:05,742 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step   190000: bleu:  50.15, loss: 27340.0820, ppl:   2.4690, duration: 92.9301s\n",
      "2021-10-09 16:27:50,470 - INFO - joeynmt.training - Epoch  14, Step:   190100, Batch Loss:     1.025326, Tokens per Sec:     5036, Lr: 0.000300\n",
      "2021-10-09 16:28:35,857 - INFO - joeynmt.training - Epoch  14, Step:   190200, Batch Loss:     1.126371, Tokens per Sec:     5092, Lr: 0.000300\n",
      "2021-10-09 16:29:20,866 - INFO - joeynmt.training - Epoch  14, Step:   190300, Batch Loss:     1.017501, Tokens per Sec:     5060, Lr: 0.000300\n",
      "2021-10-09 16:30:05,465 - INFO - joeynmt.training - Epoch  14, Step:   190400, Batch Loss:     1.025341, Tokens per Sec:     5001, Lr: 0.000300\n",
      "2021-10-09 16:30:50,438 - INFO - joeynmt.training - Epoch  14, Step:   190500, Batch Loss:     1.100054, Tokens per Sec:     4963, Lr: 0.000300\n",
      "2021-10-09 16:31:35,453 - INFO - joeynmt.training - Epoch  14, Step:   190600, Batch Loss:     1.017884, Tokens per Sec:     5067, Lr: 0.000300\n",
      "2021-10-09 16:32:20,516 - INFO - joeynmt.training - Epoch  14, Step:   190700, Batch Loss:     0.996725, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-09 16:33:04,005 - INFO - joeynmt.training - Epoch  14, Step:   190800, Batch Loss:     1.038046, Tokens per Sec:     4915, Lr: 0.000300\n",
      "2021-10-09 16:33:48,357 - INFO - joeynmt.training - Epoch  14, Step:   190900, Batch Loss:     1.017626, Tokens per Sec:     4953, Lr: 0.000300\n",
      "2021-10-09 16:34:33,093 - INFO - joeynmt.training - Epoch  14, Step:   191000, Batch Loss:     1.070727, Tokens per Sec:     5035, Lr: 0.000300\n",
      "2021-10-09 16:35:17,329 - INFO - joeynmt.training - Epoch  14, Step:   191100, Batch Loss:     0.962411, Tokens per Sec:     5013, Lr: 0.000300\n",
      "2021-10-09 16:36:02,193 - INFO - joeynmt.training - Epoch  14, Step:   191200, Batch Loss:     0.966739, Tokens per Sec:     5021, Lr: 0.000300\n",
      "2021-10-09 16:36:47,152 - INFO - joeynmt.training - Epoch  14, Step:   191300, Batch Loss:     0.995703, Tokens per Sec:     4952, Lr: 0.000300\n",
      "2021-10-09 16:37:31,967 - INFO - joeynmt.training - Epoch  14, Step:   191400, Batch Loss:     0.826168, Tokens per Sec:     5000, Lr: 0.000300\n",
      "2021-10-09 16:38:17,353 - INFO - joeynmt.training - Epoch  14, Step:   191500, Batch Loss:     0.847513, Tokens per Sec:     5092, Lr: 0.000300\n",
      "2021-10-09 16:39:01,358 - INFO - joeynmt.training - Epoch  14, Step:   191600, Batch Loss:     0.980844, Tokens per Sec:     4951, Lr: 0.000300\n",
      "2021-10-09 16:39:46,766 - INFO - joeynmt.training - Epoch  14, Step:   191700, Batch Loss:     0.996983, Tokens per Sec:     5053, Lr: 0.000300\n",
      "2021-10-09 16:40:31,747 - INFO - joeynmt.training - Epoch  14, Step:   191800, Batch Loss:     0.971047, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-09 16:41:16,411 - INFO - joeynmt.training - Epoch  14, Step:   191900, Batch Loss:     1.088710, Tokens per Sec:     5013, Lr: 0.000300\n",
      "2021-10-09 16:42:01,545 - INFO - joeynmt.training - Epoch  14, Step:   192000, Batch Loss:     0.942591, Tokens per Sec:     5009, Lr: 0.000300\n",
      "2021-10-09 16:42:46,898 - INFO - joeynmt.training - Epoch  14, Step:   192100, Batch Loss:     1.154372, Tokens per Sec:     5049, Lr: 0.000300\n",
      "2021-10-09 16:43:32,106 - INFO - joeynmt.training - Epoch  14, Step:   192200, Batch Loss:     0.943950, Tokens per Sec:     5102, Lr: 0.000300\n",
      "2021-10-09 16:44:17,176 - INFO - joeynmt.training - Epoch  14, Step:   192300, Batch Loss:     0.949186, Tokens per Sec:     5096, Lr: 0.000300\n",
      "2021-10-09 16:45:01,901 - INFO - joeynmt.training - Epoch  14, Step:   192400, Batch Loss:     1.053966, Tokens per Sec:     5030, Lr: 0.000300\n",
      "2021-10-09 16:45:46,815 - INFO - joeynmt.training - Epoch  14, Step:   192500, Batch Loss:     1.017142, Tokens per Sec:     5099, Lr: 0.000300\n",
      "2021-10-09 16:46:31,401 - INFO - joeynmt.training - Epoch  14, Step:   192600, Batch Loss:     0.912399, Tokens per Sec:     4898, Lr: 0.000300\n",
      "2021-10-09 16:47:15,965 - INFO - joeynmt.training - Epoch  14, Step:   192700, Batch Loss:     0.841064, Tokens per Sec:     5057, Lr: 0.000300\n",
      "2021-10-09 16:48:00,992 - INFO - joeynmt.training - Epoch  14, Step:   192800, Batch Loss:     1.047841, Tokens per Sec:     5065, Lr: 0.000300\n",
      "2021-10-09 16:48:45,518 - INFO - joeynmt.training - Epoch  14, Step:   192900, Batch Loss:     1.072235, Tokens per Sec:     5046, Lr: 0.000300\n",
      "2021-10-09 16:49:30,661 - INFO - joeynmt.training - Epoch  14, Step:   193000, Batch Loss:     1.004248, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-09 16:50:15,513 - INFO - joeynmt.training - Epoch  14, Step:   193100, Batch Loss:     1.073642, Tokens per Sec:     5083, Lr: 0.000300\n",
      "2021-10-09 16:51:00,104 - INFO - joeynmt.training - Epoch  14, Step:   193200, Batch Loss:     1.066431, Tokens per Sec:     4979, Lr: 0.000300\n",
      "2021-10-09 16:51:45,280 - INFO - joeynmt.training - Epoch  14, Step:   193300, Batch Loss:     1.120437, Tokens per Sec:     5028, Lr: 0.000300\n",
      "2021-10-09 16:52:30,240 - INFO - joeynmt.training - Epoch  14, Step:   193400, Batch Loss:     1.168509, Tokens per Sec:     5143, Lr: 0.000300\n",
      "2021-10-09 16:53:14,488 - INFO - joeynmt.training - Epoch  14, Step:   193500, Batch Loss:     1.124425, Tokens per Sec:     5003, Lr: 0.000300\n",
      "2021-10-09 16:53:59,582 - INFO - joeynmt.training - Epoch  14, Step:   193600, Batch Loss:     1.089068, Tokens per Sec:     5042, Lr: 0.000300\n",
      "2021-10-09 16:54:43,955 - INFO - joeynmt.training - Epoch  14, Step:   193700, Batch Loss:     1.002594, Tokens per Sec:     5031, Lr: 0.000300\n",
      "2021-10-09 16:55:29,014 - INFO - joeynmt.training - Epoch  14, Step:   193800, Batch Loss:     1.131795, Tokens per Sec:     5098, Lr: 0.000300\n",
      "2021-10-09 16:56:13,153 - INFO - joeynmt.training - Epoch  14, Step:   193900, Batch Loss:     1.153413, Tokens per Sec:     4909, Lr: 0.000300\n",
      "2021-10-09 16:56:57,666 - INFO - joeynmt.training - Epoch  14, Step:   194000, Batch Loss:     1.061447, Tokens per Sec:     4952, Lr: 0.000300\n",
      "2021-10-09 16:57:42,480 - INFO - joeynmt.training - Epoch  14, Step:   194100, Batch Loss:     1.025208, Tokens per Sec:     4970, Lr: 0.000300\n",
      "2021-10-09 16:58:26,941 - INFO - joeynmt.training - Epoch  14, Step:   194200, Batch Loss:     0.942877, Tokens per Sec:     5002, Lr: 0.000300\n",
      "2021-10-09 16:59:11,244 - INFO - joeynmt.training - Epoch  14, Step:   194300, Batch Loss:     0.885607, Tokens per Sec:     4887, Lr: 0.000300\n",
      "2021-10-09 16:59:56,672 - INFO - joeynmt.training - Epoch  14, Step:   194400, Batch Loss:     1.087004, Tokens per Sec:     5026, Lr: 0.000300\n",
      "2021-10-09 17:00:42,272 - INFO - joeynmt.training - Epoch  14, Step:   194500, Batch Loss:     0.998500, Tokens per Sec:     5049, Lr: 0.000300\n",
      "2021-10-09 17:01:27,197 - INFO - joeynmt.training - Epoch  14, Step:   194600, Batch Loss:     0.991331, Tokens per Sec:     5011, Lr: 0.000300\n",
      "2021-10-09 17:02:12,556 - INFO - joeynmt.training - Epoch  14, Step:   194700, Batch Loss:     1.055114, Tokens per Sec:     5116, Lr: 0.000300\n",
      "2021-10-09 17:02:57,773 - INFO - joeynmt.training - Epoch  14, Step:   194800, Batch Loss:     0.909131, Tokens per Sec:     4926, Lr: 0.000300\n",
      "2021-10-09 17:03:43,192 - INFO - joeynmt.training - Epoch  14, Step:   194900, Batch Loss:     1.032780, Tokens per Sec:     5055, Lr: 0.000300\n",
      "2021-10-09 17:04:28,000 - INFO - joeynmt.training - Epoch  14, Step:   195000, Batch Loss:     0.979721, Tokens per Sec:     4974, Lr: 0.000300\n",
      "2021-10-09 17:05:53,013 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 17:05:53,014 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 17:05:53,014 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 17:05:59,281 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 17:05:59,282 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 17:05:59,282 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 17:05:59,282 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term then used for the brother who took the lead in a congregation .\n",
      "2021-10-09 17:05:59,282 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 17:05:59,283 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 17:05:59,283 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 17:05:59,283 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 17:05:59,283 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 17:05:59,284 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 17:05:59,284 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 17:05:59,284 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 17:05:59,284 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 17:05:59,284 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 17:05:59,285 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 17:05:59,285 - INFO - joeynmt.training - \tHypothesis: HOW DO WE SEET WE SEECTED DOELVITS ?\n",
      "2021-10-09 17:05:59,285 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step   195000: bleu:  49.79, loss: 27422.4062, ppl:   2.4757, duration: 91.2843s\n",
      "2021-10-09 17:06:44,225 - INFO - joeynmt.training - Epoch  14, Step:   195100, Batch Loss:     0.993474, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-09 17:07:28,972 - INFO - joeynmt.training - Epoch  14, Step:   195200, Batch Loss:     1.263378, Tokens per Sec:     4976, Lr: 0.000300\n",
      "2021-10-09 17:08:13,726 - INFO - joeynmt.training - Epoch  14, Step:   195300, Batch Loss:     1.136142, Tokens per Sec:     5036, Lr: 0.000300\n",
      "2021-10-09 17:08:58,656 - INFO - joeynmt.training - Epoch  14, Step:   195400, Batch Loss:     1.046227, Tokens per Sec:     5076, Lr: 0.000300\n",
      "2021-10-09 17:09:43,701 - INFO - joeynmt.training - Epoch  14, Step:   195500, Batch Loss:     1.044037, Tokens per Sec:     5163, Lr: 0.000300\n",
      "2021-10-09 17:10:28,178 - INFO - joeynmt.training - Epoch  14, Step:   195600, Batch Loss:     1.024577, Tokens per Sec:     4969, Lr: 0.000300\n",
      "2021-10-09 17:11:12,860 - INFO - joeynmt.training - Epoch  14, Step:   195700, Batch Loss:     0.940560, Tokens per Sec:     5062, Lr: 0.000300\n",
      "2021-10-09 17:11:58,243 - INFO - joeynmt.training - Epoch  14, Step:   195800, Batch Loss:     0.910657, Tokens per Sec:     5107, Lr: 0.000300\n",
      "2021-10-09 17:12:42,948 - INFO - joeynmt.training - Epoch  14, Step:   195900, Batch Loss:     1.011087, Tokens per Sec:     4981, Lr: 0.000300\n",
      "2021-10-09 17:13:27,563 - INFO - joeynmt.training - Epoch  14, Step:   196000, Batch Loss:     1.084060, Tokens per Sec:     5028, Lr: 0.000300\n",
      "2021-10-09 17:14:12,858 - INFO - joeynmt.training - Epoch  14, Step:   196100, Batch Loss:     0.838906, Tokens per Sec:     5117, Lr: 0.000300\n",
      "2021-10-09 17:14:57,905 - INFO - joeynmt.training - Epoch  14, Step:   196200, Batch Loss:     0.992388, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2021-10-09 17:15:42,696 - INFO - joeynmt.training - Epoch  14, Step:   196300, Batch Loss:     0.998399, Tokens per Sec:     5012, Lr: 0.000300\n",
      "2021-10-09 17:16:27,590 - INFO - joeynmt.training - Epoch  14, Step:   196400, Batch Loss:     0.950812, Tokens per Sec:     5103, Lr: 0.000300\n",
      "2021-10-09 17:17:12,709 - INFO - joeynmt.training - Epoch  14, Step:   196500, Batch Loss:     1.013897, Tokens per Sec:     5036, Lr: 0.000300\n",
      "2021-10-09 17:17:57,977 - INFO - joeynmt.training - Epoch  14, Step:   196600, Batch Loss:     1.046483, Tokens per Sec:     5013, Lr: 0.000300\n",
      "2021-10-09 17:18:42,973 - INFO - joeynmt.training - Epoch  14, Step:   196700, Batch Loss:     0.977843, Tokens per Sec:     4980, Lr: 0.000300\n",
      "2021-10-09 17:19:28,556 - INFO - joeynmt.training - Epoch  14, Step:   196800, Batch Loss:     0.844738, Tokens per Sec:     5052, Lr: 0.000300\n",
      "2021-10-09 17:20:13,364 - INFO - joeynmt.training - Epoch  14, Step:   196900, Batch Loss:     0.952016, Tokens per Sec:     4944, Lr: 0.000300\n",
      "2021-10-09 17:20:58,594 - INFO - joeynmt.training - Epoch  14, Step:   197000, Batch Loss:     1.219823, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-09 17:21:44,271 - INFO - joeynmt.training - Epoch  14, Step:   197100, Batch Loss:     0.952855, Tokens per Sec:     5007, Lr: 0.000300\n",
      "2021-10-09 17:22:30,089 - INFO - joeynmt.training - Epoch  14, Step:   197200, Batch Loss:     1.087972, Tokens per Sec:     5120, Lr: 0.000300\n",
      "2021-10-09 17:23:15,533 - INFO - joeynmt.training - Epoch  14, Step:   197300, Batch Loss:     1.196392, Tokens per Sec:     5107, Lr: 0.000300\n",
      "2021-10-09 17:24:00,358 - INFO - joeynmt.training - Epoch  14, Step:   197400, Batch Loss:     1.048464, Tokens per Sec:     5040, Lr: 0.000300\n",
      "2021-10-09 17:24:44,908 - INFO - joeynmt.training - Epoch  14, Step:   197500, Batch Loss:     1.004775, Tokens per Sec:     5029, Lr: 0.000300\n",
      "2021-10-09 17:25:29,622 - INFO - joeynmt.training - Epoch  14, Step:   197600, Batch Loss:     0.972234, Tokens per Sec:     4976, Lr: 0.000300\n",
      "2021-10-09 17:26:14,291 - INFO - joeynmt.training - Epoch  14, Step:   197700, Batch Loss:     1.055400, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-09 17:26:59,224 - INFO - joeynmt.training - Epoch  14, Step:   197800, Batch Loss:     0.977536, Tokens per Sec:     4991, Lr: 0.000300\n",
      "2021-10-09 17:27:44,064 - INFO - joeynmt.training - Epoch  14, Step:   197900, Batch Loss:     1.085642, Tokens per Sec:     4973, Lr: 0.000300\n",
      "2021-10-09 17:28:29,596 - INFO - joeynmt.training - Epoch  14, Step:   198000, Batch Loss:     0.990072, Tokens per Sec:     5124, Lr: 0.000300\n",
      "2021-10-09 17:29:14,827 - INFO - joeynmt.training - Epoch  14, Step:   198100, Batch Loss:     0.998605, Tokens per Sec:     5056, Lr: 0.000300\n",
      "2021-10-09 17:29:59,640 - INFO - joeynmt.training - Epoch  14, Step:   198200, Batch Loss:     1.043149, Tokens per Sec:     5009, Lr: 0.000300\n",
      "2021-10-09 17:30:43,714 - INFO - joeynmt.training - Epoch  14, Step:   198300, Batch Loss:     1.047948, Tokens per Sec:     4941, Lr: 0.000300\n",
      "2021-10-09 17:31:28,975 - INFO - joeynmt.training - Epoch  14, Step:   198400, Batch Loss:     1.028501, Tokens per Sec:     5013, Lr: 0.000300\n",
      "2021-10-09 17:32:13,827 - INFO - joeynmt.training - Epoch  14, Step:   198500, Batch Loss:     1.012217, Tokens per Sec:     5030, Lr: 0.000300\n",
      "2021-10-09 17:32:59,732 - INFO - joeynmt.training - Epoch  14, Step:   198600, Batch Loss:     1.101858, Tokens per Sec:     5048, Lr: 0.000300\n",
      "2021-10-09 17:33:45,137 - INFO - joeynmt.training - Epoch  14, Step:   198700, Batch Loss:     1.004683, Tokens per Sec:     5027, Lr: 0.000300\n",
      "2021-10-09 17:34:30,050 - INFO - joeynmt.training - Epoch  14, Step:   198800, Batch Loss:     1.061408, Tokens per Sec:     4968, Lr: 0.000300\n",
      "2021-10-09 17:35:15,555 - INFO - joeynmt.training - Epoch  14, Step:   198900, Batch Loss:     1.109791, Tokens per Sec:     5025, Lr: 0.000300\n",
      "2021-10-09 17:36:00,529 - INFO - joeynmt.training - Epoch  14, Step:   199000, Batch Loss:     0.911555, Tokens per Sec:     4972, Lr: 0.000300\n",
      "2021-10-09 17:36:45,196 - INFO - joeynmt.training - Epoch  14, Step:   199100, Batch Loss:     0.904665, Tokens per Sec:     4940, Lr: 0.000300\n",
      "2021-10-09 17:37:30,468 - INFO - joeynmt.training - Epoch  14, Step:   199200, Batch Loss:     1.002637, Tokens per Sec:     5013, Lr: 0.000300\n",
      "2021-10-09 17:38:16,246 - INFO - joeynmt.training - Epoch  14, Step:   199300, Batch Loss:     0.944636, Tokens per Sec:     5048, Lr: 0.000300\n",
      "2021-10-09 17:39:01,026 - INFO - joeynmt.training - Epoch  14, Step:   199400, Batch Loss:     1.072603, Tokens per Sec:     5001, Lr: 0.000300\n",
      "2021-10-09 17:39:46,138 - INFO - joeynmt.training - Epoch  14, Step:   199500, Batch Loss:     1.080401, Tokens per Sec:     5021, Lr: 0.000300\n",
      "2021-10-09 17:40:31,384 - INFO - joeynmt.training - Epoch  14, Step:   199600, Batch Loss:     1.132818, Tokens per Sec:     4968, Lr: 0.000300\n",
      "2021-10-09 17:41:16,656 - INFO - joeynmt.training - Epoch  14, Step:   199700, Batch Loss:     0.937923, Tokens per Sec:     5090, Lr: 0.000300\n",
      "2021-10-09 17:42:01,837 - INFO - joeynmt.training - Epoch  14, Step:   199800, Batch Loss:     1.051686, Tokens per Sec:     5126, Lr: 0.000300\n",
      "2021-10-09 17:42:46,942 - INFO - joeynmt.training - Epoch  14, Step:   199900, Batch Loss:     1.011838, Tokens per Sec:     4958, Lr: 0.000300\n",
      "2021-10-09 17:43:32,247 - INFO - joeynmt.training - Epoch  14, Step:   200000, Batch Loss:     0.902615, Tokens per Sec:     5129, Lr: 0.000300\n",
      "2021-10-09 17:45:00,030 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 17:45:00,034 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 17:45:00,034 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 17:45:00,377 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 17:45:00,377 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 17:45:07,142 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 17:45:07,143 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 17:45:07,143 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 17:45:07,143 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term then used for the brother who took the lead in a congregation .\n",
      "2021-10-09 17:45:07,143 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 17:45:07,143 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 17:45:07,144 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 17:45:07,144 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know yourself and expresses your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 17:45:07,144 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 17:45:07,144 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 17:45:07,144 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 17:45:07,144 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 17:45:07,144 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 17:45:07,144 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 17:45:07,145 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 17:45:07,145 - INFO - joeynmt.training - \tHypothesis: HOW DO WE SEEARLY THEIR DOELVITS ?\n",
      "2021-10-09 17:45:07,145 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step   200000: bleu:  50.19, loss: 27320.0430, ppl:   2.4673, duration: 94.8965s\n",
      "2021-10-09 17:45:52,362 - INFO - joeynmt.training - Epoch  14, Step:   200100, Batch Loss:     0.910146, Tokens per Sec:     5113, Lr: 0.000300\n",
      "2021-10-09 17:46:37,511 - INFO - joeynmt.training - Epoch  14, Step:   200200, Batch Loss:     0.967836, Tokens per Sec:     5092, Lr: 0.000300\n",
      "2021-10-09 17:47:20,428 - INFO - joeynmt.training - Epoch  14: total training loss 12598.13\n",
      "2021-10-09 17:47:20,429 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-10-09 17:47:24,608 - INFO - joeynmt.training - Epoch  15, Step:   200300, Batch Loss:     0.877110, Tokens per Sec:     3462, Lr: 0.000300\n",
      "2021-10-09 17:48:08,420 - INFO - joeynmt.training - Epoch  15, Step:   200400, Batch Loss:     0.906723, Tokens per Sec:     4917, Lr: 0.000300\n",
      "2021-10-09 17:48:52,517 - INFO - joeynmt.training - Epoch  15, Step:   200500, Batch Loss:     0.943551, Tokens per Sec:     4936, Lr: 0.000300\n",
      "2021-10-09 17:49:37,577 - INFO - joeynmt.training - Epoch  15, Step:   200600, Batch Loss:     0.964812, Tokens per Sec:     4971, Lr: 0.000300\n",
      "2021-10-09 17:50:22,528 - INFO - joeynmt.training - Epoch  15, Step:   200700, Batch Loss:     1.123434, Tokens per Sec:     5086, Lr: 0.000300\n",
      "2021-10-09 17:51:06,829 - INFO - joeynmt.training - Epoch  15, Step:   200800, Batch Loss:     0.938012, Tokens per Sec:     4898, Lr: 0.000300\n",
      "2021-10-09 17:51:51,385 - INFO - joeynmt.training - Epoch  15, Step:   200900, Batch Loss:     0.969428, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2021-10-09 17:52:36,173 - INFO - joeynmt.training - Epoch  15, Step:   201000, Batch Loss:     0.995337, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-09 17:53:21,678 - INFO - joeynmt.training - Epoch  15, Step:   201100, Batch Loss:     0.999416, Tokens per Sec:     5102, Lr: 0.000300\n",
      "2021-10-09 17:54:06,399 - INFO - joeynmt.training - Epoch  15, Step:   201200, Batch Loss:     0.803216, Tokens per Sec:     4995, Lr: 0.000300\n",
      "2021-10-09 17:54:51,704 - INFO - joeynmt.training - Epoch  15, Step:   201300, Batch Loss:     0.908255, Tokens per Sec:     5012, Lr: 0.000300\n",
      "2021-10-09 17:55:36,064 - INFO - joeynmt.training - Epoch  15, Step:   201400, Batch Loss:     1.080800, Tokens per Sec:     5007, Lr: 0.000300\n",
      "2021-10-09 17:56:21,152 - INFO - joeynmt.training - Epoch  15, Step:   201500, Batch Loss:     0.903924, Tokens per Sec:     5144, Lr: 0.000300\n",
      "2021-10-09 17:57:06,178 - INFO - joeynmt.training - Epoch  15, Step:   201600, Batch Loss:     0.952263, Tokens per Sec:     5061, Lr: 0.000300\n",
      "2021-10-09 17:57:51,475 - INFO - joeynmt.training - Epoch  15, Step:   201700, Batch Loss:     0.949242, Tokens per Sec:     4985, Lr: 0.000300\n",
      "2021-10-09 17:58:36,410 - INFO - joeynmt.training - Epoch  15, Step:   201800, Batch Loss:     0.980733, Tokens per Sec:     5069, Lr: 0.000300\n",
      "2021-10-09 17:59:21,119 - INFO - joeynmt.training - Epoch  15, Step:   201900, Batch Loss:     0.901635, Tokens per Sec:     5000, Lr: 0.000300\n",
      "2021-10-09 18:00:05,552 - INFO - joeynmt.training - Epoch  15, Step:   202000, Batch Loss:     0.979505, Tokens per Sec:     4970, Lr: 0.000300\n",
      "2021-10-09 18:00:50,808 - INFO - joeynmt.training - Epoch  15, Step:   202100, Batch Loss:     1.055443, Tokens per Sec:     5085, Lr: 0.000300\n",
      "2021-10-09 18:01:34,672 - INFO - joeynmt.training - Epoch  15, Step:   202200, Batch Loss:     1.069724, Tokens per Sec:     4937, Lr: 0.000300\n",
      "2021-10-09 18:02:19,592 - INFO - joeynmt.training - Epoch  15, Step:   202300, Batch Loss:     1.054539, Tokens per Sec:     5033, Lr: 0.000300\n",
      "2021-10-09 18:03:04,587 - INFO - joeynmt.training - Epoch  15, Step:   202400, Batch Loss:     1.201709, Tokens per Sec:     4975, Lr: 0.000300\n",
      "2021-10-09 18:03:49,218 - INFO - joeynmt.training - Epoch  15, Step:   202500, Batch Loss:     1.017558, Tokens per Sec:     5051, Lr: 0.000300\n",
      "2021-10-09 18:04:33,757 - INFO - joeynmt.training - Epoch  15, Step:   202600, Batch Loss:     1.061363, Tokens per Sec:     5043, Lr: 0.000300\n",
      "2021-10-09 18:05:18,272 - INFO - joeynmt.training - Epoch  15, Step:   202700, Batch Loss:     1.045905, Tokens per Sec:     5024, Lr: 0.000300\n",
      "2021-10-09 18:06:03,341 - INFO - joeynmt.training - Epoch  15, Step:   202800, Batch Loss:     0.729747, Tokens per Sec:     5082, Lr: 0.000300\n",
      "2021-10-09 18:06:48,464 - INFO - joeynmt.training - Epoch  15, Step:   202900, Batch Loss:     0.946203, Tokens per Sec:     5093, Lr: 0.000300\n",
      "2021-10-09 18:07:32,991 - INFO - joeynmt.training - Epoch  15, Step:   203000, Batch Loss:     1.004686, Tokens per Sec:     5017, Lr: 0.000300\n",
      "2021-10-09 18:08:18,568 - INFO - joeynmt.training - Epoch  15, Step:   203100, Batch Loss:     0.871731, Tokens per Sec:     5113, Lr: 0.000300\n",
      "2021-10-09 18:09:02,839 - INFO - joeynmt.training - Epoch  15, Step:   203200, Batch Loss:     1.013883, Tokens per Sec:     4947, Lr: 0.000300\n",
      "2021-10-09 18:09:48,184 - INFO - joeynmt.training - Epoch  15, Step:   203300, Batch Loss:     0.853824, Tokens per Sec:     5079, Lr: 0.000300\n",
      "2021-10-09 18:10:32,775 - INFO - joeynmt.training - Epoch  15, Step:   203400, Batch Loss:     0.868674, Tokens per Sec:     5015, Lr: 0.000300\n",
      "2021-10-09 18:11:17,931 - INFO - joeynmt.training - Epoch  15, Step:   203500, Batch Loss:     0.966204, Tokens per Sec:     5154, Lr: 0.000300\n",
      "2021-10-09 18:12:02,792 - INFO - joeynmt.training - Epoch  15, Step:   203600, Batch Loss:     1.065581, Tokens per Sec:     4983, Lr: 0.000300\n",
      "2021-10-09 18:12:47,932 - INFO - joeynmt.training - Epoch  15, Step:   203700, Batch Loss:     0.997112, Tokens per Sec:     5018, Lr: 0.000300\n",
      "2021-10-09 18:13:33,059 - INFO - joeynmt.training - Epoch  15, Step:   203800, Batch Loss:     0.953143, Tokens per Sec:     5087, Lr: 0.000300\n",
      "2021-10-09 18:14:18,162 - INFO - joeynmt.training - Epoch  15, Step:   203900, Batch Loss:     0.936243, Tokens per Sec:     5046, Lr: 0.000300\n",
      "2021-10-09 18:15:03,167 - INFO - joeynmt.training - Epoch  15, Step:   204000, Batch Loss:     0.777155, Tokens per Sec:     4965, Lr: 0.000300\n",
      "2021-10-09 18:15:47,285 - INFO - joeynmt.training - Epoch  15, Step:   204100, Batch Loss:     0.983162, Tokens per Sec:     4912, Lr: 0.000300\n",
      "2021-10-09 18:16:32,374 - INFO - joeynmt.training - Epoch  15, Step:   204200, Batch Loss:     0.976594, Tokens per Sec:     5024, Lr: 0.000300\n",
      "2021-10-09 18:17:17,013 - INFO - joeynmt.training - Epoch  15, Step:   204300, Batch Loss:     1.088366, Tokens per Sec:     5060, Lr: 0.000300\n",
      "2021-10-09 18:18:01,738 - INFO - joeynmt.training - Epoch  15, Step:   204400, Batch Loss:     0.839436, Tokens per Sec:     5089, Lr: 0.000300\n",
      "2021-10-09 18:18:47,384 - INFO - joeynmt.training - Epoch  15, Step:   204500, Batch Loss:     1.102707, Tokens per Sec:     5133, Lr: 0.000300\n",
      "2021-10-09 18:19:33,006 - INFO - joeynmt.training - Epoch  15, Step:   204600, Batch Loss:     1.033832, Tokens per Sec:     5054, Lr: 0.000300\n",
      "2021-10-09 18:20:18,241 - INFO - joeynmt.training - Epoch  15, Step:   204700, Batch Loss:     0.904639, Tokens per Sec:     5028, Lr: 0.000300\n",
      "2021-10-09 18:21:03,766 - INFO - joeynmt.training - Epoch  15, Step:   204800, Batch Loss:     1.042486, Tokens per Sec:     5164, Lr: 0.000300\n",
      "2021-10-09 18:21:48,378 - INFO - joeynmt.training - Epoch  15, Step:   204900, Batch Loss:     1.047982, Tokens per Sec:     5049, Lr: 0.000300\n",
      "2021-10-09 18:22:33,017 - INFO - joeynmt.training - Epoch  15, Step:   205000, Batch Loss:     1.095150, Tokens per Sec:     4948, Lr: 0.000300\n",
      "2021-10-09 18:24:01,661 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 18:24:01,662 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 18:24:01,662 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 18:24:08,467 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 18:24:08,472 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 18:24:08,472 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 18:24:08,472 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-09 18:24:08,472 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 18:24:08,473 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 18:24:08,473 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 18:24:08,473 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know oneself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 18:24:08,473 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 18:24:08,474 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 18:24:08,474 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 18:24:08,474 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 18:24:08,475 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 18:24:08,475 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 18:24:08,475 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 18:24:08,475 - INFO - joeynmt.training - \tHypothesis: HOW DO WE SHOULD WE SHOULD DOELVITS ?\n",
      "2021-10-09 18:24:08,476 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step   205000: bleu:  50.24, loss: 27423.5879, ppl:   2.4758, duration: 95.4580s\n",
      "2021-10-09 18:24:53,621 - INFO - joeynmt.training - Epoch  15, Step:   205100, Batch Loss:     0.987145, Tokens per Sec:     5093, Lr: 0.000300\n",
      "2021-10-09 18:25:38,262 - INFO - joeynmt.training - Epoch  15, Step:   205200, Batch Loss:     0.862472, Tokens per Sec:     4945, Lr: 0.000300\n",
      "2021-10-09 18:26:23,323 - INFO - joeynmt.training - Epoch  15, Step:   205300, Batch Loss:     0.937933, Tokens per Sec:     5108, Lr: 0.000300\n",
      "2021-10-09 18:27:08,719 - INFO - joeynmt.training - Epoch  15, Step:   205400, Batch Loss:     1.002931, Tokens per Sec:     5039, Lr: 0.000300\n",
      "2021-10-09 18:27:55,016 - INFO - joeynmt.training - Epoch  15, Step:   205500, Batch Loss:     0.901926, Tokens per Sec:     5152, Lr: 0.000300\n",
      "2021-10-09 18:28:40,125 - INFO - joeynmt.training - Epoch  15, Step:   205600, Batch Loss:     1.143120, Tokens per Sec:     4980, Lr: 0.000300\n",
      "2021-10-09 18:29:24,450 - INFO - joeynmt.training - Epoch  15, Step:   205700, Batch Loss:     1.131432, Tokens per Sec:     4914, Lr: 0.000300\n",
      "2021-10-09 18:30:09,512 - INFO - joeynmt.training - Epoch  15, Step:   205800, Batch Loss:     1.050215, Tokens per Sec:     5028, Lr: 0.000300\n",
      "2021-10-09 18:30:54,668 - INFO - joeynmt.training - Epoch  15, Step:   205900, Batch Loss:     0.949685, Tokens per Sec:     4968, Lr: 0.000300\n",
      "2021-10-09 18:31:39,111 - INFO - joeynmt.training - Epoch  15, Step:   206000, Batch Loss:     0.944686, Tokens per Sec:     4986, Lr: 0.000300\n",
      "2021-10-09 18:32:24,015 - INFO - joeynmt.training - Epoch  15, Step:   206100, Batch Loss:     1.140163, Tokens per Sec:     5029, Lr: 0.000300\n",
      "2021-10-09 18:33:08,476 - INFO - joeynmt.training - Epoch  15, Step:   206200, Batch Loss:     1.041177, Tokens per Sec:     5010, Lr: 0.000300\n",
      "2021-10-09 18:33:53,699 - INFO - joeynmt.training - Epoch  15, Step:   206300, Batch Loss:     0.993272, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-09 18:34:38,758 - INFO - joeynmt.training - Epoch  15, Step:   206400, Batch Loss:     0.960743, Tokens per Sec:     5085, Lr: 0.000300\n",
      "2021-10-09 18:35:23,567 - INFO - joeynmt.training - Epoch  15, Step:   206500, Batch Loss:     1.145605, Tokens per Sec:     5018, Lr: 0.000300\n",
      "2021-10-09 18:36:08,311 - INFO - joeynmt.training - Epoch  15, Step:   206600, Batch Loss:     1.063230, Tokens per Sec:     5052, Lr: 0.000300\n",
      "2021-10-09 18:36:52,817 - INFO - joeynmt.training - Epoch  15, Step:   206700, Batch Loss:     1.123364, Tokens per Sec:     5024, Lr: 0.000300\n",
      "2021-10-09 18:37:37,281 - INFO - joeynmt.training - Epoch  15, Step:   206800, Batch Loss:     1.100870, Tokens per Sec:     5037, Lr: 0.000300\n",
      "2021-10-09 18:38:22,256 - INFO - joeynmt.training - Epoch  15, Step:   206900, Batch Loss:     1.030720, Tokens per Sec:     5123, Lr: 0.000300\n",
      "2021-10-09 18:39:07,119 - INFO - joeynmt.training - Epoch  15, Step:   207000, Batch Loss:     0.951089, Tokens per Sec:     5081, Lr: 0.000300\n",
      "2021-10-09 18:39:52,273 - INFO - joeynmt.training - Epoch  15, Step:   207100, Batch Loss:     0.968654, Tokens per Sec:     5077, Lr: 0.000300\n",
      "2021-10-09 18:40:36,889 - INFO - joeynmt.training - Epoch  15, Step:   207200, Batch Loss:     0.965490, Tokens per Sec:     4976, Lr: 0.000300\n",
      "2021-10-09 18:41:22,461 - INFO - joeynmt.training - Epoch  15, Step:   207300, Batch Loss:     0.959645, Tokens per Sec:     5088, Lr: 0.000300\n",
      "2021-10-09 18:42:07,452 - INFO - joeynmt.training - Epoch  15, Step:   207400, Batch Loss:     0.929113, Tokens per Sec:     5035, Lr: 0.000300\n",
      "2021-10-09 18:42:52,503 - INFO - joeynmt.training - Epoch  15, Step:   207500, Batch Loss:     0.831657, Tokens per Sec:     5080, Lr: 0.000300\n",
      "2021-10-09 18:43:37,745 - INFO - joeynmt.training - Epoch  15, Step:   207600, Batch Loss:     0.866478, Tokens per Sec:     5115, Lr: 0.000300\n",
      "2021-10-09 18:44:22,408 - INFO - joeynmt.training - Epoch  15, Step:   207700, Batch Loss:     0.929719, Tokens per Sec:     5043, Lr: 0.000300\n",
      "2021-10-09 18:45:06,839 - INFO - joeynmt.training - Epoch  15, Step:   207800, Batch Loss:     0.985494, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-09 18:45:52,171 - INFO - joeynmt.training - Epoch  15, Step:   207900, Batch Loss:     1.082935, Tokens per Sec:     5094, Lr: 0.000300\n",
      "2021-10-09 18:46:37,413 - INFO - joeynmt.training - Epoch  15, Step:   208000, Batch Loss:     1.006181, Tokens per Sec:     5126, Lr: 0.000300\n",
      "2021-10-09 18:47:21,918 - INFO - joeynmt.training - Epoch  15, Step:   208100, Batch Loss:     1.097315, Tokens per Sec:     5065, Lr: 0.000300\n",
      "2021-10-09 18:48:07,491 - INFO - joeynmt.training - Epoch  15, Step:   208200, Batch Loss:     0.894797, Tokens per Sec:     5052, Lr: 0.000300\n",
      "2021-10-09 18:48:52,378 - INFO - joeynmt.training - Epoch  15, Step:   208300, Batch Loss:     1.083574, Tokens per Sec:     5051, Lr: 0.000300\n",
      "2021-10-09 18:49:36,785 - INFO - joeynmt.training - Epoch  15, Step:   208400, Batch Loss:     1.075293, Tokens per Sec:     5010, Lr: 0.000300\n",
      "2021-10-09 18:50:21,612 - INFO - joeynmt.training - Epoch  15, Step:   208500, Batch Loss:     0.973015, Tokens per Sec:     5031, Lr: 0.000300\n",
      "2021-10-09 18:51:06,908 - INFO - joeynmt.training - Epoch  15, Step:   208600, Batch Loss:     1.025033, Tokens per Sec:     5080, Lr: 0.000300\n",
      "2021-10-09 18:51:51,363 - INFO - joeynmt.training - Epoch  15, Step:   208700, Batch Loss:     1.133385, Tokens per Sec:     5014, Lr: 0.000300\n",
      "2021-10-09 18:52:36,275 - INFO - joeynmt.training - Epoch  15, Step:   208800, Batch Loss:     0.837513, Tokens per Sec:     5064, Lr: 0.000300\n",
      "2021-10-09 18:53:20,641 - INFO - joeynmt.training - Epoch  15, Step:   208900, Batch Loss:     0.980026, Tokens per Sec:     5002, Lr: 0.000300\n",
      "2021-10-09 18:54:05,711 - INFO - joeynmt.training - Epoch  15, Step:   209000, Batch Loss:     0.937478, Tokens per Sec:     5057, Lr: 0.000300\n",
      "2021-10-09 18:54:50,436 - INFO - joeynmt.training - Epoch  15, Step:   209100, Batch Loss:     0.926034, Tokens per Sec:     4967, Lr: 0.000300\n",
      "2021-10-09 18:55:35,596 - INFO - joeynmt.training - Epoch  15, Step:   209200, Batch Loss:     1.008224, Tokens per Sec:     4976, Lr: 0.000300\n",
      "2021-10-09 18:56:19,916 - INFO - joeynmt.training - Epoch  15, Step:   209300, Batch Loss:     0.995953, Tokens per Sec:     5007, Lr: 0.000300\n",
      "2021-10-09 18:57:04,785 - INFO - joeynmt.training - Epoch  15, Step:   209400, Batch Loss:     0.990739, Tokens per Sec:     5096, Lr: 0.000300\n",
      "2021-10-09 18:57:49,733 - INFO - joeynmt.training - Epoch  15, Step:   209500, Batch Loss:     1.068876, Tokens per Sec:     4956, Lr: 0.000300\n",
      "2021-10-09 18:58:35,225 - INFO - joeynmt.training - Epoch  15, Step:   209600, Batch Loss:     0.976561, Tokens per Sec:     5117, Lr: 0.000300\n",
      "2021-10-09 18:59:20,601 - INFO - joeynmt.training - Epoch  15, Step:   209700, Batch Loss:     1.123847, Tokens per Sec:     5064, Lr: 0.000300\n",
      "2021-10-09 19:00:05,620 - INFO - joeynmt.training - Epoch  15, Step:   209800, Batch Loss:     0.992110, Tokens per Sec:     5113, Lr: 0.000300\n",
      "2021-10-09 19:00:50,735 - INFO - joeynmt.training - Epoch  15, Step:   209900, Batch Loss:     0.947744, Tokens per Sec:     5054, Lr: 0.000300\n",
      "2021-10-09 19:01:35,728 - INFO - joeynmt.training - Epoch  15, Step:   210000, Batch Loss:     1.000067, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-09 19:03:03,293 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 19:03:03,294 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 19:03:03,294 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 19:03:03,632 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 19:03:03,633 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 19:03:09,810 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 19:03:09,811 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 19:03:09,811 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 19:03:09,811 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term then used for the brother who took the lead in a congregation .\n",
      "2021-10-09 19:03:09,811 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 19:03:09,811 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 19:03:09,812 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 19:03:09,812 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know himself and expresses feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 19:03:09,812 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 19:03:09,812 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 19:03:09,812 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 19:03:09,812 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to respect that Nabal was older than he was .\n",
      "2021-10-09 19:03:09,812 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 19:03:09,812 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 19:03:09,813 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 19:03:09,813 - INFO - joeynmt.training - \tHypothesis: HOW DO WE SHOULD WE SPIRITURE DOELVITS ?\n",
      "2021-10-09 19:03:09,813 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step   210000: bleu:  50.58, loss: 27039.6895, ppl:   2.4446, duration: 94.0839s\n",
      "2021-10-09 19:03:54,537 - INFO - joeynmt.training - Epoch  15, Step:   210100, Batch Loss:     1.011118, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2021-10-09 19:04:40,203 - INFO - joeynmt.training - Epoch  15, Step:   210200, Batch Loss:     1.114887, Tokens per Sec:     5079, Lr: 0.000300\n",
      "2021-10-09 19:05:25,778 - INFO - joeynmt.training - Epoch  15, Step:   210300, Batch Loss:     0.933478, Tokens per Sec:     5124, Lr: 0.000300\n",
      "2021-10-09 19:06:11,027 - INFO - joeynmt.training - Epoch  15, Step:   210400, Batch Loss:     0.959930, Tokens per Sec:     4963, Lr: 0.000300\n",
      "2021-10-09 19:06:56,245 - INFO - joeynmt.training - Epoch  15, Step:   210500, Batch Loss:     1.013527, Tokens per Sec:     5000, Lr: 0.000300\n",
      "2021-10-09 19:07:41,318 - INFO - joeynmt.training - Epoch  15, Step:   210600, Batch Loss:     0.944274, Tokens per Sec:     4925, Lr: 0.000300\n",
      "2021-10-09 19:08:25,500 - INFO - joeynmt.training - Epoch  15, Step:   210700, Batch Loss:     1.011630, Tokens per Sec:     4943, Lr: 0.000300\n",
      "2021-10-09 19:09:10,832 - INFO - joeynmt.training - Epoch  15, Step:   210800, Batch Loss:     0.950313, Tokens per Sec:     5153, Lr: 0.000300\n",
      "2021-10-09 19:09:56,198 - INFO - joeynmt.training - Epoch  15, Step:   210900, Batch Loss:     0.960571, Tokens per Sec:     5039, Lr: 0.000300\n",
      "2021-10-09 19:10:41,367 - INFO - joeynmt.training - Epoch  15, Step:   211000, Batch Loss:     0.920267, Tokens per Sec:     5152, Lr: 0.000300\n",
      "2021-10-09 19:11:25,457 - INFO - joeynmt.training - Epoch  15, Step:   211100, Batch Loss:     0.903947, Tokens per Sec:     4931, Lr: 0.000300\n",
      "2021-10-09 19:12:09,990 - INFO - joeynmt.training - Epoch  15, Step:   211200, Batch Loss:     1.047639, Tokens per Sec:     5057, Lr: 0.000300\n",
      "2021-10-09 19:12:54,819 - INFO - joeynmt.training - Epoch  15, Step:   211300, Batch Loss:     0.949584, Tokens per Sec:     5015, Lr: 0.000300\n",
      "2021-10-09 19:13:39,688 - INFO - joeynmt.training - Epoch  15, Step:   211400, Batch Loss:     0.944916, Tokens per Sec:     5057, Lr: 0.000300\n",
      "2021-10-09 19:14:24,581 - INFO - joeynmt.training - Epoch  15, Step:   211500, Batch Loss:     1.106433, Tokens per Sec:     4928, Lr: 0.000300\n",
      "2021-10-09 19:15:09,857 - INFO - joeynmt.training - Epoch  15, Step:   211600, Batch Loss:     0.992851, Tokens per Sec:     4989, Lr: 0.000300\n",
      "2021-10-09 19:15:54,718 - INFO - joeynmt.training - Epoch  15, Step:   211700, Batch Loss:     1.027017, Tokens per Sec:     4986, Lr: 0.000300\n",
      "2021-10-09 19:16:39,634 - INFO - joeynmt.training - Epoch  15, Step:   211800, Batch Loss:     0.872522, Tokens per Sec:     4933, Lr: 0.000300\n",
      "2021-10-09 19:17:24,729 - INFO - joeynmt.training - Epoch  15, Step:   211900, Batch Loss:     1.089006, Tokens per Sec:     4948, Lr: 0.000300\n",
      "2021-10-09 19:18:09,872 - INFO - joeynmt.training - Epoch  15, Step:   212000, Batch Loss:     0.998544, Tokens per Sec:     5072, Lr: 0.000300\n",
      "2021-10-09 19:18:54,972 - INFO - joeynmt.training - Epoch  15, Step:   212100, Batch Loss:     0.886896, Tokens per Sec:     4938, Lr: 0.000300\n",
      "2021-10-09 19:19:39,989 - INFO - joeynmt.training - Epoch  15, Step:   212200, Batch Loss:     1.047777, Tokens per Sec:     5030, Lr: 0.000300\n",
      "2021-10-09 19:20:25,642 - INFO - joeynmt.training - Epoch  15, Step:   212300, Batch Loss:     0.949682, Tokens per Sec:     5094, Lr: 0.000300\n",
      "2021-10-09 19:21:10,819 - INFO - joeynmt.training - Epoch  15, Step:   212400, Batch Loss:     0.954151, Tokens per Sec:     5026, Lr: 0.000300\n",
      "2021-10-09 19:21:55,352 - INFO - joeynmt.training - Epoch  15, Step:   212500, Batch Loss:     0.989297, Tokens per Sec:     4849, Lr: 0.000300\n",
      "2021-10-09 19:22:40,437 - INFO - joeynmt.training - Epoch  15, Step:   212600, Batch Loss:     0.840841, Tokens per Sec:     4927, Lr: 0.000300\n",
      "2021-10-09 19:23:25,444 - INFO - joeynmt.training - Epoch  15, Step:   212700, Batch Loss:     0.953168, Tokens per Sec:     4990, Lr: 0.000300\n",
      "2021-10-09 19:24:11,253 - INFO - joeynmt.training - Epoch  15, Step:   212800, Batch Loss:     1.011357, Tokens per Sec:     5060, Lr: 0.000300\n",
      "2021-10-09 19:24:21,477 - INFO - joeynmt.training - Epoch  15: total training loss 12486.72\n",
      "2021-10-09 19:24:21,482 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-10-09 19:24:57,050 - INFO - joeynmt.training - Epoch  16, Step:   212900, Batch Loss:     0.887243, Tokens per Sec:     4702, Lr: 0.000300\n",
      "2021-10-09 19:25:42,030 - INFO - joeynmt.training - Epoch  16, Step:   213000, Batch Loss:     1.015430, Tokens per Sec:     5066, Lr: 0.000300\n",
      "2021-10-09 19:26:26,244 - INFO - joeynmt.training - Epoch  16, Step:   213100, Batch Loss:     1.032714, Tokens per Sec:     4908, Lr: 0.000300\n",
      "2021-10-09 19:27:11,380 - INFO - joeynmt.training - Epoch  16, Step:   213200, Batch Loss:     0.999139, Tokens per Sec:     5052, Lr: 0.000300\n",
      "2021-10-09 19:27:56,978 - INFO - joeynmt.training - Epoch  16, Step:   213300, Batch Loss:     0.924145, Tokens per Sec:     5121, Lr: 0.000300\n",
      "2021-10-09 19:28:41,523 - INFO - joeynmt.training - Epoch  16, Step:   213400, Batch Loss:     0.919685, Tokens per Sec:     4924, Lr: 0.000300\n",
      "2021-10-09 19:29:26,512 - INFO - joeynmt.training - Epoch  16, Step:   213500, Batch Loss:     0.880476, Tokens per Sec:     5000, Lr: 0.000300\n",
      "2021-10-09 19:30:11,892 - INFO - joeynmt.training - Epoch  16, Step:   213600, Batch Loss:     1.056632, Tokens per Sec:     5039, Lr: 0.000300\n",
      "2021-10-09 19:30:56,808 - INFO - joeynmt.training - Epoch  16, Step:   213700, Batch Loss:     0.885254, Tokens per Sec:     4913, Lr: 0.000300\n",
      "2021-10-09 19:31:41,635 - INFO - joeynmt.training - Epoch  16, Step:   213800, Batch Loss:     0.960612, Tokens per Sec:     5015, Lr: 0.000300\n",
      "2021-10-09 19:32:27,260 - INFO - joeynmt.training - Epoch  16, Step:   213900, Batch Loss:     0.911070, Tokens per Sec:     5116, Lr: 0.000300\n",
      "2021-10-09 19:33:12,541 - INFO - joeynmt.training - Epoch  16, Step:   214000, Batch Loss:     0.993685, Tokens per Sec:     4984, Lr: 0.000300\n",
      "2021-10-09 19:33:56,971 - INFO - joeynmt.training - Epoch  16, Step:   214100, Batch Loss:     0.964086, Tokens per Sec:     4912, Lr: 0.000300\n",
      "2021-10-09 19:34:41,653 - INFO - joeynmt.training - Epoch  16, Step:   214200, Batch Loss:     0.889022, Tokens per Sec:     5014, Lr: 0.000300\n",
      "2021-10-09 19:35:27,207 - INFO - joeynmt.training - Epoch  16, Step:   214300, Batch Loss:     1.048635, Tokens per Sec:     5085, Lr: 0.000300\n",
      "2021-10-09 19:36:12,491 - INFO - joeynmt.training - Epoch  16, Step:   214400, Batch Loss:     0.989536, Tokens per Sec:     5006, Lr: 0.000300\n",
      "2021-10-09 19:36:57,747 - INFO - joeynmt.training - Epoch  16, Step:   214500, Batch Loss:     0.969603, Tokens per Sec:     5007, Lr: 0.000300\n",
      "2021-10-09 19:37:42,917 - INFO - joeynmt.training - Epoch  16, Step:   214600, Batch Loss:     0.922575, Tokens per Sec:     5059, Lr: 0.000300\n",
      "2021-10-09 19:38:28,497 - INFO - joeynmt.training - Epoch  16, Step:   214700, Batch Loss:     1.050435, Tokens per Sec:     5026, Lr: 0.000300\n",
      "2021-10-09 19:39:14,259 - INFO - joeynmt.training - Epoch  16, Step:   214800, Batch Loss:     1.015932, Tokens per Sec:     5048, Lr: 0.000300\n",
      "2021-10-09 19:39:59,596 - INFO - joeynmt.training - Epoch  16, Step:   214900, Batch Loss:     0.900902, Tokens per Sec:     5001, Lr: 0.000300\n",
      "2021-10-09 19:40:44,549 - INFO - joeynmt.training - Epoch  16, Step:   215000, Batch Loss:     0.905299, Tokens per Sec:     5010, Lr: 0.000300\n",
      "2021-10-09 19:42:11,924 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 19:42:11,925 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 19:42:11,925 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 19:42:23,330 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 19:42:23,331 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 19:42:23,331 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 19:42:23,331 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term then used for the brother who took the lead in a congregation .\n",
      "2021-10-09 19:42:23,331 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 19:42:23,332 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 19:42:23,332 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 19:42:23,332 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when you get to know yourself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 19:42:23,332 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 19:42:23,332 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 19:42:23,332 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 19:42:23,333 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” referring to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 19:42:23,333 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 19:42:23,333 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 19:42:23,333 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 19:42:23,333 - INFO - joeynmt.training - \tHypothesis: HOW DO WE SHOW WE SHOW WE SPIRITURE DEACH ?\n",
      "2021-10-09 19:42:23,333 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step   215000: bleu:  50.58, loss: 27115.9531, ppl:   2.4508, duration: 98.7839s\n",
      "2021-10-09 19:43:08,464 - INFO - joeynmt.training - Epoch  16, Step:   215100, Batch Loss:     1.028059, Tokens per Sec:     5028, Lr: 0.000300\n",
      "2021-10-09 19:43:53,375 - INFO - joeynmt.training - Epoch  16, Step:   215200, Batch Loss:     0.982036, Tokens per Sec:     4982, Lr: 0.000300\n",
      "2021-10-09 19:44:37,980 - INFO - joeynmt.training - Epoch  16, Step:   215300, Batch Loss:     0.967150, Tokens per Sec:     4980, Lr: 0.000300\n",
      "2021-10-09 19:45:22,978 - INFO - joeynmt.training - Epoch  16, Step:   215400, Batch Loss:     0.985914, Tokens per Sec:     5059, Lr: 0.000300\n",
      "2021-10-09 19:46:08,541 - INFO - joeynmt.training - Epoch  16, Step:   215500, Batch Loss:     0.840511, Tokens per Sec:     5071, Lr: 0.000300\n",
      "2021-10-09 19:46:53,888 - INFO - joeynmt.training - Epoch  16, Step:   215600, Batch Loss:     0.954735, Tokens per Sec:     5104, Lr: 0.000300\n",
      "2021-10-09 19:47:39,087 - INFO - joeynmt.training - Epoch  16, Step:   215700, Batch Loss:     1.132367, Tokens per Sec:     5062, Lr: 0.000300\n",
      "2021-10-09 19:48:24,308 - INFO - joeynmt.training - Epoch  16, Step:   215800, Batch Loss:     0.902350, Tokens per Sec:     5075, Lr: 0.000300\n",
      "2021-10-09 19:49:09,654 - INFO - joeynmt.training - Epoch  16, Step:   215900, Batch Loss:     1.038584, Tokens per Sec:     5097, Lr: 0.000300\n",
      "2021-10-09 19:49:54,399 - INFO - joeynmt.training - Epoch  16, Step:   216000, Batch Loss:     0.954166, Tokens per Sec:     4983, Lr: 0.000300\n",
      "2021-10-09 19:50:39,136 - INFO - joeynmt.training - Epoch  16, Step:   216100, Batch Loss:     0.959660, Tokens per Sec:     4966, Lr: 0.000300\n",
      "2021-10-09 19:51:24,140 - INFO - joeynmt.training - Epoch  16, Step:   216200, Batch Loss:     1.033899, Tokens per Sec:     5052, Lr: 0.000300\n",
      "2021-10-09 19:52:09,335 - INFO - joeynmt.training - Epoch  16, Step:   216300, Batch Loss:     0.964643, Tokens per Sec:     5082, Lr: 0.000300\n",
      "2021-10-09 19:52:54,160 - INFO - joeynmt.training - Epoch  16, Step:   216400, Batch Loss:     0.992142, Tokens per Sec:     4985, Lr: 0.000300\n",
      "2021-10-09 19:53:39,329 - INFO - joeynmt.training - Epoch  16, Step:   216500, Batch Loss:     0.981881, Tokens per Sec:     5080, Lr: 0.000300\n",
      "2021-10-09 19:54:24,224 - INFO - joeynmt.training - Epoch  16, Step:   216600, Batch Loss:     0.964094, Tokens per Sec:     5053, Lr: 0.000300\n",
      "2021-10-09 19:55:09,188 - INFO - joeynmt.training - Epoch  16, Step:   216700, Batch Loss:     1.221960, Tokens per Sec:     5121, Lr: 0.000300\n",
      "2021-10-09 19:55:53,340 - INFO - joeynmt.training - Epoch  16, Step:   216800, Batch Loss:     1.090138, Tokens per Sec:     4880, Lr: 0.000300\n",
      "2021-10-09 19:56:37,753 - INFO - joeynmt.training - Epoch  16, Step:   216900, Batch Loss:     0.970923, Tokens per Sec:     5001, Lr: 0.000300\n",
      "2021-10-09 19:57:22,170 - INFO - joeynmt.training - Epoch  16, Step:   217000, Batch Loss:     1.119873, Tokens per Sec:     5043, Lr: 0.000300\n",
      "2021-10-09 19:58:07,100 - INFO - joeynmt.training - Epoch  16, Step:   217100, Batch Loss:     1.019389, Tokens per Sec:     5066, Lr: 0.000300\n",
      "2021-10-09 19:58:52,407 - INFO - joeynmt.training - Epoch  16, Step:   217200, Batch Loss:     0.925793, Tokens per Sec:     5064, Lr: 0.000300\n",
      "2021-10-09 19:59:36,881 - INFO - joeynmt.training - Epoch  16, Step:   217300, Batch Loss:     1.048744, Tokens per Sec:     5042, Lr: 0.000300\n",
      "2021-10-09 20:00:21,548 - INFO - joeynmt.training - Epoch  16, Step:   217400, Batch Loss:     0.857140, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-09 20:01:06,803 - INFO - joeynmt.training - Epoch  16, Step:   217500, Batch Loss:     1.024619, Tokens per Sec:     4975, Lr: 0.000300\n",
      "2021-10-09 20:01:51,200 - INFO - joeynmt.training - Epoch  16, Step:   217600, Batch Loss:     0.992255, Tokens per Sec:     4965, Lr: 0.000300\n",
      "2021-10-09 20:02:36,144 - INFO - joeynmt.training - Epoch  16, Step:   217700, Batch Loss:     1.063224, Tokens per Sec:     5091, Lr: 0.000300\n",
      "2021-10-09 20:03:21,368 - INFO - joeynmt.training - Epoch  16, Step:   217800, Batch Loss:     0.955898, Tokens per Sec:     5034, Lr: 0.000300\n",
      "2021-10-09 20:04:06,669 - INFO - joeynmt.training - Epoch  16, Step:   217900, Batch Loss:     0.989245, Tokens per Sec:     5017, Lr: 0.000300\n",
      "2021-10-09 20:04:51,724 - INFO - joeynmt.training - Epoch  16, Step:   218000, Batch Loss:     0.920097, Tokens per Sec:     5078, Lr: 0.000300\n",
      "2021-10-09 20:05:36,972 - INFO - joeynmt.training - Epoch  16, Step:   218100, Batch Loss:     1.045839, Tokens per Sec:     5064, Lr: 0.000300\n",
      "2021-10-09 20:06:22,011 - INFO - joeynmt.training - Epoch  16, Step:   218200, Batch Loss:     0.845815, Tokens per Sec:     5064, Lr: 0.000300\n",
      "2021-10-09 20:07:06,681 - INFO - joeynmt.training - Epoch  16, Step:   218300, Batch Loss:     1.009187, Tokens per Sec:     5012, Lr: 0.000300\n",
      "2021-10-09 20:07:51,380 - INFO - joeynmt.training - Epoch  16, Step:   218400, Batch Loss:     1.118603, Tokens per Sec:     4981, Lr: 0.000300\n",
      "2021-10-09 20:08:36,326 - INFO - joeynmt.training - Epoch  16, Step:   218500, Batch Loss:     1.008376, Tokens per Sec:     5012, Lr: 0.000300\n",
      "2021-10-09 20:09:21,567 - INFO - joeynmt.training - Epoch  16, Step:   218600, Batch Loss:     0.884773, Tokens per Sec:     5104, Lr: 0.000300\n",
      "2021-10-09 20:10:07,124 - INFO - joeynmt.training - Epoch  16, Step:   218700, Batch Loss:     0.953362, Tokens per Sec:     5081, Lr: 0.000300\n",
      "2021-10-09 20:10:52,009 - INFO - joeynmt.training - Epoch  16, Step:   218800, Batch Loss:     0.885305, Tokens per Sec:     5033, Lr: 0.000300\n",
      "2021-10-09 20:11:37,092 - INFO - joeynmt.training - Epoch  16, Step:   218900, Batch Loss:     0.964297, Tokens per Sec:     4990, Lr: 0.000300\n",
      "2021-10-09 20:12:22,075 - INFO - joeynmt.training - Epoch  16, Step:   219000, Batch Loss:     0.902711, Tokens per Sec:     5070, Lr: 0.000300\n",
      "2021-10-09 20:13:06,098 - INFO - joeynmt.training - Epoch  16, Step:   219100, Batch Loss:     0.839121, Tokens per Sec:     4965, Lr: 0.000300\n",
      "2021-10-09 20:13:50,928 - INFO - joeynmt.training - Epoch  16, Step:   219200, Batch Loss:     0.972284, Tokens per Sec:     4990, Lr: 0.000300\n",
      "2021-10-09 20:14:36,234 - INFO - joeynmt.training - Epoch  16, Step:   219300, Batch Loss:     0.878033, Tokens per Sec:     5012, Lr: 0.000300\n",
      "2021-10-09 20:15:20,964 - INFO - joeynmt.training - Epoch  16, Step:   219400, Batch Loss:     0.879595, Tokens per Sec:     4963, Lr: 0.000300\n",
      "2021-10-09 20:16:06,035 - INFO - joeynmt.training - Epoch  16, Step:   219500, Batch Loss:     0.903041, Tokens per Sec:     4942, Lr: 0.000300\n",
      "2021-10-09 20:16:51,197 - INFO - joeynmt.training - Epoch  16, Step:   219600, Batch Loss:     1.054317, Tokens per Sec:     5081, Lr: 0.000300\n",
      "2021-10-09 20:17:36,308 - INFO - joeynmt.training - Epoch  16, Step:   219700, Batch Loss:     0.864382, Tokens per Sec:     5038, Lr: 0.000300\n",
      "2021-10-09 20:18:21,991 - INFO - joeynmt.training - Epoch  16, Step:   219800, Batch Loss:     0.884457, Tokens per Sec:     5039, Lr: 0.000300\n",
      "2021-10-09 20:19:06,606 - INFO - joeynmt.training - Epoch  16, Step:   219900, Batch Loss:     0.980177, Tokens per Sec:     4916, Lr: 0.000300\n",
      "2021-10-09 20:19:51,972 - INFO - joeynmt.training - Epoch  16, Step:   220000, Batch Loss:     0.952999, Tokens per Sec:     5063, Lr: 0.000300\n",
      "2021-10-09 20:21:20,090 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 20:21:20,091 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 20:21:20,091 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 20:21:20,428 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 20:21:20,428 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 20:21:26,772 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 20:21:26,773 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 20:21:26,773 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 20:21:26,773 - INFO - joeynmt.training - \tHypothesis: Father was the company servant , the term used at that time for the brother who took the lead in a congregation .\n",
      "2021-10-09 20:21:26,773 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 20:21:26,773 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 20:21:26,773 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 20:21:26,773 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know oneself and expresses feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 20:21:26,773 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 20:21:26,774 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 20:21:26,774 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 20:21:26,774 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 20:21:26,774 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 20:21:26,774 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 20:21:26,774 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 20:21:26,774 - INFO - joeynmt.training - \tHypothesis: HOW DO WE SHOULD WE STEECTS DOELVITS ?\n",
      "2021-10-09 20:21:26,774 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step   220000: bleu:  50.74, loss: 26916.4297, ppl:   2.4346, duration: 94.8013s\n",
      "2021-10-09 20:22:10,940 - INFO - joeynmt.training - Epoch  16, Step:   220100, Batch Loss:     0.970646, Tokens per Sec:     4978, Lr: 0.000300\n",
      "2021-10-09 20:22:56,399 - INFO - joeynmt.training - Epoch  16, Step:   220200, Batch Loss:     1.115930, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2021-10-09 20:23:41,691 - INFO - joeynmt.training - Epoch  16, Step:   220300, Batch Loss:     0.982697, Tokens per Sec:     5077, Lr: 0.000300\n",
      "2021-10-09 20:24:26,730 - INFO - joeynmt.training - Epoch  16, Step:   220400, Batch Loss:     0.794376, Tokens per Sec:     5066, Lr: 0.000300\n",
      "2021-10-09 20:25:12,171 - INFO - joeynmt.training - Epoch  16, Step:   220500, Batch Loss:     0.965360, Tokens per Sec:     5041, Lr: 0.000300\n",
      "2021-10-09 20:25:57,713 - INFO - joeynmt.training - Epoch  16, Step:   220600, Batch Loss:     0.994070, Tokens per Sec:     5029, Lr: 0.000300\n",
      "2021-10-09 20:26:42,785 - INFO - joeynmt.training - Epoch  16, Step:   220700, Batch Loss:     1.041881, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-09 20:27:27,156 - INFO - joeynmt.training - Epoch  16, Step:   220800, Batch Loss:     1.077451, Tokens per Sec:     4858, Lr: 0.000300\n",
      "2021-10-09 20:28:11,488 - INFO - joeynmt.training - Epoch  16, Step:   220900, Batch Loss:     1.144518, Tokens per Sec:     4924, Lr: 0.000300\n",
      "2021-10-09 20:28:56,478 - INFO - joeynmt.training - Epoch  16, Step:   221000, Batch Loss:     1.072496, Tokens per Sec:     4969, Lr: 0.000300\n",
      "2021-10-09 20:29:41,256 - INFO - joeynmt.training - Epoch  16, Step:   221100, Batch Loss:     1.026912, Tokens per Sec:     5065, Lr: 0.000300\n",
      "2021-10-09 20:30:26,375 - INFO - joeynmt.training - Epoch  16, Step:   221200, Batch Loss:     0.901445, Tokens per Sec:     4995, Lr: 0.000300\n",
      "2021-10-09 20:31:11,132 - INFO - joeynmt.training - Epoch  16, Step:   221300, Batch Loss:     1.044756, Tokens per Sec:     4974, Lr: 0.000300\n",
      "2021-10-09 20:31:56,751 - INFO - joeynmt.training - Epoch  16, Step:   221400, Batch Loss:     1.045206, Tokens per Sec:     5084, Lr: 0.000300\n",
      "2021-10-09 20:32:41,343 - INFO - joeynmt.training - Epoch  16, Step:   221500, Batch Loss:     0.869503, Tokens per Sec:     4956, Lr: 0.000300\n",
      "2021-10-09 20:33:26,525 - INFO - joeynmt.training - Epoch  16, Step:   221600, Batch Loss:     1.064227, Tokens per Sec:     5056, Lr: 0.000300\n",
      "2021-10-09 20:34:11,753 - INFO - joeynmt.training - Epoch  16, Step:   221700, Batch Loss:     1.125585, Tokens per Sec:     5019, Lr: 0.000300\n",
      "2021-10-09 20:34:56,932 - INFO - joeynmt.training - Epoch  16, Step:   221800, Batch Loss:     0.932908, Tokens per Sec:     4918, Lr: 0.000300\n",
      "2021-10-09 20:35:42,696 - INFO - joeynmt.training - Epoch  16, Step:   221900, Batch Loss:     1.057709, Tokens per Sec:     5040, Lr: 0.000300\n",
      "2021-10-09 20:36:27,764 - INFO - joeynmt.training - Epoch  16, Step:   222000, Batch Loss:     0.888251, Tokens per Sec:     5053, Lr: 0.000300\n",
      "2021-10-09 20:37:12,809 - INFO - joeynmt.training - Epoch  16, Step:   222100, Batch Loss:     1.164935, Tokens per Sec:     5029, Lr: 0.000300\n",
      "2021-10-09 20:37:57,776 - INFO - joeynmt.training - Epoch  16, Step:   222200, Batch Loss:     1.041295, Tokens per Sec:     4977, Lr: 0.000300\n",
      "2021-10-09 20:38:42,703 - INFO - joeynmt.training - Epoch  16, Step:   222300, Batch Loss:     0.929191, Tokens per Sec:     5009, Lr: 0.000300\n",
      "2021-10-09 20:39:27,729 - INFO - joeynmt.training - Epoch  16, Step:   222400, Batch Loss:     0.905713, Tokens per Sec:     4906, Lr: 0.000300\n",
      "2021-10-09 20:40:12,930 - INFO - joeynmt.training - Epoch  16, Step:   222500, Batch Loss:     1.092603, Tokens per Sec:     5012, Lr: 0.000300\n",
      "2021-10-09 20:40:58,282 - INFO - joeynmt.training - Epoch  16, Step:   222600, Batch Loss:     0.890384, Tokens per Sec:     5069, Lr: 0.000300\n",
      "2021-10-09 20:41:43,844 - INFO - joeynmt.training - Epoch  16, Step:   222700, Batch Loss:     1.011177, Tokens per Sec:     5056, Lr: 0.000300\n",
      "2021-10-09 20:42:28,880 - INFO - joeynmt.training - Epoch  16, Step:   222800, Batch Loss:     0.946815, Tokens per Sec:     4942, Lr: 0.000300\n",
      "2021-10-09 20:43:14,478 - INFO - joeynmt.training - Epoch  16, Step:   222900, Batch Loss:     1.030322, Tokens per Sec:     5086, Lr: 0.000300\n",
      "2021-10-09 20:43:59,388 - INFO - joeynmt.training - Epoch  16, Step:   223000, Batch Loss:     0.902131, Tokens per Sec:     5011, Lr: 0.000300\n",
      "2021-10-09 20:44:44,082 - INFO - joeynmt.training - Epoch  16, Step:   223100, Batch Loss:     0.982483, Tokens per Sec:     5004, Lr: 0.000300\n",
      "2021-10-09 20:45:29,858 - INFO - joeynmt.training - Epoch  16, Step:   223200, Batch Loss:     1.034590, Tokens per Sec:     5195, Lr: 0.000300\n",
      "2021-10-09 20:46:15,546 - INFO - joeynmt.training - Epoch  16, Step:   223300, Batch Loss:     1.044768, Tokens per Sec:     5011, Lr: 0.000300\n",
      "2021-10-09 20:47:00,509 - INFO - joeynmt.training - Epoch  16, Step:   223400, Batch Loss:     1.023776, Tokens per Sec:     4985, Lr: 0.000300\n",
      "2021-10-09 20:47:45,606 - INFO - joeynmt.training - Epoch  16, Step:   223500, Batch Loss:     1.014228, Tokens per Sec:     4994, Lr: 0.000300\n",
      "2021-10-09 20:48:31,037 - INFO - joeynmt.training - Epoch  16, Step:   223600, Batch Loss:     0.890240, Tokens per Sec:     5027, Lr: 0.000300\n",
      "2021-10-09 20:49:15,549 - INFO - joeynmt.training - Epoch  16, Step:   223700, Batch Loss:     0.952469, Tokens per Sec:     4980, Lr: 0.000300\n",
      "2021-10-09 20:50:01,172 - INFO - joeynmt.training - Epoch  16, Step:   223800, Batch Loss:     0.901947, Tokens per Sec:     5029, Lr: 0.000300\n",
      "2021-10-09 20:50:47,124 - INFO - joeynmt.training - Epoch  16, Step:   223900, Batch Loss:     1.008963, Tokens per Sec:     5092, Lr: 0.000300\n",
      "2021-10-09 20:51:31,233 - INFO - joeynmt.training - Epoch  16, Step:   224000, Batch Loss:     0.886904, Tokens per Sec:     4907, Lr: 0.000300\n",
      "2021-10-09 20:52:17,121 - INFO - joeynmt.training - Epoch  16, Step:   224100, Batch Loss:     1.010013, Tokens per Sec:     5010, Lr: 0.000300\n",
      "2021-10-09 20:53:02,044 - INFO - joeynmt.training - Epoch  16, Step:   224200, Batch Loss:     0.926094, Tokens per Sec:     4945, Lr: 0.000300\n",
      "2021-10-09 20:53:46,833 - INFO - joeynmt.training - Epoch  16, Step:   224300, Batch Loss:     1.008429, Tokens per Sec:     4982, Lr: 0.000300\n",
      "2021-10-09 20:54:32,410 - INFO - joeynmt.training - Epoch  16, Step:   224400, Batch Loss:     1.045013, Tokens per Sec:     5034, Lr: 0.000300\n",
      "2021-10-09 20:55:17,305 - INFO - joeynmt.training - Epoch  16, Step:   224500, Batch Loss:     1.038277, Tokens per Sec:     4962, Lr: 0.000300\n",
      "2021-10-09 20:56:02,934 - INFO - joeynmt.training - Epoch  16, Step:   224600, Batch Loss:     0.923109, Tokens per Sec:     5014, Lr: 0.000300\n",
      "2021-10-09 20:56:47,582 - INFO - joeynmt.training - Epoch  16, Step:   224700, Batch Loss:     1.114472, Tokens per Sec:     4981, Lr: 0.000300\n",
      "2021-10-09 20:57:32,602 - INFO - joeynmt.training - Epoch  16, Step:   224800, Batch Loss:     1.128775, Tokens per Sec:     4915, Lr: 0.000300\n",
      "2021-10-09 20:58:17,726 - INFO - joeynmt.training - Epoch  16, Step:   224900, Batch Loss:     1.062078, Tokens per Sec:     5024, Lr: 0.000300\n",
      "2021-10-09 20:59:03,172 - INFO - joeynmt.training - Epoch  16, Step:   225000, Batch Loss:     0.988321, Tokens per Sec:     5029, Lr: 0.000300\n",
      "2021-10-09 21:00:32,282 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 21:00:32,283 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 21:00:32,283 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 21:00:32,622 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 21:00:32,622 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 21:00:42,440 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 21:00:42,445 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 21:00:42,445 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 21:00:42,446 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term then used for the brother who took the lead in a congregation .\n",
      "2021-10-09 21:00:42,446 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 21:00:42,446 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 21:00:42,446 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 21:00:42,447 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know himself and expresses feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 21:00:42,447 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 21:00:42,447 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 21:00:42,448 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 21:00:42,448 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 21:00:42,448 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 21:00:42,449 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 21:00:42,449 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 21:00:42,449 - INFO - joeynmt.training - \tHypothesis: HOW DO WE SHOW WE SPIRITURE DEACH ?\n",
      "2021-10-09 21:00:42,449 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step   225000: bleu:  50.58, loss: 26890.6895, ppl:   2.4326, duration: 99.2762s\n",
      "2021-10-09 21:01:27,497 - INFO - joeynmt.training - Epoch  16, Step:   225100, Batch Loss:     1.040905, Tokens per Sec:     4988, Lr: 0.000300\n",
      "2021-10-09 21:02:12,612 - INFO - joeynmt.training - Epoch  16, Step:   225200, Batch Loss:     0.916411, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2021-10-09 21:02:57,764 - INFO - joeynmt.training - Epoch  16, Step:   225300, Batch Loss:     1.078128, Tokens per Sec:     4993, Lr: 0.000300\n",
      "2021-10-09 21:03:20,114 - INFO - joeynmt.training - Epoch  16: total training loss 12378.33\n",
      "2021-10-09 21:03:20,114 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-10-09 21:03:45,283 - INFO - joeynmt.training - Epoch  17, Step:   225400, Batch Loss:     0.985169, Tokens per Sec:     4774, Lr: 0.000300\n",
      "2021-10-09 21:04:30,107 - INFO - joeynmt.training - Epoch  17, Step:   225500, Batch Loss:     1.005401, Tokens per Sec:     4974, Lr: 0.000300\n",
      "2021-10-09 21:05:15,285 - INFO - joeynmt.training - Epoch  17, Step:   225600, Batch Loss:     0.867000, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-09 21:06:00,629 - INFO - joeynmt.training - Epoch  17, Step:   225700, Batch Loss:     0.986463, Tokens per Sec:     4966, Lr: 0.000300\n",
      "2021-10-09 21:06:45,754 - INFO - joeynmt.training - Epoch  17, Step:   225800, Batch Loss:     1.016769, Tokens per Sec:     4989, Lr: 0.000300\n",
      "2021-10-09 21:07:30,628 - INFO - joeynmt.training - Epoch  17, Step:   225900, Batch Loss:     0.893824, Tokens per Sec:     4965, Lr: 0.000300\n",
      "2021-10-09 21:08:15,731 - INFO - joeynmt.training - Epoch  17, Step:   226000, Batch Loss:     0.963572, Tokens per Sec:     5027, Lr: 0.000300\n",
      "2021-10-09 21:09:01,010 - INFO - joeynmt.training - Epoch  17, Step:   226100, Batch Loss:     0.865180, Tokens per Sec:     5026, Lr: 0.000300\n",
      "2021-10-09 21:09:45,422 - INFO - joeynmt.training - Epoch  17, Step:   226200, Batch Loss:     0.936957, Tokens per Sec:     4899, Lr: 0.000300\n",
      "2021-10-09 21:10:30,589 - INFO - joeynmt.training - Epoch  17, Step:   226300, Batch Loss:     0.940832, Tokens per Sec:     5058, Lr: 0.000300\n",
      "2021-10-09 21:11:15,236 - INFO - joeynmt.training - Epoch  17, Step:   226400, Batch Loss:     1.102902, Tokens per Sec:     4933, Lr: 0.000300\n",
      "2021-10-09 21:12:00,019 - INFO - joeynmt.training - Epoch  17, Step:   226500, Batch Loss:     0.978155, Tokens per Sec:     4935, Lr: 0.000300\n",
      "2021-10-09 21:12:44,993 - INFO - joeynmt.training - Epoch  17, Step:   226600, Batch Loss:     1.004357, Tokens per Sec:     4998, Lr: 0.000300\n",
      "2021-10-09 21:13:29,689 - INFO - joeynmt.training - Epoch  17, Step:   226700, Batch Loss:     0.998843, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-09 21:14:14,633 - INFO - joeynmt.training - Epoch  17, Step:   226800, Batch Loss:     0.936227, Tokens per Sec:     5054, Lr: 0.000300\n",
      "2021-10-09 21:14:59,925 - INFO - joeynmt.training - Epoch  17, Step:   226900, Batch Loss:     1.136864, Tokens per Sec:     4985, Lr: 0.000300\n",
      "2021-10-09 21:15:45,037 - INFO - joeynmt.training - Epoch  17, Step:   227000, Batch Loss:     0.940130, Tokens per Sec:     4988, Lr: 0.000300\n",
      "2021-10-09 21:16:30,066 - INFO - joeynmt.training - Epoch  17, Step:   227100, Batch Loss:     0.874688, Tokens per Sec:     5034, Lr: 0.000300\n",
      "2021-10-09 21:17:15,590 - INFO - joeynmt.training - Epoch  17, Step:   227200, Batch Loss:     1.154187, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-09 21:18:00,231 - INFO - joeynmt.training - Epoch  17, Step:   227300, Batch Loss:     1.031172, Tokens per Sec:     4995, Lr: 0.000300\n",
      "2021-10-09 21:18:45,667 - INFO - joeynmt.training - Epoch  17, Step:   227400, Batch Loss:     0.939448, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2021-10-09 21:19:31,241 - INFO - joeynmt.training - Epoch  17, Step:   227500, Batch Loss:     0.903138, Tokens per Sec:     5073, Lr: 0.000300\n",
      "2021-10-09 21:20:16,694 - INFO - joeynmt.training - Epoch  17, Step:   227600, Batch Loss:     1.031904, Tokens per Sec:     5011, Lr: 0.000300\n",
      "2021-10-09 21:21:02,445 - INFO - joeynmt.training - Epoch  17, Step:   227700, Batch Loss:     1.027358, Tokens per Sec:     5087, Lr: 0.000300\n",
      "2021-10-09 21:21:47,117 - INFO - joeynmt.training - Epoch  17, Step:   227800, Batch Loss:     0.964308, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-09 21:22:32,232 - INFO - joeynmt.training - Epoch  17, Step:   227900, Batch Loss:     1.059404, Tokens per Sec:     5021, Lr: 0.000300\n",
      "2021-10-09 21:23:17,662 - INFO - joeynmt.training - Epoch  17, Step:   228000, Batch Loss:     0.938406, Tokens per Sec:     4995, Lr: 0.000300\n",
      "2021-10-09 21:24:02,989 - INFO - joeynmt.training - Epoch  17, Step:   228100, Batch Loss:     1.034248, Tokens per Sec:     5021, Lr: 0.000300\n",
      "2021-10-09 21:24:48,369 - INFO - joeynmt.training - Epoch  17, Step:   228200, Batch Loss:     0.980284, Tokens per Sec:     5044, Lr: 0.000300\n",
      "2021-10-09 21:25:33,511 - INFO - joeynmt.training - Epoch  17, Step:   228300, Batch Loss:     0.937908, Tokens per Sec:     5048, Lr: 0.000300\n",
      "2021-10-09 21:26:18,256 - INFO - joeynmt.training - Epoch  17, Step:   228400, Batch Loss:     0.867066, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2021-10-09 21:27:03,142 - INFO - joeynmt.training - Epoch  17, Step:   228500, Batch Loss:     0.937498, Tokens per Sec:     4974, Lr: 0.000300\n",
      "2021-10-09 21:27:47,605 - INFO - joeynmt.training - Epoch  17, Step:   228600, Batch Loss:     1.013604, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-09 21:28:32,401 - INFO - joeynmt.training - Epoch  17, Step:   228700, Batch Loss:     0.950970, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-09 21:29:18,025 - INFO - joeynmt.training - Epoch  17, Step:   228800, Batch Loss:     0.928048, Tokens per Sec:     5064, Lr: 0.000300\n",
      "2021-10-09 21:30:03,772 - INFO - joeynmt.training - Epoch  17, Step:   228900, Batch Loss:     0.986022, Tokens per Sec:     5056, Lr: 0.000300\n",
      "2021-10-09 21:30:49,124 - INFO - joeynmt.training - Epoch  17, Step:   229000, Batch Loss:     0.944967, Tokens per Sec:     5105, Lr: 0.000300\n",
      "2021-10-09 21:31:33,912 - INFO - joeynmt.training - Epoch  17, Step:   229100, Batch Loss:     1.010633, Tokens per Sec:     5007, Lr: 0.000300\n",
      "2021-10-09 21:32:19,264 - INFO - joeynmt.training - Epoch  17, Step:   229200, Batch Loss:     1.032058, Tokens per Sec:     5057, Lr: 0.000300\n",
      "2021-10-09 21:33:03,817 - INFO - joeynmt.training - Epoch  17, Step:   229300, Batch Loss:     0.980453, Tokens per Sec:     4891, Lr: 0.000300\n",
      "2021-10-09 21:33:49,191 - INFO - joeynmt.training - Epoch  17, Step:   229400, Batch Loss:     1.026519, Tokens per Sec:     5054, Lr: 0.000300\n",
      "2021-10-09 21:34:33,562 - INFO - joeynmt.training - Epoch  17, Step:   229500, Batch Loss:     0.985632, Tokens per Sec:     4836, Lr: 0.000300\n",
      "2021-10-09 21:35:19,105 - INFO - joeynmt.training - Epoch  17, Step:   229600, Batch Loss:     1.019923, Tokens per Sec:     5071, Lr: 0.000300\n",
      "2021-10-09 21:36:04,219 - INFO - joeynmt.training - Epoch  17, Step:   229700, Batch Loss:     0.882386, Tokens per Sec:     5152, Lr: 0.000300\n",
      "2021-10-09 21:36:49,820 - INFO - joeynmt.training - Epoch  17, Step:   229800, Batch Loss:     0.925936, Tokens per Sec:     5059, Lr: 0.000300\n",
      "2021-10-09 21:37:35,102 - INFO - joeynmt.training - Epoch  17, Step:   229900, Batch Loss:     0.931712, Tokens per Sec:     5086, Lr: 0.000300\n",
      "2021-10-09 21:38:20,626 - INFO - joeynmt.training - Epoch  17, Step:   230000, Batch Loss:     0.989203, Tokens per Sec:     5037, Lr: 0.000300\n",
      "2021-10-09 21:39:49,143 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 21:39:49,145 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 21:39:49,145 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 21:39:49,485 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 21:39:49,485 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 21:39:59,642 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 21:39:59,643 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 21:39:59,643 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 21:39:59,643 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term used back then for the brother who took the lead in a congregation .\n",
      "2021-10-09 21:39:59,643 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 21:39:59,643 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 21:39:59,643 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 21:39:59,643 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know oneself and expresses your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 21:39:59,643 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 21:39:59,644 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 21:39:59,644 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 21:39:59,644 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 21:39:59,644 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 21:39:59,644 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 21:39:59,644 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 21:39:59,644 - INFO - joeynmt.training - \tHypothesis: HOW DO WE SHOULD WE FAMILY DOELVITS ?\n",
      "2021-10-09 21:39:59,644 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step   230000: bleu:  50.77, loss: 26851.5996, ppl:   2.4294, duration: 99.0181s\n",
      "2021-10-09 21:40:44,938 - INFO - joeynmt.training - Epoch  17, Step:   230100, Batch Loss:     0.967852, Tokens per Sec:     5068, Lr: 0.000300\n",
      "2021-10-09 21:41:30,155 - INFO - joeynmt.training - Epoch  17, Step:   230200, Batch Loss:     0.967428, Tokens per Sec:     5039, Lr: 0.000300\n",
      "2021-10-09 21:42:14,888 - INFO - joeynmt.training - Epoch  17, Step:   230300, Batch Loss:     0.918255, Tokens per Sec:     4922, Lr: 0.000300\n",
      "2021-10-09 21:43:00,173 - INFO - joeynmt.training - Epoch  17, Step:   230400, Batch Loss:     0.999826, Tokens per Sec:     5043, Lr: 0.000300\n",
      "2021-10-09 21:43:45,563 - INFO - joeynmt.training - Epoch  17, Step:   230500, Batch Loss:     0.976854, Tokens per Sec:     5098, Lr: 0.000300\n",
      "2021-10-09 21:44:30,221 - INFO - joeynmt.training - Epoch  17, Step:   230600, Batch Loss:     0.952064, Tokens per Sec:     4998, Lr: 0.000300\n",
      "2021-10-09 21:45:15,092 - INFO - joeynmt.training - Epoch  17, Step:   230700, Batch Loss:     1.017738, Tokens per Sec:     5003, Lr: 0.000300\n",
      "2021-10-09 21:45:59,369 - INFO - joeynmt.training - Epoch  17, Step:   230800, Batch Loss:     0.771247, Tokens per Sec:     4901, Lr: 0.000300\n",
      "2021-10-09 21:46:44,417 - INFO - joeynmt.training - Epoch  17, Step:   230900, Batch Loss:     0.893187, Tokens per Sec:     5040, Lr: 0.000300\n",
      "2021-10-09 21:47:29,364 - INFO - joeynmt.training - Epoch  17, Step:   231000, Batch Loss:     0.918130, Tokens per Sec:     5002, Lr: 0.000300\n",
      "2021-10-09 21:48:14,570 - INFO - joeynmt.training - Epoch  17, Step:   231100, Batch Loss:     0.959056, Tokens per Sec:     4998, Lr: 0.000300\n",
      "2021-10-09 21:48:59,472 - INFO - joeynmt.training - Epoch  17, Step:   231200, Batch Loss:     0.924797, Tokens per Sec:     5001, Lr: 0.000300\n",
      "2021-10-09 21:49:44,536 - INFO - joeynmt.training - Epoch  17, Step:   231300, Batch Loss:     0.856185, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-09 21:50:29,779 - INFO - joeynmt.training - Epoch  17, Step:   231400, Batch Loss:     0.877966, Tokens per Sec:     5048, Lr: 0.000300\n",
      "2021-10-09 21:51:14,834 - INFO - joeynmt.training - Epoch  17, Step:   231500, Batch Loss:     0.965773, Tokens per Sec:     5051, Lr: 0.000300\n",
      "2021-10-09 21:51:59,822 - INFO - joeynmt.training - Epoch  17, Step:   231600, Batch Loss:     1.056546, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2021-10-09 21:52:45,412 - INFO - joeynmt.training - Epoch  17, Step:   231700, Batch Loss:     1.025331, Tokens per Sec:     5065, Lr: 0.000300\n",
      "2021-10-09 21:53:30,810 - INFO - joeynmt.training - Epoch  17, Step:   231800, Batch Loss:     0.982956, Tokens per Sec:     5051, Lr: 0.000300\n",
      "2021-10-09 21:54:15,774 - INFO - joeynmt.training - Epoch  17, Step:   231900, Batch Loss:     1.075735, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2021-10-09 21:55:01,129 - INFO - joeynmt.training - Epoch  17, Step:   232000, Batch Loss:     0.890842, Tokens per Sec:     5006, Lr: 0.000300\n",
      "2021-10-09 21:55:46,644 - INFO - joeynmt.training - Epoch  17, Step:   232100, Batch Loss:     0.953733, Tokens per Sec:     4993, Lr: 0.000300\n",
      "2021-10-09 21:56:32,442 - INFO - joeynmt.training - Epoch  17, Step:   232200, Batch Loss:     1.135668, Tokens per Sec:     4999, Lr: 0.000300\n",
      "2021-10-09 21:57:17,790 - INFO - joeynmt.training - Epoch  17, Step:   232300, Batch Loss:     1.058822, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2021-10-09 21:58:03,168 - INFO - joeynmt.training - Epoch  17, Step:   232400, Batch Loss:     1.002309, Tokens per Sec:     5052, Lr: 0.000300\n",
      "2021-10-09 21:58:47,832 - INFO - joeynmt.training - Epoch  17, Step:   232500, Batch Loss:     0.927370, Tokens per Sec:     5030, Lr: 0.000300\n",
      "2021-10-09 21:59:32,921 - INFO - joeynmt.training - Epoch  17, Step:   232600, Batch Loss:     1.031457, Tokens per Sec:     4995, Lr: 0.000300\n",
      "2021-10-09 22:00:18,336 - INFO - joeynmt.training - Epoch  17, Step:   232700, Batch Loss:     0.994368, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-09 22:01:02,892 - INFO - joeynmt.training - Epoch  17, Step:   232800, Batch Loss:     0.977383, Tokens per Sec:     4974, Lr: 0.000300\n",
      "2021-10-09 22:01:47,857 - INFO - joeynmt.training - Epoch  17, Step:   232900, Batch Loss:     0.921027, Tokens per Sec:     5041, Lr: 0.000300\n",
      "2021-10-09 22:02:32,872 - INFO - joeynmt.training - Epoch  17, Step:   233000, Batch Loss:     0.913986, Tokens per Sec:     4905, Lr: 0.000300\n",
      "2021-10-09 22:03:17,746 - INFO - joeynmt.training - Epoch  17, Step:   233100, Batch Loss:     1.033324, Tokens per Sec:     4918, Lr: 0.000300\n",
      "2021-10-09 22:04:02,933 - INFO - joeynmt.training - Epoch  17, Step:   233200, Batch Loss:     0.918257, Tokens per Sec:     5106, Lr: 0.000300\n",
      "2021-10-09 22:04:48,057 - INFO - joeynmt.training - Epoch  17, Step:   233300, Batch Loss:     0.964557, Tokens per Sec:     4938, Lr: 0.000300\n",
      "2021-10-09 22:05:33,251 - INFO - joeynmt.training - Epoch  17, Step:   233400, Batch Loss:     0.847120, Tokens per Sec:     4988, Lr: 0.000300\n",
      "2021-10-09 22:06:18,219 - INFO - joeynmt.training - Epoch  17, Step:   233500, Batch Loss:     0.935448, Tokens per Sec:     5042, Lr: 0.000300\n",
      "2021-10-09 22:07:03,324 - INFO - joeynmt.training - Epoch  17, Step:   233600, Batch Loss:     1.018416, Tokens per Sec:     5080, Lr: 0.000300\n",
      "2021-10-09 22:07:48,541 - INFO - joeynmt.training - Epoch  17, Step:   233700, Batch Loss:     1.016039, Tokens per Sec:     5052, Lr: 0.000300\n",
      "2021-10-09 22:08:33,470 - INFO - joeynmt.training - Epoch  17, Step:   233800, Batch Loss:     0.938424, Tokens per Sec:     5015, Lr: 0.000300\n",
      "2021-10-09 22:09:18,870 - INFO - joeynmt.training - Epoch  17, Step:   233900, Batch Loss:     0.975698, Tokens per Sec:     5042, Lr: 0.000300\n",
      "2021-10-09 22:10:03,368 - INFO - joeynmt.training - Epoch  17, Step:   234000, Batch Loss:     0.967870, Tokens per Sec:     4957, Lr: 0.000300\n",
      "2021-10-09 22:10:48,433 - INFO - joeynmt.training - Epoch  17, Step:   234100, Batch Loss:     1.027501, Tokens per Sec:     5080, Lr: 0.000300\n",
      "2021-10-09 22:11:33,886 - INFO - joeynmt.training - Epoch  17, Step:   234200, Batch Loss:     0.873129, Tokens per Sec:     5010, Lr: 0.000300\n",
      "2021-10-09 22:12:19,341 - INFO - joeynmt.training - Epoch  17, Step:   234300, Batch Loss:     0.903475, Tokens per Sec:     5025, Lr: 0.000300\n",
      "2021-10-09 22:13:04,681 - INFO - joeynmt.training - Epoch  17, Step:   234400, Batch Loss:     0.885311, Tokens per Sec:     5086, Lr: 0.000300\n",
      "2021-10-09 22:13:49,447 - INFO - joeynmt.training - Epoch  17, Step:   234500, Batch Loss:     1.123740, Tokens per Sec:     5040, Lr: 0.000300\n",
      "2021-10-09 22:14:34,194 - INFO - joeynmt.training - Epoch  17, Step:   234600, Batch Loss:     0.892938, Tokens per Sec:     4966, Lr: 0.000300\n",
      "2021-10-09 22:15:18,603 - INFO - joeynmt.training - Epoch  17, Step:   234700, Batch Loss:     0.969391, Tokens per Sec:     4974, Lr: 0.000300\n",
      "2021-10-09 22:16:04,488 - INFO - joeynmt.training - Epoch  17, Step:   234800, Batch Loss:     1.086124, Tokens per Sec:     5152, Lr: 0.000300\n",
      "2021-10-09 22:16:49,834 - INFO - joeynmt.training - Epoch  17, Step:   234900, Batch Loss:     0.928346, Tokens per Sec:     5051, Lr: 0.000300\n",
      "2021-10-09 22:17:35,229 - INFO - joeynmt.training - Epoch  17, Step:   235000, Batch Loss:     0.898590, Tokens per Sec:     5075, Lr: 0.000300\n",
      "2021-10-09 22:19:04,569 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 22:19:04,570 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 22:19:04,570 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 22:19:04,907 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 22:19:04,907 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 22:19:12,853 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 22:19:12,857 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 22:19:12,857 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 22:19:12,857 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-09 22:19:12,857 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 22:19:12,858 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 22:19:12,858 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 22:19:12,858 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know oneself and express feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 22:19:12,858 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 22:19:12,859 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 22:19:12,859 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 22:19:12,859 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 22:19:12,859 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 22:19:12,860 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 22:19:12,860 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 22:19:12,860 - INFO - joeynmt.training - \tHypothesis: HOW DO WE SHOULD WE STECTED DOELVITS ?\n",
      "2021-10-09 22:19:12,860 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step   235000: bleu:  50.97, loss: 26675.0801, ppl:   2.4153, duration: 97.6309s\n",
      "2021-10-09 22:19:57,359 - INFO - joeynmt.training - Epoch  17, Step:   235100, Batch Loss:     0.941977, Tokens per Sec:     4944, Lr: 0.000300\n",
      "2021-10-09 22:20:42,487 - INFO - joeynmt.training - Epoch  17, Step:   235200, Batch Loss:     0.962626, Tokens per Sec:     5011, Lr: 0.000300\n",
      "2021-10-09 22:21:26,992 - INFO - joeynmt.training - Epoch  17, Step:   235300, Batch Loss:     0.923237, Tokens per Sec:     5126, Lr: 0.000300\n",
      "2021-10-09 22:22:11,932 - INFO - joeynmt.training - Epoch  17, Step:   235400, Batch Loss:     0.931824, Tokens per Sec:     5048, Lr: 0.000300\n",
      "2021-10-09 22:22:56,636 - INFO - joeynmt.training - Epoch  17, Step:   235500, Batch Loss:     0.997540, Tokens per Sec:     5014, Lr: 0.000300\n",
      "2021-10-09 22:23:41,836 - INFO - joeynmt.training - Epoch  17, Step:   235600, Batch Loss:     0.911437, Tokens per Sec:     5172, Lr: 0.000300\n",
      "2021-10-09 22:24:26,708 - INFO - joeynmt.training - Epoch  17, Step:   235700, Batch Loss:     1.120969, Tokens per Sec:     4963, Lr: 0.000300\n",
      "2021-10-09 22:25:11,116 - INFO - joeynmt.training - Epoch  17, Step:   235800, Batch Loss:     0.870700, Tokens per Sec:     4969, Lr: 0.000300\n",
      "2021-10-09 22:25:55,965 - INFO - joeynmt.training - Epoch  17, Step:   235900, Batch Loss:     0.915201, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-09 22:26:40,877 - INFO - joeynmt.training - Epoch  17, Step:   236000, Batch Loss:     0.965978, Tokens per Sec:     4978, Lr: 0.000300\n",
      "2021-10-09 22:27:25,372 - INFO - joeynmt.training - Epoch  17, Step:   236100, Batch Loss:     0.948997, Tokens per Sec:     5039, Lr: 0.000300\n",
      "2021-10-09 22:28:10,648 - INFO - joeynmt.training - Epoch  17, Step:   236200, Batch Loss:     0.933808, Tokens per Sec:     5083, Lr: 0.000300\n",
      "2021-10-09 22:28:55,547 - INFO - joeynmt.training - Epoch  17, Step:   236300, Batch Loss:     1.153355, Tokens per Sec:     5109, Lr: 0.000300\n",
      "2021-10-09 22:29:40,681 - INFO - joeynmt.training - Epoch  17, Step:   236400, Batch Loss:     0.974573, Tokens per Sec:     5081, Lr: 0.000300\n",
      "2021-10-09 22:30:25,377 - INFO - joeynmt.training - Epoch  17, Step:   236500, Batch Loss:     0.997190, Tokens per Sec:     5038, Lr: 0.000300\n",
      "2021-10-09 22:31:10,723 - INFO - joeynmt.training - Epoch  17, Step:   236600, Batch Loss:     0.936581, Tokens per Sec:     5151, Lr: 0.000300\n",
      "2021-10-09 22:31:55,719 - INFO - joeynmt.training - Epoch  17, Step:   236700, Batch Loss:     0.930382, Tokens per Sec:     5107, Lr: 0.000300\n",
      "2021-10-09 22:32:40,414 - INFO - joeynmt.training - Epoch  17, Step:   236800, Batch Loss:     1.006427, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-09 22:33:25,178 - INFO - joeynmt.training - Epoch  17, Step:   236900, Batch Loss:     1.057400, Tokens per Sec:     5055, Lr: 0.000300\n",
      "2021-10-09 22:34:10,114 - INFO - joeynmt.training - Epoch  17, Step:   237000, Batch Loss:     0.986862, Tokens per Sec:     5073, Lr: 0.000300\n",
      "2021-10-09 22:34:55,028 - INFO - joeynmt.training - Epoch  17, Step:   237100, Batch Loss:     1.009346, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-09 22:35:40,445 - INFO - joeynmt.training - Epoch  17, Step:   237200, Batch Loss:     0.893228, Tokens per Sec:     5130, Lr: 0.000300\n",
      "2021-10-09 22:36:25,163 - INFO - joeynmt.training - Epoch  17, Step:   237300, Batch Loss:     0.960622, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2021-10-09 22:37:10,432 - INFO - joeynmt.training - Epoch  17, Step:   237400, Batch Loss:     1.194715, Tokens per Sec:     5061, Lr: 0.000300\n",
      "2021-10-09 22:37:55,194 - INFO - joeynmt.training - Epoch  17, Step:   237500, Batch Loss:     0.916305, Tokens per Sec:     5023, Lr: 0.000300\n",
      "2021-10-09 22:38:40,009 - INFO - joeynmt.training - Epoch  17, Step:   237600, Batch Loss:     1.084337, Tokens per Sec:     4956, Lr: 0.000300\n",
      "2021-10-09 22:39:24,709 - INFO - joeynmt.training - Epoch  17, Step:   237700, Batch Loss:     0.916842, Tokens per Sec:     4906, Lr: 0.000300\n",
      "2021-10-09 22:40:09,876 - INFO - joeynmt.training - Epoch  17, Step:   237800, Batch Loss:     1.128348, Tokens per Sec:     5099, Lr: 0.000300\n",
      "2021-10-09 22:40:36,772 - INFO - joeynmt.training - Epoch  17: total training loss 12273.27\n",
      "2021-10-09 22:40:36,778 - INFO - joeynmt.training - EPOCH 18\n",
      "2021-10-09 22:40:56,701 - INFO - joeynmt.training - Epoch  18, Step:   237900, Batch Loss:     0.989354, Tokens per Sec:     4671, Lr: 0.000300\n",
      "2021-10-09 22:41:41,866 - INFO - joeynmt.training - Epoch  18, Step:   238000, Batch Loss:     0.878250, Tokens per Sec:     4993, Lr: 0.000300\n",
      "2021-10-09 22:42:26,734 - INFO - joeynmt.training - Epoch  18, Step:   238100, Batch Loss:     1.113709, Tokens per Sec:     5067, Lr: 0.000300\n",
      "2021-10-09 22:43:11,829 - INFO - joeynmt.training - Epoch  18, Step:   238200, Batch Loss:     1.060048, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-09 22:43:56,239 - INFO - joeynmt.training - Epoch  18, Step:   238300, Batch Loss:     0.904508, Tokens per Sec:     4991, Lr: 0.000300\n",
      "2021-10-09 22:44:40,876 - INFO - joeynmt.training - Epoch  18, Step:   238400, Batch Loss:     0.996367, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-09 22:45:25,904 - INFO - joeynmt.training - Epoch  18, Step:   238500, Batch Loss:     1.074336, Tokens per Sec:     4982, Lr: 0.000300\n",
      "2021-10-09 22:46:10,827 - INFO - joeynmt.training - Epoch  18, Step:   238600, Batch Loss:     0.899688, Tokens per Sec:     5021, Lr: 0.000300\n",
      "2021-10-09 22:46:55,587 - INFO - joeynmt.training - Epoch  18, Step:   238700, Batch Loss:     0.974702, Tokens per Sec:     4994, Lr: 0.000300\n",
      "2021-10-09 22:47:40,624 - INFO - joeynmt.training - Epoch  18, Step:   238800, Batch Loss:     0.977357, Tokens per Sec:     4995, Lr: 0.000300\n",
      "2021-10-09 22:48:25,105 - INFO - joeynmt.training - Epoch  18, Step:   238900, Batch Loss:     0.994623, Tokens per Sec:     5094, Lr: 0.000300\n",
      "2021-10-09 22:49:10,465 - INFO - joeynmt.training - Epoch  18, Step:   239000, Batch Loss:     0.967604, Tokens per Sec:     5095, Lr: 0.000300\n",
      "2021-10-09 22:49:55,838 - INFO - joeynmt.training - Epoch  18, Step:   239100, Batch Loss:     0.959723, Tokens per Sec:     5124, Lr: 0.000300\n",
      "2021-10-09 22:50:40,615 - INFO - joeynmt.training - Epoch  18, Step:   239200, Batch Loss:     1.030391, Tokens per Sec:     4879, Lr: 0.000300\n",
      "2021-10-09 22:51:25,352 - INFO - joeynmt.training - Epoch  18, Step:   239300, Batch Loss:     0.996442, Tokens per Sec:     5041, Lr: 0.000300\n",
      "2021-10-09 22:52:10,624 - INFO - joeynmt.training - Epoch  18, Step:   239400, Batch Loss:     0.984617, Tokens per Sec:     5056, Lr: 0.000300\n",
      "2021-10-09 22:52:55,845 - INFO - joeynmt.training - Epoch  18, Step:   239500, Batch Loss:     1.150284, Tokens per Sec:     5070, Lr: 0.000300\n",
      "2021-10-09 22:53:41,048 - INFO - joeynmt.training - Epoch  18, Step:   239600, Batch Loss:     0.973427, Tokens per Sec:     5019, Lr: 0.000300\n",
      "2021-10-09 22:54:26,159 - INFO - joeynmt.training - Epoch  18, Step:   239700, Batch Loss:     0.977055, Tokens per Sec:     4963, Lr: 0.000300\n",
      "2021-10-09 22:55:10,528 - INFO - joeynmt.training - Epoch  18, Step:   239800, Batch Loss:     0.894951, Tokens per Sec:     4970, Lr: 0.000300\n",
      "2021-10-09 22:55:55,007 - INFO - joeynmt.training - Epoch  18, Step:   239900, Batch Loss:     0.803009, Tokens per Sec:     4932, Lr: 0.000300\n",
      "2021-10-09 22:56:40,192 - INFO - joeynmt.training - Epoch  18, Step:   240000, Batch Loss:     1.001344, Tokens per Sec:     4983, Lr: 0.000300\n",
      "2021-10-09 22:58:09,516 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 22:58:09,517 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 22:58:09,517 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 22:58:16,138 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 22:58:16,139 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 22:58:16,139 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 22:58:16,139 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother taking the lead in a congregation .\n",
      "2021-10-09 22:58:16,140 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 22:58:16,140 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 22:58:16,140 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 22:58:16,140 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know himself and expresses your feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 22:58:16,140 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 22:58:16,141 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 22:58:16,141 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 22:58:16,141 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 22:58:16,141 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 22:58:16,141 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 22:58:16,141 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 22:58:16,141 - INFO - joeynmt.training - \tHypothesis: HOW DO WE SHOULD WE STECTS DOELVITS ?\n",
      "2021-10-09 22:58:16,142 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step   240000: bleu:  50.72, loss: 26683.3613, ppl:   2.4160, duration: 95.9484s\n",
      "2021-10-09 22:59:01,702 - INFO - joeynmt.training - Epoch  18, Step:   240100, Batch Loss:     0.980452, Tokens per Sec:     5065, Lr: 0.000300\n",
      "2021-10-09 22:59:47,077 - INFO - joeynmt.training - Epoch  18, Step:   240200, Batch Loss:     1.064052, Tokens per Sec:     5023, Lr: 0.000300\n",
      "2021-10-09 23:00:32,211 - INFO - joeynmt.training - Epoch  18, Step:   240300, Batch Loss:     1.067096, Tokens per Sec:     5039, Lr: 0.000300\n",
      "2021-10-09 23:01:17,782 - INFO - joeynmt.training - Epoch  18, Step:   240400, Batch Loss:     0.910059, Tokens per Sec:     5158, Lr: 0.000300\n",
      "2021-10-09 23:02:02,802 - INFO - joeynmt.training - Epoch  18, Step:   240500, Batch Loss:     0.886359, Tokens per Sec:     5095, Lr: 0.000300\n",
      "2021-10-09 23:02:47,553 - INFO - joeynmt.training - Epoch  18, Step:   240600, Batch Loss:     1.053816, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-09 23:03:32,037 - INFO - joeynmt.training - Epoch  18, Step:   240700, Batch Loss:     0.965623, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2021-10-09 23:04:16,686 - INFO - joeynmt.training - Epoch  18, Step:   240800, Batch Loss:     0.998097, Tokens per Sec:     5025, Lr: 0.000300\n",
      "2021-10-09 23:05:01,634 - INFO - joeynmt.training - Epoch  18, Step:   240900, Batch Loss:     0.755881, Tokens per Sec:     5039, Lr: 0.000300\n",
      "2021-10-09 23:05:46,547 - INFO - joeynmt.training - Epoch  18, Step:   241000, Batch Loss:     0.910888, Tokens per Sec:     5011, Lr: 0.000300\n",
      "2021-10-09 23:06:31,645 - INFO - joeynmt.training - Epoch  18, Step:   241100, Batch Loss:     0.952237, Tokens per Sec:     5105, Lr: 0.000300\n",
      "2021-10-09 23:07:16,325 - INFO - joeynmt.training - Epoch  18, Step:   241200, Batch Loss:     1.024601, Tokens per Sec:     5004, Lr: 0.000300\n",
      "2021-10-09 23:08:00,492 - INFO - joeynmt.training - Epoch  18, Step:   241300, Batch Loss:     0.983058, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2021-10-09 23:08:45,717 - INFO - joeynmt.training - Epoch  18, Step:   241400, Batch Loss:     0.876865, Tokens per Sec:     4998, Lr: 0.000300\n",
      "2021-10-09 23:09:30,507 - INFO - joeynmt.training - Epoch  18, Step:   241500, Batch Loss:     0.907853, Tokens per Sec:     5026, Lr: 0.000300\n",
      "2021-10-09 23:10:15,760 - INFO - joeynmt.training - Epoch  18, Step:   241600, Batch Loss:     0.871922, Tokens per Sec:     5090, Lr: 0.000300\n",
      "2021-10-09 23:11:00,437 - INFO - joeynmt.training - Epoch  18, Step:   241700, Batch Loss:     1.066550, Tokens per Sec:     5057, Lr: 0.000300\n",
      "2021-10-09 23:11:45,338 - INFO - joeynmt.training - Epoch  18, Step:   241800, Batch Loss:     1.030144, Tokens per Sec:     5060, Lr: 0.000300\n",
      "2021-10-09 23:12:29,695 - INFO - joeynmt.training - Epoch  18, Step:   241900, Batch Loss:     0.929402, Tokens per Sec:     4959, Lr: 0.000300\n",
      "2021-10-09 23:13:14,436 - INFO - joeynmt.training - Epoch  18, Step:   242000, Batch Loss:     1.017321, Tokens per Sec:     5042, Lr: 0.000300\n",
      "2021-10-09 23:14:00,081 - INFO - joeynmt.training - Epoch  18, Step:   242100, Batch Loss:     1.024933, Tokens per Sec:     5126, Lr: 0.000300\n",
      "2021-10-09 23:14:45,132 - INFO - joeynmt.training - Epoch  18, Step:   242200, Batch Loss:     0.989795, Tokens per Sec:     5013, Lr: 0.000300\n",
      "2021-10-09 23:15:29,376 - INFO - joeynmt.training - Epoch  18, Step:   242300, Batch Loss:     0.866967, Tokens per Sec:     4927, Lr: 0.000300\n",
      "2021-10-09 23:16:14,667 - INFO - joeynmt.training - Epoch  18, Step:   242400, Batch Loss:     0.851192, Tokens per Sec:     5102, Lr: 0.000300\n",
      "2021-10-09 23:17:00,145 - INFO - joeynmt.training - Epoch  18, Step:   242500, Batch Loss:     1.046924, Tokens per Sec:     5080, Lr: 0.000300\n",
      "2021-10-09 23:17:45,414 - INFO - joeynmt.training - Epoch  18, Step:   242600, Batch Loss:     0.902409, Tokens per Sec:     5031, Lr: 0.000300\n",
      "2021-10-09 23:18:30,760 - INFO - joeynmt.training - Epoch  18, Step:   242700, Batch Loss:     0.997814, Tokens per Sec:     5099, Lr: 0.000300\n",
      "2021-10-09 23:19:15,721 - INFO - joeynmt.training - Epoch  18, Step:   242800, Batch Loss:     1.001019, Tokens per Sec:     5017, Lr: 0.000300\n",
      "2021-10-09 23:20:00,168 - INFO - joeynmt.training - Epoch  18, Step:   242900, Batch Loss:     1.048484, Tokens per Sec:     4957, Lr: 0.000300\n",
      "2021-10-09 23:20:45,240 - INFO - joeynmt.training - Epoch  18, Step:   243000, Batch Loss:     1.039153, Tokens per Sec:     5004, Lr: 0.000300\n",
      "2021-10-09 23:21:30,135 - INFO - joeynmt.training - Epoch  18, Step:   243100, Batch Loss:     0.960436, Tokens per Sec:     5106, Lr: 0.000300\n",
      "2021-10-09 23:22:14,828 - INFO - joeynmt.training - Epoch  18, Step:   243200, Batch Loss:     1.013527, Tokens per Sec:     5077, Lr: 0.000300\n",
      "2021-10-09 23:22:59,857 - INFO - joeynmt.training - Epoch  18, Step:   243300, Batch Loss:     0.904242, Tokens per Sec:     5031, Lr: 0.000300\n",
      "2021-10-09 23:23:44,407 - INFO - joeynmt.training - Epoch  18, Step:   243400, Batch Loss:     0.885247, Tokens per Sec:     5003, Lr: 0.000300\n",
      "2021-10-09 23:24:29,433 - INFO - joeynmt.training - Epoch  18, Step:   243500, Batch Loss:     0.973583, Tokens per Sec:     5007, Lr: 0.000300\n",
      "2021-10-09 23:25:14,885 - INFO - joeynmt.training - Epoch  18, Step:   243600, Batch Loss:     1.039737, Tokens per Sec:     5128, Lr: 0.000300\n",
      "2021-10-09 23:25:59,803 - INFO - joeynmt.training - Epoch  18, Step:   243700, Batch Loss:     0.970343, Tokens per Sec:     5050, Lr: 0.000300\n",
      "2021-10-09 23:26:44,538 - INFO - joeynmt.training - Epoch  18, Step:   243800, Batch Loss:     1.144207, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-09 23:27:29,333 - INFO - joeynmt.training - Epoch  18, Step:   243900, Batch Loss:     0.933090, Tokens per Sec:     5050, Lr: 0.000300\n",
      "2021-10-09 23:28:14,264 - INFO - joeynmt.training - Epoch  18, Step:   244000, Batch Loss:     0.834808, Tokens per Sec:     4936, Lr: 0.000300\n",
      "2021-10-09 23:28:59,572 - INFO - joeynmt.training - Epoch  18, Step:   244100, Batch Loss:     1.035136, Tokens per Sec:     5121, Lr: 0.000300\n",
      "2021-10-09 23:29:45,098 - INFO - joeynmt.training - Epoch  18, Step:   244200, Batch Loss:     1.033713, Tokens per Sec:     5036, Lr: 0.000300\n",
      "2021-10-09 23:30:30,305 - INFO - joeynmt.training - Epoch  18, Step:   244300, Batch Loss:     1.008263, Tokens per Sec:     5027, Lr: 0.000300\n",
      "2021-10-09 23:31:15,424 - INFO - joeynmt.training - Epoch  18, Step:   244400, Batch Loss:     1.041136, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-09 23:32:00,781 - INFO - joeynmt.training - Epoch  18, Step:   244500, Batch Loss:     0.885685, Tokens per Sec:     5040, Lr: 0.000300\n",
      "2021-10-09 23:32:46,623 - INFO - joeynmt.training - Epoch  18, Step:   244600, Batch Loss:     1.060720, Tokens per Sec:     5082, Lr: 0.000300\n",
      "2021-10-09 23:33:31,746 - INFO - joeynmt.training - Epoch  18, Step:   244700, Batch Loss:     1.069064, Tokens per Sec:     5110, Lr: 0.000300\n",
      "2021-10-09 23:34:17,024 - INFO - joeynmt.training - Epoch  18, Step:   244800, Batch Loss:     0.938996, Tokens per Sec:     5068, Lr: 0.000300\n",
      "2021-10-09 23:35:02,626 - INFO - joeynmt.training - Epoch  18, Step:   244900, Batch Loss:     1.029301, Tokens per Sec:     5054, Lr: 0.000300\n",
      "2021-10-09 23:35:48,424 - INFO - joeynmt.training - Epoch  18, Step:   245000, Batch Loss:     1.068457, Tokens per Sec:     5046, Lr: 0.000300\n",
      "2021-10-09 23:37:15,786 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-09 23:37:15,787 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-09 23:37:15,787 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-09 23:37:16,124 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-09 23:37:16,124 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-09 23:37:22,331 - INFO - joeynmt.training - Example #0\n",
      "2021-10-09 23:37:22,332 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-09 23:37:22,332 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-09 23:37:22,332 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-09 23:37:22,332 - INFO - joeynmt.training - Example #1\n",
      "2021-10-09 23:37:22,333 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-09 23:37:22,333 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-09 23:37:22,333 - INFO - joeynmt.training - \tHypothesis: The appeal is understandable because adolescence is a time when one comes to know oneself and express feelings in a way that speaks to others and touches them .\n",
      "2021-10-09 23:37:22,333 - INFO - joeynmt.training - Example #2\n",
      "2021-10-09 23:37:22,333 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-09 23:37:22,333 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-09 23:37:22,333 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-09 23:37:22,333 - INFO - joeynmt.training - Example #3\n",
      "2021-10-09 23:37:22,334 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-09 23:37:22,334 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-09 23:37:22,334 - INFO - joeynmt.training - \tHypothesis: HOW DO WE SHOW WE SHOW US SPIRITURE DOELVITY ?\n",
      "2021-10-09 23:37:22,334 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step   245000: bleu:  50.93, loss: 26413.0469, ppl:   2.3945, duration: 93.9098s\n",
      "2021-10-09 23:38:07,296 - INFO - joeynmt.training - Epoch  18, Step:   245100, Batch Loss:     1.028636, Tokens per Sec:     4933, Lr: 0.000300\n",
      "2021-10-09 23:38:52,874 - INFO - joeynmt.training - Epoch  18, Step:   245200, Batch Loss:     0.898313, Tokens per Sec:     5096, Lr: 0.000300\n",
      "2021-10-09 23:39:37,753 - INFO - joeynmt.training - Epoch  18, Step:   245300, Batch Loss:     1.007740, Tokens per Sec:     4997, Lr: 0.000300\n",
      "2021-10-09 23:40:22,463 - INFO - joeynmt.training - Epoch  18, Step:   245400, Batch Loss:     1.025730, Tokens per Sec:     5015, Lr: 0.000300\n",
      "2021-10-09 23:41:07,931 - INFO - joeynmt.training - Epoch  18, Step:   245500, Batch Loss:     0.934050, Tokens per Sec:     5068, Lr: 0.000300\n",
      "2021-10-09 23:41:52,767 - INFO - joeynmt.training - Epoch  18, Step:   245600, Batch Loss:     0.900393, Tokens per Sec:     4929, Lr: 0.000300\n",
      "2021-10-09 23:42:37,790 - INFO - joeynmt.training - Epoch  18, Step:   245700, Batch Loss:     1.033962, Tokens per Sec:     5017, Lr: 0.000300\n",
      "2021-10-09 23:43:23,377 - INFO - joeynmt.training - Epoch  18, Step:   245800, Batch Loss:     0.986600, Tokens per Sec:     5109, Lr: 0.000300\n",
      "2021-10-09 23:44:08,324 - INFO - joeynmt.training - Epoch  18, Step:   245900, Batch Loss:     0.908292, Tokens per Sec:     4920, Lr: 0.000300\n",
      "2021-10-09 23:44:53,534 - INFO - joeynmt.training - Epoch  18, Step:   246000, Batch Loss:     0.982206, Tokens per Sec:     5042, Lr: 0.000300\n",
      "2021-10-09 23:45:38,384 - INFO - joeynmt.training - Epoch  18, Step:   246100, Batch Loss:     0.934158, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-09 23:46:23,412 - INFO - joeynmt.training - Epoch  18, Step:   246200, Batch Loss:     1.079452, Tokens per Sec:     5056, Lr: 0.000300\n",
      "2021-10-09 23:47:09,189 - INFO - joeynmt.training - Epoch  18, Step:   246300, Batch Loss:     0.949733, Tokens per Sec:     5093, Lr: 0.000300\n",
      "2021-10-09 23:47:53,936 - INFO - joeynmt.training - Epoch  18, Step:   246400, Batch Loss:     1.064339, Tokens per Sec:     4930, Lr: 0.000300\n",
      "2021-10-09 23:48:39,170 - INFO - joeynmt.training - Epoch  18, Step:   246500, Batch Loss:     1.054134, Tokens per Sec:     5091, Lr: 0.000300\n",
      "2021-10-09 23:49:24,099 - INFO - joeynmt.training - Epoch  18, Step:   246600, Batch Loss:     0.922302, Tokens per Sec:     4960, Lr: 0.000300\n",
      "2021-10-09 23:50:09,236 - INFO - joeynmt.training - Epoch  18, Step:   246700, Batch Loss:     1.000611, Tokens per Sec:     4963, Lr: 0.000300\n",
      "2021-10-09 23:50:54,070 - INFO - joeynmt.training - Epoch  18, Step:   246800, Batch Loss:     0.953912, Tokens per Sec:     4985, Lr: 0.000300\n",
      "2021-10-09 23:51:39,114 - INFO - joeynmt.training - Epoch  18, Step:   246900, Batch Loss:     0.929099, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-09 23:52:24,125 - INFO - joeynmt.training - Epoch  18, Step:   247000, Batch Loss:     0.935214, Tokens per Sec:     5022, Lr: 0.000300\n",
      "2021-10-09 23:53:09,497 - INFO - joeynmt.training - Epoch  18, Step:   247100, Batch Loss:     0.983723, Tokens per Sec:     5093, Lr: 0.000300\n",
      "2021-10-09 23:53:55,122 - INFO - joeynmt.training - Epoch  18, Step:   247200, Batch Loss:     0.908309, Tokens per Sec:     5031, Lr: 0.000300\n",
      "2021-10-09 23:54:40,420 - INFO - joeynmt.training - Epoch  18, Step:   247300, Batch Loss:     1.105194, Tokens per Sec:     5059, Lr: 0.000300\n",
      "2021-10-09 23:55:25,073 - INFO - joeynmt.training - Epoch  18, Step:   247400, Batch Loss:     0.892566, Tokens per Sec:     4975, Lr: 0.000300\n",
      "2021-10-09 23:56:09,992 - INFO - joeynmt.training - Epoch  18, Step:   247500, Batch Loss:     0.958880, Tokens per Sec:     5034, Lr: 0.000300\n",
      "2021-10-09 23:56:55,354 - INFO - joeynmt.training - Epoch  18, Step:   247600, Batch Loss:     1.022141, Tokens per Sec:     5033, Lr: 0.000300\n",
      "2021-10-09 23:57:40,027 - INFO - joeynmt.training - Epoch  18, Step:   247700, Batch Loss:     0.985546, Tokens per Sec:     4926, Lr: 0.000300\n",
      "2021-10-09 23:58:25,648 - INFO - joeynmt.training - Epoch  18, Step:   247800, Batch Loss:     0.931605, Tokens per Sec:     4974, Lr: 0.000300\n",
      "2021-10-09 23:59:10,529 - INFO - joeynmt.training - Epoch  18, Step:   247900, Batch Loss:     0.854827, Tokens per Sec:     4913, Lr: 0.000300\n",
      "2021-10-09 23:59:55,705 - INFO - joeynmt.training - Epoch  18, Step:   248000, Batch Loss:     0.951463, Tokens per Sec:     4974, Lr: 0.000300\n",
      "2021-10-10 00:00:40,481 - INFO - joeynmt.training - Epoch  18, Step:   248100, Batch Loss:     1.158028, Tokens per Sec:     4907, Lr: 0.000300\n",
      "2021-10-10 00:01:25,384 - INFO - joeynmt.training - Epoch  18, Step:   248200, Batch Loss:     1.091311, Tokens per Sec:     4915, Lr: 0.000300\n",
      "2021-10-10 00:02:10,009 - INFO - joeynmt.training - Epoch  18, Step:   248300, Batch Loss:     0.961616, Tokens per Sec:     4931, Lr: 0.000300\n",
      "2021-10-10 00:02:55,224 - INFO - joeynmt.training - Epoch  18, Step:   248400, Batch Loss:     1.147130, Tokens per Sec:     5047, Lr: 0.000300\n",
      "2021-10-10 00:03:40,740 - INFO - joeynmt.training - Epoch  18, Step:   248500, Batch Loss:     0.956184, Tokens per Sec:     5012, Lr: 0.000300\n",
      "2021-10-10 00:04:26,565 - INFO - joeynmt.training - Epoch  18, Step:   248600, Batch Loss:     1.058997, Tokens per Sec:     5043, Lr: 0.000300\n",
      "2021-10-10 00:05:11,813 - INFO - joeynmt.training - Epoch  18, Step:   248700, Batch Loss:     1.120168, Tokens per Sec:     4953, Lr: 0.000300\n",
      "2021-10-10 00:05:56,780 - INFO - joeynmt.training - Epoch  18, Step:   248800, Batch Loss:     0.818410, Tokens per Sec:     4968, Lr: 0.000300\n",
      "2021-10-10 00:06:42,033 - INFO - joeynmt.training - Epoch  18, Step:   248900, Batch Loss:     0.939368, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-10 00:07:27,144 - INFO - joeynmt.training - Epoch  18, Step:   249000, Batch Loss:     0.889963, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-10 00:08:12,729 - INFO - joeynmt.training - Epoch  18, Step:   249100, Batch Loss:     0.910878, Tokens per Sec:     5063, Lr: 0.000300\n",
      "2021-10-10 00:08:56,956 - INFO - joeynmt.training - Epoch  18, Step:   249200, Batch Loss:     0.992020, Tokens per Sec:     4782, Lr: 0.000300\n",
      "2021-10-10 00:09:42,980 - INFO - joeynmt.training - Epoch  18, Step:   249300, Batch Loss:     0.925901, Tokens per Sec:     5126, Lr: 0.000300\n",
      "2021-10-10 00:10:28,121 - INFO - joeynmt.training - Epoch  18, Step:   249400, Batch Loss:     0.985966, Tokens per Sec:     4950, Lr: 0.000300\n",
      "2021-10-10 00:11:13,533 - INFO - joeynmt.training - Epoch  18, Step:   249500, Batch Loss:     1.035000, Tokens per Sec:     5047, Lr: 0.000300\n",
      "2021-10-10 00:11:58,839 - INFO - joeynmt.training - Epoch  18, Step:   249600, Batch Loss:     0.979776, Tokens per Sec:     5087, Lr: 0.000300\n",
      "2021-10-10 00:12:43,481 - INFO - joeynmt.training - Epoch  18, Step:   249700, Batch Loss:     0.988534, Tokens per Sec:     4882, Lr: 0.000300\n",
      "2021-10-10 00:13:28,816 - INFO - joeynmt.training - Epoch  18, Step:   249800, Batch Loss:     1.017453, Tokens per Sec:     5061, Lr: 0.000300\n",
      "2021-10-10 00:14:13,993 - INFO - joeynmt.training - Epoch  18, Step:   249900, Batch Loss:     0.880133, Tokens per Sec:     4972, Lr: 0.000300\n",
      "2021-10-10 00:14:59,873 - INFO - joeynmt.training - Epoch  18, Step:   250000, Batch Loss:     0.935834, Tokens per Sec:     5007, Lr: 0.000300\n",
      "2021-10-10 00:16:27,633 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-10 00:16:27,634 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-10 00:16:27,634 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-10 00:16:27,971 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-10 00:16:27,972 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-10 00:16:34,742 - INFO - joeynmt.training - Example #0\n",
      "2021-10-10 00:16:34,743 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-10 00:16:34,743 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-10 00:16:34,743 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term used at that time for the brother who took the lead in a congregation .\n",
      "2021-10-10 00:16:34,743 - INFO - joeynmt.training - Example #1\n",
      "2021-10-10 00:16:34,743 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-10 00:16:34,744 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-10 00:16:34,744 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know oneself and express your feelings in a way that speaks to others and touches them .\n",
      "2021-10-10 00:16:34,744 - INFO - joeynmt.training - Example #2\n",
      "2021-10-10 00:16:34,744 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-10 00:16:34,744 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-10 00:16:34,744 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-10 00:16:34,744 - INFO - joeynmt.training - Example #3\n",
      "2021-10-10 00:16:34,745 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-10 00:16:34,745 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-10 00:16:34,745 - INFO - joeynmt.training - \tHypothesis: HOW DO WE SHOULD WE STECTED DOELVITS ?\n",
      "2021-10-10 00:16:34,745 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step   250000: bleu:  51.02, loss: 26389.7559, ppl:   2.3926, duration: 94.8714s\n",
      "2021-10-10 00:17:19,383 - INFO - joeynmt.training - Epoch  18, Step:   250100, Batch Loss:     1.047981, Tokens per Sec:     5051, Lr: 0.000300\n",
      "2021-10-10 00:18:03,798 - INFO - joeynmt.training - Epoch  18, Step:   250200, Batch Loss:     0.963924, Tokens per Sec:     4938, Lr: 0.000300\n",
      "2021-10-10 00:18:49,230 - INFO - joeynmt.training - Epoch  18, Step:   250300, Batch Loss:     1.148639, Tokens per Sec:     5089, Lr: 0.000300\n",
      "2021-10-10 00:19:22,747 - INFO - joeynmt.training - Epoch  18: total training loss 12177.70\n",
      "2021-10-10 00:19:22,748 - INFO - joeynmt.training - EPOCH 19\n",
      "2021-10-10 00:19:36,153 - INFO - joeynmt.training - Epoch  19, Step:   250400, Batch Loss:     0.926716, Tokens per Sec:     4420, Lr: 0.000300\n",
      "2021-10-10 00:20:21,520 - INFO - joeynmt.training - Epoch  19, Step:   250500, Batch Loss:     1.018300, Tokens per Sec:     4991, Lr: 0.000300\n",
      "2021-10-10 00:21:07,082 - INFO - joeynmt.training - Epoch  19, Step:   250600, Batch Loss:     0.935561, Tokens per Sec:     5088, Lr: 0.000300\n",
      "2021-10-10 00:21:52,369 - INFO - joeynmt.training - Epoch  19, Step:   250700, Batch Loss:     0.952250, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-10 00:22:37,513 - INFO - joeynmt.training - Epoch  19, Step:   250800, Batch Loss:     0.931550, Tokens per Sec:     5054, Lr: 0.000300\n",
      "2021-10-10 00:23:22,692 - INFO - joeynmt.training - Epoch  19, Step:   250900, Batch Loss:     0.981957, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2021-10-10 00:24:07,832 - INFO - joeynmt.training - Epoch  19, Step:   251000, Batch Loss:     0.934815, Tokens per Sec:     5087, Lr: 0.000300\n",
      "2021-10-10 00:24:52,122 - INFO - joeynmt.training - Epoch  19, Step:   251100, Batch Loss:     0.913243, Tokens per Sec:     4984, Lr: 0.000300\n",
      "2021-10-10 00:25:37,295 - INFO - joeynmt.training - Epoch  19, Step:   251200, Batch Loss:     0.940537, Tokens per Sec:     5003, Lr: 0.000300\n",
      "2021-10-10 00:26:21,746 - INFO - joeynmt.training - Epoch  19, Step:   251300, Batch Loss:     0.963369, Tokens per Sec:     4967, Lr: 0.000300\n",
      "2021-10-10 00:27:06,597 - INFO - joeynmt.training - Epoch  19, Step:   251400, Batch Loss:     1.021743, Tokens per Sec:     5036, Lr: 0.000300\n",
      "2021-10-10 00:27:51,518 - INFO - joeynmt.training - Epoch  19, Step:   251500, Batch Loss:     1.027827, Tokens per Sec:     5038, Lr: 0.000300\n",
      "2021-10-10 00:28:36,279 - INFO - joeynmt.training - Epoch  19, Step:   251600, Batch Loss:     1.077728, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2021-10-10 00:29:21,278 - INFO - joeynmt.training - Epoch  19, Step:   251700, Batch Loss:     0.935264, Tokens per Sec:     4994, Lr: 0.000300\n",
      "2021-10-10 00:30:06,636 - INFO - joeynmt.training - Epoch  19, Step:   251800, Batch Loss:     0.878672, Tokens per Sec:     5063, Lr: 0.000300\n",
      "2021-10-10 00:30:51,537 - INFO - joeynmt.training - Epoch  19, Step:   251900, Batch Loss:     1.051165, Tokens per Sec:     5018, Lr: 0.000300\n",
      "2021-10-10 00:31:36,244 - INFO - joeynmt.training - Epoch  19, Step:   252000, Batch Loss:     0.982253, Tokens per Sec:     5052, Lr: 0.000300\n",
      "2021-10-10 00:32:20,847 - INFO - joeynmt.training - Epoch  19, Step:   252100, Batch Loss:     0.997322, Tokens per Sec:     4997, Lr: 0.000300\n",
      "2021-10-10 00:33:05,604 - INFO - joeynmt.training - Epoch  19, Step:   252200, Batch Loss:     1.073838, Tokens per Sec:     4936, Lr: 0.000300\n",
      "2021-10-10 00:33:50,856 - INFO - joeynmt.training - Epoch  19, Step:   252300, Batch Loss:     0.820563, Tokens per Sec:     5130, Lr: 0.000300\n",
      "2021-10-10 00:34:35,965 - INFO - joeynmt.training - Epoch  19, Step:   252400, Batch Loss:     0.969058, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-10 00:35:20,416 - INFO - joeynmt.training - Epoch  19, Step:   252500, Batch Loss:     1.000555, Tokens per Sec:     4952, Lr: 0.000300\n",
      "2021-10-10 00:36:05,285 - INFO - joeynmt.training - Epoch  19, Step:   252600, Batch Loss:     0.907587, Tokens per Sec:     5050, Lr: 0.000300\n",
      "2021-10-10 00:36:49,896 - INFO - joeynmt.training - Epoch  19, Step:   252700, Batch Loss:     0.975001, Tokens per Sec:     4985, Lr: 0.000300\n",
      "2021-10-10 00:37:35,539 - INFO - joeynmt.training - Epoch  19, Step:   252800, Batch Loss:     1.207984, Tokens per Sec:     5176, Lr: 0.000300\n",
      "2021-10-10 00:38:19,731 - INFO - joeynmt.training - Epoch  19, Step:   252900, Batch Loss:     0.889224, Tokens per Sec:     5012, Lr: 0.000300\n",
      "2021-10-10 00:39:04,208 - INFO - joeynmt.training - Epoch  19, Step:   253000, Batch Loss:     0.887415, Tokens per Sec:     4970, Lr: 0.000300\n",
      "2021-10-10 00:39:48,725 - INFO - joeynmt.training - Epoch  19, Step:   253100, Batch Loss:     1.071339, Tokens per Sec:     4947, Lr: 0.000300\n",
      "2021-10-10 00:40:34,038 - INFO - joeynmt.training - Epoch  19, Step:   253200, Batch Loss:     1.012061, Tokens per Sec:     5085, Lr: 0.000300\n",
      "2021-10-10 00:41:18,982 - INFO - joeynmt.training - Epoch  19, Step:   253300, Batch Loss:     1.139098, Tokens per Sec:     5022, Lr: 0.000300\n",
      "2021-10-10 00:42:03,848 - INFO - joeynmt.training - Epoch  19, Step:   253400, Batch Loss:     0.957397, Tokens per Sec:     5057, Lr: 0.000300\n",
      "2021-10-10 00:42:48,943 - INFO - joeynmt.training - Epoch  19, Step:   253500, Batch Loss:     0.959811, Tokens per Sec:     4984, Lr: 0.000300\n",
      "2021-10-10 00:43:33,931 - INFO - joeynmt.training - Epoch  19, Step:   253600, Batch Loss:     1.052406, Tokens per Sec:     5044, Lr: 0.000300\n",
      "2021-10-10 00:44:18,934 - INFO - joeynmt.training - Epoch  19, Step:   253700, Batch Loss:     0.968547, Tokens per Sec:     5089, Lr: 0.000300\n",
      "2021-10-10 00:45:04,419 - INFO - joeynmt.training - Epoch  19, Step:   253800, Batch Loss:     0.974452, Tokens per Sec:     5110, Lr: 0.000300\n",
      "2021-10-10 00:45:49,627 - INFO - joeynmt.training - Epoch  19, Step:   253900, Batch Loss:     0.983015, Tokens per Sec:     5133, Lr: 0.000300\n",
      "2021-10-10 00:46:34,206 - INFO - joeynmt.training - Epoch  19, Step:   254000, Batch Loss:     1.021786, Tokens per Sec:     5087, Lr: 0.000300\n",
      "2021-10-10 00:47:18,871 - INFO - joeynmt.training - Epoch  19, Step:   254100, Batch Loss:     0.894892, Tokens per Sec:     5076, Lr: 0.000300\n",
      "2021-10-10 00:48:03,917 - INFO - joeynmt.training - Epoch  19, Step:   254200, Batch Loss:     1.100408, Tokens per Sec:     5035, Lr: 0.000300\n",
      "2021-10-10 00:48:49,000 - INFO - joeynmt.training - Epoch  19, Step:   254300, Batch Loss:     0.955714, Tokens per Sec:     5093, Lr: 0.000300\n",
      "2021-10-10 00:49:33,108 - INFO - joeynmt.training - Epoch  19, Step:   254400, Batch Loss:     0.913179, Tokens per Sec:     4991, Lr: 0.000300\n",
      "2021-10-10 00:50:18,410 - INFO - joeynmt.training - Epoch  19, Step:   254500, Batch Loss:     0.936149, Tokens per Sec:     5111, Lr: 0.000300\n",
      "2021-10-10 00:51:02,962 - INFO - joeynmt.training - Epoch  19, Step:   254600, Batch Loss:     1.095800, Tokens per Sec:     5040, Lr: 0.000300\n",
      "2021-10-10 00:51:48,017 - INFO - joeynmt.training - Epoch  19, Step:   254700, Batch Loss:     0.959265, Tokens per Sec:     5088, Lr: 0.000300\n",
      "2021-10-10 00:52:33,108 - INFO - joeynmt.training - Epoch  19, Step:   254800, Batch Loss:     0.888119, Tokens per Sec:     5096, Lr: 0.000300\n",
      "2021-10-10 00:53:18,274 - INFO - joeynmt.training - Epoch  19, Step:   254900, Batch Loss:     0.983656, Tokens per Sec:     5095, Lr: 0.000300\n",
      "2021-10-10 00:54:02,741 - INFO - joeynmt.training - Epoch  19, Step:   255000, Batch Loss:     0.963108, Tokens per Sec:     5056, Lr: 0.000300\n",
      "2021-10-10 00:55:32,706 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-10 00:55:32,706 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-10 00:55:32,706 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-10 00:55:39,059 - INFO - joeynmt.training - Example #0\n",
      "2021-10-10 00:55:39,060 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-10 00:55:39,060 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-10 00:55:39,060 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-10 00:55:39,060 - INFO - joeynmt.training - Example #1\n",
      "2021-10-10 00:55:39,061 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-10 00:55:39,061 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-10 00:55:39,061 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know oneself and expresses your feelings in a way that speaks to others and touches them .\n",
      "2021-10-10 00:55:39,061 - INFO - joeynmt.training - Example #2\n",
      "2021-10-10 00:55:39,061 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-10 00:55:39,062 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-10 00:55:39,062 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to respectfully acknowledge that Nabal was older than he was .\n",
      "2021-10-10 00:55:39,062 - INFO - joeynmt.training - Example #3\n",
      "2021-10-10 00:55:39,062 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-10 00:55:39,062 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-10 00:55:39,062 - INFO - joeynmt.training - \tHypothesis: HOW DO WE SHOULD WE STED HISTORY ?\n",
      "2021-10-10 00:55:39,063 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step   255000: bleu:  50.95, loss: 26458.3105, ppl:   2.3980, duration: 96.3207s\n",
      "2021-10-10 00:56:23,945 - INFO - joeynmt.training - Epoch  19, Step:   255100, Batch Loss:     1.013047, Tokens per Sec:     5049, Lr: 0.000300\n",
      "2021-10-10 00:57:08,124 - INFO - joeynmt.training - Epoch  19, Step:   255200, Batch Loss:     0.869228, Tokens per Sec:     5001, Lr: 0.000300\n",
      "2021-10-10 00:57:52,729 - INFO - joeynmt.training - Epoch  19, Step:   255300, Batch Loss:     0.898597, Tokens per Sec:     4999, Lr: 0.000300\n",
      "2021-10-10 00:58:36,874 - INFO - joeynmt.training - Epoch  19, Step:   255400, Batch Loss:     1.005886, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-10 00:59:21,854 - INFO - joeynmt.training - Epoch  19, Step:   255500, Batch Loss:     0.931442, Tokens per Sec:     4986, Lr: 0.000300\n",
      "2021-10-10 01:00:07,026 - INFO - joeynmt.training - Epoch  19, Step:   255600, Batch Loss:     0.950872, Tokens per Sec:     5101, Lr: 0.000300\n",
      "2021-10-10 01:00:52,013 - INFO - joeynmt.training - Epoch  19, Step:   255700, Batch Loss:     0.893025, Tokens per Sec:     5024, Lr: 0.000300\n",
      "2021-10-10 01:01:36,783 - INFO - joeynmt.training - Epoch  19, Step:   255800, Batch Loss:     1.143106, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-10 01:02:21,969 - INFO - joeynmt.training - Epoch  19, Step:   255900, Batch Loss:     0.968910, Tokens per Sec:     5050, Lr: 0.000300\n",
      "2021-10-10 01:03:06,993 - INFO - joeynmt.training - Epoch  19, Step:   256000, Batch Loss:     1.015643, Tokens per Sec:     5114, Lr: 0.000300\n",
      "2021-10-10 01:03:51,368 - INFO - joeynmt.training - Epoch  19, Step:   256100, Batch Loss:     1.041514, Tokens per Sec:     4957, Lr: 0.000300\n",
      "2021-10-10 01:04:36,368 - INFO - joeynmt.training - Epoch  19, Step:   256200, Batch Loss:     0.885231, Tokens per Sec:     5024, Lr: 0.000300\n",
      "2021-10-10 01:05:21,204 - INFO - joeynmt.training - Epoch  19, Step:   256300, Batch Loss:     0.964897, Tokens per Sec:     4982, Lr: 0.000300\n",
      "2021-10-10 01:06:06,213 - INFO - joeynmt.training - Epoch  19, Step:   256400, Batch Loss:     0.949858, Tokens per Sec:     5042, Lr: 0.000300\n",
      "2021-10-10 01:06:51,041 - INFO - joeynmt.training - Epoch  19, Step:   256500, Batch Loss:     1.069092, Tokens per Sec:     5034, Lr: 0.000300\n",
      "2021-10-10 01:07:36,268 - INFO - joeynmt.training - Epoch  19, Step:   256600, Batch Loss:     0.961564, Tokens per Sec:     5093, Lr: 0.000300\n",
      "2021-10-10 01:08:21,272 - INFO - joeynmt.training - Epoch  19, Step:   256700, Batch Loss:     1.027311, Tokens per Sec:     4989, Lr: 0.000300\n",
      "2021-10-10 01:09:06,656 - INFO - joeynmt.training - Epoch  19, Step:   256800, Batch Loss:     0.876783, Tokens per Sec:     5083, Lr: 0.000300\n",
      "2021-10-10 01:09:51,413 - INFO - joeynmt.training - Epoch  19, Step:   256900, Batch Loss:     0.920526, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-10 01:10:35,784 - INFO - joeynmt.training - Epoch  19, Step:   257000, Batch Loss:     0.945125, Tokens per Sec:     4978, Lr: 0.000300\n",
      "2021-10-10 01:11:21,635 - INFO - joeynmt.training - Epoch  19, Step:   257100, Batch Loss:     0.720583, Tokens per Sec:     5123, Lr: 0.000300\n",
      "2021-10-10 01:12:07,174 - INFO - joeynmt.training - Epoch  19, Step:   257200, Batch Loss:     0.872948, Tokens per Sec:     5158, Lr: 0.000300\n",
      "2021-10-10 01:12:52,051 - INFO - joeynmt.training - Epoch  19, Step:   257300, Batch Loss:     0.874588, Tokens per Sec:     5044, Lr: 0.000300\n",
      "2021-10-10 01:13:37,056 - INFO - joeynmt.training - Epoch  19, Step:   257400, Batch Loss:     1.043879, Tokens per Sec:     5047, Lr: 0.000300\n",
      "2021-10-10 01:14:22,497 - INFO - joeynmt.training - Epoch  19, Step:   257500, Batch Loss:     1.009740, Tokens per Sec:     5112, Lr: 0.000300\n",
      "2021-10-10 01:15:07,332 - INFO - joeynmt.training - Epoch  19, Step:   257600, Batch Loss:     0.964396, Tokens per Sec:     5051, Lr: 0.000300\n",
      "2021-10-10 01:15:51,994 - INFO - joeynmt.training - Epoch  19, Step:   257700, Batch Loss:     0.847908, Tokens per Sec:     4940, Lr: 0.000300\n",
      "2021-10-10 01:16:36,484 - INFO - joeynmt.training - Epoch  19, Step:   257800, Batch Loss:     0.941836, Tokens per Sec:     5072, Lr: 0.000300\n",
      "2021-10-10 01:17:21,099 - INFO - joeynmt.training - Epoch  19, Step:   257900, Batch Loss:     0.898516, Tokens per Sec:     4980, Lr: 0.000300\n",
      "2021-10-10 01:18:05,752 - INFO - joeynmt.training - Epoch  19, Step:   258000, Batch Loss:     1.014058, Tokens per Sec:     5077, Lr: 0.000300\n",
      "2021-10-10 01:18:50,838 - INFO - joeynmt.training - Epoch  19, Step:   258100, Batch Loss:     0.979031, Tokens per Sec:     5083, Lr: 0.000300\n",
      "2021-10-10 01:19:35,438 - INFO - joeynmt.training - Epoch  19, Step:   258200, Batch Loss:     0.988629, Tokens per Sec:     5014, Lr: 0.000300\n",
      "2021-10-10 01:20:21,252 - INFO - joeynmt.training - Epoch  19, Step:   258300, Batch Loss:     0.897764, Tokens per Sec:     5136, Lr: 0.000300\n",
      "2021-10-10 01:21:06,005 - INFO - joeynmt.training - Epoch  19, Step:   258400, Batch Loss:     0.952116, Tokens per Sec:     5060, Lr: 0.000300\n",
      "2021-10-10 01:21:50,845 - INFO - joeynmt.training - Epoch  19, Step:   258500, Batch Loss:     0.944428, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2021-10-10 01:22:36,342 - INFO - joeynmt.training - Epoch  19, Step:   258600, Batch Loss:     1.050350, Tokens per Sec:     5135, Lr: 0.000300\n",
      "2021-10-10 01:23:21,211 - INFO - joeynmt.training - Epoch  19, Step:   258700, Batch Loss:     0.940313, Tokens per Sec:     5051, Lr: 0.000300\n",
      "2021-10-10 01:24:06,525 - INFO - joeynmt.training - Epoch  19, Step:   258800, Batch Loss:     1.004630, Tokens per Sec:     5082, Lr: 0.000300\n",
      "2021-10-10 01:24:51,239 - INFO - joeynmt.training - Epoch  19, Step:   258900, Batch Loss:     1.009177, Tokens per Sec:     4989, Lr: 0.000300\n",
      "2021-10-10 01:25:35,691 - INFO - joeynmt.training - Epoch  19, Step:   259000, Batch Loss:     0.947061, Tokens per Sec:     5004, Lr: 0.000300\n",
      "2021-10-10 01:26:21,165 - INFO - joeynmt.training - Epoch  19, Step:   259100, Batch Loss:     1.152241, Tokens per Sec:     5101, Lr: 0.000300\n",
      "2021-10-10 01:27:05,763 - INFO - joeynmt.training - Epoch  19, Step:   259200, Batch Loss:     0.965517, Tokens per Sec:     4946, Lr: 0.000300\n",
      "2021-10-10 01:27:50,846 - INFO - joeynmt.training - Epoch  19, Step:   259300, Batch Loss:     0.911276, Tokens per Sec:     5056, Lr: 0.000300\n",
      "2021-10-10 01:28:35,922 - INFO - joeynmt.training - Epoch  19, Step:   259400, Batch Loss:     0.903001, Tokens per Sec:     5119, Lr: 0.000300\n",
      "2021-10-10 01:29:20,953 - INFO - joeynmt.training - Epoch  19, Step:   259500, Batch Loss:     0.991351, Tokens per Sec:     5052, Lr: 0.000300\n",
      "2021-10-10 01:30:06,316 - INFO - joeynmt.training - Epoch  19, Step:   259600, Batch Loss:     0.942328, Tokens per Sec:     5032, Lr: 0.000300\n",
      "2021-10-10 01:30:50,806 - INFO - joeynmt.training - Epoch  19, Step:   259700, Batch Loss:     0.907744, Tokens per Sec:     5041, Lr: 0.000300\n",
      "2021-10-10 01:31:35,912 - INFO - joeynmt.training - Epoch  19, Step:   259800, Batch Loss:     0.837018, Tokens per Sec:     5036, Lr: 0.000300\n",
      "2021-10-10 01:32:20,000 - INFO - joeynmt.training - Epoch  19, Step:   259900, Batch Loss:     0.882774, Tokens per Sec:     4906, Lr: 0.000300\n",
      "2021-10-10 01:33:04,769 - INFO - joeynmt.training - Epoch  19, Step:   260000, Batch Loss:     0.975122, Tokens per Sec:     4986, Lr: 0.000300\n",
      "2021-10-10 01:34:31,708 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-10 01:34:31,709 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-10 01:34:31,709 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-10 01:34:38,163 - INFO - joeynmt.training - Example #0\n",
      "2021-10-10 01:34:38,164 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-10 01:34:38,164 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-10 01:34:38,164 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at the time for the brother who took the lead in a congregation .\n",
      "2021-10-10 01:34:38,164 - INFO - joeynmt.training - Example #1\n",
      "2021-10-10 01:34:38,165 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-10 01:34:38,165 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-10 01:34:38,165 - INFO - joeynmt.training - \tHypothesis: The appeal is understandable , for adolescence is a time when one comes to know oneself and expresses feelings in a way that speaks to others and stirs them .\n",
      "2021-10-10 01:34:38,165 - INFO - joeynmt.training - Example #2\n",
      "2021-10-10 01:34:38,165 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-10 01:34:38,165 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-10 01:34:38,165 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” referring to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-10 01:34:38,166 - INFO - joeynmt.training - Example #3\n",
      "2021-10-10 01:34:38,166 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-10 01:34:38,166 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-10 01:34:38,166 - INFO - joeynmt.training - \tHypothesis: HOW DO WE SHOULD WE STEAR SPIRITUAL DEAR ?\n",
      "2021-10-10 01:34:38,166 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step   260000: bleu:  51.14, loss: 26474.2305, ppl:   2.3993, duration: 93.3965s\n",
      "2021-10-10 01:35:22,894 - INFO - joeynmt.training - Epoch  19, Step:   260100, Batch Loss:     0.837214, Tokens per Sec:     5005, Lr: 0.000300\n",
      "2021-10-10 01:36:06,837 - INFO - joeynmt.training - Epoch  19, Step:   260200, Batch Loss:     0.882245, Tokens per Sec:     4923, Lr: 0.000300\n",
      "2021-10-10 01:36:51,834 - INFO - joeynmt.training - Epoch  19, Step:   260300, Batch Loss:     0.928018, Tokens per Sec:     5104, Lr: 0.000300\n",
      "2021-10-10 01:37:36,498 - INFO - joeynmt.training - Epoch  19, Step:   260400, Batch Loss:     1.093638, Tokens per Sec:     4945, Lr: 0.000300\n",
      "2021-10-10 01:38:21,135 - INFO - joeynmt.training - Epoch  19, Step:   260500, Batch Loss:     0.979520, Tokens per Sec:     5024, Lr: 0.000300\n",
      "2021-10-10 01:39:06,653 - INFO - joeynmt.training - Epoch  19, Step:   260600, Batch Loss:     0.922842, Tokens per Sec:     5091, Lr: 0.000300\n",
      "2021-10-10 01:39:51,089 - INFO - joeynmt.training - Epoch  19, Step:   260700, Batch Loss:     0.995118, Tokens per Sec:     5013, Lr: 0.000300\n",
      "2021-10-10 01:40:35,919 - INFO - joeynmt.training - Epoch  19, Step:   260800, Batch Loss:     0.904942, Tokens per Sec:     5002, Lr: 0.000300\n",
      "2021-10-10 01:41:21,349 - INFO - joeynmt.training - Epoch  19, Step:   260900, Batch Loss:     1.121422, Tokens per Sec:     5113, Lr: 0.000300\n",
      "2021-10-10 01:42:05,933 - INFO - joeynmt.training - Epoch  19, Step:   261000, Batch Loss:     0.912843, Tokens per Sec:     5021, Lr: 0.000300\n",
      "2021-10-10 01:42:50,559 - INFO - joeynmt.training - Epoch  19, Step:   261100, Batch Loss:     0.963111, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-10 01:43:35,409 - INFO - joeynmt.training - Epoch  19, Step:   261200, Batch Loss:     0.924857, Tokens per Sec:     5003, Lr: 0.000300\n",
      "2021-10-10 01:44:20,500 - INFO - joeynmt.training - Epoch  19, Step:   261300, Batch Loss:     0.979291, Tokens per Sec:     5041, Lr: 0.000300\n",
      "2021-10-10 01:45:05,144 - INFO - joeynmt.training - Epoch  19, Step:   261400, Batch Loss:     0.963719, Tokens per Sec:     4995, Lr: 0.000300\n",
      "2021-10-10 01:45:49,282 - INFO - joeynmt.training - Epoch  19, Step:   261500, Batch Loss:     0.952316, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-10 01:46:34,593 - INFO - joeynmt.training - Epoch  19, Step:   261600, Batch Loss:     0.878846, Tokens per Sec:     5047, Lr: 0.000300\n",
      "2021-10-10 01:47:19,809 - INFO - joeynmt.training - Epoch  19, Step:   261700, Batch Loss:     0.942707, Tokens per Sec:     4998, Lr: 0.000300\n",
      "2021-10-10 01:48:05,035 - INFO - joeynmt.training - Epoch  19, Step:   261800, Batch Loss:     0.893771, Tokens per Sec:     5079, Lr: 0.000300\n",
      "2021-10-10 01:48:50,367 - INFO - joeynmt.training - Epoch  19, Step:   261900, Batch Loss:     0.994443, Tokens per Sec:     5034, Lr: 0.000300\n",
      "2021-10-10 01:49:35,566 - INFO - joeynmt.training - Epoch  19, Step:   262000, Batch Loss:     0.995685, Tokens per Sec:     5092, Lr: 0.000300\n",
      "2021-10-10 01:50:21,031 - INFO - joeynmt.training - Epoch  19, Step:   262100, Batch Loss:     0.951616, Tokens per Sec:     5043, Lr: 0.000300\n",
      "2021-10-10 01:51:05,983 - INFO - joeynmt.training - Epoch  19, Step:   262200, Batch Loss:     0.933113, Tokens per Sec:     4982, Lr: 0.000300\n",
      "2021-10-10 01:51:51,313 - INFO - joeynmt.training - Epoch  19, Step:   262300, Batch Loss:     0.842946, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2021-10-10 01:52:35,611 - INFO - joeynmt.training - Epoch  19, Step:   262400, Batch Loss:     1.106270, Tokens per Sec:     4929, Lr: 0.000300\n",
      "2021-10-10 01:53:20,880 - INFO - joeynmt.training - Epoch  19, Step:   262500, Batch Loss:     0.972450, Tokens per Sec:     4990, Lr: 0.000300\n",
      "2021-10-10 01:54:06,071 - INFO - joeynmt.training - Epoch  19, Step:   262600, Batch Loss:     1.058565, Tokens per Sec:     5073, Lr: 0.000300\n",
      "2021-10-10 01:54:51,309 - INFO - joeynmt.training - Epoch  19, Step:   262700, Batch Loss:     1.030936, Tokens per Sec:     5029, Lr: 0.000300\n",
      "2021-10-10 01:55:36,467 - INFO - joeynmt.training - Epoch  19, Step:   262800, Batch Loss:     0.828917, Tokens per Sec:     5022, Lr: 0.000300\n",
      "2021-10-10 01:56:16,674 - INFO - joeynmt.training - Epoch  19: total training loss 12098.53\n",
      "2021-10-10 01:56:16,675 - INFO - joeynmt.training - EPOCH 20\n",
      "2021-10-10 01:56:23,405 - INFO - joeynmt.training - Epoch  20, Step:   262900, Batch Loss:     0.929250, Tokens per Sec:     3992, Lr: 0.000300\n",
      "2021-10-10 01:57:08,486 - INFO - joeynmt.training - Epoch  20, Step:   263000, Batch Loss:     0.920011, Tokens per Sec:     4965, Lr: 0.000300\n",
      "2021-10-10 01:57:52,992 - INFO - joeynmt.training - Epoch  20, Step:   263100, Batch Loss:     0.988299, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2021-10-10 01:58:37,606 - INFO - joeynmt.training - Epoch  20, Step:   263200, Batch Loss:     1.090170, Tokens per Sec:     4973, Lr: 0.000300\n",
      "2021-10-10 01:59:22,695 - INFO - joeynmt.training - Epoch  20, Step:   263300, Batch Loss:     0.878261, Tokens per Sec:     5033, Lr: 0.000300\n",
      "2021-10-10 02:00:07,324 - INFO - joeynmt.training - Epoch  20, Step:   263400, Batch Loss:     1.016980, Tokens per Sec:     4954, Lr: 0.000300\n",
      "2021-10-10 02:00:51,467 - INFO - joeynmt.training - Epoch  20, Step:   263500, Batch Loss:     0.947716, Tokens per Sec:     4957, Lr: 0.000300\n",
      "2021-10-10 02:01:37,097 - INFO - joeynmt.training - Epoch  20, Step:   263600, Batch Loss:     0.917639, Tokens per Sec:     5100, Lr: 0.000300\n",
      "2021-10-10 02:02:22,624 - INFO - joeynmt.training - Epoch  20, Step:   263700, Batch Loss:     0.956241, Tokens per Sec:     5076, Lr: 0.000300\n",
      "2021-10-10 02:03:07,809 - INFO - joeynmt.training - Epoch  20, Step:   263800, Batch Loss:     0.923558, Tokens per Sec:     5104, Lr: 0.000300\n",
      "2021-10-10 02:03:52,836 - INFO - joeynmt.training - Epoch  20, Step:   263900, Batch Loss:     0.988044, Tokens per Sec:     4933, Lr: 0.000300\n",
      "2021-10-10 02:04:38,198 - INFO - joeynmt.training - Epoch  20, Step:   264000, Batch Loss:     0.900500, Tokens per Sec:     5049, Lr: 0.000300\n",
      "2021-10-10 02:05:23,314 - INFO - joeynmt.training - Epoch  20, Step:   264100, Batch Loss:     0.950272, Tokens per Sec:     4983, Lr: 0.000300\n",
      "2021-10-10 02:06:07,871 - INFO - joeynmt.training - Epoch  20, Step:   264200, Batch Loss:     0.944289, Tokens per Sec:     5020, Lr: 0.000300\n",
      "2021-10-10 02:06:53,333 - INFO - joeynmt.training - Epoch  20, Step:   264300, Batch Loss:     0.983835, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-10 02:07:39,075 - INFO - joeynmt.training - Epoch  20, Step:   264400, Batch Loss:     0.907941, Tokens per Sec:     5127, Lr: 0.000300\n",
      "2021-10-10 02:08:23,790 - INFO - joeynmt.training - Epoch  20, Step:   264500, Batch Loss:     1.009773, Tokens per Sec:     4941, Lr: 0.000300\n",
      "2021-10-10 02:09:08,554 - INFO - joeynmt.training - Epoch  20, Step:   264600, Batch Loss:     0.890568, Tokens per Sec:     4960, Lr: 0.000300\n",
      "2021-10-10 02:09:53,594 - INFO - joeynmt.training - Epoch  20, Step:   264700, Batch Loss:     0.941947, Tokens per Sec:     5010, Lr: 0.000300\n",
      "2021-10-10 02:10:38,954 - INFO - joeynmt.training - Epoch  20, Step:   264800, Batch Loss:     0.915378, Tokens per Sec:     5055, Lr: 0.000300\n",
      "2021-10-10 02:11:24,219 - INFO - joeynmt.training - Epoch  20, Step:   264900, Batch Loss:     1.020917, Tokens per Sec:     5014, Lr: 0.000300\n",
      "2021-10-10 02:12:09,164 - INFO - joeynmt.training - Epoch  20, Step:   265000, Batch Loss:     0.921950, Tokens per Sec:     4966, Lr: 0.000300\n",
      "2021-10-10 02:13:37,500 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-10 02:13:37,501 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-10 02:13:37,501 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-10 02:13:37,841 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-10 02:13:37,841 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-10 02:13:43,873 - INFO - joeynmt.training - Example #0\n",
      "2021-10-10 02:13:43,873 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-10 02:13:43,874 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-10 02:13:43,874 - INFO - joeynmt.training - \tHypothesis: Father was the group servant , the term used at that time for the brother who took the lead in a congregation .\n",
      "2021-10-10 02:13:43,874 - INFO - joeynmt.training - Example #1\n",
      "2021-10-10 02:13:43,874 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-10 02:13:43,874 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-10 02:13:43,874 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know oneself and expresses feelings in a way that speaks to others and touches them .\n",
      "2021-10-10 02:13:43,874 - INFO - joeynmt.training - Example #2\n",
      "2021-10-10 02:13:43,874 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-10 02:13:43,875 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-10 02:13:43,875 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-10 02:13:43,875 - INFO - joeynmt.training - Example #3\n",
      "2021-10-10 02:13:43,875 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-10 02:13:43,875 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-10 02:13:43,875 - INFO - joeynmt.training - \tHypothesis: HOW DO WE SHOULD WE STECTS DOLVITS ?\n",
      "2021-10-10 02:13:43,875 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step   265000: bleu:  50.95, loss: 26370.6543, ppl:   2.3911, duration: 94.7107s\n",
      "2021-10-10 02:14:29,034 - INFO - joeynmt.training - Epoch  20, Step:   265100, Batch Loss:     0.897383, Tokens per Sec:     5036, Lr: 0.000300\n",
      "2021-10-10 02:15:13,336 - INFO - joeynmt.training - Epoch  20, Step:   265200, Batch Loss:     0.982197, Tokens per Sec:     5002, Lr: 0.000300\n",
      "2021-10-10 02:15:58,686 - INFO - joeynmt.training - Epoch  20, Step:   265300, Batch Loss:     0.825569, Tokens per Sec:     5031, Lr: 0.000300\n",
      "2021-10-10 02:16:44,077 - INFO - joeynmt.training - Epoch  20, Step:   265400, Batch Loss:     0.963911, Tokens per Sec:     5034, Lr: 0.000300\n",
      "2021-10-10 02:17:28,520 - INFO - joeynmt.training - Epoch  20, Step:   265500, Batch Loss:     0.873606, Tokens per Sec:     4942, Lr: 0.000300\n",
      "2021-10-10 02:18:14,160 - INFO - joeynmt.training - Epoch  20, Step:   265600, Batch Loss:     0.959573, Tokens per Sec:     5058, Lr: 0.000300\n",
      "2021-10-10 02:18:59,699 - INFO - joeynmt.training - Epoch  20, Step:   265700, Batch Loss:     0.935733, Tokens per Sec:     5100, Lr: 0.000300\n",
      "2021-10-10 02:19:44,676 - INFO - joeynmt.training - Epoch  20, Step:   265800, Batch Loss:     1.021258, Tokens per Sec:     5060, Lr: 0.000300\n",
      "2021-10-10 02:20:29,804 - INFO - joeynmt.training - Epoch  20, Step:   265900, Batch Loss:     0.940507, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2021-10-10 02:21:14,675 - INFO - joeynmt.training - Epoch  20, Step:   266000, Batch Loss:     0.917180, Tokens per Sec:     4944, Lr: 0.000300\n",
      "2021-10-10 02:21:59,552 - INFO - joeynmt.training - Epoch  20, Step:   266100, Batch Loss:     1.056449, Tokens per Sec:     4957, Lr: 0.000300\n",
      "2021-10-10 02:22:44,181 - INFO - joeynmt.training - Epoch  20, Step:   266200, Batch Loss:     0.956356, Tokens per Sec:     4994, Lr: 0.000300\n",
      "2021-10-10 02:23:29,441 - INFO - joeynmt.training - Epoch  20, Step:   266300, Batch Loss:     0.891516, Tokens per Sec:     5100, Lr: 0.000300\n",
      "2021-10-10 02:24:14,909 - INFO - joeynmt.training - Epoch  20, Step:   266400, Batch Loss:     0.870000, Tokens per Sec:     5134, Lr: 0.000300\n",
      "2021-10-10 02:24:59,806 - INFO - joeynmt.training - Epoch  20, Step:   266500, Batch Loss:     1.028408, Tokens per Sec:     4977, Lr: 0.000300\n",
      "2021-10-10 02:25:45,120 - INFO - joeynmt.training - Epoch  20, Step:   266600, Batch Loss:     0.944398, Tokens per Sec:     5134, Lr: 0.000300\n",
      "2021-10-10 02:26:30,102 - INFO - joeynmt.training - Epoch  20, Step:   266700, Batch Loss:     0.918529, Tokens per Sec:     4984, Lr: 0.000300\n",
      "2021-10-10 02:27:14,884 - INFO - joeynmt.training - Epoch  20, Step:   266800, Batch Loss:     0.946839, Tokens per Sec:     4949, Lr: 0.000300\n",
      "2021-10-10 02:28:00,044 - INFO - joeynmt.training - Epoch  20, Step:   266900, Batch Loss:     1.013955, Tokens per Sec:     5072, Lr: 0.000300\n",
      "2021-10-10 02:28:45,130 - INFO - joeynmt.training - Epoch  20, Step:   267000, Batch Loss:     1.010285, Tokens per Sec:     4971, Lr: 0.000300\n",
      "2021-10-10 02:29:29,209 - INFO - joeynmt.training - Epoch  20, Step:   267100, Batch Loss:     0.490172, Tokens per Sec:     4975, Lr: 0.000300\n",
      "2021-10-10 02:30:14,721 - INFO - joeynmt.training - Epoch  20, Step:   267200, Batch Loss:     0.930758, Tokens per Sec:     5076, Lr: 0.000300\n",
      "2021-10-10 02:31:00,043 - INFO - joeynmt.training - Epoch  20, Step:   267300, Batch Loss:     0.923333, Tokens per Sec:     5095, Lr: 0.000300\n",
      "2021-10-10 02:31:45,284 - INFO - joeynmt.training - Epoch  20, Step:   267400, Batch Loss:     0.959016, Tokens per Sec:     4995, Lr: 0.000300\n",
      "2021-10-10 02:32:30,884 - INFO - joeynmt.training - Epoch  20, Step:   267500, Batch Loss:     0.963068, Tokens per Sec:     5142, Lr: 0.000300\n",
      "2021-10-10 02:33:15,652 - INFO - joeynmt.training - Epoch  20, Step:   267600, Batch Loss:     0.951913, Tokens per Sec:     5009, Lr: 0.000300\n",
      "2021-10-10 02:34:00,668 - INFO - joeynmt.training - Epoch  20, Step:   267700, Batch Loss:     1.087047, Tokens per Sec:     4957, Lr: 0.000300\n",
      "2021-10-10 02:34:45,776 - INFO - joeynmt.training - Epoch  20, Step:   267800, Batch Loss:     1.066736, Tokens per Sec:     5030, Lr: 0.000300\n",
      "2021-10-10 02:35:30,581 - INFO - joeynmt.training - Epoch  20, Step:   267900, Batch Loss:     0.991402, Tokens per Sec:     4923, Lr: 0.000300\n",
      "2021-10-10 02:36:15,862 - INFO - joeynmt.training - Epoch  20, Step:   268000, Batch Loss:     1.000478, Tokens per Sec:     4987, Lr: 0.000300\n",
      "2021-10-10 02:37:01,236 - INFO - joeynmt.training - Epoch  20, Step:   268100, Batch Loss:     0.949293, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-10-10 02:37:46,655 - INFO - joeynmt.training - Epoch  20, Step:   268200, Batch Loss:     0.927456, Tokens per Sec:     4986, Lr: 0.000300\n",
      "2021-10-10 02:38:32,218 - INFO - joeynmt.training - Epoch  20, Step:   268300, Batch Loss:     0.943291, Tokens per Sec:     5063, Lr: 0.000300\n",
      "2021-10-10 02:39:17,354 - INFO - joeynmt.training - Epoch  20, Step:   268400, Batch Loss:     0.988350, Tokens per Sec:     4980, Lr: 0.000300\n",
      "2021-10-10 02:40:01,785 - INFO - joeynmt.training - Epoch  20, Step:   268500, Batch Loss:     0.977581, Tokens per Sec:     4900, Lr: 0.000300\n",
      "2021-10-10 02:40:46,957 - INFO - joeynmt.training - Epoch  20, Step:   268600, Batch Loss:     1.027051, Tokens per Sec:     4973, Lr: 0.000300\n",
      "2021-10-10 02:41:32,333 - INFO - joeynmt.training - Epoch  20, Step:   268700, Batch Loss:     0.809290, Tokens per Sec:     5082, Lr: 0.000300\n",
      "2021-10-10 02:42:17,573 - INFO - joeynmt.training - Epoch  20, Step:   268800, Batch Loss:     0.844414, Tokens per Sec:     5040, Lr: 0.000300\n",
      "2021-10-10 02:43:02,739 - INFO - joeynmt.training - Epoch  20, Step:   268900, Batch Loss:     0.957739, Tokens per Sec:     5113, Lr: 0.000300\n",
      "2021-10-10 02:43:47,724 - INFO - joeynmt.training - Epoch  20, Step:   269000, Batch Loss:     0.988482, Tokens per Sec:     4977, Lr: 0.000300\n",
      "2021-10-10 02:44:33,093 - INFO - joeynmt.training - Epoch  20, Step:   269100, Batch Loss:     1.012797, Tokens per Sec:     4990, Lr: 0.000300\n",
      "2021-10-10 02:45:17,642 - INFO - joeynmt.training - Epoch  20, Step:   269200, Batch Loss:     1.000921, Tokens per Sec:     5046, Lr: 0.000300\n",
      "2021-10-10 02:46:02,700 - INFO - joeynmt.training - Epoch  20, Step:   269300, Batch Loss:     0.880180, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-10 02:46:47,713 - INFO - joeynmt.training - Epoch  20, Step:   269400, Batch Loss:     0.834685, Tokens per Sec:     5045, Lr: 0.000300\n",
      "2021-10-10 02:47:32,358 - INFO - joeynmt.training - Epoch  20, Step:   269500, Batch Loss:     0.859745, Tokens per Sec:     4940, Lr: 0.000300\n",
      "2021-10-10 02:48:17,992 - INFO - joeynmt.training - Epoch  20, Step:   269600, Batch Loss:     0.984301, Tokens per Sec:     5102, Lr: 0.000300\n",
      "2021-10-10 02:49:02,999 - INFO - joeynmt.training - Epoch  20, Step:   269700, Batch Loss:     1.144508, Tokens per Sec:     4968, Lr: 0.000300\n",
      "2021-10-10 02:49:47,965 - INFO - joeynmt.training - Epoch  20, Step:   269800, Batch Loss:     0.966546, Tokens per Sec:     5068, Lr: 0.000300\n",
      "2021-10-10 02:50:33,218 - INFO - joeynmt.training - Epoch  20, Step:   269900, Batch Loss:     0.925532, Tokens per Sec:     5026, Lr: 0.000300\n",
      "2021-10-10 02:51:18,341 - INFO - joeynmt.training - Epoch  20, Step:   270000, Batch Loss:     0.986368, Tokens per Sec:     5029, Lr: 0.000300\n",
      "2021-10-10 02:52:45,037 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-10 02:52:45,038 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-10 02:52:45,038 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-10 02:52:45,377 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-10 02:52:45,377 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-10 02:52:51,448 - INFO - joeynmt.training - Example #0\n",
      "2021-10-10 02:52:51,449 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-10 02:52:51,449 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-10 02:52:51,449 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at that time for the brother who took the lead in a congregation .\n",
      "2021-10-10 02:52:51,449 - INFO - joeynmt.training - Example #1\n",
      "2021-10-10 02:52:51,450 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-10 02:52:51,450 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-10 02:52:51,450 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one gets to know oneself and expresses feelings in a way that speaks to others and touches them .\n",
      "2021-10-10 02:52:51,450 - INFO - joeynmt.training - Example #2\n",
      "2021-10-10 02:52:51,450 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-10 02:52:51,450 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-10 02:52:51,450 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to recognize respectfully that Nabal was older than he was .\n",
      "2021-10-10 02:52:51,450 - INFO - joeynmt.training - Example #3\n",
      "2021-10-10 02:52:51,451 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-10 02:52:51,451 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-10 02:52:51,451 - INFO - joeynmt.training - \tHypothesis: HOW DO WE SHOULD WE STECTS HAVE DOELVITS ?\n",
      "2021-10-10 02:52:51,451 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step   270000: bleu:  51.06, loss: 26325.9883, ppl:   2.3876, duration: 93.1094s\n",
      "2021-10-10 02:53:37,143 - INFO - joeynmt.training - Epoch  20, Step:   270100, Batch Loss:     0.997227, Tokens per Sec:     5070, Lr: 0.000300\n",
      "2021-10-10 02:54:22,116 - INFO - joeynmt.training - Epoch  20, Step:   270200, Batch Loss:     1.025481, Tokens per Sec:     5004, Lr: 0.000300\n",
      "2021-10-10 02:55:08,057 - INFO - joeynmt.training - Epoch  20, Step:   270300, Batch Loss:     1.087268, Tokens per Sec:     5163, Lr: 0.000300\n",
      "2021-10-10 02:55:53,078 - INFO - joeynmt.training - Epoch  20, Step:   270400, Batch Loss:     0.936678, Tokens per Sec:     5035, Lr: 0.000300\n",
      "2021-10-10 02:56:38,355 - INFO - joeynmt.training - Epoch  20, Step:   270500, Batch Loss:     0.999675, Tokens per Sec:     4982, Lr: 0.000300\n",
      "2021-10-10 02:57:22,943 - INFO - joeynmt.training - Epoch  20, Step:   270600, Batch Loss:     0.878446, Tokens per Sec:     4961, Lr: 0.000300\n",
      "2021-10-10 02:58:08,466 - INFO - joeynmt.training - Epoch  20, Step:   270700, Batch Loss:     1.029199, Tokens per Sec:     5114, Lr: 0.000300\n",
      "2021-10-10 02:58:53,390 - INFO - joeynmt.training - Epoch  20, Step:   270800, Batch Loss:     0.864977, Tokens per Sec:     4945, Lr: 0.000300\n",
      "2021-10-10 02:59:38,817 - INFO - joeynmt.training - Epoch  20, Step:   270900, Batch Loss:     0.948179, Tokens per Sec:     4986, Lr: 0.000300\n",
      "2021-10-10 03:00:23,999 - INFO - joeynmt.training - Epoch  20, Step:   271000, Batch Loss:     1.000820, Tokens per Sec:     4943, Lr: 0.000300\n",
      "2021-10-10 03:01:08,959 - INFO - joeynmt.training - Epoch  20, Step:   271100, Batch Loss:     0.824640, Tokens per Sec:     4934, Lr: 0.000300\n",
      "2021-10-10 03:01:54,268 - INFO - joeynmt.training - Epoch  20, Step:   271200, Batch Loss:     0.954927, Tokens per Sec:     4952, Lr: 0.000300\n",
      "2021-10-10 03:02:39,340 - INFO - joeynmt.training - Epoch  20, Step:   271300, Batch Loss:     1.070314, Tokens per Sec:     4988, Lr: 0.000300\n",
      "2021-10-10 03:03:24,010 - INFO - joeynmt.training - Epoch  20, Step:   271400, Batch Loss:     0.965792, Tokens per Sec:     4969, Lr: 0.000300\n",
      "2021-10-10 03:04:09,190 - INFO - joeynmt.training - Epoch  20, Step:   271500, Batch Loss:     0.962773, Tokens per Sec:     4927, Lr: 0.000300\n",
      "2021-10-10 03:04:54,680 - INFO - joeynmt.training - Epoch  20, Step:   271600, Batch Loss:     1.013063, Tokens per Sec:     5116, Lr: 0.000300\n",
      "2021-10-10 03:05:39,933 - INFO - joeynmt.training - Epoch  20, Step:   271700, Batch Loss:     1.009001, Tokens per Sec:     5039, Lr: 0.000300\n",
      "2021-10-10 03:06:25,599 - INFO - joeynmt.training - Epoch  20, Step:   271800, Batch Loss:     0.914895, Tokens per Sec:     5077, Lr: 0.000300\n",
      "2021-10-10 03:07:11,037 - INFO - joeynmt.training - Epoch  20, Step:   271900, Batch Loss:     0.960531, Tokens per Sec:     5070, Lr: 0.000300\n",
      "2021-10-10 03:07:55,885 - INFO - joeynmt.training - Epoch  20, Step:   272000, Batch Loss:     1.062190, Tokens per Sec:     4999, Lr: 0.000300\n",
      "2021-10-10 03:08:40,755 - INFO - joeynmt.training - Epoch  20, Step:   272100, Batch Loss:     0.880726, Tokens per Sec:     4978, Lr: 0.000300\n",
      "2021-10-10 03:09:25,921 - INFO - joeynmt.training - Epoch  20, Step:   272200, Batch Loss:     0.889223, Tokens per Sec:     4996, Lr: 0.000300\n",
      "2021-10-10 03:10:11,442 - INFO - joeynmt.training - Epoch  20, Step:   272300, Batch Loss:     0.950958, Tokens per Sec:     5080, Lr: 0.000300\n",
      "2021-10-10 03:10:56,062 - INFO - joeynmt.training - Epoch  20, Step:   272400, Batch Loss:     0.954755, Tokens per Sec:     4941, Lr: 0.000300\n",
      "2021-10-10 03:11:40,885 - INFO - joeynmt.training - Epoch  20, Step:   272500, Batch Loss:     0.960679, Tokens per Sec:     5052, Lr: 0.000300\n",
      "2021-10-10 03:12:26,385 - INFO - joeynmt.training - Epoch  20, Step:   272600, Batch Loss:     0.915428, Tokens per Sec:     5066, Lr: 0.000300\n",
      "2021-10-10 03:13:11,601 - INFO - joeynmt.training - Epoch  20, Step:   272700, Batch Loss:     1.033835, Tokens per Sec:     4952, Lr: 0.000300\n",
      "2021-10-10 03:13:56,249 - INFO - joeynmt.training - Epoch  20, Step:   272800, Batch Loss:     0.886294, Tokens per Sec:     4942, Lr: 0.000300\n",
      "2021-10-10 03:14:41,515 - INFO - joeynmt.training - Epoch  20, Step:   272900, Batch Loss:     1.139931, Tokens per Sec:     5031, Lr: 0.000300\n",
      "2021-10-10 03:15:27,110 - INFO - joeynmt.training - Epoch  20, Step:   273000, Batch Loss:     1.096712, Tokens per Sec:     5092, Lr: 0.000300\n",
      "2021-10-10 03:16:12,130 - INFO - joeynmt.training - Epoch  20, Step:   273100, Batch Loss:     1.034668, Tokens per Sec:     5022, Lr: 0.000300\n",
      "2021-10-10 03:16:56,983 - INFO - joeynmt.training - Epoch  20, Step:   273200, Batch Loss:     0.900375, Tokens per Sec:     5014, Lr: 0.000300\n",
      "2021-10-10 03:17:42,603 - INFO - joeynmt.training - Epoch  20, Step:   273300, Batch Loss:     0.977886, Tokens per Sec:     5125, Lr: 0.000300\n",
      "2021-10-10 03:18:28,166 - INFO - joeynmt.training - Epoch  20, Step:   273400, Batch Loss:     0.986362, Tokens per Sec:     5136, Lr: 0.000300\n",
      "2021-10-10 03:19:13,789 - INFO - joeynmt.training - Epoch  20, Step:   273500, Batch Loss:     0.941678, Tokens per Sec:     5033, Lr: 0.000300\n",
      "2021-10-10 03:19:58,834 - INFO - joeynmt.training - Epoch  20, Step:   273600, Batch Loss:     1.043176, Tokens per Sec:     4946, Lr: 0.000300\n",
      "2021-10-10 03:20:44,065 - INFO - joeynmt.training - Epoch  20, Step:   273700, Batch Loss:     1.019292, Tokens per Sec:     5008, Lr: 0.000300\n",
      "2021-10-10 03:21:28,711 - INFO - joeynmt.training - Epoch  20, Step:   273800, Batch Loss:     1.039828, Tokens per Sec:     4975, Lr: 0.000300\n",
      "2021-10-10 03:22:13,369 - INFO - joeynmt.training - Epoch  20, Step:   273900, Batch Loss:     1.299169, Tokens per Sec:     5044, Lr: 0.000300\n",
      "2021-10-10 03:22:58,953 - INFO - joeynmt.training - Epoch  20, Step:   274000, Batch Loss:     0.991677, Tokens per Sec:     5049, Lr: 0.000300\n",
      "2021-10-10 03:23:44,264 - INFO - joeynmt.training - Epoch  20, Step:   274100, Batch Loss:     0.894020, Tokens per Sec:     4993, Lr: 0.000300\n",
      "2021-10-10 03:24:29,467 - INFO - joeynmt.training - Epoch  20, Step:   274200, Batch Loss:     1.064754, Tokens per Sec:     4957, Lr: 0.000300\n",
      "2021-10-10 03:25:14,538 - INFO - joeynmt.training - Epoch  20, Step:   274300, Batch Loss:     0.864185, Tokens per Sec:     4993, Lr: 0.000300\n",
      "2021-10-10 03:26:00,456 - INFO - joeynmt.training - Epoch  20, Step:   274400, Batch Loss:     1.086383, Tokens per Sec:     5129, Lr: 0.000300\n",
      "2021-10-10 03:26:45,790 - INFO - joeynmt.training - Epoch  20, Step:   274500, Batch Loss:     0.969036, Tokens per Sec:     5004, Lr: 0.000300\n",
      "2021-10-10 03:27:31,233 - INFO - joeynmt.training - Epoch  20, Step:   274600, Batch Loss:     1.091075, Tokens per Sec:     5047, Lr: 0.000300\n",
      "2021-10-10 03:28:16,655 - INFO - joeynmt.training - Epoch  20, Step:   274700, Batch Loss:     0.978918, Tokens per Sec:     5123, Lr: 0.000300\n",
      "2021-10-10 03:29:01,861 - INFO - joeynmt.training - Epoch  20, Step:   274800, Batch Loss:     1.048026, Tokens per Sec:     5052, Lr: 0.000300\n",
      "2021-10-10 03:29:46,653 - INFO - joeynmt.training - Epoch  20, Step:   274900, Batch Loss:     0.933207, Tokens per Sec:     4992, Lr: 0.000300\n",
      "2021-10-10 03:30:31,563 - INFO - joeynmt.training - Epoch  20, Step:   275000, Batch Loss:     1.076032, Tokens per Sec:     4884, Lr: 0.000300\n",
      "2021-10-10 03:31:57,490 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-10 03:31:57,491 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-10 03:31:57,491 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-10 03:31:57,828 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-10-10 03:31:57,829 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-10-10 03:32:04,043 - INFO - joeynmt.training - Example #0\n",
      "2021-10-10 03:32:04,044 - INFO - joeynmt.training - \tSource:     My pa was die groepkneg , die term wat destyds gebruik is vir die broer wat die leiding in ’ n gemeente geneem het .\n",
      "2021-10-10 03:32:04,044 - INFO - joeynmt.training - \tReference:  Father was the company servant , the term then used for the one taking the lead in a congregation .\n",
      "2021-10-10 03:32:04,044 - INFO - joeynmt.training - \tHypothesis: My father was the group servant , the term used at that time for the brother who took the lead in a congregation .\n",
      "2021-10-10 03:32:04,044 - INFO - joeynmt.training - Example #1\n",
      "2021-10-10 03:32:04,044 - INFO - joeynmt.training - \tSource:     Die aantrekkingskrag is verstaanbaar , want adolessensie is ’ n tyd wanneer ’ n mens jouself leer ken en jou gevoelens op ’ n manier uitdruk wat tot ander spreek en hulle ontroer .\n",
      "2021-10-10 03:32:04,044 - INFO - joeynmt.training - \tReference:  The appeal is understandable , for adolescence is a time of learning about oneself and revealing one ’ s feelings in a way that reaches and moves others .\n",
      "2021-10-10 03:32:04,045 - INFO - joeynmt.training - \tHypothesis: The attraction is understandable , for adolescence is a time when one comes to know oneself and expresses feelings in a way that speaks to others and touches them .\n",
      "2021-10-10 03:32:04,045 - INFO - joeynmt.training - Example #2\n",
      "2021-10-10 03:32:04,045 - INFO - joeynmt.training - \tSource:     Hy het selfs die woorde “ u seun Dawid ” met verwysing na homself gebruik , moontlik om eerbiedig te erken dat Nabal ouer as hy was .\n",
      "2021-10-10 03:32:04,045 - INFO - joeynmt.training - \tReference:  He even referred to himself as “ your son David , ” perhaps a respectful acknowledgment of Nabal ’ s greater age .\n",
      "2021-10-10 03:32:04,045 - INFO - joeynmt.training - \tHypothesis: He even used the words “ your son David ” with reference to himself , perhaps to respectfully acknowledge that Nabal was older than he was .\n",
      "2021-10-10 03:32:04,045 - INFO - joeynmt.training - Example #3\n",
      "2021-10-10 03:32:04,045 - INFO - joeynmt.training - \tSource:     HOEKOM MOET ONS GEESTELIKE DOELWITTE STEL ?\n",
      "2021-10-10 03:32:04,045 - INFO - joeynmt.training - \tReference:  WHY SET SPIRITUAL GOALS ?\n",
      "2021-10-10 03:32:04,046 - INFO - joeynmt.training - \tHypothesis: HOW DO WE SHOULD WE STECTS DOELVITS ?\n",
      "2021-10-10 03:32:04,046 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step   275000: bleu:  51.38, loss: 26065.8789, ppl:   2.3671, duration: 92.4818s\n",
      "2021-10-10 03:32:49,272 - INFO - joeynmt.training - Epoch  20, Step:   275100, Batch Loss:     1.051750, Tokens per Sec:     5036, Lr: 0.000300\n",
      "2021-10-10 03:33:34,470 - INFO - joeynmt.training - Epoch  20, Step:   275200, Batch Loss:     0.940718, Tokens per Sec:     4974, Lr: 0.000300\n",
      "2021-10-10 03:34:18,870 - INFO - joeynmt.training - Epoch  20, Step:   275300, Batch Loss:     1.157552, Tokens per Sec:     4913, Lr: 0.000300\n",
      "2021-10-10 03:35:02,284 - INFO - joeynmt.training - Epoch  20: total training loss 12007.07\n",
      "2021-10-10 03:35:02,285 - INFO - joeynmt.training - Training ended after  20 epochs.\n",
      "2021-10-10 03:35:02,285 - INFO - joeynmt.training - Best validation result (greedy) at step   275000:   2.37 ppl.\n",
      "2021-10-10 03:35:02,312 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
      "2021-10-10 03:35:02,714 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-10-10 03:35:03,464 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-10-10 03:35:03,676 - INFO - joeynmt.prediction - Decoding on dev set (data/afen/dev.bpe.en)...\n",
      "2021-10-10 03:36:45,638 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-10 03:36:45,638 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-10 03:36:45,639 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-10 03:36:45,976 - INFO - joeynmt.prediction -  dev bleu[13a]:  51.47 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-10-10 03:36:45,980 - INFO - joeynmt.prediction - Translations saved to: models/afen_reverse_transformer_big-model-end1008/00275000.hyps.dev\n",
      "2021-10-10 03:36:45,981 - INFO - joeynmt.prediction - Decoding on test set (data/afen/test.bpe.en)...\n",
      "2021-10-10 03:39:02,418 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-10 03:39:02,419 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-10 03:39:02,419 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-10 03:39:03,220 - INFO - joeynmt.prediction - test bleu[13a]:  57.22 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-10-10 03:39:03,232 - INFO - joeynmt.prediction - Translations saved to: models/afen_reverse_transformer_big-model-end1008/00275000.hyps.test\n"
     ]
    }
   ],
   "source": [
    "!cat logs_run_20_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['FILE'] = 'nlp_things/masakhane/train_long_large/models/after_20_epochs/'\n",
    "! mkdir -p $FILE\n",
    "! cp -r joeynmt/models/afen_reverse_transformer_big-model-end1008/* \"$FILE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 35000\tLoss: 37326.53516\tPPL: 3.43472\tbleu: 42.65708\tLR: 0.00030000\t*\n",
      "Steps: 40000\tLoss: 36330.92578\tPPL: 3.32351\tbleu: 43.04933\tLR: 0.00030000\t*\n",
      "Steps: 45000\tLoss: 35266.66797\tPPL: 3.20862\tbleu: 44.00645\tLR: 0.00030000\t*\n",
      "Steps: 50000\tLoss: 34401.17969\tPPL: 3.11812\tbleu: 44.15336\tLR: 0.00030000\t*\n",
      "Steps: 55000\tLoss: 33955.31641\tPPL: 3.07249\tbleu: 44.80005\tLR: 0.00030000\t*\n",
      "Steps: 60000\tLoss: 33105.29688\tPPL: 2.98736\tbleu: 45.69988\tLR: 0.00030000\t*\n",
      "Steps: 65000\tLoss: 32428.82031\tPPL: 2.92130\tbleu: 46.10860\tLR: 0.00030000\t*\n",
      "Steps: 70000\tLoss: 32225.17578\tPPL: 2.90170\tbleu: 46.56415\tLR: 0.00030000\t*\n",
      "Steps: 75000\tLoss: 31921.34961\tPPL: 2.87270\tbleu: 46.65734\tLR: 0.00030000\t*\n",
      "Steps: 80000\tLoss: 31358.10742\tPPL: 2.81970\tbleu: 47.06222\tLR: 0.00030000\t*\n",
      "Steps: 85000\tLoss: 31065.81250\tPPL: 2.79259\tbleu: 47.65710\tLR: 0.00030000\t*\n",
      "Steps: 90000\tLoss: 30789.02539\tPPL: 2.76715\tbleu: 47.42006\tLR: 0.00030000\t*\n",
      "Steps: 95000\tLoss: 30523.22852\tPPL: 2.74295\tbleu: 47.44714\tLR: 0.00030000\t*\n",
      "Steps: 100000\tLoss: 30466.59766\tPPL: 2.73782\tbleu: 47.85531\tLR: 0.00030000\t*\n",
      "Steps: 105000\tLoss: 30169.74219\tPPL: 2.71108\tbleu: 48.23219\tLR: 0.00030000\t*\n",
      "Steps: 110000\tLoss: 29764.14453\tPPL: 2.67497\tbleu: 48.60606\tLR: 0.00030000\t*\n",
      "Steps: 115000\tLoss: 29571.84180\tPPL: 2.65802\tbleu: 48.50553\tLR: 0.00030000\t*\n",
      "Steps: 120000\tLoss: 29475.83984\tPPL: 2.64960\tbleu: 48.47561\tLR: 0.00030000\t*\n",
      "Steps: 125000\tLoss: 29280.60352\tPPL: 2.63255\tbleu: 48.56847\tLR: 0.00030000\t*\n",
      "Steps: 130000\tLoss: 28920.01562\tPPL: 2.60136\tbleu: 48.90298\tLR: 0.00030000\t*\n",
      "Steps: 135000\tLoss: 28881.55469\tPPL: 2.59805\tbleu: 49.08995\tLR: 0.00030000\t*\n",
      "Steps: 140000\tLoss: 28604.07227\tPPL: 2.57433\tbleu: 48.98009\tLR: 0.00030000\t*\n",
      "Steps: 145000\tLoss: 28601.52148\tPPL: 2.57411\tbleu: 49.51867\tLR: 0.00030000\t*\n",
      "Steps: 150000\tLoss: 28462.11133\tPPL: 2.56228\tbleu: 49.51232\tLR: 0.00030000\t*\n",
      "Steps: 155000\tLoss: 28400.63477\tPPL: 2.55707\tbleu: 49.18649\tLR: 0.00030000\t*\n",
      "Steps: 160000\tLoss: 28113.36133\tPPL: 2.53291\tbleu: 49.65640\tLR: 0.00030000\t*\n",
      "Steps: 165000\tLoss: 28122.86719\tPPL: 2.53370\tbleu: 49.64876\tLR: 0.00030000\t\n",
      "Steps: 170000\tLoss: 27802.93164\tPPL: 2.50705\tbleu: 50.05837\tLR: 0.00030000\t*\n",
      "Steps: 175000\tLoss: 27779.26367\tPPL: 2.50509\tbleu: 49.59280\tLR: 0.00030000\t*\n",
      "Steps: 180000\tLoss: 27813.21094\tPPL: 2.50790\tbleu: 49.38676\tLR: 0.00030000\t\n",
      "Steps: 185000\tLoss: 27587.95898\tPPL: 2.48929\tbleu: 50.01621\tLR: 0.00030000\t*\n",
      "Steps: 190000\tLoss: 27340.08203\tPPL: 2.46898\tbleu: 50.15388\tLR: 0.00030000\t*\n",
      "Steps: 195000\tLoss: 27422.40625\tPPL: 2.47571\tbleu: 49.78572\tLR: 0.00030000\t\n",
      "Steps: 200000\tLoss: 27320.04297\tPPL: 2.46734\tbleu: 50.19143\tLR: 0.00030000\t*\n",
      "Steps: 205000\tLoss: 27423.58789\tPPL: 2.47580\tbleu: 50.23807\tLR: 0.00030000\t\n",
      "Steps: 210000\tLoss: 27039.68945\tPPL: 2.44458\tbleu: 50.57919\tLR: 0.00030000\t*\n",
      "Steps: 215000\tLoss: 27115.95312\tPPL: 2.45075\tbleu: 50.58172\tLR: 0.00030000\t\n",
      "Steps: 220000\tLoss: 26916.42969\tPPL: 2.43464\tbleu: 50.73682\tLR: 0.00030000\t*\n",
      "Steps: 225000\tLoss: 26890.68945\tPPL: 2.43257\tbleu: 50.57711\tLR: 0.00030000\t*\n",
      "Steps: 230000\tLoss: 26851.59961\tPPL: 2.42943\tbleu: 50.76699\tLR: 0.00030000\t*\n",
      "Steps: 235000\tLoss: 26675.08008\tPPL: 2.41529\tbleu: 50.97329\tLR: 0.00030000\t*\n",
      "Steps: 240000\tLoss: 26683.36133\tPPL: 2.41595\tbleu: 50.71789\tLR: 0.00030000\t\n",
      "Steps: 245000\tLoss: 26413.04688\tPPL: 2.39446\tbleu: 50.92813\tLR: 0.00030000\t*\n",
      "Steps: 250000\tLoss: 26389.75586\tPPL: 2.39262\tbleu: 51.02250\tLR: 0.00030000\t*\n",
      "Steps: 255000\tLoss: 26458.31055\tPPL: 2.39805\tbleu: 50.94778\tLR: 0.00030000\t\n",
      "Steps: 260000\tLoss: 26474.23047\tPPL: 2.39931\tbleu: 51.14358\tLR: 0.00030000\t\n",
      "Steps: 265000\tLoss: 26370.65430\tPPL: 2.39111\tbleu: 50.94998\tLR: 0.00030000\t*\n",
      "Steps: 270000\tLoss: 26325.98828\tPPL: 2.38758\tbleu: 51.05649\tLR: 0.00030000\t*\n",
      "Steps: 275000\tLoss: 26065.87891\tPPL: 2.36714\tbleu: 51.38160\tLR: 0.00030000\t*\n"
     ]
    }
   ],
   "source": [
    "! cat $FILE/validations.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd joeynmt; python3 -m joeynmt test ../\"$FILE/config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-10 05:57:47,548 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-10-10 05:57:47,550 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-10-10 05:57:47,830 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-10-10 05:57:47,847 - INFO - joeynmt.data - Loading test data...\n",
      "2021-10-10 05:57:47,874 - INFO - joeynmt.data - Data loaded.\n",
      "2021-10-10 05:57:47,922 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
      "2021-10-10 05:57:50,673 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-10-10 05:57:51,476 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-10-10 05:57:51,664 - INFO - joeynmt.prediction - Decoding on dev set (data/afen/dev.bpe.en)...\n",
      "2021-10-10 05:59:31,494 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-10 05:59:31,495 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-10 05:59:31,495 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-10 05:59:31,826 - INFO - joeynmt.prediction -  dev bleu[13a]:  51.47 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-10-10 05:59:31,826 - INFO - joeynmt.prediction - Decoding on test set (data/afen/test.bpe.en)...\n",
      "2021-10-10 06:01:46,261 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-10-10 06:01:46,262 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-10-10 06:01:46,262 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-10-10 06:01:47,043 - INFO - joeynmt.prediction - test bleu[13a]:  57.22 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
     ]
    }
   ],
   "source": [
    "!cat log_test"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "starter_notebook_into_English_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
