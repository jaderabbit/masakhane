{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copie de starter_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/berthine/masakhane-mt/blob/master/benchmarks/en-kwy/jw300-baseline/berthine_en_kwy_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Igc5itf-xMGj"
      },
      "source": [
        "# Masakhane - Machine Translation for African Languages (Using JoeyNMT)\n",
        "\n",
        "### Languages: English-Kikongo\n",
        "\n",
        "### Author: Berthine Nyunga Mpinda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l929HimrxS0a"
      },
      "source": [
        "## Retrieve your data & make a parallel corpus\n",
        "\n",
        "If you are wanting to use the JW300 data referenced on the Masakhane website or in our GitHub repo, you can use `opus-tools` to convert the data into a convenient format. `opus_read` from that package provides a convenient tool for reading the native aligned XML files and to convert them to TMX format. The tool can also be used to fetch relevant files from OPUS on the fly and to filter the data as necessary. [Read the documentation](https://pypi.org/project/opustools-pkg/) for more details.\n",
        "\n",
        "Once you have your corpus files in TMX format (an xml structure which will include the sentences in your target language and your source language in a single file), we recommend reading them into a pandas dataframe. Thankfully, Jade wrote a silly `tmx2dataframe` package which converts your tmx file to a pandas dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oGRmDELn7Az0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "40d7890e-6623-415e-9eaf-0567ec577660"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cn3tgQLzUxwn",
        "colab": {}
      },
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\n",
        "import os\n",
        "source_language = \"en\"\n",
        "target_language = \"kwy\" \n",
        "lc = False  # If True, lowercase the data.\n",
        "seed = 42  # Random seed for shuffling.\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = tag\n",
        "\n",
        "# This will save it to a folder in our gdrive instead!\n",
        "!mkdir -p \"/content/drive/My Drive/masakhane/$src-$tgt-$tag\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/masakhane/%s-%s-%s\" % (source_language, target_language, tag)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kBSgJHEw7Nvx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "73f5ba59-cc81-4d29-e840-6c6834c6d761"
      },
      "source": [
        "!echo $gdrive_path"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masakhane/en-kwy-baseline\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gA75Fs9ys8Y9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "cb9fbbc3-2161-4d3d-b4d6-8f63abd964e2"
      },
      "source": [
        "# Install opus-tools\n",
        "! pip install opustools-pkg"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opustools-pkg\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/9f/e829a0cceccc603450cd18e1ff80807b6237a88d9a8df2c0bb320796e900/opustools_pkg-0.0.52-py3-none-any.whl (80kB)\n",
            "\r\u001b[K     |████                            | 10kB 22.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.2MB/s \n",
            "\u001b[?25hInstalling collected packages: opustools-pkg\n",
            "Successfully installed opustools-pkg-0.0.52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xq-tDZVks7ZD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "13794410-18d5-47e9-e924-1d33c271b0c3"
      },
      "source": [
        "# Downloading our corpus\n",
        "! opus_read -d JW300 -s $src -t $tgt -wm moses -w jw300.$src jw300.$tgt -q\n",
        "\n",
        "# extract the corpus file\n",
        "! gunzip JW300_latest_xml_$src-$tgt.xml.gz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Alignment file /proj/nlpl/data/OPUS/JW300/latest/xml/en-kwy.xml.gz not found. The following files are available for downloading:\n",
            "\n",
            "   2 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/en-kwy.xml.gz\n",
            " 263 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/en.zip\n",
            "  17 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/kwy.zip\n",
            "\n",
            " 282 MB Total size\n",
            "./JW300_latest_xml_en-kwy.xml.gz ... 100% of 2 MB\n",
            "./JW300_latest_xml_en.zip ... 100% of 263 MB\n",
            "./JW300_latest_xml_kwy.zip ... 100% of 17 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n48GDRnP8y2G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "outputId": "3791d68b-9751-493f-c185-91b554e9bb71"
      },
      "source": [
        "# Download the global test set.\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
        "  \n",
        "# And the specific test set for this language pair.\n",
        "os.environ[\"trg\"] = target_language \n",
        "os.environ[\"src\"] = source_language \n",
        "\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.en \n",
        "! mv test.en-$trg.en test.en\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.$trg \n",
        "! mv test.en-$trg.$trg test.$trg"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-23 17:35:31--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 277791 (271K) [text/plain]\n",
            "Saving to: ‘test.en-any.en’\n",
            "\n",
            "\rtest.en-any.en        0%[                    ]       0  --.-KB/s               \rtest.en-any.en      100%[===================>] 271.28K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-08-23 17:35:31 (5.18 MB/s) - ‘test.en-any.en’ saved [277791/277791]\n",
            "\n",
            "--2020-08-23 17:35:32--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-kwy.en\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 205972 (201K) [text/plain]\n",
            "Saving to: ‘test.en-kwy.en’\n",
            "\n",
            "test.en-kwy.en      100%[===================>] 201.14K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-08-23 17:35:33 (4.45 MB/s) - ‘test.en-kwy.en’ saved [205972/205972]\n",
            "\n",
            "--2020-08-23 17:35:36--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-kwy.kwy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 212465 (207K) [text/plain]\n",
            "Saving to: ‘test.en-kwy.kwy’\n",
            "\n",
            "test.en-kwy.kwy     100%[===================>] 207.49K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-08-23 17:35:36 (4.13 MB/s) - ‘test.en-kwy.kwy’ saved [212465/212465]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NqDG-CI28y2L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a0aa3803-885d-454c-8018-41f0146e6ec4"
      },
      "source": [
        "# Read the test data to filter from train and dev splits.\n",
        "# Store english portion in set for quick filtering checks.\n",
        "en_test_sents = set()\n",
        "filter_test_sents = \"test.en-any.en\"\n",
        "j = 0\n",
        "with open(filter_test_sents) as f:\n",
        "  for line in f:\n",
        "    en_test_sents.add(line.strip())\n",
        "    j += 1\n",
        "print('Loaded {} global test sentences to filter from the training/dev data.'.format(j))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 3571 global test sentences to filter from the training/dev data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6D-_PqdXGJiB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "54721fd7-acdf-4257-84e2-c52c2ee8bfb9"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive\t   JW300_latest_xml_en-kwy.xml\tsample_data\ttest.kwy\n",
            "jw300.en   JW300_latest_xml_en.zip\ttest.en\n",
            "jw300.kwy  JW300_latest_xml_kwy.zip\ttest.en-any.en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3CNdwLBCfSIl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "ec2456cd-7562-4f67-a925-e6bb42e81668"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# TMX file to dataframe\n",
        "source_file = 'jw300.' + source_language\n",
        "target_file = 'jw300.' + target_language\n",
        "\n",
        "source = []\n",
        "target = []\n",
        "skip_lines = []  # Collect the line numbers of the source portion to skip the same lines for the target portion.\n",
        "with open(source_file) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        # Skip sentences that are contained in the test set.\n",
        "        if line.strip() not in en_test_sents:\n",
        "            source.append(line.strip())\n",
        "        else:\n",
        "            skip_lines.append(i)             \n",
        "with open(target_file) as f:\n",
        "    for j, line in enumerate(f):\n",
        "        # Only add to corpus if corresponding source was not skipped.\n",
        "        if j not in skip_lines:\n",
        "            target.append(line.strip())\n",
        "    \n",
        "print('Loaded data and skipped {}/{} lines since contained in test set.'.format(len(skip_lines), i))\n",
        "    \n",
        "df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n",
        "# if you get TypeError: data argument can't be an iterator is because of your zip version run this below\n",
        "#df = pd.DataFrame(list(zip(source, target)), columns=['source_sentence', 'target_sentence'])\n",
        "df.head(10)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded data and skipped 5241/193639 lines since contained in test set.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_sentence</th>\n",
              "      <th>target_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Jehovah’s Word Is Alive</td>\n",
              "      <td>Diambu dia Yave Diamoyo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Highlights From the Book of Joshua</td>\n",
              "      <td>Nsasa za Nkand’a Yosua</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENCAMPED on the Plains of Moab in 1473 B.C.E ....</td>\n",
              "      <td>VAVA Aneyisaele balwaka muna Ndimb’a Moabe mun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Their 40 - year wilderness sojourn is about to...</td>\n",
              "      <td>( Yosua 1 : ​ 11 ) E nkangalu wa ya 40 ma mvu ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A little over two decades later , the leader J...</td>\n",
              "      <td>Vioka makumole ma mvu , o Yosua wa mfidi a Isa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>And Jehovah your God was the one who kept push...</td>\n",
              "      <td>O Yave wa Nzambi eno , yandi okubakûla vovo nw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Written by Joshua in 1450 B.C.E . , the book o...</td>\n",
              "      <td>Muna Nkand’a Yosua muna ye tusansu twamfunu tw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>As we stand at the threshold of the promised n...</td>\n",
              "      <td>Ekolo tufinamang’o kota muna nz’ampa eyi yasil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>With keen interest , then , let us give attent...</td>\n",
              "      <td>Yambula twabadika o nkand’a Yosua ye sungididi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>TO “ THE DESERT PLAINS OF JERICHO ”</td>\n",
              "      <td>MUNA “ NDIMB’A YERIKO ”</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     source_sentence                                    target_sentence\n",
              "0                            Jehovah’s Word Is Alive                            Diambu dia Yave Diamoyo\n",
              "1                 Highlights From the Book of Joshua                             Nsasa za Nkand’a Yosua\n",
              "2  ENCAMPED on the Plains of Moab in 1473 B.C.E ....  VAVA Aneyisaele balwaka muna Ndimb’a Moabe mun...\n",
              "3  Their 40 - year wilderness sojourn is about to...  ( Yosua 1 : ​ 11 ) E nkangalu wa ya 40 ma mvu ...\n",
              "4  A little over two decades later , the leader J...  Vioka makumole ma mvu , o Yosua wa mfidi a Isa...\n",
              "5  And Jehovah your God was the one who kept push...  O Yave wa Nzambi eno , yandi okubakûla vovo nw...\n",
              "6  Written by Joshua in 1450 B.C.E . , the book o...  Muna Nkand’a Yosua muna ye tusansu twamfunu tw...\n",
              "7  As we stand at the threshold of the promised n...  Ekolo tufinamang’o kota muna nz’ampa eyi yasil...\n",
              "8  With keen interest , then , let us give attent...  Yambula twabadika o nkand’a Yosua ye sungididi...\n",
              "9                TO “ THE DESERT PLAINS OF JERICHO ”                            MUNA “ NDIMB’A YERIKO ”"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hvgFJhMy6178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "b03edda1-ad0f-4049-f6ab-81f9103581ab"
      },
      "source": [
        "df.source_sentence[100]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Later , Joshua assembles all the tribes of Israel at Shechem .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "luzCebRY7FP0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1fb6bfdd-5c32-433d-fe7f-b1f9f52845da"
      },
      "source": [
        "df.target_sentence[100]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Kuna kwalanda , Yosua walunganesa makanda mawonso ma Isaele kuna Sekeme .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TM6Uy1-W7Pxw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f324ce78-c7ae-4f4f-fd68-aa21413ca597"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "188399"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YkuK3B4p2AkN"
      },
      "source": [
        "## Pre-processing and export\n",
        "\n",
        "It is generally a good idea to remove duplicate translations and conflicting translations from the corpus. In practice, these public corpora include some number of these that need to be cleaned.\n",
        "\n",
        "In addition we will split our data into dev/test/train and export to the filesystem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M_2ouEOH1_1q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "04685fd9-0142-4b82-9792-0aa91a09e2c1"
      },
      "source": [
        "# drop duplicate translations\n",
        "df_pp = df.drop_duplicates()\n",
        "\n",
        "# drop conflicting translations\n",
        "# (this is optional and something that you might want to comment out \n",
        "# depending on the size of your corpus)\n",
        "df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n",
        "df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n",
        "\n",
        "# Shuffle the data to remove bias in dev set selection.\n",
        "df_pp = df_pp.sample(frac=1, random_state=seed).reset_index(drop=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z_1BwAApEtMk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fce37adb-be16-4cbf-8c1c-9e0880dbabea"
      },
      "source": [
        "# Install fuzzy wuzzy to remove \"almost duplicate\" sentences in the\n",
        "# test and training sets.\n",
        "! pip install fuzzywuzzy\n",
        "! pip install python-Levenshtein\n",
        "import time\n",
        "from fuzzywuzzy import process\n",
        "import numpy as np\n",
        "\n",
        "# reset the index of the training set after previous filtering\n",
        "df_pp.reset_index(drop=False, inplace=True)\n",
        "\n",
        "# Remove samples from the training data set if they \"almost overlap\" with the\n",
        "# samples in the test set.\n",
        "\n",
        "# Filtering function. Adjust pad to narrow down the candidate matches to\n",
        "# within a certain length of characters of the given sample.\n",
        "def fuzzfilter(sample, candidates, pad):\n",
        "  candidates = [x for x in candidates if len(x) <= len(sample)+pad and len(x) >= len(sample)-pad] \n",
        "  if len(candidates) > 0:\n",
        "    return process.extractOne(sample, candidates)[1]\n",
        "  else:\n",
        "    return np.nan\n",
        "\n",
        "# NOTE - This might run slow depending on the size of your training set. We are\n",
        "# printing some information to help you track how long it would take. \n",
        "scores = []\n",
        "start_time = time.time()\n",
        "for idx, row in df_pp.iterrows():\n",
        "  scores.append(fuzzfilter(row['source_sentence'], list(en_test_sents), 5))\n",
        "  if idx % 1000 == 0:\n",
        "    hours, rem = divmod(time.time() - start_time, 3600)\n",
        "    minutes, seconds = divmod(rem, 60)\n",
        "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds), \"%0.2f percent complete\" % (100.0*float(idx)/float(len(df_pp))))\n",
        "\n",
        "# Filter out \"almost overlapping samples\"\n",
        "df_pp['scores'] = scores\n",
        "df_pp = df_pp[df_pp['scores'] < 95]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (49.2.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144796 sha256=660c97b30f99599e162882e9329a5acbbc133875c032467abfccdf8235973176\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.0\n",
            "00:00:00.08 0.00 percent complete\n",
            "00:00:24.81 0.59 percent complete\n",
            "00:00:49.43 1.17 percent complete\n",
            "00:01:15.32 1.76 percent complete\n",
            "00:01:40.78 2.34 percent complete\n",
            "00:02:05.72 2.93 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '⇩']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:02:30.47 3.51 percent complete\n",
            "00:02:55.23 4.10 percent complete\n",
            "00:03:19.60 4.68 percent complete\n",
            "00:03:44.79 5.27 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:04:09.32 5.85 percent complete\n",
            "00:04:33.91 6.44 percent complete\n",
            "00:04:58.29 7.02 percent complete\n",
            "00:05:23.59 7.61 percent complete\n",
            "00:05:48.19 8.20 percent complete\n",
            "00:06:12.17 8.78 percent complete\n",
            "00:06:36.90 9.37 percent complete\n",
            "00:07:00.78 9.95 percent complete\n",
            "00:07:25.35 10.54 percent complete\n",
            "00:07:50.91 11.12 percent complete\n",
            "00:08:15.51 11.71 percent complete\n",
            "00:08:39.43 12.29 percent complete\n",
            "00:09:04.19 12.88 percent complete\n",
            "00:09:29.10 13.46 percent complete\n",
            "00:09:53.19 14.05 percent complete\n",
            "00:10:17.70 14.63 percent complete\n",
            "00:10:41.99 15.22 percent complete\n",
            "00:11:06.92 15.80 percent complete\n",
            "00:11:31.73 16.39 percent complete\n",
            "00:11:56.16 16.98 percent complete\n",
            "00:12:20.76 17.56 percent complete\n",
            "00:12:45.79 18.15 percent complete\n",
            "00:13:11.18 18.73 percent complete\n",
            "00:13:35.50 19.32 percent complete\n",
            "00:13:59.53 19.90 percent complete\n",
            "00:14:25.25 20.49 percent complete\n",
            "00:14:49.75 21.07 percent complete\n",
            "00:15:15.11 21.66 percent complete\n",
            "00:15:39.30 22.24 percent complete\n",
            "00:16:04.32 22.83 percent complete\n",
            "00:16:28.30 23.41 percent complete\n",
            "00:16:52.89 24.00 percent complete\n",
            "00:17:17.46 24.59 percent complete\n",
            "00:17:43.05 25.17 percent complete\n",
            "00:18:07.47 25.76 percent complete\n",
            "00:18:32.59 26.34 percent complete\n",
            "00:18:56.89 26.93 percent complete\n",
            "00:19:21.05 27.51 percent complete\n",
            "00:19:45.98 28.10 percent complete\n",
            "00:20:10.32 28.68 percent complete\n",
            "00:20:35.36 29.27 percent complete\n",
            "00:21:01.79 29.85 percent complete\n",
            "00:21:26.03 30.44 percent complete\n",
            "00:21:50.37 31.02 percent complete\n",
            "00:22:14.40 31.61 percent complete\n",
            "00:22:39.21 32.20 percent complete\n",
            "00:23:04.25 32.78 percent complete\n",
            "00:23:28.29 33.37 percent complete\n",
            "00:23:52.42 33.95 percent complete\n",
            "00:24:17.95 34.54 percent complete\n",
            "00:24:42.33 35.12 percent complete\n",
            "00:25:07.02 35.71 percent complete\n",
            "00:25:31.85 36.29 percent complete\n",
            "00:25:56.04 36.88 percent complete\n",
            "00:26:20.55 37.46 percent complete\n",
            "00:26:45.12 38.05 percent complete\n",
            "00:27:10.52 38.63 percent complete\n",
            "00:27:35.90 39.22 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '․ ․ ․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:28:00.45 39.80 percent complete\n",
            "00:28:25.12 40.39 percent complete\n",
            "00:28:49.95 40.98 percent complete\n",
            "00:29:13.62 41.56 percent complete\n",
            "00:29:37.46 42.15 percent complete\n",
            "00:30:01.60 42.73 percent complete\n",
            "00:30:26.17 43.32 percent complete\n",
            "00:30:52.65 43.90 percent complete\n",
            "00:31:17.02 44.49 percent complete\n",
            "00:31:40.99 45.07 percent complete\n",
            "00:32:05.46 45.66 percent complete\n",
            "00:32:29.98 46.24 percent complete\n",
            "00:32:54.56 46.83 percent complete\n",
            "00:33:18.75 47.41 percent complete\n",
            "00:33:42.41 48.00 percent complete\n",
            "00:34:07.22 48.59 percent complete\n",
            "00:34:31.20 49.17 percent complete\n",
            "00:34:56.26 49.76 percent complete\n",
            "00:35:20.78 50.34 percent complete\n",
            "00:35:45.23 50.93 percent complete\n",
            "00:36:09.72 51.51 percent complete\n",
            "00:36:33.63 52.10 percent complete\n",
            "00:36:57.67 52.68 percent complete\n",
            "00:37:22.16 53.27 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:37:46.71 53.85 percent complete\n",
            "00:38:10.85 54.44 percent complete\n",
            "00:38:34.51 55.02 percent complete\n",
            "00:38:59.36 55.61 percent complete\n",
            "00:39:23.85 56.20 percent complete\n",
            "00:39:47.87 56.78 percent complete\n",
            "00:40:13.02 57.37 percent complete\n",
            "00:40:38.29 57.95 percent complete\n",
            "00:41:02.88 58.54 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '*']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:41:27.90 59.12 percent complete\n",
            "00:41:52.33 59.71 percent complete\n",
            "00:42:17.24 60.29 percent complete\n",
            "00:42:41.63 60.88 percent complete\n",
            "00:43:06.17 61.46 percent complete\n",
            "00:43:31.04 62.05 percent complete\n",
            "00:43:54.91 62.63 percent complete\n",
            "00:44:20.43 63.22 percent complete\n",
            "00:44:44.52 63.81 percent complete\n",
            "00:45:09.14 64.39 percent complete\n",
            "00:45:32.88 64.98 percent complete\n",
            "00:45:56.96 65.56 percent complete\n",
            "00:46:21.50 66.15 percent complete\n",
            "00:46:45.81 66.73 percent complete\n",
            "00:47:10.36 67.32 percent complete\n",
            "00:47:36.27 67.90 percent complete\n",
            "00:47:59.92 68.49 percent complete\n",
            "00:48:24.85 69.07 percent complete\n",
            "00:48:49.40 69.66 percent complete\n",
            "00:49:13.56 70.24 percent complete\n",
            "00:49:38.38 70.83 percent complete\n",
            "00:50:03.28 71.41 percent complete\n",
            "00:50:27.58 72.00 percent complete\n",
            "00:50:52.89 72.59 percent complete\n",
            "00:51:17.20 73.17 percent complete\n",
            "00:51:41.96 73.76 percent complete\n",
            "00:52:06.94 74.34 percent complete\n",
            "00:52:30.92 74.93 percent complete\n",
            "00:52:55.36 75.51 percent complete\n",
            "00:53:19.43 76.10 percent complete\n",
            "00:53:44.58 76.68 percent complete\n",
            "00:54:09.84 77.27 percent complete\n",
            "00:54:33.94 77.85 percent complete\n",
            "00:54:58.39 78.44 percent complete\n",
            "00:55:22.38 79.02 percent complete\n",
            "00:55:46.99 79.61 percent complete\n",
            "00:56:11.16 80.20 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '․ ․ ․ ․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "00:56:35.86 80.78 percent complete\n",
            "00:57:00.41 81.37 percent complete\n",
            "00:57:26.19 81.95 percent complete\n",
            "00:57:50.54 82.54 percent complete\n",
            "00:58:15.44 83.12 percent complete\n",
            "00:58:39.95 83.71 percent complete\n",
            "00:59:04.72 84.29 percent complete\n",
            "00:59:28.78 84.88 percent complete\n",
            "00:59:53.34 85.46 percent complete\n",
            "01:00:17.75 86.05 percent complete\n",
            "01:00:42.75 86.63 percent complete\n",
            "01:01:07.27 87.22 percent complete\n",
            "01:01:31.51 87.81 percent complete\n",
            "01:01:55.87 88.39 percent complete\n",
            "01:02:20.11 88.98 percent complete\n",
            "01:02:44.34 89.56 percent complete\n",
            "01:03:08.83 90.15 percent complete\n",
            "01:03:33.38 90.73 percent complete\n",
            "01:03:58.60 91.32 percent complete\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '․ ․ ․ ․ ․ ․']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01:04:23.47 91.90 percent complete\n",
            "01:04:48.53 92.49 percent complete\n",
            "01:05:13.32 93.07 percent complete\n",
            "01:05:37.43 93.66 percent complete\n",
            "01:06:01.90 94.24 percent complete\n",
            "01:06:25.31 94.83 percent complete\n",
            "01:06:49.66 95.41 percent complete\n",
            "01:07:15.45 96.00 percent complete\n",
            "01:07:40.10 96.59 percent complete\n",
            "01:08:04.11 97.17 percent complete\n",
            "01:08:28.66 97.76 percent complete\n",
            "01:08:52.23 98.34 percent complete\n",
            "01:09:17.02 98.93 percent complete\n",
            "01:09:41.18 99.51 percent complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hxxBOCA-xXhy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "outputId": "1793ac3e-021a-43be-c67d-cd5bc7d8ed07"
      },
      "source": [
        "# This section does the split between train/dev for the parallel corpora then saves them as separate files\n",
        "# We use 1000 dev test and the given test set.\n",
        "import csv\n",
        "\n",
        "# Do the split between dev/train and create parallel corpora\n",
        "num_dev_patterns = 1000\n",
        "\n",
        "# Optional: lower case the corpora - this will make it easier to generalize, but without proper casing.\n",
        "if lc:  # Julia: making lowercasing optional\n",
        "    df_pp[\"source_sentence\"] = df_pp[\"source_sentence\"].str.lower()\n",
        "    df_pp[\"target_sentence\"] = df_pp[\"target_sentence\"].str.lower()\n",
        "\n",
        "# Julia: test sets are already generated\n",
        "dev = df_pp.tail(num_dev_patterns) # Herman: Error in original\n",
        "stripped = df_pp.drop(df_pp.tail(num_dev_patterns).index)\n",
        "\n",
        "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in stripped.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "    \n",
        "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in dev.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "\n",
        "#stripped[[\"source_sentence\"]].to_csv(\"train.\"+source_language, header=False, index=False)  # Herman: Added `header=False` everywhere\n",
        "#stripped[[\"target_sentence\"]].to_csv(\"train.\"+target_language, header=False, index=False)  # Julia: Problematic handling of quotation marks.\n",
        "\n",
        "#dev[[\"source_sentence\"]].to_csv(\"dev.\"+source_language, header=False, index=False)\n",
        "#dev[[\"target_sentence\"]].to_csv(\"dev.\"+target_language, header=False, index=False)\n",
        "\n",
        "# Doublecheck the format below. There should be no extra quotation marks or weird characters.\n",
        "! head train.*\n",
        "! head dev.*"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> train.en <==\n",
            "* However , the family study is not the only time to teach children .\n",
            "However , reaching out for privileges in the congregation does mean striving to meet the qualifications set out in the Scriptures .\n",
            "How did Adam use his free will in a good way ?\n",
            "Should such situations cause you to conclude that it was a mistake to start on the journey and that you should abandon the automobile ?\n",
            "From studying the Bible , I came to understand that I have a responsibility to the Giver of life , Jehovah .\n",
            "Many people in Tuva live in remote communities that are hard to reach with the Kingdom message .\n",
            "Why might the Israelites have been fearful at seeing the glory of God that Moses reflected ?\n",
            "Although we ‘ hate every false path , ’ we may need to ask God to act in our behalf so that we do not succumb to some temptation to break his law .\n",
            "God’s holy spirit was poured out upon them , the apostle Peter gave a stirring talk explaining the meaning of this miracle , and some 3,000 became believers and were baptized .\n",
            "Then the voice said : “ Stop calling defiled the things God has cleansed . ”\n",
            "\n",
            "==> train.kwy <==\n",
            "* Kansi , e ntangw’elongi dia esi nzo ke yau kaka ko i ntangwa amase bafwete longa o wana .\n",
            "Kansi , kele vo ozolele tambula kiyekwa muna nkutakani kafwete lungisa oma melombwanga muna Nkand’a Nzambi .\n",
            "Aweyi Adami kasadila nswa wa kuyisolela mu mpila yambote ?\n",
            "Nga mambu mama mafwete kufila mu yindula vo dia uzowa wavanga mu yantika e nkangalu wau ? Nga obembola e kalu ?\n",
            "Mun’elongi diame dia Nkand’a Nzambi , yabakula vo mbebe ngina yau kwa Yave wa Mvani a moyo .\n",
            "Kuna nsi ya Tuva wantu ayingi mu zunga yandá bezingilanga , diampasi dikalanga mu wá nsangu za Kintinu .\n",
            "Ekuma Aneyisaele bamwena wonga vava bamona Mose wamwesanga nkembo a Nzambi ?\n",
            "( Tini kia 124 , 125 ) Kana una vo ‘ tusaulanga konso ngyend’a luvunu , ’ tufwete lombanga kwa Nzambi katusadisa kimana twalembi kulula e nsiku miandi .\n",
            "Mwand’avelela wa Nzambi wabayitalela , i bosi o Petelo wa ntumwa wasonga e nsasa y’esivi diadi kwa nkangu . Mazunda matatu ma wantu bakwikila yo vubwa .\n",
            "Muna nkumbu ntatu miami , e ndinga yamvovesanga vo : “ Yambula yikila e lekwa ina Nzambi kavelelese vo yafunzuka . ”\n",
            "==> dev.en <==\n",
            "Gone will be the need for hospitals and medications .\n",
            "3 : 27 .\n",
            "Such a circumstance can be very distressing .\n",
            "If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "Saved From God’s Wrath\n",
            "I feared that people would stop noticing me and see only a wheelchair with a sickly woman .\n",
            "A fine example is one of the best teachers .\n",
            "It is a pleasure and a precious privilege to share that Bible - based hope with others , is it not ?\n",
            "‘ You don’t think about doing something contrary to Jehovah’s law , ’ he said .\n",
            "In the Bethel family there , I was thrilled to be surrounded by many spiritually mature older brothers and sisters .\n",
            "\n",
            "==> dev.kwy <==\n",
            "Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "3 : ​ 27 .\n",
            "Ediadi dilenda kikilu kutukendeleka .\n",
            "Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "Tulenda Vuluka Muna Lumbu kia Makasi ma Nzambi\n",
            "Wonga yakala wau , kadi yayindulanga vo wantu ke badi kunsia diaka sungididi ko yo mona kaka nkento wayela muna kalu dia mvanguki .\n",
            "O songa mbandu ambote i mpila isundidi balenda kubalongela .\n",
            "I lau diampwena dia samuna e vuvu kiaki kia Nkand’a Nzambi kw’akaka .\n",
            "Wavova vo : ‘ Kuyindula vanga diambu ko dilenda kulula nsiku a Yave . ’\n",
            "Kuna Betele yakalanga entwadi ye mpangi zayingi z’anunu azikuka muna mwanda .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epeCydmCyS8X"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Installation of JoeyNMT\n",
        "\n",
        "JoeyNMT is a simple, minimalist NMT package which is useful for learning and teaching. Check out the documentation for JoeyNMT [here](https://joeynmt.readthedocs.io)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iBRMm4kMxZ8L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "818a04a7-3ecf-49ba-98a2-99c7ed8f0e9f"
      },
      "source": [
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'joeynmt'...\n",
            "remote: Enumerating objects: 2479, done.\u001b[K\n",
            "remote: Total 2479 (delta 0), reused 0 (delta 0), pack-reused 2479\u001b[K\n",
            "Receiving objects: 100% (2479/2479), 2.65 MiB | 22.40 MiB/s, done.\n",
            "Resolving deltas: 100% (1732/1732), done.\n",
            "Processing /content/joeynmt\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (7.0.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.18.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (49.2.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.6.0+cu101)\n",
            "Requirement already satisfied: tensorflow>=1.14 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (2.3.0)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.3.1)\n",
            "Collecting sacrebleu>=1.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/d3/be980ad7cda7c4bbfa97ee3de062fb3014fc1a34d6dd5b82d7b92f8d6522/sacrebleu-1.4.13-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.7MB/s \n",
            "\u001b[?25hCollecting subword-nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.10.1)\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 7.0MB/s \n",
            "\u001b[?25hCollecting pylint\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/13/519c1264a134beab2be4bac8dd3e64948980a5ca7833b31cf0255b21f20a/pylint-2.6.0-py3-none-any.whl (325kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 20.1MB/s \n",
            "\u001b[?25hCollecting six==1.12\n",
            "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
            "Collecting wrapt==1.11.1\n",
            "  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.31.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.12.4)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (2.3.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (2.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (2.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.9.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.34.2)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (4.41.1)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt==0.0.1) (1.0.5)\n",
            "Collecting astroid<=2.5,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/a8/5133f51967fb21e46ee50831c3f5dda49e976b7f915408d670b1603d41d6/astroid-2.4.2-py3-none-any.whl (213kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 19.0MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7,>=0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from pylint->joeynmt==0.0.1) (0.10.1)\n",
            "Collecting isort<6,>=4.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/9b/f9e9307c89a80552f298cef17a62fa856b3f5220436338886d5eab64d4fa/isort-5.4.2-py3-none-any.whl (94kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (1.7.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (1.17.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->seaborn->joeynmt==0.0.1) (2018.9)\n",
            "Collecting lazy-object-proxy==1.4.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/dd/b1e3407e9e6913cf178e506cd0dee818e58694d9a5cd1984e3f6a8b9a10f/lazy_object_proxy-1.4.3-cp36-cp36m-manylinux1_x86_64.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.5MB/s \n",
            "\u001b[?25hCollecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/ed/5459080d95eb87a02fe860d447197be63b6e2b5e9ff73c2b0a85622994f4/typed_ast-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (737kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 20.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (0.2.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.14->joeynmt==0.0.1) (0.4.8)\n",
            "Building wheels for collected packages: joeynmt, pyyaml, wrapt\n",
            "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for joeynmt: filename=joeynmt-0.0.1-cp36-none-any.whl size=77296 sha256=e95e1ca891dc1cf7fa528c6f56dc2ca5df3b13abe132f3783c4daa74f0fb4b03\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6fb4wlz2/wheels/db/01/db/751cc9f3e7f6faec127c43644ba250a3ea7ad200594aeda70a\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=31cb3d7512dd7d981c1a5ab701fdf80bb9e5f453923cee9ad199fafba8ea8e0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.11.1-cp36-cp36m-linux_x86_64.whl size=67438 sha256=6ba1ebcce239a3fbe8c8f92e2f93be46067a5e5addfcb534e836bfa99e61bd57\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n",
            "Successfully built joeynmt pyyaml wrapt\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: portalocker, sacrebleu, subword-nmt, pyyaml, wrapt, lazy-object-proxy, typed-ast, six, astroid, mccabe, isort, pylint, joeynmt\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "  Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "Successfully installed astroid-2.4.2 isort-5.4.2 joeynmt-0.0.1 lazy-object-proxy-1.4.3 mccabe-0.6.1 portalocker-2.0.0 pylint-2.6.0 pyyaml-5.3.1 sacrebleu-1.4.13 six-1.12.0 subword-nmt-0.3.7 typed-ast-1.4.1 wrapt-1.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AaE77Tcppex9"
      },
      "source": [
        "# Preprocessing the Data into Subword BPE Tokens\n",
        "\n",
        "- One of the most powerful improvements for agglutinative languages (a feature of most Bantu languages) is using BPE tokenization [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n",
        "\n",
        "- It was also shown that by optimizing the umber of BPE codes we significantly improve results for low-resourced languages [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021) [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)\n",
        "\n",
        "- Below we have the scripts for doing BPE tokenization of our data. We use 4000 tokens as recommended by [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021). You do not need to change anything. Simply running the below will be suitable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H-TyjtmXB1mL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "outputId": "d029cea5-e167-4fe3-b948-6beabeb6ee12"
      },
      "source": [
        "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
        "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
        "\n",
        "# Do subword NMT\n",
        "from os import path\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "\n",
        "# Learn BPEs on the training data.\n",
        "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "# Apply BPE splits to the development and test data.\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
        "\n",
        "# Create directory, move everyone we care about to the correct location\n",
        "! mkdir -p $data_path\n",
        "! cp train.* $data_path\n",
        "! cp test.* $data_path\n",
        "! cp dev.* $data_path\n",
        "! cp bpe.codes.4000 $data_path\n",
        "! ls $data_path\n",
        "\n",
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\"\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# Some output\n",
        "! echo \"BPE Kikongo Sentences\"\n",
        "! tail -n 5 test.bpe.$tgt\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! tail -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.en\t     test.bpe.kwy    test.kwy\t    train.en\n",
            "dev.bpe.en\tdev.kwy      test.en\t     train.bpe.en   train.kwy\n",
            "dev.bpe.kwy\ttest.bpe.en  test.en-any.en  train.bpe.kwy\n",
            "bpe.codes.4000\tdev.en\t     test.bpe.kwy    test.kwy\t    train.en\n",
            "dev.bpe.en\tdev.kwy      test.en\t     train.bpe.en   train.kwy\n",
            "dev.bpe.kwy\ttest.bpe.en  test.en-any.en  train.bpe.kwy\n",
            "BPE Kikongo Sentences\n",
            "Ng@@ ub@@ u anene ya lukwikilu ( Tala e tini kia 12 - 14 )\n",
            "E mp@@ u a mv@@ it@@ a a luv@@ ul@@ uku ( Tala e tini kia 15 - 18 )\n",
            "O wantu bet@@ oma yangal@@ alanga vava bemonanga e kiese tukalanga kiau kia sadila Nkand’a Nzambi ye ngolo tuv@@ anganga muna kubasadisa . ”\n",
            "Ns@@ os@@ olo a mwanda ( Tala e tini kia 19 - 20 )\n",
            "T@@ ut@@ omene zaya e mbeni eto , nt@@ ambu miandi ye makani mandi .\n",
            "Combined BPE Vocab\n",
            "Hebre@@\n",
            "‛\n",
            ";@@\n",
            "C.@@\n",
            "č@@\n",
            "ā@@\n",
            "Ł@@\n",
            "−\n",
            "Ṭ@@\n",
            "û\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IlMitUHR8Qy-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "01264e3d-5e8e-46ea-d32d-3be8fbc4187b"
      },
      "source": [
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.en\t     test.bpe.kwy    test.kwy\t    train.en\n",
            "dev.bpe.en\tdev.kwy      test.en\t     train.bpe.en   train.kwy\n",
            "dev.bpe.kwy\ttest.bpe.en  test.en-any.en  train.bpe.kwy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ixmzi60WsUZ8"
      },
      "source": [
        "# Creating the JoeyNMT Config\n",
        "\n",
        "JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n",
        "\n",
        "- We used Transformer architecture \n",
        "- We set our dropout to reasonably high: 0.3 (recommended in  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
        "\n",
        "Things worth playing with:\n",
        "- The batch size (also recommended to change for low-resourced languages)\n",
        "- The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n",
        "- The decoder options (beam_size, alpha)\n",
        "- Evaluation metrics (BLEU versus Crhf4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PIs1lY2hxMsl",
        "colab": {}
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/train.bpe\"\n",
        "    dev:   \"data/{name}/dev.bpe\"\n",
        "    test:  \"data/{name}/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: False               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pIifxE3Qzuvs"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "This single line of joeynmt runs the training using the config we made above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6ZBPFwT94WpI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "040aabf8-ed2e-4058-f373-ed1473f73aa0"
      },
      "source": [
        "# Train the model\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-23 18:59:20,396 Hello! This is Joey-NMT.\n",
            "2020-08-23 18:59:20.552448: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-23 18:59:22,323 Total params: 12155136\n",
            "2020-08-23 18:59:22,325 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']\n",
            "2020-08-23 18:59:35,822 cfg.name                           : enkwy_transformer\n",
            "2020-08-23 18:59:35,822 cfg.data.src                       : en\n",
            "2020-08-23 18:59:35,823 cfg.data.trg                       : kwy\n",
            "2020-08-23 18:59:35,823 cfg.data.train                     : data/enkwy/train.bpe\n",
            "2020-08-23 18:59:35,823 cfg.data.dev                       : data/enkwy/dev.bpe\n",
            "2020-08-23 18:59:35,823 cfg.data.test                      : data/enkwy/test.bpe\n",
            "2020-08-23 18:59:35,823 cfg.data.level                     : bpe\n",
            "2020-08-23 18:59:35,823 cfg.data.lowercase                 : False\n",
            "2020-08-23 18:59:35,823 cfg.data.max_sent_length           : 100\n",
            "2020-08-23 18:59:35,823 cfg.data.src_vocab                 : data/enkwy/vocab.txt\n",
            "2020-08-23 18:59:35,823 cfg.data.trg_vocab                 : data/enkwy/vocab.txt\n",
            "2020-08-23 18:59:35,824 cfg.testing.beam_size              : 5\n",
            "2020-08-23 18:59:35,824 cfg.testing.alpha                  : 1.0\n",
            "2020-08-23 18:59:35,824 cfg.training.random_seed           : 42\n",
            "2020-08-23 18:59:35,824 cfg.training.optimizer             : adam\n",
            "2020-08-23 18:59:35,824 cfg.training.normalization         : tokens\n",
            "2020-08-23 18:59:35,824 cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2020-08-23 18:59:35,824 cfg.training.scheduling            : plateau\n",
            "2020-08-23 18:59:35,824 cfg.training.patience              : 5\n",
            "2020-08-23 18:59:35,824 cfg.training.learning_rate_factor  : 0.5\n",
            "2020-08-23 18:59:35,825 cfg.training.learning_rate_warmup  : 1000\n",
            "2020-08-23 18:59:35,825 cfg.training.decrease_factor       : 0.7\n",
            "2020-08-23 18:59:35,825 cfg.training.loss                  : crossentropy\n",
            "2020-08-23 18:59:35,825 cfg.training.learning_rate         : 0.0003\n",
            "2020-08-23 18:59:35,825 cfg.training.learning_rate_min     : 1e-08\n",
            "2020-08-23 18:59:35,825 cfg.training.weight_decay          : 0.0\n",
            "2020-08-23 18:59:35,825 cfg.training.label_smoothing       : 0.1\n",
            "2020-08-23 18:59:35,825 cfg.training.batch_size            : 4096\n",
            "2020-08-23 18:59:35,826 cfg.training.batch_type            : token\n",
            "2020-08-23 18:59:35,826 cfg.training.eval_batch_size       : 3600\n",
            "2020-08-23 18:59:35,826 cfg.training.eval_batch_type       : token\n",
            "2020-08-23 18:59:35,826 cfg.training.batch_multiplier      : 1\n",
            "2020-08-23 18:59:35,826 cfg.training.early_stopping_metric : ppl\n",
            "2020-08-23 18:59:35,826 cfg.training.epochs                : 30\n",
            "2020-08-23 18:59:35,826 cfg.training.validation_freq       : 1000\n",
            "2020-08-23 18:59:35,826 cfg.training.logging_freq          : 100\n",
            "2020-08-23 18:59:35,826 cfg.training.eval_metric           : bleu\n",
            "2020-08-23 18:59:35,826 cfg.training.model_dir             : models/enkwy_transformer\n",
            "2020-08-23 18:59:35,826 cfg.training.overwrite             : False\n",
            "2020-08-23 18:59:35,827 cfg.training.shuffle               : True\n",
            "2020-08-23 18:59:35,827 cfg.training.use_cuda              : True\n",
            "2020-08-23 18:59:35,827 cfg.training.max_output_length     : 100\n",
            "2020-08-23 18:59:35,827 cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2020-08-23 18:59:35,827 cfg.training.keep_last_ckpts       : 3\n",
            "2020-08-23 18:59:35,827 cfg.model.initializer              : xavier\n",
            "2020-08-23 18:59:35,827 cfg.model.bias_initializer         : zeros\n",
            "2020-08-23 18:59:35,827 cfg.model.init_gain                : 1.0\n",
            "2020-08-23 18:59:35,827 cfg.model.embed_initializer        : xavier\n",
            "2020-08-23 18:59:35,828 cfg.model.embed_init_gain          : 1.0\n",
            "2020-08-23 18:59:35,828 cfg.model.tied_embeddings          : True\n",
            "2020-08-23 18:59:35,828 cfg.model.tied_softmax             : True\n",
            "2020-08-23 18:59:35,828 cfg.model.encoder.type             : transformer\n",
            "2020-08-23 18:59:35,828 cfg.model.encoder.num_layers       : 6\n",
            "2020-08-23 18:59:35,828 cfg.model.encoder.num_heads        : 4\n",
            "2020-08-23 18:59:35,828 cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2020-08-23 18:59:35,828 cfg.model.encoder.embeddings.scale : True\n",
            "2020-08-23 18:59:35,828 cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2020-08-23 18:59:35,828 cfg.model.encoder.hidden_size      : 256\n",
            "2020-08-23 18:59:35,829 cfg.model.encoder.ff_size          : 1024\n",
            "2020-08-23 18:59:35,829 cfg.model.encoder.dropout          : 0.3\n",
            "2020-08-23 18:59:35,829 cfg.model.decoder.type             : transformer\n",
            "2020-08-23 18:59:35,829 cfg.model.decoder.num_layers       : 6\n",
            "2020-08-23 18:59:35,829 cfg.model.decoder.num_heads        : 4\n",
            "2020-08-23 18:59:35,829 cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2020-08-23 18:59:35,829 cfg.model.decoder.embeddings.scale : True\n",
            "2020-08-23 18:59:35,829 cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2020-08-23 18:59:35,829 cfg.model.decoder.hidden_size      : 256\n",
            "2020-08-23 18:59:35,829 cfg.model.decoder.ff_size          : 1024\n",
            "2020-08-23 18:59:35,830 cfg.model.decoder.dropout          : 0.3\n",
            "2020-08-23 18:59:35,830 Data set sizes: \n",
            "\ttrain 169092,\n",
            "\tvalid 1000,\n",
            "\ttest 2707\n",
            "2020-08-23 18:59:35,830 First training example:\n",
            "\t[SRC] * However , the family study is not the only time to teach children .\n",
            "\t[TRG] * Kansi , e ntang@@ w’@@ elongi dia esi nzo ke yau kaka ko i ntangwa am@@ ase bafwete longa o wana .\n",
            "2020-08-23 18:59:35,830 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) a (9) :\n",
            "2020-08-23 18:59:35,831 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) a (9) :\n",
            "2020-08-23 18:59:35,831 Number of Src words (types): 4277\n",
            "2020-08-23 18:59:35,831 Number of Trg words (types): 4277\n",
            "2020-08-23 18:59:35,831 Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4277),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4277))\n",
            "2020-08-23 18:59:35,836 EPOCH 1\n",
            "/content/joeynmt/joeynmt/loss.py:46: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero(Tensor input, *, Tensor out)\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  padding_positions = torch.nonzero(targets.data == self.pad_index)\n",
            "2020-08-23 18:59:49,492 Epoch   1 Step:      100 Batch Loss:     5.606369 Tokens per Sec:    15068, Lr: 0.000300\n",
            "2020-08-23 19:00:02,364 Epoch   1 Step:      200 Batch Loss:     5.380364 Tokens per Sec:    15520, Lr: 0.000300\n",
            "2020-08-23 19:00:15,524 Epoch   1 Step:      300 Batch Loss:     5.250687 Tokens per Sec:    15812, Lr: 0.000300\n",
            "2020-08-23 19:00:28,522 Epoch   1 Step:      400 Batch Loss:     4.950791 Tokens per Sec:    15577, Lr: 0.000300\n",
            "2020-08-23 19:00:41,663 Epoch   1 Step:      500 Batch Loss:     4.551891 Tokens per Sec:    15784, Lr: 0.000300\n",
            "2020-08-23 19:00:54,995 Epoch   1 Step:      600 Batch Loss:     4.345782 Tokens per Sec:    15694, Lr: 0.000300\n",
            "2020-08-23 19:01:07,994 Epoch   1 Step:      700 Batch Loss:     4.267505 Tokens per Sec:    16238, Lr: 0.000300\n",
            "2020-08-23 19:01:20,924 Epoch   1 Step:      800 Batch Loss:     4.204247 Tokens per Sec:    15891, Lr: 0.000300\n",
            "2020-08-23 19:01:34,077 Epoch   1 Step:      900 Batch Loss:     4.401848 Tokens per Sec:    16017, Lr: 0.000300\n",
            "2020-08-23 19:01:47,197 Epoch   1 Step:     1000 Batch Loss:     4.484132 Tokens per Sec:    16127, Lr: 0.000300\n",
            "2020-08-23 19:02:18,730 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:02:18,730 Saving new checkpoint.\n",
            "2020-08-23 19:02:19,221 Example #0\n",
            "2020-08-23 19:02:19,221 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:02:19,221 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:02:19,222 \tHypothesis: E mpila yayi , ke ke ke ke ke ke ke ke ke ke ko ko .\n",
            "2020-08-23 19:02:19,222 Example #1\n",
            "2020-08-23 19:02:19,222 \tSource:     3 : 27 .\n",
            "2020-08-23 19:02:19,222 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:02:19,222 \tHypothesis: 1 : ​ ​ ​ ​ ​\n",
            "2020-08-23 19:02:19,222 Example #2\n",
            "2020-08-23 19:02:19,222 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:02:19,223 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:02:19,223 \tHypothesis: Kansi , ke ke ke ke ke ke ke ke ko ko ko .\n",
            "2020-08-23 19:02:19,223 Example #3\n",
            "2020-08-23 19:02:19,223 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:02:19,223 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:02:19,223 \tHypothesis: Nga tufwete vanga mu kuma kia Nzambi , tufwete vanga mu kuma kia Nzambi ?\n",
            "2020-08-23 19:02:19,223 Validation result (greedy) at epoch   1, step     1000: bleu:   1.05, loss: 94990.7969, ppl:  58.1533, duration: 32.0260s\n",
            "2020-08-23 19:02:32,416 Epoch   1 Step:     1100 Batch Loss:     4.221429 Tokens per Sec:    15952, Lr: 0.000300\n",
            "2020-08-23 19:02:45,545 Epoch   1 Step:     1200 Batch Loss:     4.163354 Tokens per Sec:    15621, Lr: 0.000300\n",
            "2020-08-23 19:02:58,544 Epoch   1 Step:     1300 Batch Loss:     3.255452 Tokens per Sec:    15878, Lr: 0.000300\n",
            "2020-08-23 19:03:11,907 Epoch   1 Step:     1400 Batch Loss:     3.560280 Tokens per Sec:    15899, Lr: 0.000300\n",
            "2020-08-23 19:03:24,946 Epoch   1 Step:     1500 Batch Loss:     3.888114 Tokens per Sec:    16197, Lr: 0.000300\n",
            "2020-08-23 19:03:37,953 Epoch   1 Step:     1600 Batch Loss:     3.865369 Tokens per Sec:    16038, Lr: 0.000300\n",
            "2020-08-23 19:03:51,170 Epoch   1 Step:     1700 Batch Loss:     3.691843 Tokens per Sec:    15711, Lr: 0.000300\n",
            "2020-08-23 19:04:04,264 Epoch   1 Step:     1800 Batch Loss:     3.503922 Tokens per Sec:    16148, Lr: 0.000300\n",
            "2020-08-23 19:04:14,672 Epoch   1: total training loss 8200.65\n",
            "2020-08-23 19:04:14,672 EPOCH 2\n",
            "2020-08-23 19:04:17,794 Epoch   2 Step:     1900 Batch Loss:     3.705741 Tokens per Sec:    13301, Lr: 0.000300\n",
            "2020-08-23 19:04:30,727 Epoch   2 Step:     2000 Batch Loss:     3.280484 Tokens per Sec:    16043, Lr: 0.000300\n",
            "2020-08-23 19:05:01,048 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:05:01,048 Saving new checkpoint.\n",
            "2020-08-23 19:05:01,554 Example #0\n",
            "2020-08-23 19:05:01,554 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:05:01,555 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:05:01,555 \tHypothesis: O zola muna vovesanga e mpasi za vovana .\n",
            "2020-08-23 19:05:01,555 Example #1\n",
            "2020-08-23 19:05:01,555 \tSource:     3 : 27 .\n",
            "2020-08-23 19:05:01,556 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:05:01,556 \tHypothesis: 1 : ​ 1 - 5 .\n",
            "2020-08-23 19:05:01,556 Example #2\n",
            "2020-08-23 19:05:01,556 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:05:01,556 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:05:01,556 \tHypothesis: E mpila yayi yayi i diambu diadi .\n",
            "2020-08-23 19:05:01,556 Example #3\n",
            "2020-08-23 19:05:01,557 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:05:01,557 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:05:01,557 \tHypothesis: Avo tuzayanga e mvutu za yuvu yayi , nga tuzingilanga mu kuma kia samuna nsangu zambote ?\n",
            "2020-08-23 19:05:01,557 Validation result (greedy) at epoch   2, step     2000: bleu:   3.79, loss: 79171.5703, ppl:  29.5605, duration: 30.8294s\n",
            "2020-08-23 19:05:14,516 Epoch   2 Step:     2100 Batch Loss:     3.557981 Tokens per Sec:    16227, Lr: 0.000300\n",
            "2020-08-23 19:05:27,583 Epoch   2 Step:     2200 Batch Loss:     3.510894 Tokens per Sec:    16165, Lr: 0.000300\n",
            "2020-08-23 19:05:40,669 Epoch   2 Step:     2300 Batch Loss:     4.309134 Tokens per Sec:    15604, Lr: 0.000300\n",
            "2020-08-23 19:05:54,119 Epoch   2 Step:     2400 Batch Loss:     3.651278 Tokens per Sec:    15755, Lr: 0.000300\n",
            "2020-08-23 19:06:06,981 Epoch   2 Step:     2500 Batch Loss:     3.312327 Tokens per Sec:    16191, Lr: 0.000300\n",
            "2020-08-23 19:06:20,023 Epoch   2 Step:     2600 Batch Loss:     3.418061 Tokens per Sec:    15828, Lr: 0.000300\n",
            "2020-08-23 19:06:33,487 Epoch   2 Step:     2700 Batch Loss:     3.346560 Tokens per Sec:    15514, Lr: 0.000300\n",
            "2020-08-23 19:06:46,863 Epoch   2 Step:     2800 Batch Loss:     3.339795 Tokens per Sec:    15405, Lr: 0.000300\n",
            "2020-08-23 19:07:00,127 Epoch   2 Step:     2900 Batch Loss:     3.388305 Tokens per Sec:    15772, Lr: 0.000300\n",
            "2020-08-23 19:07:13,264 Epoch   2 Step:     3000 Batch Loss:     2.627511 Tokens per Sec:    15108, Lr: 0.000300\n",
            "2020-08-23 19:07:42,841 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:07:42,841 Saving new checkpoint.\n",
            "2020-08-23 19:07:43,306 Example #0\n",
            "2020-08-23 19:07:43,306 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:07:43,306 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:07:43,306 \tHypothesis: O kala ye ziku vo e fu kia vangwanga yo vangila e fu kia vangwanga .\n",
            "2020-08-23 19:07:43,307 Example #1\n",
            "2020-08-23 19:07:43,307 \tSource:     3 : 27 .\n",
            "2020-08-23 19:07:43,307 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:07:43,307 \tHypothesis: 3 : ​ 3 .\n",
            "2020-08-23 19:07:43,307 Example #2\n",
            "2020-08-23 19:07:43,308 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:07:43,308 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:07:43,308 \tHypothesis: E mpila yayi ilenda kala ye fu kia vangila .\n",
            "2020-08-23 19:07:43,308 Example #3\n",
            "2020-08-23 19:07:43,308 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:07:43,308 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:07:43,308 \tHypothesis: Avo osadilanga e mvutu za yuvu yayi , olenda longoka muna yuvu yayi , adieyi olenda vanga mu kuma kia Diambu dia Nzambi ?\n",
            "2020-08-23 19:07:43,309 Validation result (greedy) at epoch   2, step     3000: bleu:   4.74, loss: 71365.7031, ppl:  21.1694, duration: 30.0442s\n",
            "2020-08-23 19:07:56,448 Epoch   2 Step:     3100 Batch Loss:     3.426840 Tokens per Sec:    16176, Lr: 0.000300\n",
            "2020-08-23 19:08:09,601 Epoch   2 Step:     3200 Batch Loss:     2.799961 Tokens per Sec:    15952, Lr: 0.000300\n",
            "2020-08-23 19:08:22,612 Epoch   2 Step:     3300 Batch Loss:     3.460987 Tokens per Sec:    16138, Lr: 0.000300\n",
            "2020-08-23 19:08:35,722 Epoch   2 Step:     3400 Batch Loss:     3.407089 Tokens per Sec:    15460, Lr: 0.000300\n",
            "2020-08-23 19:08:48,840 Epoch   2 Step:     3500 Batch Loss:     3.161410 Tokens per Sec:    15895, Lr: 0.000300\n",
            "2020-08-23 19:09:02,057 Epoch   2 Step:     3600 Batch Loss:     3.458607 Tokens per Sec:    15954, Lr: 0.000300\n",
            "2020-08-23 19:09:15,156 Epoch   2 Step:     3700 Batch Loss:     3.325883 Tokens per Sec:    15753, Lr: 0.000300\n",
            "2020-08-23 19:09:23,439 Epoch   2: total training loss 6102.99\n",
            "2020-08-23 19:09:23,439 EPOCH 3\n",
            "2020-08-23 19:09:28,668 Epoch   3 Step:     3800 Batch Loss:     3.130052 Tokens per Sec:    14691, Lr: 0.000300\n",
            "2020-08-23 19:09:41,924 Epoch   3 Step:     3900 Batch Loss:     3.026096 Tokens per Sec:    15849, Lr: 0.000300\n",
            "2020-08-23 19:09:54,977 Epoch   3 Step:     4000 Batch Loss:     2.389492 Tokens per Sec:    15668, Lr: 0.000300\n",
            "2020-08-23 19:10:12,835 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:10:12,836 Saving new checkpoint.\n",
            "2020-08-23 19:10:13,321 Example #0\n",
            "2020-08-23 19:10:13,321 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:10:13,321 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:10:13,321 \tHypothesis: O kala ye fu kia vumu ye fu kia vumu .\n",
            "2020-08-23 19:10:13,321 Example #1\n",
            "2020-08-23 19:10:13,322 \tSource:     3 : 27 .\n",
            "2020-08-23 19:10:13,322 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:10:13,322 \tHypothesis: 3 : ​ 5 .\n",
            "2020-08-23 19:10:13,322 Example #2\n",
            "2020-08-23 19:10:13,322 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:10:13,322 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:10:13,323 \tHypothesis: E mpila yayi ilenda kala ye fu kia vumu .\n",
            "2020-08-23 19:10:13,323 Example #3\n",
            "2020-08-23 19:10:13,323 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:10:13,323 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:10:13,323 \tHypothesis: Avo oyindulanga e mvutu za yuvu yayi , nga olenda longoka e yuvu yayi ?\n",
            "2020-08-23 19:10:13,323 Validation result (greedy) at epoch   3, step     4000: bleu:   7.62, loss: 65827.9766, ppl:  16.7047, duration: 18.3459s\n",
            "2020-08-23 19:10:26,718 Epoch   3 Step:     4100 Batch Loss:     2.546787 Tokens per Sec:    15529, Lr: 0.000300\n",
            "2020-08-23 19:10:39,741 Epoch   3 Step:     4200 Batch Loss:     2.940185 Tokens per Sec:    16163, Lr: 0.000300\n",
            "2020-08-23 19:10:52,965 Epoch   3 Step:     4300 Batch Loss:     2.631771 Tokens per Sec:    15698, Lr: 0.000300\n",
            "2020-08-23 19:11:06,021 Epoch   3 Step:     4400 Batch Loss:     3.586626 Tokens per Sec:    15507, Lr: 0.000300\n",
            "2020-08-23 19:11:19,294 Epoch   3 Step:     4500 Batch Loss:     3.017503 Tokens per Sec:    15870, Lr: 0.000300\n",
            "2020-08-23 19:11:32,374 Epoch   3 Step:     4600 Batch Loss:     2.752824 Tokens per Sec:    15812, Lr: 0.000300\n",
            "2020-08-23 19:11:45,414 Epoch   3 Step:     4700 Batch Loss:     2.520127 Tokens per Sec:    16116, Lr: 0.000300\n",
            "2020-08-23 19:11:58,328 Epoch   3 Step:     4800 Batch Loss:     2.931787 Tokens per Sec:    15979, Lr: 0.000300\n",
            "2020-08-23 19:12:11,310 Epoch   3 Step:     4900 Batch Loss:     2.786722 Tokens per Sec:    16261, Lr: 0.000300\n",
            "2020-08-23 19:12:24,266 Epoch   3 Step:     5000 Batch Loss:     2.978990 Tokens per Sec:    16143, Lr: 0.000300\n",
            "2020-08-23 19:12:43,303 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:12:43,303 Saving new checkpoint.\n",
            "2020-08-23 19:12:43,803 Example #0\n",
            "2020-08-23 19:12:43,804 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:12:43,804 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:12:43,804 \tHypothesis: O kala ye fu kia vwama yo vevola .\n",
            "2020-08-23 19:12:43,804 Example #1\n",
            "2020-08-23 19:12:43,804 \tSource:     3 : 27 .\n",
            "2020-08-23 19:12:43,804 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:12:43,804 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 19:12:43,804 Example #2\n",
            "2020-08-23 19:12:43,805 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:12:43,805 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:12:43,805 \tHypothesis: E mpila yayi ilenda kala diampasi mu kala ye fu kia vwama .\n",
            "2020-08-23 19:12:43,805 Example #3\n",
            "2020-08-23 19:12:43,805 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:12:43,805 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:12:43,805 \tHypothesis: Avo i wau , nga olenda baka e mvutu za yuvu yayi , nga olenda kala ye ziku vo Nzambi osadilanga ?\n",
            "2020-08-23 19:12:43,806 Validation result (greedy) at epoch   3, step     5000: bleu:   9.52, loss: 62261.0078, ppl:  14.3409, duration: 19.5393s\n",
            "2020-08-23 19:12:56,883 Epoch   3 Step:     5100 Batch Loss:     2.442445 Tokens per Sec:    15674, Lr: 0.000300\n",
            "2020-08-23 19:13:09,747 Epoch   3 Step:     5200 Batch Loss:     2.803423 Tokens per Sec:    15756, Lr: 0.000300\n",
            "2020-08-23 19:13:22,640 Epoch   3 Step:     5300 Batch Loss:     2.482343 Tokens per Sec:    16157, Lr: 0.000300\n",
            "2020-08-23 19:13:35,763 Epoch   3 Step:     5400 Batch Loss:     2.725189 Tokens per Sec:    16179, Lr: 0.000300\n",
            "2020-08-23 19:13:48,709 Epoch   3 Step:     5500 Batch Loss:     2.785775 Tokens per Sec:    15196, Lr: 0.000300\n",
            "2020-08-23 19:14:01,819 Epoch   3 Step:     5600 Batch Loss:     2.592599 Tokens per Sec:    15629, Lr: 0.000300\n",
            "2020-08-23 19:14:08,484 Epoch   3: total training loss 5375.26\n",
            "2020-08-23 19:14:08,484 EPOCH 4\n",
            "2020-08-23 19:14:15,254 Epoch   4 Step:     5700 Batch Loss:     2.636070 Tokens per Sec:    15811, Lr: 0.000300\n",
            "2020-08-23 19:14:28,357 Epoch   4 Step:     5800 Batch Loss:     2.460879 Tokens per Sec:    15803, Lr: 0.000300\n",
            "2020-08-23 19:14:41,258 Epoch   4 Step:     5900 Batch Loss:     2.792295 Tokens per Sec:    16390, Lr: 0.000300\n",
            "2020-08-23 19:14:54,243 Epoch   4 Step:     6000 Batch Loss:     2.610444 Tokens per Sec:    15968, Lr: 0.000300\n",
            "2020-08-23 19:15:18,658 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:15:18,659 Saving new checkpoint.\n",
            "2020-08-23 19:15:19,200 Example #0\n",
            "2020-08-23 19:15:19,201 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:15:19,201 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:15:19,201 \tHypothesis: O muntu ndioyo olenda kala ye fu kia vangu dia vata dia zumba .\n",
            "2020-08-23 19:15:19,201 Example #1\n",
            "2020-08-23 19:15:19,201 \tSource:     3 : 27 .\n",
            "2020-08-23 19:15:19,201 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:15:19,201 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 19:15:19,201 Example #2\n",
            "2020-08-23 19:15:19,202 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:15:19,202 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:15:19,202 \tHypothesis: E diambu diadi dilenda kala diampasi mu vanga diambu diadi .\n",
            "2020-08-23 19:15:19,202 Example #3\n",
            "2020-08-23 19:15:19,202 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:15:19,203 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:15:19,203 \tHypothesis: Avo ovangidi e mvutu za yuvu yayi , nga olenda sadila e mvutu za yuvu yayi ?\n",
            "2020-08-23 19:15:19,203 Validation result (greedy) at epoch   4, step     6000: bleu:  10.86, loss: 59579.2617, ppl:  12.7867, duration: 24.9597s\n",
            "2020-08-23 19:15:32,569 Epoch   4 Step:     6100 Batch Loss:     2.691780 Tokens per Sec:    15688, Lr: 0.000300\n",
            "2020-08-23 19:15:45,481 Epoch   4 Step:     6200 Batch Loss:     2.838215 Tokens per Sec:    16182, Lr: 0.000300\n",
            "2020-08-23 19:15:58,336 Epoch   4 Step:     6300 Batch Loss:     2.954740 Tokens per Sec:    15708, Lr: 0.000300\n",
            "2020-08-23 19:16:11,511 Epoch   4 Step:     6400 Batch Loss:     2.494474 Tokens per Sec:    15585, Lr: 0.000300\n",
            "2020-08-23 19:16:24,716 Epoch   4 Step:     6500 Batch Loss:     2.414114 Tokens per Sec:    15778, Lr: 0.000300\n",
            "2020-08-23 19:16:37,770 Epoch   4 Step:     6600 Batch Loss:     3.112894 Tokens per Sec:    15614, Lr: 0.000300\n",
            "2020-08-23 19:16:51,016 Epoch   4 Step:     6700 Batch Loss:     2.529013 Tokens per Sec:    15640, Lr: 0.000300\n",
            "2020-08-23 19:17:04,233 Epoch   4 Step:     6800 Batch Loss:     2.413128 Tokens per Sec:    16103, Lr: 0.000300\n",
            "2020-08-23 19:17:17,466 Epoch   4 Step:     6900 Batch Loss:     2.855419 Tokens per Sec:    15917, Lr: 0.000300\n",
            "2020-08-23 19:17:30,644 Epoch   4 Step:     7000 Batch Loss:     2.572777 Tokens per Sec:    16060, Lr: 0.000300\n",
            "2020-08-23 19:17:54,241 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:17:54,241 Saving new checkpoint.\n",
            "2020-08-23 19:17:54,776 Example #0\n",
            "2020-08-23 19:17:54,776 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:17:54,776 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:17:54,777 \tHypothesis: Mosi muna yau i diambu dia vanga yo vanga diambu .\n",
            "2020-08-23 19:17:54,777 Example #1\n",
            "2020-08-23 19:17:54,777 \tSource:     3 : 27 .\n",
            "2020-08-23 19:17:54,777 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:17:54,777 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 19:17:54,777 Example #2\n",
            "2020-08-23 19:17:54,777 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:17:54,778 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:17:54,778 \tHypothesis: E mpasi zazi zilenda twasa nluta .\n",
            "2020-08-23 19:17:54,778 Example #3\n",
            "2020-08-23 19:17:54,778 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:17:54,778 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:17:54,778 \tHypothesis: Avo i wau , nga oyangalelanga e mvutu za yuvu yayi , nga olenda kala ye ziku vo Nzambi otoma sadilanga ?\n",
            "2020-08-23 19:17:54,778 Validation result (greedy) at epoch   4, step     7000: bleu:  12.42, loss: 57351.0078, ppl:  11.6243, duration: 24.1342s\n",
            "2020-08-23 19:18:08,053 Epoch   4 Step:     7100 Batch Loss:     2.710736 Tokens per Sec:    15385, Lr: 0.000300\n",
            "2020-08-23 19:18:21,026 Epoch   4 Step:     7200 Batch Loss:     2.391852 Tokens per Sec:    16105, Lr: 0.000300\n",
            "2020-08-23 19:18:34,361 Epoch   4 Step:     7300 Batch Loss:     2.224488 Tokens per Sec:    15539, Lr: 0.000300\n",
            "2020-08-23 19:18:47,393 Epoch   4 Step:     7400 Batch Loss:     2.218583 Tokens per Sec:    15796, Lr: 0.000300\n",
            "2020-08-23 19:19:00,353 Epoch   4 Step:     7500 Batch Loss:     2.342388 Tokens per Sec:    16127, Lr: 0.000300\n",
            "2020-08-23 19:19:04,148 Epoch   4: total training loss 4940.16\n",
            "2020-08-23 19:19:04,148 EPOCH 5\n",
            "2020-08-23 19:19:13,884 Epoch   5 Step:     7600 Batch Loss:     3.177247 Tokens per Sec:    15036, Lr: 0.000300\n",
            "2020-08-23 19:19:26,857 Epoch   5 Step:     7700 Batch Loss:     3.167083 Tokens per Sec:    16177, Lr: 0.000300\n",
            "2020-08-23 19:19:39,863 Epoch   5 Step:     7800 Batch Loss:     2.445474 Tokens per Sec:    16272, Lr: 0.000300\n",
            "2020-08-23 19:19:53,126 Epoch   5 Step:     7900 Batch Loss:     2.628938 Tokens per Sec:    15851, Lr: 0.000300\n",
            "2020-08-23 19:20:06,146 Epoch   5 Step:     8000 Batch Loss:     2.607110 Tokens per Sec:    16099, Lr: 0.000300\n",
            "2020-08-23 19:20:25,823 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:20:25,823 Saving new checkpoint.\n",
            "2020-08-23 19:20:26,321 Example #0\n",
            "2020-08-23 19:20:26,321 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:20:26,322 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:20:26,322 \tHypothesis: Dilenda kala vo muntu kafwete vavana e fu kia vundu ye mavangu ma zumba .\n",
            "2020-08-23 19:20:26,322 Example #1\n",
            "2020-08-23 19:20:26,322 \tSource:     3 : 27 .\n",
            "2020-08-23 19:20:26,322 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:20:26,322 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 19:20:26,322 Example #2\n",
            "2020-08-23 19:20:26,323 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:20:26,323 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:20:26,323 \tHypothesis: E mpasi zazi zilenda twasa .\n",
            "2020-08-23 19:20:26,323 Example #3\n",
            "2020-08-23 19:20:26,323 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:20:26,323 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:20:26,323 \tHypothesis: Avo ovangidi e mvutu za yuvu yayi , nga olenda lomba e mvutu za yuvu yayi ?\n",
            "2020-08-23 19:20:26,324 Validation result (greedy) at epoch   5, step     8000: bleu:  13.34, loss: 55338.1445, ppl:  10.6654, duration: 20.1777s\n",
            "2020-08-23 19:20:39,623 Epoch   5 Step:     8100 Batch Loss:     2.878045 Tokens per Sec:    15473, Lr: 0.000300\n",
            "2020-08-23 19:20:52,729 Epoch   5 Step:     8200 Batch Loss:     2.394960 Tokens per Sec:    16065, Lr: 0.000300\n",
            "2020-08-23 19:21:05,739 Epoch   5 Step:     8300 Batch Loss:     2.058295 Tokens per Sec:    15768, Lr: 0.000300\n",
            "2020-08-23 19:21:18,821 Epoch   5 Step:     8400 Batch Loss:     2.645820 Tokens per Sec:    16143, Lr: 0.000300\n",
            "2020-08-23 19:21:31,859 Epoch   5 Step:     8500 Batch Loss:     2.666241 Tokens per Sec:    15595, Lr: 0.000300\n",
            "2020-08-23 19:21:44,992 Epoch   5 Step:     8600 Batch Loss:     2.154483 Tokens per Sec:    15892, Lr: 0.000300\n",
            "2020-08-23 19:21:58,157 Epoch   5 Step:     8700 Batch Loss:     2.780155 Tokens per Sec:    15776, Lr: 0.000300\n",
            "2020-08-23 19:22:11,336 Epoch   5 Step:     8800 Batch Loss:     2.446979 Tokens per Sec:    15553, Lr: 0.000300\n",
            "2020-08-23 19:22:24,591 Epoch   5 Step:     8900 Batch Loss:     2.853442 Tokens per Sec:    15512, Lr: 0.000300\n",
            "2020-08-23 19:22:37,779 Epoch   5 Step:     9000 Batch Loss:     2.326742 Tokens per Sec:    15804, Lr: 0.000300\n",
            "2020-08-23 19:23:01,343 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:23:01,344 Saving new checkpoint.\n",
            "2020-08-23 19:23:01,833 Example #0\n",
            "2020-08-23 19:23:01,833 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:23:01,833 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:23:01,833 \tHypothesis: Ngeye mpe olenda kala ye fu kia vevola ye vuku .\n",
            "2020-08-23 19:23:01,834 Example #1\n",
            "2020-08-23 19:23:01,834 \tSource:     3 : 27 .\n",
            "2020-08-23 19:23:01,834 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:23:01,834 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 19:23:01,834 Example #2\n",
            "2020-08-23 19:23:01,834 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:23:01,834 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:23:01,835 \tHypothesis: E mpasi zazi zilenda twasa .\n",
            "2020-08-23 19:23:01,835 Example #3\n",
            "2020-08-23 19:23:01,835 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:23:01,835 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:23:01,835 \tHypothesis: Avo ovangidi e mvutu za yuvu yayi , nga olenda lomba kwa Nzambi ?\n",
            "2020-08-23 19:23:01,835 Validation result (greedy) at epoch   5, step     9000: bleu:  14.69, loss: 53618.9531, ppl:   9.9092, duration: 24.0555s\n",
            "2020-08-23 19:23:15,032 Epoch   5 Step:     9100 Batch Loss:     2.416835 Tokens per Sec:    15749, Lr: 0.000300\n",
            "2020-08-23 19:23:27,902 Epoch   5 Step:     9200 Batch Loss:     2.432490 Tokens per Sec:    16145, Lr: 0.000300\n",
            "2020-08-23 19:23:41,119 Epoch   5 Step:     9300 Batch Loss:     2.592410 Tokens per Sec:    16094, Lr: 0.000300\n",
            "2020-08-23 19:23:54,372 Epoch   5 Step:     9400 Batch Loss:     2.620940 Tokens per Sec:    16129, Lr: 0.000300\n",
            "2020-08-23 19:23:55,036 Epoch   5: total training loss 4647.10\n",
            "2020-08-23 19:23:55,036 EPOCH 6\n",
            "2020-08-23 19:24:07,545 Epoch   6 Step:     9500 Batch Loss:     2.371198 Tokens per Sec:    15808, Lr: 0.000300\n",
            "2020-08-23 19:24:20,750 Epoch   6 Step:     9600 Batch Loss:     2.335292 Tokens per Sec:    16152, Lr: 0.000300\n",
            "2020-08-23 19:24:33,704 Epoch   6 Step:     9700 Batch Loss:     2.397092 Tokens per Sec:    16034, Lr: 0.000300\n",
            "2020-08-23 19:24:46,819 Epoch   6 Step:     9800 Batch Loss:     2.060227 Tokens per Sec:    15976, Lr: 0.000300\n",
            "2020-08-23 19:24:59,890 Epoch   6 Step:     9900 Batch Loss:     2.512309 Tokens per Sec:    15856, Lr: 0.000300\n",
            "2020-08-23 19:25:13,178 Epoch   6 Step:    10000 Batch Loss:     2.029903 Tokens per Sec:    15707, Lr: 0.000300\n",
            "2020-08-23 19:25:31,541 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:25:31,541 Saving new checkpoint.\n",
            "2020-08-23 19:25:32,030 Example #0\n",
            "2020-08-23 19:25:32,030 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:25:32,030 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:25:32,031 \tHypothesis: Ngeye mosi olenda kala ye fu kia tambula nzenza yo vevola .\n",
            "2020-08-23 19:25:32,031 Example #1\n",
            "2020-08-23 19:25:32,031 \tSource:     3 : 27 .\n",
            "2020-08-23 19:25:32,031 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:25:32,031 \tHypothesis: 3 :⁠ 27 .\n",
            "2020-08-23 19:25:32,031 Example #2\n",
            "2020-08-23 19:25:32,031 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:25:32,032 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:25:32,032 \tHypothesis: Ediadi dilenda kala diampasi mu zizidila e mpasi .\n",
            "2020-08-23 19:25:32,032 Example #3\n",
            "2020-08-23 19:25:32,032 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:25:32,032 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:25:32,032 \tHypothesis: Avo ovangidi e mvutu za yuvu yayi , nga olenda lomba lusadisu lwa Nzambi ?\n",
            "2020-08-23 19:25:32,032 Validation result (greedy) at epoch   6, step    10000: bleu:  15.15, loss: 52201.0039, ppl:   9.3261, duration: 18.8537s\n",
            "2020-08-23 19:25:45,081 Epoch   6 Step:    10100 Batch Loss:     2.202989 Tokens per Sec:    16116, Lr: 0.000300\n",
            "2020-08-23 19:25:58,193 Epoch   6 Step:    10200 Batch Loss:     2.219559 Tokens per Sec:    15891, Lr: 0.000300\n",
            "2020-08-23 19:26:11,226 Epoch   6 Step:    10300 Batch Loss:     2.612126 Tokens per Sec:    16279, Lr: 0.000300\n",
            "2020-08-23 19:26:24,450 Epoch   6 Step:    10400 Batch Loss:     2.452096 Tokens per Sec:    16051, Lr: 0.000300\n",
            "2020-08-23 19:26:37,444 Epoch   6 Step:    10500 Batch Loss:     2.558732 Tokens per Sec:    15775, Lr: 0.000300\n",
            "2020-08-23 19:26:50,365 Epoch   6 Step:    10600 Batch Loss:     2.330309 Tokens per Sec:    16004, Lr: 0.000300\n",
            "2020-08-23 19:27:03,432 Epoch   6 Step:    10700 Batch Loss:     2.519541 Tokens per Sec:    16216, Lr: 0.000300\n",
            "2020-08-23 19:27:16,246 Epoch   6 Step:    10800 Batch Loss:     2.442129 Tokens per Sec:    15877, Lr: 0.000300\n",
            "2020-08-23 19:27:29,428 Epoch   6 Step:    10900 Batch Loss:     1.847132 Tokens per Sec:    15459, Lr: 0.000300\n",
            "2020-08-23 19:27:42,628 Epoch   6 Step:    11000 Batch Loss:     2.191853 Tokens per Sec:    15813, Lr: 0.000300\n",
            "2020-08-23 19:28:00,433 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:28:00,433 Saving new checkpoint.\n",
            "2020-08-23 19:28:00,951 Example #0\n",
            "2020-08-23 19:28:00,951 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:28:00,951 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:28:00,951 \tHypothesis: Muntu ndioyo ozolanga vo e nzenza ye bulu .\n",
            "2020-08-23 19:28:00,951 Example #1\n",
            "2020-08-23 19:28:00,952 \tSource:     3 : 27 .\n",
            "2020-08-23 19:28:00,952 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:28:00,952 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 19:28:00,952 Example #2\n",
            "2020-08-23 19:28:00,952 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:28:00,952 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:28:00,953 \tHypothesis: E mpasi zazi zilenda twasa nluta .\n",
            "2020-08-23 19:28:00,953 Example #3\n",
            "2020-08-23 19:28:00,953 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:28:00,953 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:28:00,953 \tHypothesis: Avo ovangidi e mvutu za yuvu yayi , nga olenda lomba lusadisu kwa Nzambi ?\n",
            "2020-08-23 19:28:00,953 Validation result (greedy) at epoch   6, step    11000: bleu:  15.96, loss: 50936.3945, ppl:   8.8350, duration: 18.3248s\n",
            "2020-08-23 19:28:14,001 Epoch   6 Step:    11100 Batch Loss:     2.670365 Tokens per Sec:    16247, Lr: 0.000300\n",
            "2020-08-23 19:28:26,949 Epoch   6 Step:    11200 Batch Loss:     2.109166 Tokens per Sec:    15725, Lr: 0.000300\n",
            "2020-08-23 19:28:37,302 Epoch   6: total training loss 4430.96\n",
            "2020-08-23 19:28:37,302 EPOCH 7\n",
            "2020-08-23 19:28:40,259 Epoch   7 Step:    11300 Batch Loss:     2.516426 Tokens per Sec:    15237, Lr: 0.000300\n",
            "2020-08-23 19:28:53,338 Epoch   7 Step:    11400 Batch Loss:     2.311522 Tokens per Sec:    16232, Lr: 0.000300\n",
            "2020-08-23 19:29:06,563 Epoch   7 Step:    11500 Batch Loss:     1.799180 Tokens per Sec:    15593, Lr: 0.000300\n",
            "2020-08-23 19:29:19,596 Epoch   7 Step:    11600 Batch Loss:     2.408846 Tokens per Sec:    15681, Lr: 0.000300\n",
            "2020-08-23 19:29:32,662 Epoch   7 Step:    11700 Batch Loss:     2.171269 Tokens per Sec:    15548, Lr: 0.000300\n",
            "2020-08-23 19:29:45,932 Epoch   7 Step:    11800 Batch Loss:     1.903543 Tokens per Sec:    16117, Lr: 0.000300\n",
            "2020-08-23 19:29:59,257 Epoch   7 Step:    11900 Batch Loss:     2.508087 Tokens per Sec:    15840, Lr: 0.000300\n",
            "2020-08-23 19:30:12,278 Epoch   7 Step:    12000 Batch Loss:     2.240769 Tokens per Sec:    16121, Lr: 0.000300\n",
            "2020-08-23 19:30:30,303 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:30:30,304 Saving new checkpoint.\n",
            "2020-08-23 19:30:30,784 Example #0\n",
            "2020-08-23 19:30:30,784 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:30:30,784 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:30:30,784 \tHypothesis: O muntu ndioyo ovwidi o mfunu wa tambula nzenza yo veza .\n",
            "2020-08-23 19:30:30,784 Example #1\n",
            "2020-08-23 19:30:30,785 \tSource:     3 : 27 .\n",
            "2020-08-23 19:30:30,785 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:30:30,785 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 19:30:30,785 Example #2\n",
            "2020-08-23 19:30:30,785 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:30:30,785 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:30:30,785 \tHypothesis: E mpasi zazi zilenda twasa mpasi .\n",
            "2020-08-23 19:30:30,785 Example #3\n",
            "2020-08-23 19:30:30,786 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:30:30,786 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:30:30,786 \tHypothesis: Avo ovangidi mvutu za yuvu yayi , nga olenda lomba lusadisu kwa Nzambi ?\n",
            "2020-08-23 19:30:30,786 Validation result (greedy) at epoch   7, step    12000: bleu:  17.11, loss: 50030.3203, ppl:   8.4992, duration: 18.5076s\n",
            "2020-08-23 19:30:43,923 Epoch   7 Step:    12100 Batch Loss:     1.799628 Tokens per Sec:    15718, Lr: 0.000300\n",
            "2020-08-23 19:30:57,181 Epoch   7 Step:    12200 Batch Loss:     2.433813 Tokens per Sec:    15981, Lr: 0.000300\n",
            "2020-08-23 19:31:10,627 Epoch   7 Step:    12300 Batch Loss:     2.183580 Tokens per Sec:    15578, Lr: 0.000300\n",
            "2020-08-23 19:31:23,724 Epoch   7 Step:    12400 Batch Loss:     2.327144 Tokens per Sec:    15720, Lr: 0.000300\n",
            "2020-08-23 19:31:36,818 Epoch   7 Step:    12500 Batch Loss:     2.435620 Tokens per Sec:    16077, Lr: 0.000300\n",
            "2020-08-23 19:31:50,190 Epoch   7 Step:    12600 Batch Loss:     2.575833 Tokens per Sec:    15280, Lr: 0.000300\n",
            "2020-08-23 19:32:03,315 Epoch   7 Step:    12700 Batch Loss:     2.227450 Tokens per Sec:    16042, Lr: 0.000300\n",
            "2020-08-23 19:32:16,494 Epoch   7 Step:    12800 Batch Loss:     2.421599 Tokens per Sec:    15665, Lr: 0.000300\n",
            "2020-08-23 19:32:29,655 Epoch   7 Step:    12900 Batch Loss:     2.486219 Tokens per Sec:    16169, Lr: 0.000300\n",
            "2020-08-23 19:32:42,692 Epoch   7 Step:    13000 Batch Loss:     2.135772 Tokens per Sec:    15266, Lr: 0.000300\n",
            "2020-08-23 19:32:59,818 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:32:59,818 Saving new checkpoint.\n",
            "2020-08-23 19:33:00,333 Example #0\n",
            "2020-08-23 19:33:00,333 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:33:00,333 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:33:00,333 \tHypothesis: O muntu kafwete kala ye fu kia tambula nzenza yo veza .\n",
            "2020-08-23 19:33:00,333 Example #1\n",
            "2020-08-23 19:33:00,334 \tSource:     3 : 27 .\n",
            "2020-08-23 19:33:00,334 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:33:00,334 \tHypothesis: 3 :⁠ 27 .\n",
            "2020-08-23 19:33:00,334 Example #2\n",
            "2020-08-23 19:33:00,334 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:33:00,334 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:33:00,335 \tHypothesis: E mpasi zazi zilenda twasa .\n",
            "2020-08-23 19:33:00,335 Example #3\n",
            "2020-08-23 19:33:00,335 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:33:00,335 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:33:00,335 \tHypothesis: Avo ovangidi e mvutu za yuvu yayi , nga olenda vanga mu kuma kia diambu diadi ?\n",
            "2020-08-23 19:33:00,335 Validation result (greedy) at epoch   7, step    13000: bleu:  17.06, loss: 49374.6094, ppl:   8.2641, duration: 17.6433s\n",
            "2020-08-23 19:33:13,314 Epoch   7 Step:    13100 Batch Loss:     2.232665 Tokens per Sec:    15792, Lr: 0.000300\n",
            "2020-08-23 19:33:21,227 Epoch   7: total training loss 4285.41\n",
            "2020-08-23 19:33:21,227 EPOCH 8\n",
            "2020-08-23 19:33:26,539 Epoch   8 Step:    13200 Batch Loss:     2.242340 Tokens per Sec:    15555, Lr: 0.000300\n",
            "2020-08-23 19:33:39,673 Epoch   8 Step:    13300 Batch Loss:     2.387643 Tokens per Sec:    16092, Lr: 0.000300\n",
            "2020-08-23 19:33:53,044 Epoch   8 Step:    13400 Batch Loss:     2.318910 Tokens per Sec:    15285, Lr: 0.000300\n",
            "2020-08-23 19:34:06,009 Epoch   8 Step:    13500 Batch Loss:     2.283592 Tokens per Sec:    16047, Lr: 0.000300\n",
            "2020-08-23 19:34:19,120 Epoch   8 Step:    13600 Batch Loss:     1.858197 Tokens per Sec:    15832, Lr: 0.000300\n",
            "2020-08-23 19:34:32,146 Epoch   8 Step:    13700 Batch Loss:     2.460381 Tokens per Sec:    15891, Lr: 0.000300\n",
            "2020-08-23 19:34:45,173 Epoch   8 Step:    13800 Batch Loss:     1.890576 Tokens per Sec:    16012, Lr: 0.000300\n",
            "2020-08-23 19:34:58,411 Epoch   8 Step:    13900 Batch Loss:     1.404048 Tokens per Sec:    15716, Lr: 0.000300\n",
            "2020-08-23 19:35:11,416 Epoch   8 Step:    14000 Batch Loss:     1.933014 Tokens per Sec:    16199, Lr: 0.000300\n",
            "2020-08-23 19:35:32,002 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:35:32,002 Saving new checkpoint.\n",
            "2020-08-23 19:35:32,480 Example #0\n",
            "2020-08-23 19:35:32,480 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:35:32,480 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:35:32,480 \tHypothesis: O muntu kafwete kala ye kiese muna tambula e nzenza yo vevola e nzimbu .\n",
            "2020-08-23 19:35:32,481 Example #1\n",
            "2020-08-23 19:35:32,481 \tSource:     3 : 27 .\n",
            "2020-08-23 19:35:32,481 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:35:32,481 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 19:35:32,481 Example #2\n",
            "2020-08-23 19:35:32,481 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:35:32,482 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:35:32,482 \tHypothesis: E mpasi zazi zilenda twasa mpasi .\n",
            "2020-08-23 19:35:32,482 Example #3\n",
            "2020-08-23 19:35:32,482 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:35:32,482 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:35:32,482 \tHypothesis: Avo ovangidi e mvutu za yuvu yayi , nga olenda lomba lusadisu lwa Nzambi ?\n",
            "2020-08-23 19:35:32,482 Validation result (greedy) at epoch   8, step    14000: bleu:  17.46, loss: 48375.0273, ppl:   7.9182, duration: 21.0657s\n",
            "2020-08-23 19:35:45,609 Epoch   8 Step:    14100 Batch Loss:     1.631682 Tokens per Sec:    15623, Lr: 0.000300\n",
            "2020-08-23 19:35:58,941 Epoch   8 Step:    14200 Batch Loss:     2.441327 Tokens per Sec:    15891, Lr: 0.000300\n",
            "2020-08-23 19:36:11,983 Epoch   8 Step:    14300 Batch Loss:     2.851804 Tokens per Sec:    15552, Lr: 0.000300\n",
            "2020-08-23 19:36:24,997 Epoch   8 Step:    14400 Batch Loss:     2.161211 Tokens per Sec:    15746, Lr: 0.000300\n",
            "2020-08-23 19:36:37,985 Epoch   8 Step:    14500 Batch Loss:     2.014613 Tokens per Sec:    16059, Lr: 0.000300\n",
            "2020-08-23 19:36:51,113 Epoch   8 Step:    14600 Batch Loss:     2.342689 Tokens per Sec:    15396, Lr: 0.000300\n",
            "2020-08-23 19:37:04,355 Epoch   8 Step:    14700 Batch Loss:     2.070846 Tokens per Sec:    15653, Lr: 0.000300\n",
            "2020-08-23 19:37:17,231 Epoch   8 Step:    14800 Batch Loss:     2.102597 Tokens per Sec:    15753, Lr: 0.000300\n",
            "2020-08-23 19:37:30,194 Epoch   8 Step:    14900 Batch Loss:     2.530095 Tokens per Sec:    16429, Lr: 0.000300\n",
            "2020-08-23 19:37:43,260 Epoch   8 Step:    15000 Batch Loss:     2.111813 Tokens per Sec:    16077, Lr: 0.000300\n",
            "2020-08-23 19:38:01,544 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:38:01,545 Saving new checkpoint.\n",
            "2020-08-23 19:38:02,061 Example #0\n",
            "2020-08-23 19:38:02,061 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:38:02,061 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:38:02,061 \tHypothesis: O muntu kafwete kala ye fu kia tambula nzenza yo veza .\n",
            "2020-08-23 19:38:02,061 Example #1\n",
            "2020-08-23 19:38:02,062 \tSource:     3 : 27 .\n",
            "2020-08-23 19:38:02,062 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:38:02,062 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 19:38:02,062 Example #2\n",
            "2020-08-23 19:38:02,062 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:38:02,062 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:38:02,062 \tHypothesis: E mpasi zazi zilenda kala zau .\n",
            "2020-08-23 19:38:02,063 Example #3\n",
            "2020-08-23 19:38:02,063 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:38:02,063 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:38:02,063 \tHypothesis: Avo ovangidi mvutu za yuvu yayi , nga olenda lomba lusadisu kwa Nzambi ?\n",
            "2020-08-23 19:38:02,063 Validation result (greedy) at epoch   8, step    15000: bleu:  18.28, loss: 47547.0664, ppl:   7.6427, duration: 18.8029s\n",
            "2020-08-23 19:38:07,783 Epoch   8: total training loss 4154.28\n",
            "2020-08-23 19:38:07,783 EPOCH 9\n",
            "2020-08-23 19:38:15,382 Epoch   9 Step:    15100 Batch Loss:     2.377771 Tokens per Sec:    15543, Lr: 0.000300\n",
            "2020-08-23 19:38:28,465 Epoch   9 Step:    15200 Batch Loss:     2.287650 Tokens per Sec:    15436, Lr: 0.000300\n",
            "2020-08-23 19:38:41,605 Epoch   9 Step:    15300 Batch Loss:     2.112589 Tokens per Sec:    15833, Lr: 0.000300\n",
            "2020-08-23 19:38:54,654 Epoch   9 Step:    15400 Batch Loss:     2.205203 Tokens per Sec:    16233, Lr: 0.000300\n",
            "2020-08-23 19:39:07,792 Epoch   9 Step:    15500 Batch Loss:     1.861975 Tokens per Sec:    15348, Lr: 0.000300\n",
            "2020-08-23 19:39:20,690 Epoch   9 Step:    15600 Batch Loss:     2.064341 Tokens per Sec:    16066, Lr: 0.000300\n",
            "2020-08-23 19:39:33,789 Epoch   9 Step:    15700 Batch Loss:     2.267606 Tokens per Sec:    16373, Lr: 0.000300\n",
            "2020-08-23 19:39:46,892 Epoch   9 Step:    15800 Batch Loss:     1.764338 Tokens per Sec:    15982, Lr: 0.000300\n",
            "2020-08-23 19:40:00,008 Epoch   9 Step:    15900 Batch Loss:     2.666434 Tokens per Sec:    16017, Lr: 0.000300\n",
            "2020-08-23 19:40:13,108 Epoch   9 Step:    16000 Batch Loss:     1.656864 Tokens per Sec:    15916, Lr: 0.000300\n",
            "2020-08-23 19:40:29,856 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:40:29,856 Saving new checkpoint.\n",
            "2020-08-23 19:40:30,392 Example #0\n",
            "2020-08-23 19:40:30,392 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:40:30,392 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:40:30,392 \tHypothesis: O muntu ovwidi o mfunu wa tambula nzenza yo vevola .\n",
            "2020-08-23 19:40:30,392 Example #1\n",
            "2020-08-23 19:40:30,393 \tSource:     3 : 27 .\n",
            "2020-08-23 19:40:30,393 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:40:30,393 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 19:40:30,393 Example #2\n",
            "2020-08-23 19:40:30,393 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:40:30,393 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:40:30,394 \tHypothesis: E mpasi zazi zilenda twasa mpasi .\n",
            "2020-08-23 19:40:30,394 Example #3\n",
            "2020-08-23 19:40:30,394 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:40:30,394 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:40:30,394 \tHypothesis: Avo ovangidi e mvutu za yuvu yayi , nga olenda bakula vo Nzambi i Nkangu a Nzambi ?\n",
            "2020-08-23 19:40:30,394 Validation result (greedy) at epoch   9, step    16000: bleu:  18.55, loss: 46939.2344, ppl:   7.4466, duration: 17.2860s\n",
            "2020-08-23 19:40:43,687 Epoch   9 Step:    16100 Batch Loss:     1.961584 Tokens per Sec:    15732, Lr: 0.000300\n",
            "2020-08-23 19:40:56,690 Epoch   9 Step:    16200 Batch Loss:     2.282219 Tokens per Sec:    15857, Lr: 0.000300\n",
            "2020-08-23 19:41:09,881 Epoch   9 Step:    16300 Batch Loss:     2.012967 Tokens per Sec:    16309, Lr: 0.000300\n",
            "2020-08-23 19:41:23,059 Epoch   9 Step:    16400 Batch Loss:     1.979529 Tokens per Sec:    15501, Lr: 0.000300\n",
            "2020-08-23 19:41:35,913 Epoch   9 Step:    16500 Batch Loss:     2.069005 Tokens per Sec:    16222, Lr: 0.000300\n",
            "2020-08-23 19:41:49,095 Epoch   9 Step:    16600 Batch Loss:     2.229548 Tokens per Sec:    15709, Lr: 0.000300\n",
            "2020-08-23 19:42:02,170 Epoch   9 Step:    16700 Batch Loss:     1.830247 Tokens per Sec:    16005, Lr: 0.000300\n",
            "2020-08-23 19:42:15,276 Epoch   9 Step:    16800 Batch Loss:     1.884315 Tokens per Sec:    16026, Lr: 0.000300\n",
            "2020-08-23 19:42:28,303 Epoch   9 Step:    16900 Batch Loss:     1.953017 Tokens per Sec:    15704, Lr: 0.000300\n",
            "2020-08-23 19:42:31,114 Epoch   9: total training loss 4021.48\n",
            "2020-08-23 19:42:31,115 EPOCH 10\n",
            "2020-08-23 19:42:41,575 Epoch  10 Step:    17000 Batch Loss:     1.419568 Tokens per Sec:    15387, Lr: 0.000300\n",
            "2020-08-23 19:42:58,437 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:42:58,437 Saving new checkpoint.\n",
            "2020-08-23 19:42:58,920 Example #0\n",
            "2020-08-23 19:42:58,921 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:42:58,921 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:42:58,921 \tHypothesis: Ndiona ovuidi o mfunu muna tambula nzenza yo veza .\n",
            "2020-08-23 19:42:58,921 Example #1\n",
            "2020-08-23 19:42:58,921 \tSource:     3 : 27 .\n",
            "2020-08-23 19:42:58,921 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:42:58,921 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 19:42:58,922 Example #2\n",
            "2020-08-23 19:42:58,922 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:42:58,922 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:42:58,922 \tHypothesis: E mpasi zazi zilenda kala zau .\n",
            "2020-08-23 19:42:58,922 Example #3\n",
            "2020-08-23 19:42:58,922 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:42:58,923 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:42:58,923 \tHypothesis: Avo ovangidi e mvutu za yuvu yayi , nga olenda lomba lusadisu lwa Nzambi ?\n",
            "2020-08-23 19:42:58,923 Validation result (greedy) at epoch  10, step    17000: bleu:  19.37, loss: 46427.3945, ppl:   7.2853, duration: 17.3480s\n",
            "2020-08-23 19:43:12,241 Epoch  10 Step:    17100 Batch Loss:     2.528056 Tokens per Sec:    15787, Lr: 0.000300\n",
            "2020-08-23 19:43:25,338 Epoch  10 Step:    17200 Batch Loss:     2.239781 Tokens per Sec:    15972, Lr: 0.000300\n",
            "2020-08-23 19:43:38,475 Epoch  10 Step:    17300 Batch Loss:     2.380607 Tokens per Sec:    16067, Lr: 0.000300\n",
            "2020-08-23 19:43:51,686 Epoch  10 Step:    17400 Batch Loss:     2.316724 Tokens per Sec:    16103, Lr: 0.000300\n",
            "2020-08-23 19:44:05,204 Epoch  10 Step:    17500 Batch Loss:     2.061285 Tokens per Sec:    15264, Lr: 0.000300\n",
            "2020-08-23 19:44:18,463 Epoch  10 Step:    17600 Batch Loss:     1.917878 Tokens per Sec:    16169, Lr: 0.000300\n",
            "2020-08-23 19:44:31,889 Epoch  10 Step:    17700 Batch Loss:     1.865795 Tokens per Sec:    15464, Lr: 0.000300\n",
            "2020-08-23 19:44:44,818 Epoch  10 Step:    17800 Batch Loss:     1.901951 Tokens per Sec:    15568, Lr: 0.000300\n",
            "2020-08-23 19:44:58,095 Epoch  10 Step:    17900 Batch Loss:     1.839917 Tokens per Sec:    15552, Lr: 0.000300\n",
            "2020-08-23 19:45:11,300 Epoch  10 Step:    18000 Batch Loss:     2.354678 Tokens per Sec:    16084, Lr: 0.000300\n",
            "2020-08-23 19:45:29,749 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:45:29,749 Saving new checkpoint.\n",
            "2020-08-23 19:45:30,274 Example #0\n",
            "2020-08-23 19:45:30,275 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:45:30,275 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:45:30,275 \tHypothesis: O muntu ovavanga vo nzenza yo veza e mboka .\n",
            "2020-08-23 19:45:30,275 Example #1\n",
            "2020-08-23 19:45:30,275 \tSource:     3 : 27 .\n",
            "2020-08-23 19:45:30,276 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:45:30,276 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 19:45:30,276 Example #2\n",
            "2020-08-23 19:45:30,276 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:45:30,276 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:45:30,276 \tHypothesis: Ediadi dilenda kala diampasi .\n",
            "2020-08-23 19:45:30,276 Example #3\n",
            "2020-08-23 19:45:30,277 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:45:30,277 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:45:30,277 \tHypothesis: Avo ovwidi mvutu za yuvu yayi , nga olenda baka nzengo za zaya kana vo Nzambi Okutuvwanga o mfunu ?\n",
            "2020-08-23 19:45:30,277 Validation result (greedy) at epoch  10, step    18000: bleu:  19.68, loss: 45810.4453, ppl:   7.0956, duration: 18.9771s\n",
            "2020-08-23 19:45:43,572 Epoch  10 Step:    18100 Batch Loss:     2.110987 Tokens per Sec:    15194, Lr: 0.000300\n",
            "2020-08-23 19:45:56,496 Epoch  10 Step:    18200 Batch Loss:     2.200032 Tokens per Sec:    15878, Lr: 0.000300\n",
            "2020-08-23 19:46:09,906 Epoch  10 Step:    18300 Batch Loss:     2.338068 Tokens per Sec:    15373, Lr: 0.000300\n",
            "2020-08-23 19:46:23,275 Epoch  10 Step:    18400 Batch Loss:     1.877142 Tokens per Sec:    15832, Lr: 0.000300\n",
            "2020-08-23 19:46:36,276 Epoch  10 Step:    18500 Batch Loss:     2.202134 Tokens per Sec:    16225, Lr: 0.000300\n",
            "2020-08-23 19:46:49,119 Epoch  10 Step:    18600 Batch Loss:     2.445951 Tokens per Sec:    15858, Lr: 0.000300\n",
            "2020-08-23 19:47:02,147 Epoch  10 Step:    18700 Batch Loss:     2.090306 Tokens per Sec:    16004, Lr: 0.000300\n",
            "2020-08-23 19:47:15,343 Epoch  10 Step:    18800 Batch Loss:     2.106205 Tokens per Sec:    15743, Lr: 0.000300\n",
            "2020-08-23 19:47:15,650 Epoch  10: total training loss 3943.89\n",
            "2020-08-23 19:47:15,651 EPOCH 11\n",
            "2020-08-23 19:47:28,840 Epoch  11 Step:    18900 Batch Loss:     2.095929 Tokens per Sec:    15350, Lr: 0.000300\n",
            "2020-08-23 19:47:42,218 Epoch  11 Step:    19000 Batch Loss:     1.993114 Tokens per Sec:    15741, Lr: 0.000300\n",
            "2020-08-23 19:48:00,343 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:48:00,343 Saving new checkpoint.\n",
            "2020-08-23 19:48:00,832 Example #0\n",
            "2020-08-23 19:48:00,832 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:48:00,832 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:48:00,832 \tHypothesis: O muntu kafwete kala ye fu kia tambula nzenza yo veza .\n",
            "2020-08-23 19:48:00,832 Example #1\n",
            "2020-08-23 19:48:00,833 \tSource:     3 : 27 .\n",
            "2020-08-23 19:48:00,833 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:48:00,833 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 19:48:00,833 Example #2\n",
            "2020-08-23 19:48:00,833 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:48:00,833 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:48:00,833 \tHypothesis: Ediadi dilenda kala diampasi mu sunda e mpasi .\n",
            "2020-08-23 19:48:00,834 Example #3\n",
            "2020-08-23 19:48:00,834 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:48:00,834 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:48:00,834 \tHypothesis: Avo ovangidi e mvutu za yuvu yayi , nga olenda lomba lusadisu lwa Nkand’a Nzambi Adieyi Kieleka Kelonganga ?\n",
            "2020-08-23 19:48:00,834 Validation result (greedy) at epoch  11, step    19000: bleu:  19.78, loss: 45406.7148, ppl:   6.9741, duration: 18.6161s\n",
            "2020-08-23 19:48:13,930 Epoch  11 Step:    19100 Batch Loss:     2.346591 Tokens per Sec:    16232, Lr: 0.000300\n",
            "2020-08-23 19:48:27,002 Epoch  11 Step:    19200 Batch Loss:     2.135668 Tokens per Sec:    15769, Lr: 0.000300\n",
            "2020-08-23 19:48:40,071 Epoch  11 Step:    19300 Batch Loss:     2.360722 Tokens per Sec:    15914, Lr: 0.000300\n",
            "2020-08-23 19:48:53,432 Epoch  11 Step:    19400 Batch Loss:     1.898210 Tokens per Sec:    15756, Lr: 0.000300\n",
            "2020-08-23 19:49:06,762 Epoch  11 Step:    19500 Batch Loss:     2.471486 Tokens per Sec:    15458, Lr: 0.000300\n",
            "2020-08-23 19:49:19,757 Epoch  11 Step:    19600 Batch Loss:     1.934648 Tokens per Sec:    15776, Lr: 0.000300\n",
            "2020-08-23 19:49:32,712 Epoch  11 Step:    19700 Batch Loss:     2.161445 Tokens per Sec:    16159, Lr: 0.000300\n",
            "2020-08-23 19:49:46,038 Epoch  11 Step:    19800 Batch Loss:     1.923644 Tokens per Sec:    15358, Lr: 0.000300\n",
            "2020-08-23 19:49:59,055 Epoch  11 Step:    19900 Batch Loss:     2.368609 Tokens per Sec:    15990, Lr: 0.000300\n",
            "2020-08-23 19:50:12,133 Epoch  11 Step:    20000 Batch Loss:     1.815306 Tokens per Sec:    16093, Lr: 0.000300\n",
            "2020-08-23 19:50:28,220 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:50:28,220 Saving new checkpoint.\n",
            "2020-08-23 19:50:28,726 Example #0\n",
            "2020-08-23 19:50:28,727 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:50:28,727 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:50:28,727 \tHypothesis: O muntu kafwete kala ye fu kia tambula nzenza yo veza e nzimbu .\n",
            "2020-08-23 19:50:28,727 Example #1\n",
            "2020-08-23 19:50:28,727 \tSource:     3 : 27 .\n",
            "2020-08-23 19:50:28,727 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:50:28,728 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 19:50:28,728 Example #2\n",
            "2020-08-23 19:50:28,728 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:50:28,728 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:50:28,728 \tHypothesis: E mpasi zazi zilenda kala zampasi .\n",
            "2020-08-23 19:50:28,728 Example #3\n",
            "2020-08-23 19:50:28,729 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:50:28,729 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:50:28,729 \tHypothesis: Avo ovangidi e mvutu za yuvu yayi , nga olenda lomba lusadisu lwa Nkand’a Nzambi mu kuma kia nkanda wau ?\n",
            "2020-08-23 19:50:28,729 Validation result (greedy) at epoch  11, step    20000: bleu:  19.83, loss: 44904.8711, ppl:   6.8260, duration: 16.5961s\n",
            "2020-08-23 19:50:41,857 Epoch  11 Step:    20100 Batch Loss:     2.043173 Tokens per Sec:    15644, Lr: 0.000300\n",
            "2020-08-23 19:50:54,860 Epoch  11 Step:    20200 Batch Loss:     2.262854 Tokens per Sec:    16108, Lr: 0.000300\n",
            "2020-08-23 19:51:07,868 Epoch  11 Step:    20300 Batch Loss:     2.276496 Tokens per Sec:    16240, Lr: 0.000300\n",
            "2020-08-23 19:51:20,749 Epoch  11 Step:    20400 Batch Loss:     2.057249 Tokens per Sec:    15920, Lr: 0.000300\n",
            "2020-08-23 19:51:34,010 Epoch  11 Step:    20500 Batch Loss:     2.050223 Tokens per Sec:    15720, Lr: 0.000300\n",
            "2020-08-23 19:51:47,725 Epoch  11 Step:    20600 Batch Loss:     1.991785 Tokens per Sec:    14951, Lr: 0.000300\n",
            "2020-08-23 19:51:58,640 Epoch  11: total training loss 3861.75\n",
            "2020-08-23 19:51:58,640 EPOCH 12\n",
            "2020-08-23 19:52:01,299 Epoch  12 Step:    20700 Batch Loss:     2.118076 Tokens per Sec:    14585, Lr: 0.000300\n",
            "2020-08-23 19:52:14,510 Epoch  12 Step:    20800 Batch Loss:     2.741066 Tokens per Sec:    15821, Lr: 0.000300\n",
            "2020-08-23 19:52:27,765 Epoch  12 Step:    20900 Batch Loss:     2.055276 Tokens per Sec:    16045, Lr: 0.000300\n",
            "2020-08-23 19:52:40,948 Epoch  12 Step:    21000 Batch Loss:     2.324747 Tokens per Sec:    15543, Lr: 0.000300\n",
            "2020-08-23 19:53:00,917 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:53:00,917 Saving new checkpoint.\n",
            "2020-08-23 19:53:01,396 Example #0\n",
            "2020-08-23 19:53:01,397 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:53:01,397 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:53:01,397 \tHypothesis: O muntu ovwidi o mfunu wa tambula nzenza yo veza .\n",
            "2020-08-23 19:53:01,397 Example #1\n",
            "2020-08-23 19:53:01,397 \tSource:     3 : 27 .\n",
            "2020-08-23 19:53:01,397 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:53:01,397 \tHypothesis: 3 :⁠ 27 .\n",
            "2020-08-23 19:53:01,397 Example #2\n",
            "2020-08-23 19:53:01,398 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:53:01,398 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:53:01,398 \tHypothesis: E mpasi zazi zilenda kala zampasi .\n",
            "2020-08-23 19:53:01,398 Example #3\n",
            "2020-08-23 19:53:01,398 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:53:01,398 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:53:01,398 \tHypothesis: Avo ovwidi mvutu za yuvu yayi , nga olenda lomba lusadisu lwa Nzambi ?\n",
            "2020-08-23 19:53:01,399 Validation result (greedy) at epoch  12, step    21000: bleu:  20.71, loss: 44580.3867, ppl:   6.7319, duration: 20.4507s\n",
            "2020-08-23 19:53:14,413 Epoch  12 Step:    21100 Batch Loss:     1.966862 Tokens per Sec:    16052, Lr: 0.000300\n",
            "2020-08-23 19:53:27,407 Epoch  12 Step:    21200 Batch Loss:     2.155842 Tokens per Sec:    16352, Lr: 0.000300\n",
            "2020-08-23 19:53:41,265 Epoch  12 Step:    21300 Batch Loss:     1.725028 Tokens per Sec:    14993, Lr: 0.000300\n",
            "2020-08-23 19:53:54,767 Epoch  12 Step:    21400 Batch Loss:     2.173938 Tokens per Sec:    15394, Lr: 0.000300\n",
            "2020-08-23 19:54:07,761 Epoch  12 Step:    21500 Batch Loss:     2.038915 Tokens per Sec:    15875, Lr: 0.000300\n",
            "2020-08-23 19:54:21,100 Epoch  12 Step:    21600 Batch Loss:     1.656844 Tokens per Sec:    15637, Lr: 0.000300\n",
            "2020-08-23 19:54:34,287 Epoch  12 Step:    21700 Batch Loss:     2.428443 Tokens per Sec:    16139, Lr: 0.000300\n",
            "2020-08-23 19:54:47,363 Epoch  12 Step:    21800 Batch Loss:     1.996291 Tokens per Sec:    15632, Lr: 0.000300\n",
            "2020-08-23 19:55:00,787 Epoch  12 Step:    21900 Batch Loss:     2.105781 Tokens per Sec:    15414, Lr: 0.000300\n",
            "2020-08-23 19:55:14,025 Epoch  12 Step:    22000 Batch Loss:     2.019961 Tokens per Sec:    15921, Lr: 0.000300\n",
            "2020-08-23 19:55:33,350 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:55:33,350 Saving new checkpoint.\n",
            "2020-08-23 19:55:33,836 Example #0\n",
            "2020-08-23 19:55:33,836 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:55:33,837 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:55:33,837 \tHypothesis: Muntu ovwidi mfunu wa tambula nzenza yo veza .\n",
            "2020-08-23 19:55:33,837 Example #1\n",
            "2020-08-23 19:55:33,837 \tSource:     3 : 27 .\n",
            "2020-08-23 19:55:33,837 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:55:33,837 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 19:55:33,838 Example #2\n",
            "2020-08-23 19:55:33,838 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:55:33,838 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:55:33,838 \tHypothesis: E mpasi zazi zilenda kala zampasi .\n",
            "2020-08-23 19:55:33,838 Example #3\n",
            "2020-08-23 19:55:33,839 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:55:33,839 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:55:33,839 \tHypothesis: Avo ovene mvutu za yuvu yayi , nga olenda lomba lusadisu lwa nkanda wau ?\n",
            "2020-08-23 19:55:33,839 Validation result (greedy) at epoch  12, step    22000: bleu:  20.51, loss: 44010.3945, ppl:   6.5697, duration: 19.8138s\n",
            "2020-08-23 19:55:46,979 Epoch  12 Step:    22100 Batch Loss:     2.009534 Tokens per Sec:    15980, Lr: 0.000300\n",
            "2020-08-23 19:56:00,151 Epoch  12 Step:    22200 Batch Loss:     2.687652 Tokens per Sec:    15681, Lr: 0.000300\n",
            "2020-08-23 19:56:13,108 Epoch  12 Step:    22300 Batch Loss:     2.272647 Tokens per Sec:    15836, Lr: 0.000300\n",
            "2020-08-23 19:56:26,075 Epoch  12 Step:    22400 Batch Loss:     2.274848 Tokens per Sec:    15566, Lr: 0.000300\n",
            "2020-08-23 19:56:39,315 Epoch  12 Step:    22500 Batch Loss:     1.867536 Tokens per Sec:    15704, Lr: 0.000300\n",
            "2020-08-23 19:56:47,558 Epoch  12: total training loss 3788.25\n",
            "2020-08-23 19:56:47,558 EPOCH 13\n",
            "2020-08-23 19:56:52,831 Epoch  13 Step:    22600 Batch Loss:     1.703483 Tokens per Sec:    14475, Lr: 0.000300\n",
            "2020-08-23 19:57:06,125 Epoch  13 Step:    22700 Batch Loss:     1.916950 Tokens per Sec:    15496, Lr: 0.000300\n",
            "2020-08-23 19:57:19,241 Epoch  13 Step:    22800 Batch Loss:     2.036803 Tokens per Sec:    16216, Lr: 0.000300\n",
            "2020-08-23 19:57:32,460 Epoch  13 Step:    22900 Batch Loss:     1.844888 Tokens per Sec:    15612, Lr: 0.000300\n",
            "2020-08-23 19:57:45,580 Epoch  13 Step:    23000 Batch Loss:     2.269286 Tokens per Sec:    16296, Lr: 0.000300\n",
            "2020-08-23 19:58:04,513 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 19:58:04,514 Saving new checkpoint.\n",
            "2020-08-23 19:58:05,013 Example #0\n",
            "2020-08-23 19:58:05,014 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 19:58:05,014 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 19:58:05,014 \tHypothesis: O muntu kafwete kala ye fu kia tambula nzenza yo veza .\n",
            "2020-08-23 19:58:05,014 Example #1\n",
            "2020-08-23 19:58:05,014 \tSource:     3 : 27 .\n",
            "2020-08-23 19:58:05,014 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 19:58:05,014 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 19:58:05,014 Example #2\n",
            "2020-08-23 19:58:05,015 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 19:58:05,015 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 19:58:05,015 \tHypothesis: E diambu diadi dilenda kala diampasi .\n",
            "2020-08-23 19:58:05,015 Example #3\n",
            "2020-08-23 19:58:05,015 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 19:58:05,015 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 19:58:05,015 \tHypothesis: Avo ovangidi e mvutu za yuvu yayi , nga olenda lomba lusadisu lwa Nkand’a Nzambi ?\n",
            "2020-08-23 19:58:05,016 Validation result (greedy) at epoch  13, step    23000: bleu:  20.78, loss: 43653.6523, ppl:   6.4703, duration: 19.4356s\n",
            "2020-08-23 19:58:18,151 Epoch  13 Step:    23100 Batch Loss:     1.735838 Tokens per Sec:    16091, Lr: 0.000300\n",
            "2020-08-23 19:58:31,261 Epoch  13 Step:    23200 Batch Loss:     2.071996 Tokens per Sec:    15706, Lr: 0.000300\n",
            "2020-08-23 19:58:44,474 Epoch  13 Step:    23300 Batch Loss:     2.021514 Tokens per Sec:    15612, Lr: 0.000300\n",
            "2020-08-23 19:58:57,867 Epoch  13 Step:    23400 Batch Loss:     2.456178 Tokens per Sec:    15210, Lr: 0.000300\n",
            "2020-08-23 19:59:11,194 Epoch  13 Step:    23500 Batch Loss:     1.736168 Tokens per Sec:    15667, Lr: 0.000300\n",
            "2020-08-23 19:59:24,211 Epoch  13 Step:    23600 Batch Loss:     2.322508 Tokens per Sec:    15919, Lr: 0.000300\n",
            "2020-08-23 19:59:37,616 Epoch  13 Step:    23700 Batch Loss:     1.213629 Tokens per Sec:    15852, Lr: 0.000300\n",
            "2020-08-23 19:59:51,319 Epoch  13 Step:    23800 Batch Loss:     1.731848 Tokens per Sec:    15021, Lr: 0.000300\n",
            "2020-08-23 20:00:04,765 Epoch  13 Step:    23900 Batch Loss:     1.937467 Tokens per Sec:    15646, Lr: 0.000300\n",
            "2020-08-23 20:00:17,829 Epoch  13 Step:    24000 Batch Loss:     1.770584 Tokens per Sec:    15580, Lr: 0.000300\n",
            "2020-08-23 20:00:35,947 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:00:35,947 Saving new checkpoint.\n",
            "2020-08-23 20:00:36,429 Example #0\n",
            "2020-08-23 20:00:36,430 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:00:36,430 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:00:36,430 \tHypothesis: Muntu ovuidi o mfunu wa tambula nzenza yo veza e mvutu .\n",
            "2020-08-23 20:00:36,430 Example #1\n",
            "2020-08-23 20:00:36,430 \tSource:     3 : 27 .\n",
            "2020-08-23 20:00:36,430 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:00:36,430 \tHypothesis: 3 :⁠ 27 .\n",
            "2020-08-23 20:00:36,430 Example #2\n",
            "2020-08-23 20:00:36,431 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:00:36,431 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:00:36,431 \tHypothesis: E diambu diadi dilenda kala diampasi mu singika e mpasi .\n",
            "2020-08-23 20:00:36,431 Example #3\n",
            "2020-08-23 20:00:36,431 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:00:36,431 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:00:36,431 \tHypothesis: Avo ovangidi e mvutu za yuvu yayi , nga olenda lomba lusadisu lwa nkanda wau ?\n",
            "2020-08-23 20:00:36,431 Validation result (greedy) at epoch  13, step    24000: bleu:  20.90, loss: 43375.7617, ppl:   6.3938, duration: 18.6026s\n",
            "2020-08-23 20:00:49,882 Epoch  13 Step:    24100 Batch Loss:     1.996735 Tokens per Sec:    15616, Lr: 0.000300\n",
            "2020-08-23 20:01:03,128 Epoch  13 Step:    24200 Batch Loss:     2.155006 Tokens per Sec:    15901, Lr: 0.000300\n",
            "2020-08-23 20:01:16,489 Epoch  13 Step:    24300 Batch Loss:     2.207428 Tokens per Sec:    15482, Lr: 0.000300\n",
            "2020-08-23 20:01:29,787 Epoch  13 Step:    24400 Batch Loss:     1.679408 Tokens per Sec:    15629, Lr: 0.000300\n",
            "2020-08-23 20:01:35,330 Epoch  13: total training loss 3723.88\n",
            "2020-08-23 20:01:35,330 EPOCH 14\n",
            "2020-08-23 20:01:42,937 Epoch  14 Step:    24500 Batch Loss:     1.543464 Tokens per Sec:    15333, Lr: 0.000300\n",
            "2020-08-23 20:01:56,092 Epoch  14 Step:    24600 Batch Loss:     1.819636 Tokens per Sec:    15595, Lr: 0.000300\n",
            "2020-08-23 20:02:09,182 Epoch  14 Step:    24700 Batch Loss:     1.317588 Tokens per Sec:    15831, Lr: 0.000300\n",
            "2020-08-23 20:02:22,559 Epoch  14 Step:    24800 Batch Loss:     1.922388 Tokens per Sec:    15932, Lr: 0.000300\n",
            "2020-08-23 20:02:35,766 Epoch  14 Step:    24900 Batch Loss:     2.026421 Tokens per Sec:    15785, Lr: 0.000300\n",
            "2020-08-23 20:02:48,834 Epoch  14 Step:    25000 Batch Loss:     1.898275 Tokens per Sec:    15889, Lr: 0.000300\n",
            "2020-08-23 20:03:09,546 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:03:09,546 Saving new checkpoint.\n",
            "2020-08-23 20:03:10,053 Example #0\n",
            "2020-08-23 20:03:10,053 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:03:10,053 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:03:10,053 \tHypothesis: Ndiona ovuidi o mfunu wa tambula nzenza ye mbumba .\n",
            "2020-08-23 20:03:10,054 Example #1\n",
            "2020-08-23 20:03:10,054 \tSource:     3 : 27 .\n",
            "2020-08-23 20:03:10,054 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:03:10,054 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 20:03:10,054 Example #2\n",
            "2020-08-23 20:03:10,054 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:03:10,054 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:03:10,055 \tHypothesis: E mpasi zazi zilenda twasa mpasi .\n",
            "2020-08-23 20:03:10,055 Example #3\n",
            "2020-08-23 20:03:10,055 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:03:10,055 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:03:10,055 \tHypothesis: Avo ke wau ko vo mvutu za yuvu yayi , nga olenda lomba lusadisu lwa nkanda wau ?\n",
            "2020-08-23 20:03:10,055 Validation result (greedy) at epoch  14, step    25000: bleu:  21.18, loss: 43175.9375, ppl:   6.3394, duration: 21.2212s\n",
            "2020-08-23 20:03:23,080 Epoch  14 Step:    25100 Batch Loss:     1.808245 Tokens per Sec:    15648, Lr: 0.000300\n",
            "2020-08-23 20:03:36,369 Epoch  14 Step:    25200 Batch Loss:     2.226291 Tokens per Sec:    15765, Lr: 0.000300\n",
            "2020-08-23 20:03:49,345 Epoch  14 Step:    25300 Batch Loss:     2.493289 Tokens per Sec:    15959, Lr: 0.000300\n",
            "2020-08-23 20:04:02,266 Epoch  14 Step:    25400 Batch Loss:     1.907088 Tokens per Sec:    15834, Lr: 0.000300\n",
            "2020-08-23 20:04:15,724 Epoch  14 Step:    25500 Batch Loss:     1.855041 Tokens per Sec:    16206, Lr: 0.000300\n",
            "2020-08-23 20:04:28,979 Epoch  14 Step:    25600 Batch Loss:     1.833363 Tokens per Sec:    15806, Lr: 0.000300\n",
            "2020-08-23 20:04:42,060 Epoch  14 Step:    25700 Batch Loss:     1.903535 Tokens per Sec:    16140, Lr: 0.000300\n",
            "2020-08-23 20:04:55,278 Epoch  14 Step:    25800 Batch Loss:     1.956331 Tokens per Sec:    15699, Lr: 0.000300\n",
            "2020-08-23 20:05:08,310 Epoch  14 Step:    25900 Batch Loss:     1.933120 Tokens per Sec:    16155, Lr: 0.000300\n",
            "2020-08-23 20:05:21,538 Epoch  14 Step:    26000 Batch Loss:     1.804148 Tokens per Sec:    15837, Lr: 0.000300\n",
            "2020-08-23 20:05:36,644 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:05:36,644 Saving new checkpoint.\n",
            "2020-08-23 20:05:37,119 Example #0\n",
            "2020-08-23 20:05:37,119 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:05:37,120 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:05:37,120 \tHypothesis: Muntu ovwidi o mfunu wa tambula nzenza yo veza .\n",
            "2020-08-23 20:05:37,120 Example #1\n",
            "2020-08-23 20:05:37,120 \tSource:     3 : 27 .\n",
            "2020-08-23 20:05:37,120 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:05:37,120 \tHypothesis: 3 :⁠ 27 .\n",
            "2020-08-23 20:05:37,120 Example #2\n",
            "2020-08-23 20:05:37,121 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:05:37,121 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:05:37,121 \tHypothesis: E diambu diadi dilenda kala diampasi .\n",
            "2020-08-23 20:05:37,121 Example #3\n",
            "2020-08-23 20:05:37,121 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:05:37,121 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:05:37,121 \tHypothesis: Avo ke dialudi ko vo e mvutu za yuvu yayi , nga olenda lomba lusadisu kwa Nzambi ?\n",
            "2020-08-23 20:05:37,121 Validation result (greedy) at epoch  14, step    26000: bleu:  21.25, loss: 42681.6797, ppl:   6.2068, duration: 15.5835s\n",
            "2020-08-23 20:05:50,120 Epoch  14 Step:    26100 Batch Loss:     1.616809 Tokens per Sec:    16034, Lr: 0.000300\n",
            "2020-08-23 20:06:03,096 Epoch  14 Step:    26200 Batch Loss:     2.113882 Tokens per Sec:    16077, Lr: 0.000300\n",
            "2020-08-23 20:06:16,246 Epoch  14 Step:    26300 Batch Loss:     2.015969 Tokens per Sec:    15820, Lr: 0.000300\n",
            "2020-08-23 20:06:18,429 Epoch  14: total training loss 3654.62\n",
            "2020-08-23 20:06:18,429 EPOCH 15\n",
            "2020-08-23 20:06:29,569 Epoch  15 Step:    26400 Batch Loss:     1.710638 Tokens per Sec:    14799, Lr: 0.000300\n",
            "2020-08-23 20:06:42,588 Epoch  15 Step:    26500 Batch Loss:     2.070730 Tokens per Sec:    15982, Lr: 0.000300\n",
            "2020-08-23 20:06:55,648 Epoch  15 Step:    26600 Batch Loss:     1.576471 Tokens per Sec:    16019, Lr: 0.000300\n",
            "2020-08-23 20:07:08,717 Epoch  15 Step:    26700 Batch Loss:     1.930906 Tokens per Sec:    16289, Lr: 0.000300\n",
            "2020-08-23 20:07:21,738 Epoch  15 Step:    26800 Batch Loss:     2.052508 Tokens per Sec:    16068, Lr: 0.000300\n",
            "2020-08-23 20:07:34,856 Epoch  15 Step:    26900 Batch Loss:     1.975524 Tokens per Sec:    16033, Lr: 0.000300\n",
            "2020-08-23 20:07:48,015 Epoch  15 Step:    27000 Batch Loss:     1.743212 Tokens per Sec:    15820, Lr: 0.000300\n",
            "2020-08-23 20:08:04,705 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:08:04,705 Saving new checkpoint.\n",
            "2020-08-23 20:08:05,215 Example #0\n",
            "2020-08-23 20:08:05,215 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:08:05,215 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:08:05,215 \tHypothesis: Mosi muna yau i zola nzenza ye mbumba .\n",
            "2020-08-23 20:08:05,215 Example #1\n",
            "2020-08-23 20:08:05,216 \tSource:     3 : 27 .\n",
            "2020-08-23 20:08:05,216 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:08:05,216 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 20:08:05,216 Example #2\n",
            "2020-08-23 20:08:05,216 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:08:05,216 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:08:05,216 \tHypothesis: E mpasi zazi zilenda kala zampasi .\n",
            "2020-08-23 20:08:05,217 Example #3\n",
            "2020-08-23 20:08:05,217 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:08:05,217 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:08:05,217 \tHypothesis: Avo ke tuna ye mvutu za yuvu yayi ko , nga olenda lomba lusadisu lwa nkanda wau ?\n",
            "2020-08-23 20:08:05,217 Validation result (greedy) at epoch  15, step    27000: bleu:  21.87, loss: 42504.4688, ppl:   6.1599, duration: 17.2015s\n",
            "2020-08-23 20:08:18,059 Epoch  15 Step:    27100 Batch Loss:     1.581994 Tokens per Sec:    15352, Lr: 0.000300\n",
            "2020-08-23 20:08:31,177 Epoch  15 Step:    27200 Batch Loss:     2.037042 Tokens per Sec:    15893, Lr: 0.000300\n",
            "2020-08-23 20:08:44,269 Epoch  15 Step:    27300 Batch Loss:     2.597946 Tokens per Sec:    15960, Lr: 0.000300\n",
            "2020-08-23 20:08:57,333 Epoch  15 Step:    27400 Batch Loss:     1.972149 Tokens per Sec:    15745, Lr: 0.000300\n",
            "2020-08-23 20:09:10,305 Epoch  15 Step:    27500 Batch Loss:     1.915354 Tokens per Sec:    16074, Lr: 0.000300\n",
            "2020-08-23 20:09:23,528 Epoch  15 Step:    27600 Batch Loss:     1.906243 Tokens per Sec:    15830, Lr: 0.000300\n",
            "2020-08-23 20:09:36,665 Epoch  15 Step:    27700 Batch Loss:     1.882082 Tokens per Sec:    16108, Lr: 0.000300\n",
            "2020-08-23 20:09:49,596 Epoch  15 Step:    27800 Batch Loss:     2.557809 Tokens per Sec:    16254, Lr: 0.000300\n",
            "2020-08-23 20:10:02,694 Epoch  15 Step:    27900 Batch Loss:     1.854912 Tokens per Sec:    16434, Lr: 0.000300\n",
            "2020-08-23 20:10:16,148 Epoch  15 Step:    28000 Batch Loss:     2.072885 Tokens per Sec:    15294, Lr: 0.000300\n",
            "2020-08-23 20:10:33,494 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:10:33,494 Saving new checkpoint.\n",
            "2020-08-23 20:10:34,015 Example #0\n",
            "2020-08-23 20:10:34,016 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:10:34,016 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:10:34,016 \tHypothesis: Ndiona ovuidi o mfunu wa tambula nzenza ye mbumba .\n",
            "2020-08-23 20:10:34,016 Example #1\n",
            "2020-08-23 20:10:34,016 \tSource:     3 : 27 .\n",
            "2020-08-23 20:10:34,016 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:10:34,017 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 20:10:34,017 Example #2\n",
            "2020-08-23 20:10:34,017 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:10:34,017 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:10:34,017 \tHypothesis: E mpasi zazi zilenda kala zampasi .\n",
            "2020-08-23 20:10:34,017 Example #3\n",
            "2020-08-23 20:10:34,018 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:10:34,018 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:10:34,018 \tHypothesis: Avo ke vena mvutu za yuvu yayi ko , nga olenda lomba lusadisu lwa nkanda wau mu kuma kia Nkand’a Nzambi Adieyi Kieleka Kelonganga ?\n",
            "2020-08-23 20:10:34,018 Validation result (greedy) at epoch  15, step    28000: bleu:  21.78, loss: 42270.0352, ppl:   6.0984, duration: 17.8700s\n",
            "2020-08-23 20:10:47,111 Epoch  15 Step:    28100 Batch Loss:     2.050957 Tokens per Sec:    15838, Lr: 0.000300\n",
            "2020-08-23 20:10:59,729 Epoch  15: total training loss 3618.69\n",
            "2020-08-23 20:10:59,729 EPOCH 16\n",
            "2020-08-23 20:11:00,518 Epoch  16 Step:    28200 Batch Loss:     1.826695 Tokens per Sec:    11191, Lr: 0.000300\n",
            "2020-08-23 20:11:13,929 Epoch  16 Step:    28300 Batch Loss:     2.089294 Tokens per Sec:    15538, Lr: 0.000300\n",
            "2020-08-23 20:11:27,280 Epoch  16 Step:    28400 Batch Loss:     1.999416 Tokens per Sec:    15995, Lr: 0.000300\n",
            "2020-08-23 20:11:40,462 Epoch  16 Step:    28500 Batch Loss:     1.978830 Tokens per Sec:    15671, Lr: 0.000300\n",
            "2020-08-23 20:11:53,724 Epoch  16 Step:    28600 Batch Loss:     1.809888 Tokens per Sec:    15771, Lr: 0.000300\n",
            "2020-08-23 20:12:06,777 Epoch  16 Step:    28700 Batch Loss:     1.643249 Tokens per Sec:    16182, Lr: 0.000300\n",
            "2020-08-23 20:12:19,904 Epoch  16 Step:    28800 Batch Loss:     2.092191 Tokens per Sec:    16093, Lr: 0.000300\n",
            "2020-08-23 20:12:33,338 Epoch  16 Step:    28900 Batch Loss:     2.057646 Tokens per Sec:    15616, Lr: 0.000300\n",
            "2020-08-23 20:12:46,183 Epoch  16 Step:    29000 Batch Loss:     1.922766 Tokens per Sec:    15988, Lr: 0.000300\n",
            "2020-08-23 20:13:03,173 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:13:03,173 Saving new checkpoint.\n",
            "2020-08-23 20:13:03,682 Example #0\n",
            "2020-08-23 20:13:03,682 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:13:03,682 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:13:03,682 \tHypothesis: Ndiona ovwanga o mfunu wa tambula nzenza ye mbumba .\n",
            "2020-08-23 20:13:03,682 Example #1\n",
            "2020-08-23 20:13:03,682 \tSource:     3 : 27 .\n",
            "2020-08-23 20:13:03,682 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:13:03,683 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 20:13:03,683 Example #2\n",
            "2020-08-23 20:13:03,683 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:13:03,683 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:13:03,683 \tHypothesis: E mpasi zazi zilenda kala zampasi .\n",
            "2020-08-23 20:13:03,683 Example #3\n",
            "2020-08-23 20:13:03,684 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:13:03,684 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:13:03,684 \tHypothesis: Avo ovangidi mvutu za yuvu yayi , nga olenda lomba lusadisu lwa nkanda wau ?\n",
            "2020-08-23 20:13:03,684 Validation result (greedy) at epoch  16, step    29000: bleu:  22.41, loss: 41851.3242, ppl:   5.9902, duration: 17.5005s\n",
            "2020-08-23 20:13:16,525 Epoch  16 Step:    29100 Batch Loss:     1.660100 Tokens per Sec:    15822, Lr: 0.000300\n",
            "2020-08-23 20:13:29,499 Epoch  16 Step:    29200 Batch Loss:     1.616845 Tokens per Sec:    15563, Lr: 0.000300\n",
            "2020-08-23 20:13:42,330 Epoch  16 Step:    29300 Batch Loss:     2.253055 Tokens per Sec:    16421, Lr: 0.000300\n",
            "2020-08-23 20:13:55,426 Epoch  16 Step:    29400 Batch Loss:     1.881557 Tokens per Sec:    16056, Lr: 0.000300\n",
            "2020-08-23 20:14:08,548 Epoch  16 Step:    29500 Batch Loss:     1.679056 Tokens per Sec:    16373, Lr: 0.000300\n",
            "2020-08-23 20:14:21,745 Epoch  16 Step:    29600 Batch Loss:     1.540159 Tokens per Sec:    15716, Lr: 0.000300\n",
            "2020-08-23 20:14:35,008 Epoch  16 Step:    29700 Batch Loss:     1.934872 Tokens per Sec:    15637, Lr: 0.000300\n",
            "2020-08-23 20:14:48,437 Epoch  16 Step:    29800 Batch Loss:     1.806863 Tokens per Sec:    15333, Lr: 0.000300\n",
            "2020-08-23 20:15:01,518 Epoch  16 Step:    29900 Batch Loss:     1.724935 Tokens per Sec:    15507, Lr: 0.000300\n",
            "2020-08-23 20:15:14,543 Epoch  16 Step:    30000 Batch Loss:     2.186793 Tokens per Sec:    16211, Lr: 0.000300\n",
            "2020-08-23 20:15:30,309 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:15:30,309 Saving new checkpoint.\n",
            "2020-08-23 20:15:30,823 Example #0\n",
            "2020-08-23 20:15:30,824 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:15:30,824 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:15:30,824 \tHypothesis: Mosi muna yau i nzenza , i vuata o mfunu .\n",
            "2020-08-23 20:15:30,824 Example #1\n",
            "2020-08-23 20:15:30,824 \tSource:     3 : 27 .\n",
            "2020-08-23 20:15:30,825 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:15:30,825 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 20:15:30,825 Example #2\n",
            "2020-08-23 20:15:30,825 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:15:30,825 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:15:30,825 \tHypothesis: E mpasi zazi zilenda kala zampasi .\n",
            "2020-08-23 20:15:30,825 Example #3\n",
            "2020-08-23 20:15:30,826 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:15:30,826 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:15:30,826 \tHypothesis: Avo ke vwidi mvutu za yuvu yayi ko , nga olenda lomba lusadisu kwa Nzambi mu kuma kia nkanda Os ?\n",
            "2020-08-23 20:15:30,826 Validation result (greedy) at epoch  16, step    30000: bleu:  22.28, loss: 41725.0312, ppl:   5.9579, duration: 16.2828s\n",
            "2020-08-23 20:15:40,774 Epoch  16: total training loss 3566.25\n",
            "2020-08-23 20:15:40,775 EPOCH 17\n",
            "2020-08-23 20:15:44,398 Epoch  17 Step:    30100 Batch Loss:     2.030315 Tokens per Sec:    15341, Lr: 0.000300\n",
            "2020-08-23 20:15:57,476 Epoch  17 Step:    30200 Batch Loss:     1.579385 Tokens per Sec:    15959, Lr: 0.000300\n",
            "2020-08-23 20:16:10,670 Epoch  17 Step:    30300 Batch Loss:     2.193505 Tokens per Sec:    15735, Lr: 0.000300\n",
            "2020-08-23 20:16:23,586 Epoch  17 Step:    30400 Batch Loss:     1.953663 Tokens per Sec:    16094, Lr: 0.000300\n",
            "2020-08-23 20:16:36,894 Epoch  17 Step:    30500 Batch Loss:     1.567601 Tokens per Sec:    15460, Lr: 0.000300\n",
            "2020-08-23 20:16:49,979 Epoch  17 Step:    30600 Batch Loss:     1.805760 Tokens per Sec:    15323, Lr: 0.000300\n",
            "2020-08-23 20:17:03,067 Epoch  17 Step:    30700 Batch Loss:     1.654608 Tokens per Sec:    16371, Lr: 0.000300\n",
            "2020-08-23 20:17:15,955 Epoch  17 Step:    30800 Batch Loss:     1.936083 Tokens per Sec:    15995, Lr: 0.000300\n",
            "2020-08-23 20:17:29,069 Epoch  17 Step:    30900 Batch Loss:     1.563677 Tokens per Sec:    16335, Lr: 0.000300\n",
            "2020-08-23 20:17:42,367 Epoch  17 Step:    31000 Batch Loss:     2.001935 Tokens per Sec:    15685, Lr: 0.000300\n",
            "2020-08-23 20:17:58,585 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:17:58,585 Saving new checkpoint.\n",
            "2020-08-23 20:17:59,097 Example #0\n",
            "2020-08-23 20:17:59,098 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:17:59,098 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:17:59,098 \tHypothesis: O muntu ovwa e nzenza yo vezwa .\n",
            "2020-08-23 20:17:59,098 Example #1\n",
            "2020-08-23 20:17:59,099 \tSource:     3 : 27 .\n",
            "2020-08-23 20:17:59,099 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:17:59,099 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 20:17:59,099 Example #2\n",
            "2020-08-23 20:17:59,099 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:17:59,099 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:17:59,099 \tHypothesis: E mpasi zazi zilenda kala zampasi .\n",
            "2020-08-23 20:17:59,099 Example #3\n",
            "2020-08-23 20:17:59,100 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:17:59,100 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:17:59,100 \tHypothesis: Avo ke lukatikisu ko vo e mvutu za yuvu yayi , nga olenda lomba lusadisu lwa nkanda mivaikisanga o Nzambi Adieyi Kieleka Kelonganga ?\n",
            "2020-08-23 20:17:59,100 Validation result (greedy) at epoch  17, step    31000: bleu:  22.49, loss: 41453.7305, ppl:   5.8892, duration: 16.7332s\n",
            "2020-08-23 20:18:12,081 Epoch  17 Step:    31100 Batch Loss:     1.930433 Tokens per Sec:    15564, Lr: 0.000300\n",
            "2020-08-23 20:18:25,117 Epoch  17 Step:    31200 Batch Loss:     1.853744 Tokens per Sec:    15853, Lr: 0.000300\n",
            "2020-08-23 20:18:38,450 Epoch  17 Step:    31300 Batch Loss:     1.684939 Tokens per Sec:    15944, Lr: 0.000300\n",
            "2020-08-23 20:18:51,428 Epoch  17 Step:    31400 Batch Loss:     1.888205 Tokens per Sec:    16345, Lr: 0.000300\n",
            "2020-08-23 20:19:04,398 Epoch  17 Step:    31500 Batch Loss:     1.561939 Tokens per Sec:    16057, Lr: 0.000300\n",
            "2020-08-23 20:19:17,512 Epoch  17 Step:    31600 Batch Loss:     1.735479 Tokens per Sec:    15406, Lr: 0.000300\n",
            "2020-08-23 20:19:30,494 Epoch  17 Step:    31700 Batch Loss:     1.716031 Tokens per Sec:    16184, Lr: 0.000300\n",
            "2020-08-23 20:19:43,825 Epoch  17 Step:    31800 Batch Loss:     1.916066 Tokens per Sec:    15870, Lr: 0.000300\n",
            "2020-08-23 20:19:57,061 Epoch  17 Step:    31900 Batch Loss:     1.682570 Tokens per Sec:    15831, Lr: 0.000300\n",
            "2020-08-23 20:20:03,755 Epoch  17: total training loss 3527.61\n",
            "2020-08-23 20:20:03,756 EPOCH 18\n",
            "2020-08-23 20:20:10,386 Epoch  18 Step:    32000 Batch Loss:     2.023590 Tokens per Sec:    14847, Lr: 0.000300\n",
            "2020-08-23 20:20:27,548 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:20:27,548 Saving new checkpoint.\n",
            "2020-08-23 20:20:28,050 Example #0\n",
            "2020-08-23 20:20:28,050 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:20:28,050 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:20:28,050 \tHypothesis: O muntu kafwete kala ye fu kia tambula nzenza yo veza .\n",
            "2020-08-23 20:20:28,051 Example #1\n",
            "2020-08-23 20:20:28,051 \tSource:     3 : 27 .\n",
            "2020-08-23 20:20:28,051 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:20:28,051 \tHypothesis: 3 :⁠ 27 .\n",
            "2020-08-23 20:20:28,051 Example #2\n",
            "2020-08-23 20:20:28,051 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:20:28,051 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:20:28,051 \tHypothesis: E mpasi zazi zilenda twasa mpasi .\n",
            "2020-08-23 20:20:28,052 Example #3\n",
            "2020-08-23 20:20:28,052 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:20:28,052 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:20:28,052 \tHypothesis: Avo ke tuna ye mvutu za yuvu yayi ko , nga olenda lomba lusadisu lwa nkanda wau ?\n",
            "2020-08-23 20:20:28,052 Validation result (greedy) at epoch  18, step    32000: bleu:  22.89, loss: 41279.2148, ppl:   5.8454, duration: 17.6663s\n",
            "2020-08-23 20:20:41,179 Epoch  18 Step:    32100 Batch Loss:     2.276055 Tokens per Sec:    15937, Lr: 0.000300\n",
            "2020-08-23 20:20:54,490 Epoch  18 Step:    32200 Batch Loss:     1.878699 Tokens per Sec:    15665, Lr: 0.000300\n",
            "2020-08-23 20:21:07,751 Epoch  18 Step:    32300 Batch Loss:     1.943299 Tokens per Sec:    15817, Lr: 0.000300\n",
            "2020-08-23 20:21:20,974 Epoch  18 Step:    32400 Batch Loss:     1.639456 Tokens per Sec:    15916, Lr: 0.000300\n",
            "2020-08-23 20:21:34,133 Epoch  18 Step:    32500 Batch Loss:     2.092848 Tokens per Sec:    15408, Lr: 0.000300\n",
            "2020-08-23 20:21:47,022 Epoch  18 Step:    32600 Batch Loss:     1.853977 Tokens per Sec:    15948, Lr: 0.000300\n",
            "2020-08-23 20:22:00,170 Epoch  18 Step:    32700 Batch Loss:     2.113664 Tokens per Sec:    15496, Lr: 0.000300\n",
            "2020-08-23 20:22:13,238 Epoch  18 Step:    32800 Batch Loss:     1.521289 Tokens per Sec:    16491, Lr: 0.000300\n",
            "2020-08-23 20:22:25,964 Epoch  18 Step:    32900 Batch Loss:     1.504320 Tokens per Sec:    15563, Lr: 0.000300\n",
            "2020-08-23 20:22:39,289 Epoch  18 Step:    33000 Batch Loss:     2.181927 Tokens per Sec:    15765, Lr: 0.000300\n",
            "2020-08-23 20:22:56,679 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:22:56,679 Saving new checkpoint.\n",
            "2020-08-23 20:22:57,173 Example #0\n",
            "2020-08-23 20:22:57,173 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:22:57,173 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:22:57,173 \tHypothesis: Muntu ovuidi o mfunu wa tambula nzenza ye mvuatu .\n",
            "2020-08-23 20:22:57,173 Example #1\n",
            "2020-08-23 20:22:57,174 \tSource:     3 : 27 .\n",
            "2020-08-23 20:22:57,174 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:22:57,174 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 20:22:57,174 Example #2\n",
            "2020-08-23 20:22:57,174 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:22:57,174 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:22:57,174 \tHypothesis: E mpasi zazi zilenda twasa mpasi .\n",
            "2020-08-23 20:22:57,174 Example #3\n",
            "2020-08-23 20:22:57,175 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:22:57,175 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:22:57,175 \tHypothesis: Avo ke wau ko e mvutu za yuvu yayi , nga olenda lomba lusadisu kwa Nzambi mu kuma kia nkanda Os Que um Qução ?\n",
            "2020-08-23 20:22:57,175 Validation result (greedy) at epoch  18, step    33000: bleu:  23.31, loss: 41042.7188, ppl:   5.7866, duration: 17.8861s\n",
            "2020-08-23 20:23:10,310 Epoch  18 Step:    33100 Batch Loss:     1.857806 Tokens per Sec:    16021, Lr: 0.000300\n",
            "2020-08-23 20:23:23,498 Epoch  18 Step:    33200 Batch Loss:     1.849512 Tokens per Sec:    15993, Lr: 0.000300\n",
            "2020-08-23 20:23:36,590 Epoch  18 Step:    33300 Batch Loss:     1.738974 Tokens per Sec:    15985, Lr: 0.000300\n",
            "2020-08-23 20:23:49,789 Epoch  18 Step:    33400 Batch Loss:     1.720114 Tokens per Sec:    15938, Lr: 0.000300\n",
            "2020-08-23 20:24:02,915 Epoch  18 Step:    33500 Batch Loss:     2.039748 Tokens per Sec:    15683, Lr: 0.000300\n",
            "2020-08-23 20:24:16,222 Epoch  18 Step:    33600 Batch Loss:     2.090066 Tokens per Sec:    15744, Lr: 0.000300\n",
            "2020-08-23 20:24:29,542 Epoch  18 Step:    33700 Batch Loss:     2.122832 Tokens per Sec:    15518, Lr: 0.000300\n",
            "2020-08-23 20:24:42,532 Epoch  18 Step:    33800 Batch Loss:     1.942533 Tokens per Sec:    16278, Lr: 0.000300\n",
            "2020-08-23 20:24:46,934 Epoch  18: total training loss 3494.11\n",
            "2020-08-23 20:24:46,934 EPOCH 19\n",
            "2020-08-23 20:24:56,278 Epoch  19 Step:    33900 Batch Loss:     1.673561 Tokens per Sec:    15345, Lr: 0.000300\n",
            "2020-08-23 20:25:09,540 Epoch  19 Step:    34000 Batch Loss:     2.095339 Tokens per Sec:    15404, Lr: 0.000300\n",
            "2020-08-23 20:25:27,144 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:25:27,144 Saving new checkpoint.\n",
            "2020-08-23 20:25:27,632 Example #0\n",
            "2020-08-23 20:25:27,633 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:25:27,633 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:25:27,633 \tHypothesis: O muntu kafwete kala ye fu kia tambula nzenza yo veza e mboka .\n",
            "2020-08-23 20:25:27,633 Example #1\n",
            "2020-08-23 20:25:27,633 \tSource:     3 : 27 .\n",
            "2020-08-23 20:25:27,633 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:25:27,633 \tHypothesis: 3 :⁠ 27 .\n",
            "2020-08-23 20:25:27,633 Example #2\n",
            "2020-08-23 20:25:27,634 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:25:27,634 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:25:27,634 \tHypothesis: E mpasi zazi zilenda kala zampasi .\n",
            "2020-08-23 20:25:27,634 Example #3\n",
            "2020-08-23 20:25:27,634 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:25:27,634 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:25:27,634 \tHypothesis: Avo ke tuna ye mvutu za yuvu yayi ko , nga olenda lomba lusadisu lwa finkanda fiafi fiafi fiafi fiafi fiafi fiafi fiafi fiafi fiafi ?\n",
            "2020-08-23 20:25:27,634 Validation result (greedy) at epoch  19, step    34000: bleu:  23.42, loss: 40938.1016, ppl:   5.7607, duration: 18.0937s\n",
            "2020-08-23 20:25:40,782 Epoch  19 Step:    34100 Batch Loss:     1.871292 Tokens per Sec:    15629, Lr: 0.000300\n",
            "2020-08-23 20:25:54,130 Epoch  19 Step:    34200 Batch Loss:     1.863755 Tokens per Sec:    15564, Lr: 0.000300\n",
            "2020-08-23 20:26:07,150 Epoch  19 Step:    34300 Batch Loss:     1.855239 Tokens per Sec:    16000, Lr: 0.000300\n",
            "2020-08-23 20:26:20,155 Epoch  19 Step:    34400 Batch Loss:     2.277900 Tokens per Sec:    16012, Lr: 0.000300\n",
            "2020-08-23 20:26:33,419 Epoch  19 Step:    34500 Batch Loss:     2.145338 Tokens per Sec:    15504, Lr: 0.000300\n",
            "2020-08-23 20:26:46,544 Epoch  19 Step:    34600 Batch Loss:     1.866484 Tokens per Sec:    15979, Lr: 0.000300\n",
            "2020-08-23 20:26:59,606 Epoch  19 Step:    34700 Batch Loss:     1.775490 Tokens per Sec:    16286, Lr: 0.000300\n",
            "2020-08-23 20:27:12,557 Epoch  19 Step:    34800 Batch Loss:     1.950627 Tokens per Sec:    15896, Lr: 0.000300\n",
            "2020-08-23 20:27:25,471 Epoch  19 Step:    34900 Batch Loss:     1.691214 Tokens per Sec:    15826, Lr: 0.000300\n",
            "2020-08-23 20:27:38,443 Epoch  19 Step:    35000 Batch Loss:     1.357880 Tokens per Sec:    16528, Lr: 0.000300\n",
            "2020-08-23 20:27:54,589 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:27:54,589 Saving new checkpoint.\n",
            "2020-08-23 20:27:55,087 Example #0\n",
            "2020-08-23 20:27:55,087 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:27:55,088 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:27:55,088 \tHypothesis: O muntu kafwete tambula nzenza yo veza e mboka .\n",
            "2020-08-23 20:27:55,088 Example #1\n",
            "2020-08-23 20:27:55,088 \tSource:     3 : 27 .\n",
            "2020-08-23 20:27:55,088 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:27:55,088 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 20:27:55,088 Example #2\n",
            "2020-08-23 20:27:55,089 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:27:55,089 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:27:55,089 \tHypothesis: E mpasi zazi zilenda kala zampasi .\n",
            "2020-08-23 20:27:55,089 Example #3\n",
            "2020-08-23 20:27:55,089 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:27:55,089 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:27:55,089 \tHypothesis: Avo ke lukatikisu ko vo e mvutu za yuvu yayi , nga olenda lomba lusadisu lwa finkanda fiafi fiafi fiafi fiafi fiafi fiafi fiafi ?\n",
            "2020-08-23 20:27:55,089 Validation result (greedy) at epoch  19, step    35000: bleu:  23.45, loss: 40585.3281, ppl:   5.6745, duration: 16.6459s\n",
            "2020-08-23 20:28:08,189 Epoch  19 Step:    35100 Batch Loss:     2.001993 Tokens per Sec:    15798, Lr: 0.000300\n",
            "2020-08-23 20:28:21,268 Epoch  19 Step:    35200 Batch Loss:     1.947906 Tokens per Sec:    16187, Lr: 0.000300\n",
            "2020-08-23 20:28:34,113 Epoch  19 Step:    35300 Batch Loss:     1.607850 Tokens per Sec:    15912, Lr: 0.000300\n",
            "2020-08-23 20:28:47,273 Epoch  19 Step:    35400 Batch Loss:     2.027233 Tokens per Sec:    15927, Lr: 0.000300\n",
            "2020-08-23 20:29:00,140 Epoch  19 Step:    35500 Batch Loss:     1.868822 Tokens per Sec:    16253, Lr: 0.000300\n",
            "2020-08-23 20:29:13,430 Epoch  19 Step:    35600 Batch Loss:     1.855397 Tokens per Sec:    15198, Lr: 0.000300\n",
            "2020-08-23 20:29:26,292 Epoch  19 Step:    35700 Batch Loss:     1.558538 Tokens per Sec:    16026, Lr: 0.000300\n",
            "2020-08-23 20:29:28,316 Epoch  19: total training loss 3466.50\n",
            "2020-08-23 20:29:28,316 EPOCH 20\n",
            "2020-08-23 20:29:39,490 Epoch  20 Step:    35800 Batch Loss:     1.801317 Tokens per Sec:    15536, Lr: 0.000300\n",
            "2020-08-23 20:29:52,565 Epoch  20 Step:    35900 Batch Loss:     1.826256 Tokens per Sec:    16202, Lr: 0.000300\n",
            "2020-08-23 20:30:05,558 Epoch  20 Step:    36000 Batch Loss:     1.761747 Tokens per Sec:    16045, Lr: 0.000300\n",
            "2020-08-23 20:30:22,664 Example #0\n",
            "2020-08-23 20:30:22,665 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:30:22,665 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:30:22,665 \tHypothesis: O muntu kafwete tambula nzenza yo veza .\n",
            "2020-08-23 20:30:22,665 Example #1\n",
            "2020-08-23 20:30:22,665 \tSource:     3 : 27 .\n",
            "2020-08-23 20:30:22,666 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:30:22,666 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 20:30:22,666 Example #2\n",
            "2020-08-23 20:30:22,666 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:30:22,666 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:30:22,666 \tHypothesis: E mpasi zazi zilenda kala zampasi .\n",
            "2020-08-23 20:30:22,666 Example #3\n",
            "2020-08-23 20:30:22,666 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:30:22,667 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:30:22,667 \tHypothesis: Avo kuzeye mvutu za yuvu yayi ko , nga olenda lomba lusadisu lwa finkanda - nkanda O Nzambi Adieyi Kieleka Kelonganga ?\n",
            "2020-08-23 20:30:22,667 Validation result (greedy) at epoch  20, step    36000: bleu:  23.46, loss: 40620.5664, ppl:   5.6830, duration: 17.1089s\n",
            "2020-08-23 20:30:35,632 Epoch  20 Step:    36100 Batch Loss:     1.608533 Tokens per Sec:    15804, Lr: 0.000300\n",
            "2020-08-23 20:30:48,995 Epoch  20 Step:    36200 Batch Loss:     1.541148 Tokens per Sec:    15668, Lr: 0.000300\n",
            "2020-08-23 20:31:02,149 Epoch  20 Step:    36300 Batch Loss:     2.505226 Tokens per Sec:    15921, Lr: 0.000300\n",
            "2020-08-23 20:31:14,972 Epoch  20 Step:    36400 Batch Loss:     1.763502 Tokens per Sec:    15917, Lr: 0.000300\n",
            "2020-08-23 20:31:28,077 Epoch  20 Step:    36500 Batch Loss:     1.373076 Tokens per Sec:    16662, Lr: 0.000300\n",
            "2020-08-23 20:31:41,273 Epoch  20 Step:    36600 Batch Loss:     1.303451 Tokens per Sec:    15612, Lr: 0.000300\n",
            "2020-08-23 20:31:54,311 Epoch  20 Step:    36700 Batch Loss:     1.819366 Tokens per Sec:    15924, Lr: 0.000300\n",
            "2020-08-23 20:32:07,312 Epoch  20 Step:    36800 Batch Loss:     2.119054 Tokens per Sec:    16358, Lr: 0.000300\n",
            "2020-08-23 20:32:20,291 Epoch  20 Step:    36900 Batch Loss:     1.628613 Tokens per Sec:    16204, Lr: 0.000300\n",
            "2020-08-23 20:32:33,151 Epoch  20 Step:    37000 Batch Loss:     1.997603 Tokens per Sec:    15989, Lr: 0.000300\n",
            "2020-08-23 20:32:49,074 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:32:49,074 Saving new checkpoint.\n",
            "2020-08-23 20:32:49,574 Example #0\n",
            "2020-08-23 20:32:49,574 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:32:49,574 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:32:49,575 \tHypothesis: O muntu ovwidi o mfunu wa tambula nzenza yo kubakila e nzengo .\n",
            "2020-08-23 20:32:49,575 Example #1\n",
            "2020-08-23 20:32:49,575 \tSource:     3 : 27 .\n",
            "2020-08-23 20:32:49,575 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:32:49,575 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 20:32:49,575 Example #2\n",
            "2020-08-23 20:32:49,575 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:32:49,576 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:32:49,576 \tHypothesis: E mpasi zazi zilenda kala zampasi .\n",
            "2020-08-23 20:32:49,576 Example #3\n",
            "2020-08-23 20:32:49,576 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:32:49,576 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:32:49,576 \tHypothesis: Avo ke lukatikisu ko mu kuma kia mvutu za yuvu yayi , nga olenda lomba lusadisu lwa finkanda - nkanda O Nzambi Adieyi Kieleka Kelonganga ?\n",
            "2020-08-23 20:32:49,576 Validation result (greedy) at epoch  20, step    37000: bleu:  23.37, loss: 40301.0625, ppl:   5.6059, duration: 16.4248s\n",
            "2020-08-23 20:33:02,775 Epoch  20 Step:    37100 Batch Loss:     1.578084 Tokens per Sec:    15862, Lr: 0.000300\n",
            "2020-08-23 20:33:16,078 Epoch  20 Step:    37200 Batch Loss:     1.672168 Tokens per Sec:    15463, Lr: 0.000300\n",
            "2020-08-23 20:33:29,222 Epoch  20 Step:    37300 Batch Loss:     1.841203 Tokens per Sec:    15758, Lr: 0.000300\n",
            "2020-08-23 20:33:42,264 Epoch  20 Step:    37400 Batch Loss:     2.020566 Tokens per Sec:    15912, Lr: 0.000300\n",
            "2020-08-23 20:33:55,551 Epoch  20 Step:    37500 Batch Loss:     1.825721 Tokens per Sec:    16030, Lr: 0.000300\n",
            "2020-08-23 20:34:07,384 Epoch  20: total training loss 3418.29\n",
            "2020-08-23 20:34:07,384 EPOCH 21\n",
            "2020-08-23 20:34:08,946 Epoch  21 Step:    37600 Batch Loss:     1.598891 Tokens per Sec:    12495, Lr: 0.000300\n",
            "2020-08-23 20:34:21,950 Epoch  21 Step:    37700 Batch Loss:     1.413135 Tokens per Sec:    16252, Lr: 0.000300\n",
            "2020-08-23 20:34:35,206 Epoch  21 Step:    37800 Batch Loss:     1.711442 Tokens per Sec:    15822, Lr: 0.000300\n",
            "2020-08-23 20:34:48,302 Epoch  21 Step:    37900 Batch Loss:     1.459154 Tokens per Sec:    16133, Lr: 0.000300\n",
            "2020-08-23 20:35:01,523 Epoch  21 Step:    38000 Batch Loss:     1.373652 Tokens per Sec:    15655, Lr: 0.000300\n",
            "2020-08-23 20:35:17,886 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:35:17,886 Saving new checkpoint.\n",
            "2020-08-23 20:35:18,406 Example #0\n",
            "2020-08-23 20:35:18,407 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:35:18,407 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:35:18,407 \tHypothesis: O muntu kafwete tambula nzenza yo nua mfomo .\n",
            "2020-08-23 20:35:18,407 Example #1\n",
            "2020-08-23 20:35:18,407 \tSource:     3 : 27 .\n",
            "2020-08-23 20:35:18,407 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:35:18,408 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 20:35:18,408 Example #2\n",
            "2020-08-23 20:35:18,408 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:35:18,408 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:35:18,408 \tHypothesis: E mpasi zazi zilenda kala zampasi .\n",
            "2020-08-23 20:35:18,408 Example #3\n",
            "2020-08-23 20:35:18,409 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:35:18,409 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:35:18,409 \tHypothesis: Avo kuzeye mvutu za yuvu yayi ko , nga olenda lomba lusadisu lwa finkanda - nkanda O Nzambi Adieyi Kieleka Kelonganga ?\n",
            "2020-08-23 20:35:18,409 Validation result (greedy) at epoch  21, step    38000: bleu:  23.67, loss: 40129.0938, ppl:   5.5648, duration: 16.8853s\n",
            "2020-08-23 20:35:31,261 Epoch  21 Step:    38100 Batch Loss:     1.638443 Tokens per Sec:    15858, Lr: 0.000300\n",
            "2020-08-23 20:35:44,364 Epoch  21 Step:    38200 Batch Loss:     1.344409 Tokens per Sec:    16122, Lr: 0.000300\n",
            "2020-08-23 20:35:57,672 Epoch  21 Step:    38300 Batch Loss:     1.850847 Tokens per Sec:    15482, Lr: 0.000300\n",
            "2020-08-23 20:36:10,771 Epoch  21 Step:    38400 Batch Loss:     1.261985 Tokens per Sec:    16190, Lr: 0.000300\n",
            "2020-08-23 20:36:23,838 Epoch  21 Step:    38500 Batch Loss:     1.808859 Tokens per Sec:    15959, Lr: 0.000300\n",
            "2020-08-23 20:36:36,928 Epoch  21 Step:    38600 Batch Loss:     1.864567 Tokens per Sec:    15895, Lr: 0.000300\n",
            "2020-08-23 20:36:49,913 Epoch  21 Step:    38700 Batch Loss:     1.573814 Tokens per Sec:    16259, Lr: 0.000300\n",
            "2020-08-23 20:37:02,756 Epoch  21 Step:    38800 Batch Loss:     2.280051 Tokens per Sec:    16024, Lr: 0.000300\n",
            "2020-08-23 20:37:15,851 Epoch  21 Step:    38900 Batch Loss:     1.838373 Tokens per Sec:    15489, Lr: 0.000300\n",
            "2020-08-23 20:37:28,802 Epoch  21 Step:    39000 Batch Loss:     1.654182 Tokens per Sec:    16167, Lr: 0.000300\n",
            "2020-08-23 20:37:45,843 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:37:45,844 Saving new checkpoint.\n",
            "2020-08-23 20:37:46,330 Example #0\n",
            "2020-08-23 20:37:46,330 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:37:46,331 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:37:46,331 \tHypothesis: O muntu kafwete tambula nzenza yo veza e mboka .\n",
            "2020-08-23 20:37:46,331 Example #1\n",
            "2020-08-23 20:37:46,331 \tSource:     3 : 27 .\n",
            "2020-08-23 20:37:46,331 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:37:46,331 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 20:37:46,331 Example #2\n",
            "2020-08-23 20:37:46,331 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:37:46,332 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:37:46,332 \tHypothesis: E mpasi zazi zilenda kala zampasi .\n",
            "2020-08-23 20:37:46,332 Example #3\n",
            "2020-08-23 20:37:46,332 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:37:46,332 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:37:46,332 \tHypothesis: Avo ke vena mvutu za yuvu yayi ko , nga olenda lomba lusadisu lwa nkanda wau wa Nkand’a Nzambi Adieyi Kieleka Kelonganga ?\n",
            "2020-08-23 20:37:46,332 Validation result (greedy) at epoch  21, step    39000: bleu:  23.65, loss: 39976.7656, ppl:   5.5286, duration: 17.5300s\n",
            "2020-08-23 20:37:59,420 Epoch  21 Step:    39100 Batch Loss:     1.836631 Tokens per Sec:    16230, Lr: 0.000300\n",
            "2020-08-23 20:38:12,351 Epoch  21 Step:    39200 Batch Loss:     2.160517 Tokens per Sec:    15693, Lr: 0.000300\n",
            "2020-08-23 20:38:25,308 Epoch  21 Step:    39300 Batch Loss:     1.777910 Tokens per Sec:    16225, Lr: 0.000300\n",
            "2020-08-23 20:38:38,355 Epoch  21 Step:    39400 Batch Loss:     1.873858 Tokens per Sec:    15272, Lr: 0.000300\n",
            "2020-08-23 20:38:47,703 Epoch  21: total training loss 3402.06\n",
            "2020-08-23 20:38:47,703 EPOCH 22\n",
            "2020-08-23 20:38:51,698 Epoch  22 Step:    39500 Batch Loss:     2.065849 Tokens per Sec:    14421, Lr: 0.000300\n",
            "2020-08-23 20:39:04,947 Epoch  22 Step:    39600 Batch Loss:     1.898766 Tokens per Sec:    16003, Lr: 0.000300\n",
            "2020-08-23 20:39:18,073 Epoch  22 Step:    39700 Batch Loss:     1.432582 Tokens per Sec:    15772, Lr: 0.000300\n",
            "2020-08-23 20:39:31,229 Epoch  22 Step:    39800 Batch Loss:     1.460756 Tokens per Sec:    15989, Lr: 0.000300\n",
            "2020-08-23 20:39:44,556 Epoch  22 Step:    39900 Batch Loss:     1.740748 Tokens per Sec:    16131, Lr: 0.000300\n",
            "2020-08-23 20:39:57,672 Epoch  22 Step:    40000 Batch Loss:     1.476443 Tokens per Sec:    15965, Lr: 0.000300\n",
            "2020-08-23 20:40:14,540 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:40:14,540 Saving new checkpoint.\n",
            "2020-08-23 20:40:15,065 Example #0\n",
            "2020-08-23 20:40:15,065 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:40:15,065 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:40:15,065 \tHypothesis: O muntu kafwete kala ye fu kia tambula nzenza yo kubakila e nzengo .\n",
            "2020-08-23 20:40:15,065 Example #1\n",
            "2020-08-23 20:40:15,066 \tSource:     3 : 27 .\n",
            "2020-08-23 20:40:15,066 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:40:15,066 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 20:40:15,066 Example #2\n",
            "2020-08-23 20:40:15,066 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:40:15,066 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:40:15,066 \tHypothesis: E mpasi zazi zilenda kala zampasi .\n",
            "2020-08-23 20:40:15,066 Example #3\n",
            "2020-08-23 20:40:15,067 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:40:15,067 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:40:15,067 \tHypothesis: Avo kuzeye mvutu za yuvu yayi ko , nga olenda lomba lusadisu lwa finkanda - nkanda O Nzambi Adieyi Kieleka Kelonganga ?\n",
            "2020-08-23 20:40:15,067 Validation result (greedy) at epoch  22, step    40000: bleu:  24.30, loss: 39948.7070, ppl:   5.5220, duration: 17.3942s\n",
            "2020-08-23 20:40:28,296 Epoch  22 Step:    40100 Batch Loss:     1.709185 Tokens per Sec:    15744, Lr: 0.000300\n",
            "2020-08-23 20:40:41,626 Epoch  22 Step:    40200 Batch Loss:     1.750613 Tokens per Sec:    15652, Lr: 0.000300\n",
            "2020-08-23 20:40:54,747 Epoch  22 Step:    40300 Batch Loss:     1.928624 Tokens per Sec:    15603, Lr: 0.000300\n",
            "2020-08-23 20:41:07,842 Epoch  22 Step:    40400 Batch Loss:     1.476952 Tokens per Sec:    15558, Lr: 0.000300\n",
            "2020-08-23 20:41:21,086 Epoch  22 Step:    40500 Batch Loss:     1.927574 Tokens per Sec:    15799, Lr: 0.000300\n",
            "2020-08-23 20:41:34,006 Epoch  22 Step:    40600 Batch Loss:     1.565982 Tokens per Sec:    15729, Lr: 0.000300\n",
            "2020-08-23 20:41:47,453 Epoch  22 Step:    40700 Batch Loss:     1.527859 Tokens per Sec:    15273, Lr: 0.000300\n",
            "2020-08-23 20:42:00,652 Epoch  22 Step:    40800 Batch Loss:     2.086368 Tokens per Sec:    15670, Lr: 0.000300\n",
            "2020-08-23 20:42:13,678 Epoch  22 Step:    40900 Batch Loss:     1.958075 Tokens per Sec:    16012, Lr: 0.000300\n",
            "2020-08-23 20:42:26,889 Epoch  22 Step:    41000 Batch Loss:     2.033453 Tokens per Sec:    16145, Lr: 0.000300\n",
            "2020-08-23 20:42:43,878 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:42:43,878 Saving new checkpoint.\n",
            "2020-08-23 20:42:44,354 Example #0\n",
            "2020-08-23 20:42:44,354 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:42:44,354 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:42:44,354 \tHypothesis: Ndiona ovwanga o mfunu wa tambula nzenza ye mbokena .\n",
            "2020-08-23 20:42:44,354 Example #1\n",
            "2020-08-23 20:42:44,355 \tSource:     3 : 27 .\n",
            "2020-08-23 20:42:44,355 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:42:44,355 \tHypothesis: 3 :⁠ 27 .\n",
            "2020-08-23 20:42:44,355 Example #2\n",
            "2020-08-23 20:42:44,355 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:42:44,355 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:42:44,355 \tHypothesis: E mpasi zazi zilenda kala zampasi .\n",
            "2020-08-23 20:42:44,355 Example #3\n",
            "2020-08-23 20:42:44,356 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:42:44,356 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:42:44,356 \tHypothesis: Avo kuzeye mvutu za yuvu yayi ko , nga olenda lomba lusadisu lwa finkanda fiafi fiafi fiavaikiswa kwa nkanda O Nzambi Adieyi Kieleka Kelonganga ?\n",
            "2020-08-23 20:42:44,356 Validation result (greedy) at epoch  22, step    41000: bleu:  24.26, loss: 39782.0742, ppl:   5.4828, duration: 17.4661s\n",
            "2020-08-23 20:42:57,506 Epoch  22 Step:    41100 Batch Loss:     1.665500 Tokens per Sec:    15981, Lr: 0.000300\n",
            "2020-08-23 20:43:10,483 Epoch  22 Step:    41200 Batch Loss:     1.336343 Tokens per Sec:    16041, Lr: 0.000300\n",
            "2020-08-23 20:43:23,620 Epoch  22 Step:    41300 Batch Loss:     1.955758 Tokens per Sec:    15809, Lr: 0.000300\n",
            "2020-08-23 20:43:29,918 Epoch  22: total training loss 3365.20\n",
            "2020-08-23 20:43:29,918 EPOCH 23\n",
            "2020-08-23 20:43:36,739 Epoch  23 Step:    41400 Batch Loss:     1.960372 Tokens per Sec:    15580, Lr: 0.000300\n",
            "2020-08-23 20:43:50,091 Epoch  23 Step:    41500 Batch Loss:     1.399568 Tokens per Sec:    15282, Lr: 0.000300\n",
            "2020-08-23 20:44:03,192 Epoch  23 Step:    41600 Batch Loss:     1.973683 Tokens per Sec:    15826, Lr: 0.000300\n",
            "2020-08-23 20:44:16,611 Epoch  23 Step:    41700 Batch Loss:     1.841347 Tokens per Sec:    15217, Lr: 0.000300\n",
            "2020-08-23 20:44:29,693 Epoch  23 Step:    41800 Batch Loss:     1.878807 Tokens per Sec:    15953, Lr: 0.000300\n",
            "2020-08-23 20:44:42,920 Epoch  23 Step:    41900 Batch Loss:     1.788167 Tokens per Sec:    15439, Lr: 0.000300\n",
            "2020-08-23 20:44:55,985 Epoch  23 Step:    42000 Batch Loss:     1.504583 Tokens per Sec:    16489, Lr: 0.000300\n",
            "2020-08-23 20:45:11,587 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:45:11,587 Saving new checkpoint.\n",
            "2020-08-23 20:45:12,086 Example #0\n",
            "2020-08-23 20:45:12,086 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:45:12,086 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:45:12,086 \tHypothesis: O muntu kafwete tambula nzenza yo veza e mboka .\n",
            "2020-08-23 20:45:12,086 Example #1\n",
            "2020-08-23 20:45:12,087 \tSource:     3 : 27 .\n",
            "2020-08-23 20:45:12,087 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:45:12,087 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 20:45:12,087 Example #2\n",
            "2020-08-23 20:45:12,087 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:45:12,087 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:45:12,087 \tHypothesis: E mpasi zazi zilenda kala zampasi .\n",
            "2020-08-23 20:45:12,088 Example #3\n",
            "2020-08-23 20:45:12,088 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:45:12,088 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:45:12,088 \tHypothesis: Avo kuzeye mvutu za yuvu yayi ko , nga olenda lomba lusadisu lwa finkanda fiafi , O Nzambi Adieyi Kevavanga mu Kuma kia Yeto ?\n",
            "2020-08-23 20:45:12,088 Validation result (greedy) at epoch  23, step    42000: bleu:  23.88, loss: 39491.9336, ppl:   5.4152, duration: 16.1024s\n",
            "2020-08-23 20:45:25,315 Epoch  23 Step:    42100 Batch Loss:     1.876203 Tokens per Sec:    15660, Lr: 0.000300\n",
            "2020-08-23 20:45:38,438 Epoch  23 Step:    42200 Batch Loss:     1.688680 Tokens per Sec:    15872, Lr: 0.000300\n",
            "2020-08-23 20:45:51,344 Epoch  23 Step:    42300 Batch Loss:     1.990386 Tokens per Sec:    15867, Lr: 0.000300\n",
            "2020-08-23 20:46:04,475 Epoch  23 Step:    42400 Batch Loss:     2.042406 Tokens per Sec:    15575, Lr: 0.000300\n",
            "2020-08-23 20:46:17,541 Epoch  23 Step:    42500 Batch Loss:     1.580526 Tokens per Sec:    15837, Lr: 0.000300\n",
            "2020-08-23 20:46:30,567 Epoch  23 Step:    42600 Batch Loss:     1.944468 Tokens per Sec:    16631, Lr: 0.000300\n",
            "2020-08-23 20:46:43,759 Epoch  23 Step:    42700 Batch Loss:     1.499555 Tokens per Sec:    15874, Lr: 0.000300\n",
            "2020-08-23 20:46:57,016 Epoch  23 Step:    42800 Batch Loss:     1.637809 Tokens per Sec:    15610, Lr: 0.000300\n",
            "2020-08-23 20:47:10,205 Epoch  23 Step:    42900 Batch Loss:     2.193726 Tokens per Sec:    16202, Lr: 0.000300\n",
            "2020-08-23 20:47:23,460 Epoch  23 Step:    43000 Batch Loss:     1.944480 Tokens per Sec:    15598, Lr: 0.000300\n",
            "2020-08-23 20:47:38,172 Example #0\n",
            "2020-08-23 20:47:38,173 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:47:38,173 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:47:38,173 \tHypothesis: O muntu kafwete tambula nzenza yo dia mbolo .\n",
            "2020-08-23 20:47:38,173 Example #1\n",
            "2020-08-23 20:47:38,173 \tSource:     3 : 27 .\n",
            "2020-08-23 20:47:38,173 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:47:38,173 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 20:47:38,173 Example #2\n",
            "2020-08-23 20:47:38,174 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:47:38,174 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:47:38,174 \tHypothesis: E mpasi zazi zilenda kala zampasi .\n",
            "2020-08-23 20:47:38,174 Example #3\n",
            "2020-08-23 20:47:38,174 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:47:38,174 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:47:38,174 \tHypothesis: Avo kuzeye mvutu za yuvu yayi ko , nga olenda lomba lusadisu lwa finkanda O Nzambi Adieyi Kieleka Kevavanga ?\n",
            "2020-08-23 20:47:38,174 Validation result (greedy) at epoch  23, step    43000: bleu:  24.41, loss: 39492.4688, ppl:   5.4153, duration: 14.7146s\n",
            "2020-08-23 20:47:51,071 Epoch  23 Step:    43100 Batch Loss:     1.192213 Tokens per Sec:    15947, Lr: 0.000300\n",
            "2020-08-23 20:48:04,161 Epoch  23 Step:    43200 Batch Loss:     2.208975 Tokens per Sec:    16476, Lr: 0.000300\n",
            "2020-08-23 20:48:07,656 Epoch  23: total training loss 3336.99\n",
            "2020-08-23 20:48:07,657 EPOCH 24\n",
            "2020-08-23 20:48:17,389 Epoch  24 Step:    43300 Batch Loss:     1.844985 Tokens per Sec:    15752, Lr: 0.000300\n",
            "2020-08-23 20:48:30,642 Epoch  24 Step:    43400 Batch Loss:     1.504180 Tokens per Sec:    15612, Lr: 0.000300\n",
            "2020-08-23 20:48:43,618 Epoch  24 Step:    43500 Batch Loss:     1.888167 Tokens per Sec:    16159, Lr: 0.000300\n",
            "2020-08-23 20:48:56,749 Epoch  24 Step:    43600 Batch Loss:     1.181265 Tokens per Sec:    15496, Lr: 0.000300\n",
            "2020-08-23 20:49:10,115 Epoch  24 Step:    43700 Batch Loss:     1.648444 Tokens per Sec:    15926, Lr: 0.000300\n",
            "2020-08-23 20:49:23,312 Epoch  24 Step:    43800 Batch Loss:     1.598838 Tokens per Sec:    15607, Lr: 0.000300\n",
            "2020-08-23 20:49:36,226 Epoch  24 Step:    43900 Batch Loss:     1.598224 Tokens per Sec:    16083, Lr: 0.000300\n",
            "2020-08-23 20:49:49,253 Epoch  24 Step:    44000 Batch Loss:     1.674202 Tokens per Sec:    16157, Lr: 0.000300\n",
            "2020-08-23 20:50:05,178 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:50:05,178 Saving new checkpoint.\n",
            "2020-08-23 20:50:05,688 Example #0\n",
            "2020-08-23 20:50:05,689 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:50:05,689 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:50:05,689 \tHypothesis: O muntu kafwete tambula nzenza yo nua mfomo .\n",
            "2020-08-23 20:50:05,689 Example #1\n",
            "2020-08-23 20:50:05,689 \tSource:     3 : 27 .\n",
            "2020-08-23 20:50:05,689 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:50:05,689 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 20:50:05,690 Example #2\n",
            "2020-08-23 20:50:05,690 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:50:05,690 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:50:05,690 \tHypothesis: E ntangwa yayi ilenda kala diampasi .\n",
            "2020-08-23 20:50:05,690 Example #3\n",
            "2020-08-23 20:50:05,690 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:50:05,691 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:50:05,691 \tHypothesis: Avo kuzeye mvutu za yuvu yayi ko , nga olenda lomba lusadisu lwa nkanda O Nzambi Adieyi Kieleka Kelonganga ?\n",
            "2020-08-23 20:50:05,691 Validation result (greedy) at epoch  24, step    44000: bleu:  24.48, loss: 39269.8203, ppl:   5.3640, duration: 16.4376s\n",
            "2020-08-23 20:50:18,784 Epoch  24 Step:    44100 Batch Loss:     1.894857 Tokens per Sec:    15598, Lr: 0.000300\n",
            "2020-08-23 20:50:31,870 Epoch  24 Step:    44200 Batch Loss:     2.070745 Tokens per Sec:    16053, Lr: 0.000300\n",
            "2020-08-23 20:50:44,846 Epoch  24 Step:    44300 Batch Loss:     2.109861 Tokens per Sec:    16007, Lr: 0.000300\n",
            "2020-08-23 20:50:57,953 Epoch  24 Step:    44400 Batch Loss:     1.924737 Tokens per Sec:    15833, Lr: 0.000300\n",
            "2020-08-23 20:51:11,251 Epoch  24 Step:    44500 Batch Loss:     1.656475 Tokens per Sec:    16015, Lr: 0.000300\n",
            "2020-08-23 20:51:24,541 Epoch  24 Step:    44600 Batch Loss:     1.737712 Tokens per Sec:    15931, Lr: 0.000300\n",
            "2020-08-23 20:51:37,994 Epoch  24 Step:    44700 Batch Loss:     1.709458 Tokens per Sec:    14866, Lr: 0.000300\n",
            "2020-08-23 20:51:51,101 Epoch  24 Step:    44800 Batch Loss:     1.885767 Tokens per Sec:    16359, Lr: 0.000300\n",
            "2020-08-23 20:52:04,243 Epoch  24 Step:    44900 Batch Loss:     1.842357 Tokens per Sec:    15601, Lr: 0.000300\n",
            "2020-08-23 20:52:17,777 Epoch  24 Step:    45000 Batch Loss:     1.846789 Tokens per Sec:    15022, Lr: 0.000300\n",
            "2020-08-23 20:52:33,488 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:52:33,488 Saving new checkpoint.\n",
            "2020-08-23 20:52:34,003 Example #0\n",
            "2020-08-23 20:52:34,003 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:52:34,003 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:52:34,004 \tHypothesis: O muntu kafwete kala ye fu kia tambula nzenza yo vava e mboka .\n",
            "2020-08-23 20:52:34,004 Example #1\n",
            "2020-08-23 20:52:34,004 \tSource:     3 : 27 .\n",
            "2020-08-23 20:52:34,004 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:52:34,004 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 20:52:34,004 Example #2\n",
            "2020-08-23 20:52:34,005 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:52:34,005 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:52:34,005 \tHypothesis: E diambu diadi dilenda kala diampasi .\n",
            "2020-08-23 20:52:34,005 Example #3\n",
            "2020-08-23 20:52:34,005 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:52:34,006 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:52:34,006 \tHypothesis: Avo kuzeye mvutu za yuvu yayi ko , nga olenda lomba lusadisu lwa finkanda O Nzambi Adieyi Kevavanga mu Kuma kia Yeto ?\n",
            "2020-08-23 20:52:34,006 Validation result (greedy) at epoch  24, step    45000: bleu:  24.34, loss: 39150.1172, ppl:   5.3366, duration: 16.2286s\n",
            "2020-08-23 20:52:47,237 Epoch  24 Step:    45100 Batch Loss:     1.832558 Tokens per Sec:    15820, Lr: 0.000300\n",
            "2020-08-23 20:52:48,146 Epoch  24: total training loss 3315.62\n",
            "2020-08-23 20:52:48,146 EPOCH 25\n",
            "2020-08-23 20:53:00,410 Epoch  25 Step:    45200 Batch Loss:     1.729081 Tokens per Sec:    15682, Lr: 0.000300\n",
            "2020-08-23 20:53:13,968 Epoch  25 Step:    45300 Batch Loss:     1.873348 Tokens per Sec:    15356, Lr: 0.000300\n",
            "2020-08-23 20:53:27,065 Epoch  25 Step:    45400 Batch Loss:     1.916689 Tokens per Sec:    16006, Lr: 0.000300\n",
            "2020-08-23 20:53:40,065 Epoch  25 Step:    45500 Batch Loss:     1.518224 Tokens per Sec:    15726, Lr: 0.000300\n",
            "2020-08-23 20:53:53,185 Epoch  25 Step:    45600 Batch Loss:     1.529465 Tokens per Sec:    15740, Lr: 0.000300\n",
            "2020-08-23 20:54:06,161 Epoch  25 Step:    45700 Batch Loss:     2.059209 Tokens per Sec:    16068, Lr: 0.000300\n",
            "2020-08-23 20:54:19,545 Epoch  25 Step:    45800 Batch Loss:     1.792734 Tokens per Sec:    15751, Lr: 0.000300\n",
            "2020-08-23 20:54:32,854 Epoch  25 Step:    45900 Batch Loss:     2.197787 Tokens per Sec:    15480, Lr: 0.000300\n",
            "2020-08-23 20:54:46,088 Epoch  25 Step:    46000 Batch Loss:     1.836631 Tokens per Sec:    15987, Lr: 0.000300\n",
            "2020-08-23 20:55:02,014 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:55:02,014 Saving new checkpoint.\n",
            "2020-08-23 20:55:02,500 Example #0\n",
            "2020-08-23 20:55:02,500 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:55:02,500 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:55:02,501 \tHypothesis: O muntu kafwete tambula nzenza ye mawuku .\n",
            "2020-08-23 20:55:02,501 Example #1\n",
            "2020-08-23 20:55:02,501 \tSource:     3 : 27 .\n",
            "2020-08-23 20:55:02,501 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:55:02,501 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 20:55:02,501 Example #2\n",
            "2020-08-23 20:55:02,501 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:55:02,502 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:55:02,502 \tHypothesis: E diambu diadi dilenda vangama .\n",
            "2020-08-23 20:55:02,502 Example #3\n",
            "2020-08-23 20:55:02,502 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:55:02,502 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:55:02,502 \tHypothesis: Avo kuzeye mvutu za yuvu yayi ko , nga olenda lomba lusadisu lwa finkanda - nkanda O Nzambi Adieyi Kieleka Kelonganga ?\n",
            "2020-08-23 20:55:02,502 Validation result (greedy) at epoch  25, step    46000: bleu:  24.33, loss: 39131.1250, ppl:   5.3322, duration: 16.4138s\n",
            "2020-08-23 20:55:15,682 Epoch  25 Step:    46100 Batch Loss:     1.852013 Tokens per Sec:    15362, Lr: 0.000300\n",
            "2020-08-23 20:55:28,948 Epoch  25 Step:    46200 Batch Loss:     1.690774 Tokens per Sec:    16015, Lr: 0.000300\n",
            "2020-08-23 20:55:41,945 Epoch  25 Step:    46300 Batch Loss:     1.773064 Tokens per Sec:    16032, Lr: 0.000300\n",
            "2020-08-23 20:55:54,955 Epoch  25 Step:    46400 Batch Loss:     1.741925 Tokens per Sec:    16092, Lr: 0.000300\n",
            "2020-08-23 20:56:08,141 Epoch  25 Step:    46500 Batch Loss:     2.060802 Tokens per Sec:    15908, Lr: 0.000300\n",
            "2020-08-23 20:56:21,164 Epoch  25 Step:    46600 Batch Loss:     1.886016 Tokens per Sec:    16165, Lr: 0.000300\n",
            "2020-08-23 20:56:34,248 Epoch  25 Step:    46700 Batch Loss:     2.165474 Tokens per Sec:    15798, Lr: 0.000300\n",
            "2020-08-23 20:56:47,640 Epoch  25 Step:    46800 Batch Loss:     1.640793 Tokens per Sec:    15798, Lr: 0.000300\n",
            "2020-08-23 20:57:00,875 Epoch  25 Step:    46900 Batch Loss:     1.490940 Tokens per Sec:    15473, Lr: 0.000300\n",
            "2020-08-23 20:57:12,210 Epoch  25: total training loss 3297.36\n",
            "2020-08-23 20:57:12,211 EPOCH 26\n",
            "2020-08-23 20:57:14,166 Epoch  26 Step:    47000 Batch Loss:     1.119526 Tokens per Sec:    14228, Lr: 0.000300\n",
            "2020-08-23 20:57:30,253 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 20:57:30,253 Saving new checkpoint.\n",
            "2020-08-23 20:57:30,741 Example #0\n",
            "2020-08-23 20:57:30,742 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:57:30,742 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:57:30,742 \tHypothesis: O muntu kafwete tambula nzenza yo kudikila e mbolo .\n",
            "2020-08-23 20:57:30,742 Example #1\n",
            "2020-08-23 20:57:30,742 \tSource:     3 : 27 .\n",
            "2020-08-23 20:57:30,742 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:57:30,742 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 20:57:30,743 Example #2\n",
            "2020-08-23 20:57:30,743 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:57:30,743 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:57:30,743 \tHypothesis: E mpasi zazi zilenda kutubwila .\n",
            "2020-08-23 20:57:30,743 Example #3\n",
            "2020-08-23 20:57:30,743 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:57:30,743 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:57:30,744 \tHypothesis: Avo kuzeye mvutu za yuvu yayi ko , nga olenda lomba lusadisu lwa finkanda fiafi fiafi fiavaikiswa kwa Nzambi ?\n",
            "2020-08-23 20:57:30,744 Validation result (greedy) at epoch  26, step    47000: bleu:  24.83, loss: 38823.8320, ppl:   5.2626, duration: 16.5769s\n",
            "2020-08-23 20:57:43,820 Epoch  26 Step:    47100 Batch Loss:     1.743430 Tokens per Sec:    15895, Lr: 0.000300\n",
            "2020-08-23 20:57:56,582 Epoch  26 Step:    47200 Batch Loss:     1.462136 Tokens per Sec:    16024, Lr: 0.000300\n",
            "2020-08-23 20:58:09,608 Epoch  26 Step:    47300 Batch Loss:     1.820628 Tokens per Sec:    15231, Lr: 0.000300\n",
            "2020-08-23 20:58:22,998 Epoch  26 Step:    47400 Batch Loss:     1.727031 Tokens per Sec:    15755, Lr: 0.000300\n",
            "2020-08-23 20:58:36,136 Epoch  26 Step:    47500 Batch Loss:     1.794997 Tokens per Sec:    16149, Lr: 0.000300\n",
            "2020-08-23 20:58:48,956 Epoch  26 Step:    47600 Batch Loss:     1.890694 Tokens per Sec:    16109, Lr: 0.000300\n",
            "2020-08-23 20:59:01,715 Epoch  26 Step:    47700 Batch Loss:     1.353728 Tokens per Sec:    16286, Lr: 0.000300\n",
            "2020-08-23 20:59:14,673 Epoch  26 Step:    47800 Batch Loss:     1.741570 Tokens per Sec:    15990, Lr: 0.000300\n",
            "2020-08-23 20:59:27,775 Epoch  26 Step:    47900 Batch Loss:     2.033155 Tokens per Sec:    15806, Lr: 0.000300\n",
            "2020-08-23 20:59:41,010 Epoch  26 Step:    48000 Batch Loss:     2.109891 Tokens per Sec:    16070, Lr: 0.000300\n",
            "2020-08-23 20:59:57,756 Example #0\n",
            "2020-08-23 20:59:57,756 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 20:59:57,756 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 20:59:57,756 \tHypothesis: O muntu kafwete tambula nzenza yo kubakila e nzengo .\n",
            "2020-08-23 20:59:57,756 Example #1\n",
            "2020-08-23 20:59:57,756 \tSource:     3 : 27 .\n",
            "2020-08-23 20:59:57,757 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 20:59:57,757 \tHypothesis: 3 :⁠ 27 .\n",
            "2020-08-23 20:59:57,757 Example #2\n",
            "2020-08-23 20:59:57,757 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 20:59:57,757 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 20:59:57,757 \tHypothesis: E mpasi zazi zilenda kututwasila mpasi .\n",
            "2020-08-23 20:59:57,757 Example #3\n",
            "2020-08-23 20:59:57,757 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 20:59:57,758 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 20:59:57,758 \tHypothesis: Avo kuzeye mvutu ko za yuvu yayi , nga olenda lomba lusadisu lwa finkanda O Nzambi Adieyi Kieleka Kevavanga ?\n",
            "2020-08-23 20:59:57,758 Validation result (greedy) at epoch  26, step    48000: bleu:  24.64, loss: 38977.6367, ppl:   5.2974, duration: 16.7474s\n",
            "2020-08-23 21:00:11,163 Epoch  26 Step:    48100 Batch Loss:     1.168252 Tokens per Sec:    15406, Lr: 0.000300\n",
            "2020-08-23 21:00:24,140 Epoch  26 Step:    48200 Batch Loss:     1.463630 Tokens per Sec:    16008, Lr: 0.000300\n",
            "2020-08-23 21:00:37,203 Epoch  26 Step:    48300 Batch Loss:     1.874620 Tokens per Sec:    15354, Lr: 0.000300\n",
            "2020-08-23 21:00:50,332 Epoch  26 Step:    48400 Batch Loss:     1.555865 Tokens per Sec:    15692, Lr: 0.000300\n",
            "2020-08-23 21:01:03,420 Epoch  26 Step:    48500 Batch Loss:     1.657785 Tokens per Sec:    16131, Lr: 0.000300\n",
            "2020-08-23 21:01:16,680 Epoch  26 Step:    48600 Batch Loss:     1.836222 Tokens per Sec:    15699, Lr: 0.000300\n",
            "2020-08-23 21:01:29,894 Epoch  26 Step:    48700 Batch Loss:     1.705688 Tokens per Sec:    16166, Lr: 0.000300\n",
            "2020-08-23 21:01:43,090 Epoch  26 Step:    48800 Batch Loss:     2.041534 Tokens per Sec:    16312, Lr: 0.000300\n",
            "2020-08-23 21:01:52,237 Epoch  26: total training loss 3285.97\n",
            "2020-08-23 21:01:52,238 EPOCH 27\n",
            "2020-08-23 21:01:56,322 Epoch  27 Step:    48900 Batch Loss:     1.766772 Tokens per Sec:    14563, Lr: 0.000300\n",
            "2020-08-23 21:02:09,550 Epoch  27 Step:    49000 Batch Loss:     1.741135 Tokens per Sec:    15809, Lr: 0.000300\n",
            "2020-08-23 21:02:25,347 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 21:02:25,347 Saving new checkpoint.\n",
            "2020-08-23 21:02:25,852 Example #0\n",
            "2020-08-23 21:02:25,853 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 21:02:25,853 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 21:02:25,853 \tHypothesis: O muntu kafwete tambula nzenza yo wuka mawuku .\n",
            "2020-08-23 21:02:25,854 Example #1\n",
            "2020-08-23 21:02:25,854 \tSource:     3 : 27 .\n",
            "2020-08-23 21:02:25,854 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 21:02:25,854 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 21:02:25,854 Example #2\n",
            "2020-08-23 21:02:25,855 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 21:02:25,855 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 21:02:25,855 \tHypothesis: E mpasi zazi zilenda kutubwila .\n",
            "2020-08-23 21:02:25,855 Example #3\n",
            "2020-08-23 21:02:25,855 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 21:02:25,855 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 21:02:25,855 \tHypothesis: Avo ke tuna ye mvutu za yuvu yayi ko , nga olenda lomba lusadisu kwa nkanda wau wavaikiswa kwa Mbangi za Yave ?\n",
            "2020-08-23 21:02:25,856 Validation result (greedy) at epoch  27, step    49000: bleu:  24.57, loss: 38664.2812, ppl:   5.2268, duration: 16.3049s\n",
            "2020-08-23 21:02:39,159 Epoch  27 Step:    49100 Batch Loss:     1.576525 Tokens per Sec:    15696, Lr: 0.000300\n",
            "2020-08-23 21:02:52,166 Epoch  27 Step:    49200 Batch Loss:     1.990482 Tokens per Sec:    16071, Lr: 0.000300\n",
            "2020-08-23 21:03:05,172 Epoch  27 Step:    49300 Batch Loss:     1.354175 Tokens per Sec:    15819, Lr: 0.000300\n",
            "2020-08-23 21:03:18,396 Epoch  27 Step:    49400 Batch Loss:     1.770605 Tokens per Sec:    15799, Lr: 0.000300\n",
            "2020-08-23 21:03:31,283 Epoch  27 Step:    49500 Batch Loss:     1.612819 Tokens per Sec:    15860, Lr: 0.000300\n",
            "2020-08-23 21:03:44,755 Epoch  27 Step:    49600 Batch Loss:     1.702939 Tokens per Sec:    15650, Lr: 0.000300\n",
            "2020-08-23 21:03:57,802 Epoch  27 Step:    49700 Batch Loss:     1.627257 Tokens per Sec:    16040, Lr: 0.000300\n",
            "2020-08-23 21:04:10,997 Epoch  27 Step:    49800 Batch Loss:     1.765994 Tokens per Sec:    15499, Lr: 0.000300\n",
            "2020-08-23 21:04:23,917 Epoch  27 Step:    49900 Batch Loss:     1.744427 Tokens per Sec:    16109, Lr: 0.000300\n",
            "2020-08-23 21:04:36,961 Epoch  27 Step:    50000 Batch Loss:     1.708370 Tokens per Sec:    16217, Lr: 0.000300\n",
            "2020-08-23 21:04:52,252 Example #0\n",
            "2020-08-23 21:04:52,252 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 21:04:52,253 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 21:04:52,253 \tHypothesis: O muntu kafwete tambula nzenza yo wuka e mboka .\n",
            "2020-08-23 21:04:52,253 Example #1\n",
            "2020-08-23 21:04:52,253 \tSource:     3 : 27 .\n",
            "2020-08-23 21:04:52,253 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 21:04:52,253 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 21:04:52,253 Example #2\n",
            "2020-08-23 21:04:52,254 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 21:04:52,254 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 21:04:52,254 \tHypothesis: E mpasi zazi zilenda twasa mpasi .\n",
            "2020-08-23 21:04:52,254 Example #3\n",
            "2020-08-23 21:04:52,254 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 21:04:52,254 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 21:04:52,254 \tHypothesis: Avo ke tuna ye mvutu za yuvu yayi ko , nga olenda lomba lusadisu muna finkanda - nkanda , O Nzambi Adieyi Kieleka Kevavanga kwa Yeto ?\n",
            "2020-08-23 21:04:52,254 Validation result (greedy) at epoch  27, step    50000: bleu:  25.02, loss: 38704.2812, ppl:   5.2358, duration: 15.2928s\n",
            "2020-08-23 21:05:05,187 Epoch  27 Step:    50100 Batch Loss:     1.145672 Tokens per Sec:    16241, Lr: 0.000300\n",
            "2020-08-23 21:05:18,152 Epoch  27 Step:    50200 Batch Loss:     1.411856 Tokens per Sec:    15704, Lr: 0.000300\n",
            "2020-08-23 21:05:31,183 Epoch  27 Step:    50300 Batch Loss:     1.766696 Tokens per Sec:    15850, Lr: 0.000300\n",
            "2020-08-23 21:05:44,098 Epoch  27 Step:    50400 Batch Loss:     1.837446 Tokens per Sec:    16348, Lr: 0.000300\n",
            "2020-08-23 21:05:57,296 Epoch  27 Step:    50500 Batch Loss:     1.677646 Tokens per Sec:    15253, Lr: 0.000300\n",
            "2020-08-23 21:06:10,351 Epoch  27 Step:    50600 Batch Loss:     1.678989 Tokens per Sec:    15781, Lr: 0.000300\n",
            "2020-08-23 21:06:23,493 Epoch  27 Step:    50700 Batch Loss:     1.657059 Tokens per Sec:    15721, Lr: 0.000300\n",
            "2020-08-23 21:06:30,914 Epoch  27: total training loss 3265.98\n",
            "2020-08-23 21:06:30,914 EPOCH 28\n",
            "2020-08-23 21:06:36,946 Epoch  28 Step:    50800 Batch Loss:     1.584334 Tokens per Sec:    14768, Lr: 0.000300\n",
            "2020-08-23 21:06:49,936 Epoch  28 Step:    50900 Batch Loss:     1.613540 Tokens per Sec:    15861, Lr: 0.000300\n",
            "2020-08-23 21:07:02,855 Epoch  28 Step:    51000 Batch Loss:     1.947115 Tokens per Sec:    15840, Lr: 0.000300\n",
            "2020-08-23 21:07:19,093 Example #0\n",
            "2020-08-23 21:07:19,093 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 21:07:19,093 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 21:07:19,093 \tHypothesis: O muntu kafwete tambula nzenza ye mboka .\n",
            "2020-08-23 21:07:19,093 Example #1\n",
            "2020-08-23 21:07:19,093 \tSource:     3 : 27 .\n",
            "2020-08-23 21:07:19,094 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 21:07:19,094 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 21:07:19,094 Example #2\n",
            "2020-08-23 21:07:19,094 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 21:07:19,094 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 21:07:19,094 \tHypothesis: E mpasi zazi zilenda kala zampasi .\n",
            "2020-08-23 21:07:19,094 Example #3\n",
            "2020-08-23 21:07:19,095 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 21:07:19,095 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 21:07:19,095 \tHypothesis: Avo ke tuna ye mvutu za yuvu yayi ko , nga olenda lomba lusadisu kwa finkanda O Nzambi Adieyi Kieleka Kevavanga ?\n",
            "2020-08-23 21:07:19,095 Validation result (greedy) at epoch  28, step    51000: bleu:  24.59, loss: 38703.8555, ppl:   5.2357, duration: 16.2393s\n",
            "2020-08-23 21:07:32,466 Epoch  28 Step:    51100 Batch Loss:     1.917030 Tokens per Sec:    15623, Lr: 0.000300\n",
            "2020-08-23 21:07:45,383 Epoch  28 Step:    51200 Batch Loss:     1.698078 Tokens per Sec:    15819, Lr: 0.000300\n",
            "2020-08-23 21:07:58,630 Epoch  28 Step:    51300 Batch Loss:     1.490515 Tokens per Sec:    16219, Lr: 0.000300\n",
            "2020-08-23 21:08:11,581 Epoch  28 Step:    51400 Batch Loss:     1.742472 Tokens per Sec:    16312, Lr: 0.000300\n",
            "2020-08-23 21:08:24,467 Epoch  28 Step:    51500 Batch Loss:     1.557703 Tokens per Sec:    15841, Lr: 0.000300\n",
            "2020-08-23 21:08:37,936 Epoch  28 Step:    51600 Batch Loss:     1.845821 Tokens per Sec:    15609, Lr: 0.000300\n",
            "2020-08-23 21:08:50,907 Epoch  28 Step:    51700 Batch Loss:     1.742696 Tokens per Sec:    16115, Lr: 0.000300\n",
            "2020-08-23 21:09:03,959 Epoch  28 Step:    51800 Batch Loss:     1.705466 Tokens per Sec:    16143, Lr: 0.000300\n",
            "2020-08-23 21:09:17,080 Epoch  28 Step:    51900 Batch Loss:     1.979768 Tokens per Sec:    16028, Lr: 0.000300\n",
            "2020-08-23 21:09:30,223 Epoch  28 Step:    52000 Batch Loss:     1.442705 Tokens per Sec:    16005, Lr: 0.000300\n",
            "2020-08-23 21:09:45,341 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 21:09:45,342 Saving new checkpoint.\n",
            "2020-08-23 21:09:45,839 Example #0\n",
            "2020-08-23 21:09:45,839 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 21:09:45,839 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 21:09:45,839 \tHypothesis: O muntu kafwete tambula nzenza yo nua mfomo .\n",
            "2020-08-23 21:09:45,839 Example #1\n",
            "2020-08-23 21:09:45,839 \tSource:     3 : 27 .\n",
            "2020-08-23 21:09:45,840 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 21:09:45,840 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 21:09:45,840 Example #2\n",
            "2020-08-23 21:09:45,840 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 21:09:45,840 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 21:09:45,840 \tHypothesis: E diambu diadi dilenda kendeleka kikilu .\n",
            "2020-08-23 21:09:45,840 Example #3\n",
            "2020-08-23 21:09:45,841 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 21:09:45,841 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 21:09:45,841 \tHypothesis: Avo ke tuna ye mvutu za yuvu yayi ko , nga olenda lomba lusadisu kwa mpangi za finkanda - nkanda O Nzambi Adieyi Kevavanga kwa Yeto ?\n",
            "2020-08-23 21:09:45,841 Validation result (greedy) at epoch  28, step    52000: bleu:  24.85, loss: 38607.2500, ppl:   5.2141, duration: 15.6179s\n",
            "2020-08-23 21:09:58,784 Epoch  28 Step:    52100 Batch Loss:     1.665644 Tokens per Sec:    16235, Lr: 0.000300\n",
            "2020-08-23 21:10:11,838 Epoch  28 Step:    52200 Batch Loss:     1.860959 Tokens per Sec:    16614, Lr: 0.000300\n",
            "2020-08-23 21:10:25,034 Epoch  28 Step:    52300 Batch Loss:     1.871267 Tokens per Sec:    15590, Lr: 0.000300\n",
            "2020-08-23 21:10:37,816 Epoch  28 Step:    52400 Batch Loss:     1.836306 Tokens per Sec:    15749, Lr: 0.000300\n",
            "2020-08-23 21:10:50,988 Epoch  28 Step:    52500 Batch Loss:     1.838542 Tokens per Sec:    15925, Lr: 0.000300\n",
            "2020-08-23 21:11:03,974 Epoch  28 Step:    52600 Batch Loss:     1.864073 Tokens per Sec:    16328, Lr: 0.000300\n",
            "2020-08-23 21:11:07,714 Epoch  28: total training loss 3222.29\n",
            "2020-08-23 21:11:07,714 EPOCH 29\n",
            "2020-08-23 21:11:17,183 Epoch  29 Step:    52700 Batch Loss:     1.765346 Tokens per Sec:    15752, Lr: 0.000300\n",
            "2020-08-23 21:11:30,432 Epoch  29 Step:    52800 Batch Loss:     1.778953 Tokens per Sec:    15883, Lr: 0.000300\n",
            "2020-08-23 21:11:43,661 Epoch  29 Step:    52900 Batch Loss:     1.772263 Tokens per Sec:    15572, Lr: 0.000300\n",
            "2020-08-23 21:11:56,552 Epoch  29 Step:    53000 Batch Loss:     1.660791 Tokens per Sec:    16069, Lr: 0.000300\n",
            "2020-08-23 21:12:12,690 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 21:12:12,691 Saving new checkpoint.\n",
            "2020-08-23 21:12:13,593 Example #0\n",
            "2020-08-23 21:12:13,593 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 21:12:13,593 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 21:12:13,593 \tHypothesis: O muntu kafwete kala ye fu kia tambula nzenza ye mawuku .\n",
            "2020-08-23 21:12:13,593 Example #1\n",
            "2020-08-23 21:12:13,593 \tSource:     3 : 27 .\n",
            "2020-08-23 21:12:13,594 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 21:12:13,594 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 21:12:13,594 Example #2\n",
            "2020-08-23 21:12:13,594 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 21:12:13,594 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 21:12:13,594 \tHypothesis: E mpasi zazi zilenda twasa mpasi .\n",
            "2020-08-23 21:12:13,594 Example #3\n",
            "2020-08-23 21:12:13,595 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 21:12:13,595 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 21:12:13,595 \tHypothesis: Avo kuzeye mvutu za yuvu yayi ko , nga olenda lomba ndomba za finkanda O Nzambi Adieyi Kevavanga ?\n",
            "2020-08-23 21:12:13,595 Validation result (greedy) at epoch  29, step    53000: bleu:  25.42, loss: 38442.1211, ppl:   5.1774, duration: 17.0426s\n",
            "2020-08-23 21:12:26,694 Epoch  29 Step:    53100 Batch Loss:     1.747851 Tokens per Sec:    15924, Lr: 0.000300\n",
            "2020-08-23 21:12:39,793 Epoch  29 Step:    53200 Batch Loss:     1.599213 Tokens per Sec:    15637, Lr: 0.000300\n",
            "2020-08-23 21:12:52,766 Epoch  29 Step:    53300 Batch Loss:     1.720603 Tokens per Sec:    15824, Lr: 0.000300\n",
            "2020-08-23 21:13:05,717 Epoch  29 Step:    53400 Batch Loss:     1.537757 Tokens per Sec:    16228, Lr: 0.000300\n",
            "2020-08-23 21:13:18,677 Epoch  29 Step:    53500 Batch Loss:     1.391693 Tokens per Sec:    15122, Lr: 0.000300\n",
            "2020-08-23 21:13:31,620 Epoch  29 Step:    53600 Batch Loss:     1.904706 Tokens per Sec:    16113, Lr: 0.000300\n",
            "2020-08-23 21:13:44,803 Epoch  29 Step:    53700 Batch Loss:     1.528918 Tokens per Sec:    16381, Lr: 0.000300\n",
            "2020-08-23 21:13:57,786 Epoch  29 Step:    53800 Batch Loss:     1.848806 Tokens per Sec:    15987, Lr: 0.000300\n",
            "2020-08-23 21:14:10,994 Epoch  29 Step:    53900 Batch Loss:     1.780161 Tokens per Sec:    15801, Lr: 0.000300\n",
            "2020-08-23 21:14:23,987 Epoch  29 Step:    54000 Batch Loss:     1.754625 Tokens per Sec:    16118, Lr: 0.000300\n",
            "2020-08-23 21:14:39,531 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 21:14:39,531 Saving new checkpoint.\n",
            "2020-08-23 21:14:40,028 Example #0\n",
            "2020-08-23 21:14:40,029 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 21:14:40,029 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 21:14:40,029 \tHypothesis: O muntu kafwete tambula nzenza ye mboka .\n",
            "2020-08-23 21:14:40,029 Example #1\n",
            "2020-08-23 21:14:40,030 \tSource:     3 : 27 .\n",
            "2020-08-23 21:14:40,030 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 21:14:40,030 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 21:14:40,030 Example #2\n",
            "2020-08-23 21:14:40,030 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 21:14:40,030 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 21:14:40,030 \tHypothesis: E diambu diadi dilenda kendalala .\n",
            "2020-08-23 21:14:40,030 Example #3\n",
            "2020-08-23 21:14:40,031 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 21:14:40,031 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 21:14:40,031 \tHypothesis: Avo ke tuna ye mvutu za yuvu yayi ko , nga olenda lomba lusadisu lwa finkanda - nkanda O Nzambi Adieyi Kieleka Kevavanga kwa Yeto ?\n",
            "2020-08-23 21:14:40,031 Validation result (greedy) at epoch  29, step    54000: bleu:  25.53, loss: 38406.6094, ppl:   5.1695, duration: 16.0432s\n",
            "2020-08-23 21:14:53,061 Epoch  29 Step:    54100 Batch Loss:     1.279798 Tokens per Sec:    16068, Lr: 0.000300\n",
            "2020-08-23 21:15:06,006 Epoch  29 Step:    54200 Batch Loss:     2.203188 Tokens per Sec:    16131, Lr: 0.000300\n",
            "2020-08-23 21:15:19,222 Epoch  29 Step:    54300 Batch Loss:     1.661653 Tokens per Sec:    15858, Lr: 0.000300\n",
            "2020-08-23 21:15:32,413 Epoch  29 Step:    54400 Batch Loss:     1.596070 Tokens per Sec:    15681, Lr: 0.000300\n",
            "2020-08-23 21:15:45,503 Epoch  29 Step:    54500 Batch Loss:     1.488358 Tokens per Sec:    16191, Lr: 0.000300\n",
            "2020-08-23 21:15:46,645 Epoch  29: total training loss 3225.11\n",
            "2020-08-23 21:15:46,646 EPOCH 30\n",
            "2020-08-23 21:15:58,952 Epoch  30 Step:    54600 Batch Loss:     1.493443 Tokens per Sec:    15059, Lr: 0.000300\n",
            "2020-08-23 21:16:12,178 Epoch  30 Step:    54700 Batch Loss:     2.091909 Tokens per Sec:    15894, Lr: 0.000300\n",
            "2020-08-23 21:16:25,513 Epoch  30 Step:    54800 Batch Loss:     1.392003 Tokens per Sec:    15485, Lr: 0.000300\n",
            "2020-08-23 21:16:38,806 Epoch  30 Step:    54900 Batch Loss:     1.684136 Tokens per Sec:    16059, Lr: 0.000300\n",
            "2020-08-23 21:16:51,927 Epoch  30 Step:    55000 Batch Loss:     1.341481 Tokens per Sec:    15501, Lr: 0.000300\n",
            "2020-08-23 21:17:07,175 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 21:17:07,176 Saving new checkpoint.\n",
            "2020-08-23 21:17:07,713 Example #0\n",
            "2020-08-23 21:17:07,713 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 21:17:07,713 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 21:17:07,713 \tHypothesis: O muntu kafwete kala ye fu kia tambula nzenza ye mawuku .\n",
            "2020-08-23 21:17:07,713 Example #1\n",
            "2020-08-23 21:17:07,714 \tSource:     3 : 27 .\n",
            "2020-08-23 21:17:07,714 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 21:17:07,714 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 21:17:07,714 Example #2\n",
            "2020-08-23 21:17:07,714 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 21:17:07,714 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 21:17:07,714 \tHypothesis: E diambu diadi dilenda kendeleka kikilu .\n",
            "2020-08-23 21:17:07,715 Example #3\n",
            "2020-08-23 21:17:07,715 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 21:17:07,715 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 21:17:07,715 \tHypothesis: Avo kuzeye mvutu za yuvu yayi ko , nga olenda lomba vo nkanda wau wavaikiswa kwa Mbangi za Yave ?\n",
            "2020-08-23 21:17:07,715 Validation result (greedy) at epoch  30, step    55000: bleu:  25.49, loss: 38234.7188, ppl:   5.1317, duration: 15.7880s\n",
            "2020-08-23 21:17:20,894 Epoch  30 Step:    55100 Batch Loss:     1.879239 Tokens per Sec:    16389, Lr: 0.000300\n",
            "2020-08-23 21:17:33,957 Epoch  30 Step:    55200 Batch Loss:     1.797829 Tokens per Sec:    15414, Lr: 0.000300\n",
            "2020-08-23 21:17:47,133 Epoch  30 Step:    55300 Batch Loss:     1.673882 Tokens per Sec:    15691, Lr: 0.000300\n",
            "2020-08-23 21:17:59,999 Epoch  30 Step:    55400 Batch Loss:     1.743155 Tokens per Sec:    16159, Lr: 0.000300\n",
            "2020-08-23 21:18:12,922 Epoch  30 Step:    55500 Batch Loss:     1.796807 Tokens per Sec:    16270, Lr: 0.000300\n",
            "2020-08-23 21:18:26,313 Epoch  30 Step:    55600 Batch Loss:     1.477888 Tokens per Sec:    15977, Lr: 0.000300\n",
            "2020-08-23 21:18:39,326 Epoch  30 Step:    55700 Batch Loss:     1.785620 Tokens per Sec:    15803, Lr: 0.000300\n",
            "2020-08-23 21:18:52,339 Epoch  30 Step:    55800 Batch Loss:     1.557833 Tokens per Sec:    16241, Lr: 0.000300\n",
            "2020-08-23 21:19:05,472 Epoch  30 Step:    55900 Batch Loss:     1.623258 Tokens per Sec:    15745, Lr: 0.000300\n",
            "2020-08-23 21:19:18,337 Epoch  30 Step:    56000 Batch Loss:     1.676135 Tokens per Sec:    16040, Lr: 0.000300\n",
            "2020-08-23 21:19:34,350 Hooray! New best validation result [ppl]!\n",
            "2020-08-23 21:19:34,350 Saving new checkpoint.\n",
            "2020-08-23 21:19:34,876 Example #0\n",
            "2020-08-23 21:19:34,876 \tSource:     Gone will be the need for hospitals and medications .\n",
            "2020-08-23 21:19:34,876 \tReference:  Ke vekala mfunu a tupitalu ko ngatu nlongo .\n",
            "2020-08-23 21:19:34,876 \tHypothesis: O muntu kafwete tambula nzenza ye mawuku .\n",
            "2020-08-23 21:19:34,876 Example #1\n",
            "2020-08-23 21:19:34,876 \tSource:     3 : 27 .\n",
            "2020-08-23 21:19:34,877 \tReference:  3 : ​ 27 .\n",
            "2020-08-23 21:19:34,877 \tHypothesis: 3 : 27 .\n",
            "2020-08-23 21:19:34,877 Example #2\n",
            "2020-08-23 21:19:34,877 \tSource:     Such a circumstance can be very distressing .\n",
            "2020-08-23 21:19:34,877 \tReference:  Ediadi dilenda kikilu kutukendeleka .\n",
            "2020-08-23 21:19:34,877 \tHypothesis: E diambu diadi dilenda kendeleka kikilu .\n",
            "2020-08-23 21:19:34,877 Example #3\n",
            "2020-08-23 21:19:34,878 \tSource:     If you are unsure of the answers to any of these questions , you may request a copy of the brochure What Does God Require of Us ?\n",
            "2020-08-23 21:19:34,878 \tReference:  Nkia nkanikinu miakaka mia Nkand’a Nzambi milenda wokesa e kiese muna nzo ?\n",
            "2020-08-23 21:19:34,878 \tHypothesis: Avo kuzeye mvutu za yuvu yayi ko , nga olenda lomba kwa finkanda O Nzambi Adieyi Kieleka Kevanganga kwa Yeto ?\n",
            "2020-08-23 21:19:34,878 Validation result (greedy) at epoch  30, step    56000: bleu:  25.56, loss: 37946.6406, ppl:   5.0688, duration: 16.5404s\n",
            "2020-08-23 21:19:47,962 Epoch  30 Step:    56100 Batch Loss:     1.773254 Tokens per Sec:    15912, Lr: 0.000300\n",
            "2020-08-23 21:20:01,057 Epoch  30 Step:    56200 Batch Loss:     1.836234 Tokens per Sec:    15884, Lr: 0.000300\n",
            "2020-08-23 21:20:14,162 Epoch  30 Step:    56300 Batch Loss:     1.464019 Tokens per Sec:    16471, Lr: 0.000300\n",
            "2020-08-23 21:20:25,334 Epoch  30: total training loss 3188.61\n",
            "2020-08-23 21:20:25,334 Training ended after  30 epochs.\n",
            "2020-08-23 21:20:25,334 Best validation result (greedy) at step    56000:   5.07 ppl.\n",
            "2020-08-23 21:20:49,464  dev bleu:  26.69 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2020-08-23 21:20:49,465 Translations saved to: models/enkwy_transformer/00056000.hyps.dev\n",
            "2020-08-23 21:21:28,680 test bleu:  36.28 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2020-08-23 21:21:28,682 Translations saved to: models/enkwy_transformer/00056000.hyps.test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MBoDS09JM807",
        "colab": {}
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "!cp -r joeynmt/models/${src}${tgt}_transformer/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AYCOImAVVCFJ",
        "colab": {}
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "!cp joeynmt/models/${src}${tgt}_transformer/best.ckpt \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cSoGZWeeUFob",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "1e95705e-8701-4b49-cf81-72076f61d10c"
      },
      "source": [
        "!ls joeynmt/models/${src}${tgt}_transformer"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "00056000.hyps.dev   2000.hyps\t33000.hyps  46000.hyps\t56000.hyps\n",
            "00056000.hyps.test  21000.hyps\t34000.hyps  47000.hyps\t6000.hyps\n",
            "10000.hyps\t    22000.hyps\t35000.hyps  48000.hyps\t7000.hyps\n",
            "1000.hyps\t    23000.hyps\t36000.hyps  49000.hyps\t8000.hyps\n",
            "11000.hyps\t    24000.hyps\t37000.hyps  50000.hyps\t9000.hyps\n",
            "12000.hyps\t    25000.hyps\t38000.hyps  5000.hyps\tbest.ckpt\n",
            "13000.hyps\t    26000.hyps\t39000.hyps  51000.hyps\tconfig.yaml\n",
            "14000.hyps\t    27000.hyps\t40000.hyps  52000.hyps\tsrc_vocab.txt\n",
            "15000.hyps\t    28000.hyps\t4000.hyps   53000.hyps\ttensorboard\n",
            "16000.hyps\t    29000.hyps\t41000.hyps  54000.ckpt\ttrain.log\n",
            "17000.hyps\t    30000.hyps\t42000.hyps  54000.hyps\ttrg_vocab.txt\n",
            "18000.hyps\t    3000.hyps\t43000.hyps  55000.ckpt\tvalidations.txt\n",
            "19000.hyps\t    31000.hyps\t44000.hyps  55000.hyps\n",
            "20000.hyps\t    32000.hyps\t45000.hyps  56000.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n94wlrCjVc17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "19113275-7d6a-48ae-8979-9959ce133f27"
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 1000\tLoss: 94990.79688\tPPL: 58.15327\tbleu: 1.05368\tLR: 0.00030000\t*\n",
            "Steps: 2000\tLoss: 79171.57031\tPPL: 29.56052\tbleu: 3.79407\tLR: 0.00030000\t*\n",
            "Steps: 3000\tLoss: 71365.70312\tPPL: 21.16939\tbleu: 4.73938\tLR: 0.00030000\t*\n",
            "Steps: 4000\tLoss: 65827.97656\tPPL: 16.70467\tbleu: 7.62276\tLR: 0.00030000\t*\n",
            "Steps: 5000\tLoss: 62261.00781\tPPL: 14.34092\tbleu: 9.52336\tLR: 0.00030000\t*\n",
            "Steps: 6000\tLoss: 59579.26172\tPPL: 12.78675\tbleu: 10.86494\tLR: 0.00030000\t*\n",
            "Steps: 7000\tLoss: 57351.00781\tPPL: 11.62432\tbleu: 12.41515\tLR: 0.00030000\t*\n",
            "Steps: 8000\tLoss: 55338.14453\tPPL: 10.66537\tbleu: 13.34303\tLR: 0.00030000\t*\n",
            "Steps: 9000\tLoss: 53618.95312\tPPL: 9.90923\tbleu: 14.68891\tLR: 0.00030000\t*\n",
            "Steps: 10000\tLoss: 52201.00391\tPPL: 9.32609\tbleu: 15.15425\tLR: 0.00030000\t*\n",
            "Steps: 11000\tLoss: 50936.39453\tPPL: 8.83503\tbleu: 15.95811\tLR: 0.00030000\t*\n",
            "Steps: 12000\tLoss: 50030.32031\tPPL: 8.49917\tbleu: 17.10952\tLR: 0.00030000\t*\n",
            "Steps: 13000\tLoss: 49374.60938\tPPL: 8.26410\tbleu: 17.06075\tLR: 0.00030000\t*\n",
            "Steps: 14000\tLoss: 48375.02734\tPPL: 7.91822\tbleu: 17.46149\tLR: 0.00030000\t*\n",
            "Steps: 15000\tLoss: 47547.06641\tPPL: 7.64270\tbleu: 18.28160\tLR: 0.00030000\t*\n",
            "Steps: 16000\tLoss: 46939.23438\tPPL: 7.44656\tbleu: 18.54897\tLR: 0.00030000\t*\n",
            "Steps: 17000\tLoss: 46427.39453\tPPL: 7.28530\tbleu: 19.36885\tLR: 0.00030000\t*\n",
            "Steps: 18000\tLoss: 45810.44531\tPPL: 7.09556\tbleu: 19.68186\tLR: 0.00030000\t*\n",
            "Steps: 19000\tLoss: 45406.71484\tPPL: 6.97408\tbleu: 19.77998\tLR: 0.00030000\t*\n",
            "Steps: 20000\tLoss: 44904.87109\tPPL: 6.82598\tbleu: 19.83059\tLR: 0.00030000\t*\n",
            "Steps: 21000\tLoss: 44580.38672\tPPL: 6.73189\tbleu: 20.71205\tLR: 0.00030000\t*\n",
            "Steps: 22000\tLoss: 44010.39453\tPPL: 6.56975\tbleu: 20.50529\tLR: 0.00030000\t*\n",
            "Steps: 23000\tLoss: 43653.65234\tPPL: 6.47026\tbleu: 20.78045\tLR: 0.00030000\t*\n",
            "Steps: 24000\tLoss: 43375.76172\tPPL: 6.39381\tbleu: 20.90051\tLR: 0.00030000\t*\n",
            "Steps: 25000\tLoss: 43175.93750\tPPL: 6.33939\tbleu: 21.17680\tLR: 0.00030000\t*\n",
            "Steps: 26000\tLoss: 42681.67969\tPPL: 6.20678\tbleu: 21.24669\tLR: 0.00030000\t*\n",
            "Steps: 27000\tLoss: 42504.46875\tPPL: 6.15991\tbleu: 21.87470\tLR: 0.00030000\t*\n",
            "Steps: 28000\tLoss: 42270.03516\tPPL: 6.09845\tbleu: 21.78488\tLR: 0.00030000\t*\n",
            "Steps: 29000\tLoss: 41851.32422\tPPL: 5.99020\tbleu: 22.40898\tLR: 0.00030000\t*\n",
            "Steps: 30000\tLoss: 41725.03125\tPPL: 5.95793\tbleu: 22.27735\tLR: 0.00030000\t*\n",
            "Steps: 31000\tLoss: 41453.73047\tPPL: 5.88919\tbleu: 22.48757\tLR: 0.00030000\t*\n",
            "Steps: 32000\tLoss: 41279.21484\tPPL: 5.84539\tbleu: 22.88817\tLR: 0.00030000\t*\n",
            "Steps: 33000\tLoss: 41042.71875\tPPL: 5.78656\tbleu: 23.30968\tLR: 0.00030000\t*\n",
            "Steps: 34000\tLoss: 40938.10156\tPPL: 5.76072\tbleu: 23.41904\tLR: 0.00030000\t*\n",
            "Steps: 35000\tLoss: 40585.32812\tPPL: 5.67445\tbleu: 23.45256\tLR: 0.00030000\t*\n",
            "Steps: 36000\tLoss: 40620.56641\tPPL: 5.68301\tbleu: 23.45846\tLR: 0.00030000\t\n",
            "Steps: 37000\tLoss: 40301.06250\tPPL: 5.60587\tbleu: 23.37400\tLR: 0.00030000\t*\n",
            "Steps: 38000\tLoss: 40129.09375\tPPL: 5.56479\tbleu: 23.67158\tLR: 0.00030000\t*\n",
            "Steps: 39000\tLoss: 39976.76562\tPPL: 5.52865\tbleu: 23.65259\tLR: 0.00030000\t*\n",
            "Steps: 40000\tLoss: 39948.70703\tPPL: 5.52202\tbleu: 24.30340\tLR: 0.00030000\t*\n",
            "Steps: 41000\tLoss: 39782.07422\tPPL: 5.48280\tbleu: 24.25712\tLR: 0.00030000\t*\n",
            "Steps: 42000\tLoss: 39491.93359\tPPL: 5.41518\tbleu: 23.88405\tLR: 0.00030000\t*\n",
            "Steps: 43000\tLoss: 39492.46875\tPPL: 5.41530\tbleu: 24.40745\tLR: 0.00030000\t\n",
            "Steps: 44000\tLoss: 39269.82031\tPPL: 5.36397\tbleu: 24.48297\tLR: 0.00030000\t*\n",
            "Steps: 45000\tLoss: 39150.11719\tPPL: 5.33658\tbleu: 24.34019\tLR: 0.00030000\t*\n",
            "Steps: 46000\tLoss: 39131.12500\tPPL: 5.33225\tbleu: 24.33152\tLR: 0.00030000\t*\n",
            "Steps: 47000\tLoss: 38823.83203\tPPL: 5.26262\tbleu: 24.83040\tLR: 0.00030000\t*\n",
            "Steps: 48000\tLoss: 38977.63672\tPPL: 5.29735\tbleu: 24.63594\tLR: 0.00030000\t\n",
            "Steps: 49000\tLoss: 38664.28125\tPPL: 5.22682\tbleu: 24.57220\tLR: 0.00030000\t*\n",
            "Steps: 50000\tLoss: 38704.28125\tPPL: 5.23577\tbleu: 25.01577\tLR: 0.00030000\t\n",
            "Steps: 51000\tLoss: 38703.85547\tPPL: 5.23568\tbleu: 24.58984\tLR: 0.00030000\t\n",
            "Steps: 52000\tLoss: 38607.25000\tPPL: 5.21409\tbleu: 24.85228\tLR: 0.00030000\t*\n",
            "Steps: 53000\tLoss: 38442.12109\tPPL: 5.17739\tbleu: 25.42388\tLR: 0.00030000\t*\n",
            "Steps: 54000\tLoss: 38406.60938\tPPL: 5.16953\tbleu: 25.53146\tLR: 0.00030000\t*\n",
            "Steps: 55000\tLoss: 38234.71875\tPPL: 5.13166\tbleu: 25.49015\tLR: 0.00030000\t*\n",
            "Steps: 56000\tLoss: 37946.64062\tPPL: 5.06882\tbleu: 25.56169\tLR: 0.00030000\t*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "66WhRE9lIhoD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "3c2d1021-2b66-41b5-e5ed-d1a1003a55ec"
      },
      "source": [
        "# Test our model\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-23 21:35:15,307 Hello! This is Joey-NMT.\n",
            "/content/joeynmt/joeynmt/search.py:351: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  finished_hyp = is_finished[i].nonzero().view(-1)\n",
            "2020-08-23 21:35:42,245  dev bleu:  26.69 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2020-08-23 21:36:22,367 test bleu:  36.28 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}