{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English2Tigrigna",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE3onLtiHvP5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c88b96b3-bba5-43f3-bf53-13c46cda400a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iapTRdQLH2MO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\n",
        "import os\n",
        "source_language = \"en\"\n",
        "target_language = \"ti\"\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = tag\n",
        "\n",
        "# This will save it to a folder in our gdrive instead!\n",
        "!mkdir -p \"/content/drive/My Drive/masakhane/$src-$tgt\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/masakhane/%s-%s\" % (source_language, target_language)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI0nLF_Zt6Of",
        "colab_type": "code",
        "outputId": "e6ad9d51-4e57-4a61-b93b-5dd70ac30dec",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-78a81e65-0e18-4423-9919-c50c20b2b600\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-78a81e65-0e18-4423-9919-c50c20b2b600\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving en_ti.csv to en_ti.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQvTy7obuOPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "data = pd.read_csv(io.BytesIO(uploaded['en_ti.csv']))\n",
        "# Dataset is now stored in a Pandas Dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQF4yAXfuOSA",
        "colab_type": "code",
        "outputId": "d8dab361-774d-4ff1-9d49-cf887aeb68ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Tigrigna</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17 Jehovah now said to Moses : 2 “ Speak to th...</td>\n",
              "      <td>17 የሆዋ ድማ ንሙሴ ኸምዚ ኢሉ ተዛረቦ ፦ 2 “ ንእስራኤላውያን ተዛረቦ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6 So Moses spoke to the Israelites , and all t...</td>\n",
              "      <td>6 ሙሴ ድማ ንእስራኤላውያን ተዛረቦም , ኵሎም ሹማምንቲ ኸኣ ኣባትር  ማ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8 On the next day , when Moses went into the t...</td>\n",
              "      <td>8 ንጽብሒቱ  ሙሴ ናብ ድንኳን ምስክር ምስ ኣተወ  እንሆ በትሪ ኣሮን  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10 Jehovah then said to Moses : “ Put Aaron ’ ...</td>\n",
              "      <td>10 የሆዋ ድማ ንሙሴ  “ በትሪ ኣሮን ንደቂ ዕልወት ምልክት ክትኰኖም ም...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12 The Israelites then said to Moses : “ Now w...</td>\n",
              "      <td>12 እስራኤላውያን ድማ ንሙሴ  “ ሕጂ ሞትና  ጠፋእና  ኵላትና ጠፋእና ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             English                                           Tigrigna\n",
              "0  17 Jehovah now said to Moses : 2 “ Speak to th...  17 የሆዋ ድማ ንሙሴ ኸምዚ ኢሉ ተዛረቦ ፦ 2 “ ንእስራኤላውያን ተዛረቦ...\n",
              "1  6 So Moses spoke to the Israelites , and all t...  6 ሙሴ ድማ ንእስራኤላውያን ተዛረቦም , ኵሎም ሹማምንቲ ኸኣ ኣባትር  ማ...\n",
              "2  8 On the next day , when Moses went into the t...  8 ንጽብሒቱ  ሙሴ ናብ ድንኳን ምስክር ምስ ኣተወ  እንሆ በትሪ ኣሮን  ...\n",
              "3  10 Jehovah then said to Moses : “ Put Aaron ’ ...  10 የሆዋ ድማ ንሙሴ  “ በትሪ ኣሮን ንደቂ ዕልወት ምልክት ክትኰኖም ም...\n",
              "4  12 The Israelites then said to Moses : “ Now w...  12 እስራኤላውያን ድማ ንሙሴ  “ ሕጂ ሞትና  ጠፋእና  ኵላትና ጠፋእና ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzs9F6kVuOYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.rename(columns={\"English\":\"source_sentence\", \"Tigrigna\":\"target_sentence\"})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HPQuXBK6o8J",
        "colab_type": "code",
        "outputId": "e766756e-aa0a-484c-fde0-6dc4814670fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_sentence</th>\n",
              "      <th>target_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17 Jehovah now said to Moses : 2 “ Speak to th...</td>\n",
              "      <td>17 የሆዋ ድማ ንሙሴ ኸምዚ ኢሉ ተዛረቦ ፦ 2 “ ንእስራኤላውያን ተዛረቦ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6 So Moses spoke to the Israelites , and all t...</td>\n",
              "      <td>6 ሙሴ ድማ ንእስራኤላውያን ተዛረቦም , ኵሎም ሹማምንቲ ኸኣ ኣባትር  ማ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8 On the next day , when Moses went into the t...</td>\n",
              "      <td>8 ንጽብሒቱ  ሙሴ ናብ ድንኳን ምስክር ምስ ኣተወ  እንሆ በትሪ ኣሮን  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10 Jehovah then said to Moses : “ Put Aaron ’ ...</td>\n",
              "      <td>10 የሆዋ ድማ ንሙሴ  “ በትሪ ኣሮን ንደቂ ዕልወት ምልክት ክትኰኖም ም...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12 The Israelites then said to Moses : “ Now w...</td>\n",
              "      <td>12 እስራኤላውያን ድማ ንሙሴ  “ ሕጂ ሞትና  ጠፋእና  ኵላትና ጠፋእና ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     source_sentence                                    target_sentence\n",
              "0  17 Jehovah now said to Moses : 2 “ Speak to th...  17 የሆዋ ድማ ንሙሴ ኸምዚ ኢሉ ተዛረቦ ፦ 2 “ ንእስራኤላውያን ተዛረቦ...\n",
              "1  6 So Moses spoke to the Israelites , and all t...  6 ሙሴ ድማ ንእስራኤላውያን ተዛረቦም , ኵሎም ሹማምንቲ ኸኣ ኣባትር  ማ...\n",
              "2  8 On the next day , when Moses went into the t...  8 ንጽብሒቱ  ሙሴ ናብ ድንኳን ምስክር ምስ ኣተወ  እንሆ በትሪ ኣሮን  ...\n",
              "3  10 Jehovah then said to Moses : “ Put Aaron ’ ...  10 የሆዋ ድማ ንሙሴ  “ በትሪ ኣሮን ንደቂ ዕልወት ምልክት ክትኰኖም ም...\n",
              "4  12 The Israelites then said to Moses : “ Now w...  12 እስራኤላውያን ድማ ንሙሴ  “ ሕጂ ሞትና  ጠፋእና  ኵላትና ጠፋእና ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihDdYUR0ukwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Do the split between dev/test/train and create parallel corpora\n",
        "num_dev_patterns = 1000\n",
        "num_test_patterns = 1000\n",
        "df = data\n",
        "# Lower case the corpora\n",
        "df[\"source_sentence\"] = df[\"source_sentence\"].str.lower()\n",
        "df[\"target_sentence\"] = df[\"target_sentence\"].str.lower()\n",
        "\n",
        "\n",
        "devtest = df.tail(num_dev_patterns + num_test_patterns)\n",
        "test = devtest.tail(num_test_patterns)\n",
        "dev = devtest.head(num_dev_patterns)\n",
        "stripped = df.drop(df.tail(num_dev_patterns + num_test_patterns).index)\n",
        "\n",
        "stripped[[\"source_sentence\"]].to_csv(\"train.en\", index=False)\n",
        "stripped[[\"target_sentence\"]].to_csv(\"train.ti\", index=False)\n",
        "\n",
        "dev[[\"source_sentence\"]].to_csv(\"dev.en\", index=False)\n",
        "dev[[\"target_sentence\"]].to_csv(\"dev.ti\", index=False)\n",
        "\n",
        "test[[\"source_sentence\"]].to_csv(\"test.en\", index=False)\n",
        "test[[\"target_sentence\"]].to_csv(\"test.ti\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeszbBPNukz5",
        "colab_type": "code",
        "outputId": "fa2581d1-fdf0-4329-a2c3-bebf67a1bd1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'joeynmt'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/45)\u001b[K\rremote: Counting objects:   4% (2/45)\u001b[K\rremote: Counting objects:   6% (3/45)\u001b[K\rremote: Counting objects:   8% (4/45)\u001b[K\rremote: Counting objects:  11% (5/45)\u001b[K\rremote: Counting objects:  13% (6/45)\u001b[K\rremote: Counting objects:  15% (7/45)\u001b[K\rremote: Counting objects:  17% (8/45)\u001b[K\rremote: Counting objects:  20% (9/45)\u001b[K\rremote: Counting objects:  22% (10/45)\u001b[K\rremote: Counting objects:  24% (11/45)\u001b[K\rremote: Counting objects:  26% (12/45)\u001b[K\rremote: Counting objects:  28% (13/45)\u001b[K\rremote: Counting objects:  31% (14/45)\u001b[K\rremote: Counting objects:  33% (15/45)\u001b[K\rremote: Counting objects:  35% (16/45)\u001b[K\rremote: Counting objects:  37% (17/45)\u001b[K\rremote: Counting objects:  40% (18/45)\u001b[K\rremote: Counting objects:  42% (19/45)\u001b[K\rremote: Counting objects:  44% (20/45)\u001b[K\rremote: Counting objects:  46% (21/45)\u001b[K\rremote: Counting objects:  48% (22/45)\u001b[K\rremote: Counting objects:  51% (23/45)\u001b[K\rremote: Counting objects:  53% (24/45)\u001b[K\rremote: Counting objects:  55% (25/45)\u001b[K\rremote: Counting objects:  57% (26/45)\u001b[K\rremote: Counting objects:  60% (27/45)\u001b[K\rremote: Counting objects:  62% (28/45)\u001b[K\rremote: Counting objects:  64% (29/45)\u001b[K\rremote: Counting objects:  66% (30/45)\u001b[K\rremote: Counting objects:  68% (31/45)\u001b[K\rremote: Counting objects:  71% (32/45)\u001b[K\rremote: Counting objects:  73% (33/45)\u001b[K\rremote: Counting objects:  75% (34/45)\u001b[K\rremote: Counting objects:  77% (35/45)\u001b[K\rremote: Counting objects:  80% (36/45)\u001b[K\rremote: Counting objects:  82% (37/45)\u001b[K\rremote: Counting objects:  84% (38/45)\u001b[K\rremote: Counting objects:  86% (39/45)\u001b[K\rremote: Counting objects:  88% (40/45)\u001b[K\rremote: Counting objects:  91% (41/45)\u001b[K\rremote: Counting objects:  93% (42/45)\u001b[K\rremote: Counting objects:  95% (43/45)\u001b[K\rremote: Counting objects:  97% (44/45)\u001b[K\rremote: Counting objects: 100% (45/45)\u001b[K\rremote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 2051 (delta 25), reused 22 (delta 10), pack-reused 2006\u001b[K\n",
            "Receiving objects: 100% (2051/2051), 2.39 MiB | 16.10 MiB/s, done.\n",
            "Resolving deltas: 100% (1414/1414), done.\n",
            "Processing /content/joeynmt\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (4.3.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.16.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (41.2.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.2.0)\n",
            "Requirement already satisfied: tensorflow>=1.14 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.15.0rc3)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.3.1)\n",
            "Collecting sacrebleu>=1.3.6 (from joeynmt==0.0.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/0e/e5/93d252182f7cbd4b59bb3ec5797e2ce33cfd6f5aadaf327db170cf4b7887/sacrebleu-1.4.2-py3-none-any.whl\n",
            "Collecting subword-nmt (from joeynmt==0.0.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/26/08/58267cb3ac00f5f895457777ed9e0d106dbb5e6388fa7923d8663b04b849/subword_nmt-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (3.0.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.9.0)\n",
            "Collecting pyyaml>=5.1 (from joeynmt==0.0.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/e8/b3212641ee2718d556df0f23f78de8303f068fe29cdaa7a91018849582fe/PyYAML-5.1.2.tar.gz (265kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 9.8MB/s \n",
            "\u001b[?25hCollecting pylint (from joeynmt==0.0.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/ed/1cb8e7b85a31807aa0bff8b3e60935370bed7e141df8b530aac6352bddff/pylint-2.4.2-py3-none-any.whl (302kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 46.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->joeynmt==0.0.1) (0.46)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.7.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.11.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.1.7)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.2.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.33.6)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (2.21.0)\n",
            "Collecting typing (from sacrebleu>=1.3.6->joeynmt==0.0.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/2e/b480ee1b75e6d17d2993738670e75c1feeb9ff7f64452153cf018051cc92/typing-3.7.4.1-py3-none-any.whl\n",
            "Collecting portalocker (from sacrebleu>=1.3.6->joeynmt==0.0.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/60/ec/836a494dbaa72541f691ec4e66f29fdc8db9bcc7f49e1c2d457ba13ced42/portalocker-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.5.3)\n",
            "Requirement already satisfied: pandas>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt==0.0.1) (0.24.2)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt==0.0.1) (1.3.1)\n",
            "Collecting astroid<2.4,>=2.3.0 (from pylint->joeynmt==0.0.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/e1/74a63c85c501c29c52da5be604c025e368f4dd77daf1fa13c878a33e5a36/astroid-2.3.1-py3-none-any.whl (205kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 41.9MB/s \n",
            "\u001b[?25hCollecting isort<5,>=4.2.5 (from pylint->joeynmt==0.0.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/b0/c121fd1fa3419ea9bfd55c7f9c4fedfec5143208d8c7ad3ce3db6c623c21/isort-4.3.21-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 23.9MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7,>=0.6 (from pylint->joeynmt==0.0.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14->joeynmt==0.0.1) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14->joeynmt==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow>=1.14->joeynmt==0.0.1) (2.8.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2019.9.11)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn->joeynmt==0.0.1) (2018.9)\n",
            "Collecting lazy-object-proxy==1.4.* (from astroid<2.4,>=2.3.0->pylint->joeynmt==0.0.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/26/534a6d32572a9dbca11619321535c0a7ab34688545d9d67c2c204b9e3a3d/lazy_object_proxy-1.4.2-cp36-cp36m-manylinux1_x86_64.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 22.6MB/s \n",
            "\u001b[?25hCollecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\" (from astroid<2.4,>=2.3.0->pylint->joeynmt==0.0.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/d3/9d1802c161626d0278bafb1ffb32f76b9d01e123881bbf9d91e8ccf28e18/typed_ast-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (736kB)\n",
            "\u001b[K     |████████████████████████████████| 737kB 48.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: joeynmt, pyyaml\n",
            "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for joeynmt: filename=joeynmt-0.0.1-cp36-none-any.whl size=69430 sha256=44cf14657322cee0a283c96264e4a7ddaff77daa6da79354f4c0229e0734e8b1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ur4sp2w1/wheels/db/01/db/751cc9f3e7f6faec127c43644ba250a3ea7ad200594aeda70a\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1.2-cp36-cp36m-linux_x86_64.whl size=44104 sha256=ef35a19915c6376d9a5a9326a42beb86528888e02373c2137a0d86c0530cd96c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/45/dd/65f0b38450c47cf7e5312883deb97d065e030c5cca0a365030\n",
            "Successfully built joeynmt pyyaml\n",
            "Installing collected packages: typing, portalocker, sacrebleu, subword-nmt, pyyaml, lazy-object-proxy, typed-ast, astroid, isort, mccabe, pylint, joeynmt\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed astroid-2.3.1 isort-4.3.21 joeynmt-0.0.1 lazy-object-proxy-1.4.2 mccabe-0.6.1 portalocker-1.5.1 pylint-2.4.2 pyyaml-5.1.2 sacrebleu-1.4.2 subword-nmt-0.3.6 typed-ast-1.4.0 typing-3.7.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDWEwGnAuk3M",
        "colab_type": "code",
        "outputId": "d7dc6d8b-4023-4380-f97f-65fa579b4ef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
        "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
        "\n",
        "# Do subword NMT\n",
        "! mkdir joeynmt/data/\n",
        "! mkdir joeynmt/data/enti/\n",
        "! export data_path=joeynmt/data/$src$tgt/\n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
        "\n",
        "# Create directory, move everyone we care about to the correct location\n",
        "# Create the /data/enti/ folder explicityly\n",
        "! cp train.* joeynmt/data/enti/ \n",
        "! cp test.* joeynmt/data/enti/\n",
        "! cp dev.* joeynmt/data/enti/\n",
        "! cp bpe.codes.4000 $data_path\n",
        "! ls $data_path\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# Some output\n",
        "! echo \"BPE Tigrigna Sentences\"\n",
        "! tail -n 5 test.bpe.$tgt\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! tail -n 10 joeynmt/data/enti/vocab.txt"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: missing destination file operand after 'bpe.codes.4000'\n",
            "Try 'cp --help' for more information.\n",
            "bpe.codes.4000\tdev.ti\t   sample_data\ttest.ti       train.ti\n",
            "dev.bpe.en\tdrive\t   test.bpe.en\ttrain.bpe.en  vocab.en\n",
            "dev.bpe.ti\ten_ti.csv  test.bpe.ti\ttrain.bpe.ti  vocab.ti\n",
            "dev.en\t\tjoeynmt    test.en\ttrain.en\n",
            "BPE Tigrigna Sentences\n",
            "\"25 ደቂ ጋ@@ ድን ደቂ ሮ@@ ቤ@@ ልን ከኣ ንሙሴ ኸምዚ በልዎ ፦ “ ገላዉ@@ ኻ ኸም@@ ቲ ጐይታይ ዝኣዘዞ ኺ@@ ገብሩ እዮም . 26 ቈ@@ ልዑ@@ ናን ኣንስ@@ ት@@ ናን ማል@@ ናን ኵሉ እንስ@@ ሳታት ዘ@@ ቤ@@ ት@@ ናን ኣብኡ ኣብ ከተማታት ጊልዓድ ይኹ@@ ኑ , 27 ገላዉ@@ ኻ ግና ከምቲ ጐይታይ ዝ@@ በሎ ንው@@ ግእ እተ@@ ዓ@@ ጥ@@ ቁ ዘ@@ በሉ ዅሎም ኣብ ቅድሚ የሆዋ ኺ@@ ዋግ@@ ኡ ኺ@@ ሳገ@@ ሩ እዮም . ”\"\n",
            "28 ሙሴ ድማ ን@@ ኻህን ኣል@@ ኣ@@ ዛ@@ ርን ንእ@@ ያሱ ወዲ ነ@@ ዌ@@ ን ን@@ ርእስ@@ ታት ማይ ቤ@@ ታት ነገ@@ ዳት እስራኤልን ብዛዕባ@@ ኦም ኣዘ@@ ዘ . 29 ሙሴ ኸኣ ከምዚ በሎም ፦ “ ደቂ ጋ@@ ድን ደቂ ሮ@@ ቤ@@ ልን ኵላቶም ኣብ ቅድሚ የሆዋ ንው@@ ግእ ተዓ@@ ጢ@@ ቖም ምሳኻትኩም ዮርዳኖስ እንተ ተ@@ ሳ@@ ጊ@@ ሮም እታ ምድሪ ድማ ኣብ ቅድሜ@@ ኹም እንተ ተገ@@ ዚ@@ ኣ ሽዑ ምድሪ ጊልዓድ ርስቲ ጌርኩም ሃብ@@ ዎም . 30 ኣ@@ ጽዋ@@ ር ኣልዒ@@ ሎም ምሳኻትኩም እንተ ዘይ@@ ተ@@ ሳ@@ ጊ@@ ሮም ግና ኣብ ምድሪ ከነ@@ ኣን ኣብ መንጎ@@ ኹም ይስ@@ ፈ@@ ሩ . ”\n",
            "\"31 ደቂ ጋ@@ ድን ደቂ ሮ@@ ቤ@@ ልን ድማ ከምዚ ኢሎም መለ@@ ሱ ፦ “ ከምቲ የሆዋ ን@@ ገላዉ@@ ኻ ዝ@@ በሎም ከምኡ ኽን@@ ገብር ኢና . 32 ኣ@@ ጽዋ@@ ር ኣልዒ@@ ልና ኣብ ቅድሚ የሆዋ ናብ ምድሪ ከነ@@ ኣን ክ@@ ንሳ@@ ገ@@ ር ኢና , እቲ እን@@ ወር@@ ሶ ርስቲ ግና ኣብዚ ኣብ ክ@@ ነ@@ ጀ@@ ው ሸነኽ ዮርዳኖስ ይኹ@@ ነ@@ ልና . ” 33 ሙሴ ኸኣ ንዓታቶም —@@ ንደቂ ጋ@@ ድን ንደቂ ሮ@@ ቤ@@ ልን ን@@ ፍርቂ ነገድ ምናሴ ወዲ ዮሴ@@ ፍን —@@ መንግስቲ ሲ@@ ሆ@@ ን ንጉስ ኣሞ@@ ራ@@ ውያን መንግስቲ ዖ@@ ግ ንጉስ ባ@@ ሳን ኣብታ ሃገ@@ ር እቲኣ ዘላ ምድሪ እተን ከተማታት ኣብ ዙርያ እታ ምድሪ ዘለዋ ኸተማታት ሃቦም .\"\n",
            "\"34 ደቂ ጋ@@ ድ ድማ ዲ@@ ቦ@@ ንን ዓ@@ ጣ@@ ሮ@@ ትን ዓ@@ ሮ@@ ዔ@@ ርን 35 ዓ@@ ጥ@@ ሮ@@ ት-@@ ሾ@@ ፋ@@ ንን ያ@@ ዕ@@ ዜ@@ ርን ዮ@@ ግበ@@ ሃ@@ ን 36 ቤት-@@ ኒ@@ ም@@ ራን ቤት-@@ ሃ@@ ራ@@ ንን ዚ@@ ብሃ@@ ላ እተ@@ ዓር@@ ዳ ኸተማ@@ ታትን ን@@ መጓሰ@@ ኦም ዚኸውን ደ@@ ም@@ በ@@ ታትን ሰር@@ ሑ . 37 ደቂ ሮ@@ ቤል ከኣ ሔ@@ ስ@@ ቦ@@ ንን ኤል@@ ዓ@@ ለን ቂ@@ ርያ@@ ታ@@ ዪ@@ ምን 38 ኔ@@ ቦን በዓ@@ ል@@ -@@ ሜ@@ ዖ@@ ንን —@@ ስ@@ ማ@@ ተን እተ@@ ለ@@ ወ@@ ጠ —@@ ሲ@@ ብ@@ ማን ሰር@@ ሑ , ነተን ዳ@@ ግም ዝ@@ ሰር@@ ሕ@@ ወን ከተማታት ድማ ስም ለ@@ ወ@@ ጡ@@ ለን .\"\n",
            "\"3@@ 9 ደቂ ማ@@ ኪ@@ ር ወዲ ምናሴ ኸኣ ናብ ጊልዓድ ከይ@@ ዶም ሓዝ@@ ዋ , ነቶም ኣብኣ ዝነበሩ ኣሞ@@ ራ@@ ውያን ድማ ሰ@@ ጐ@@ ጕ@@ ዎም . 40 ሙሴ ኸኣ ን@@ ማ@@ ኪ@@ ር ወዲ ምናሴ ጊልዓድ ሃቦ ንሱ ድማ ኣብኣ ተቐመጠ . 41 ያ@@ ኢ@@ ር ወዲ ምናሴ ኸኣ ከይ@@ ዱ ዓድ@@ ታት ድንኳ@@ ና@@ ቶም ሓ@@ ዘ , ሓ@@ ዎ@@ ት-@@ ያ@@ ኢ@@ ር ኢሉ ድማ ሰ@@ መ@@ የን . 42 ኖ@@ ባ ኸኣ ከይ@@ ዱ ን@@ ቄ@@ ናትን ዓድ@@ ታታን ሓ@@ ዘ , ብስ@@ ሙ ድማ ኖ@@ ባ ኢሉ ሰ@@ መ@@ ያ .\"\n",
            "Combined BPE Vocab\n",
            "ቈ\n",
            "neb·u·@@\n",
            "beca@@\n",
            "calam@@\n",
            "jud@@\n",
            "wilder@@\n",
            "joh@@\n",
            "phi·lisʹtin@@\n",
            "ቊ\n",
            "ጂ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIRhhPqHIMpt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a60edf09-8d51-4f8e-938c-b8c12717d3bd"
      },
      "source": [
        "\n",
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.en\t     test.bpe.ti  train.bpe.en\ttrain.ti\n",
            "dev.bpe.en\tdev.ti\t     test.en\t  train.bpe.ti\n",
            "dev.bpe.ti\ttest.bpe.en  test.ti\t  train.en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qW0fXlfuk7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/train.bpe\"\n",
        "    dev:   \"data/{name}/dev.bpe\"\n",
        "    test:  \"data/{name}/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"models/{name}_transformer/12000.ckpt\" # if given, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"noam\"            # Try switching from plateau to Noam scheduling\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    patience: 8\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0002\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30 # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 400 # Decrease this for testing\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: True\n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 8\n",
        "        embeddings:\n",
        "            embedding_dim: 512\n",
        "            scale: True\n",
        "            dropout: 0.\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 512\n",
        "        ff_size: 2048\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 8\n",
        "        embeddings:\n",
        "            embedding_dim: 512\n",
        "            scale: True\n",
        "            dropout: 0.\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 512\n",
        "        ff_size: 2048\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dFwTRna7UXb",
        "colab_type": "code",
        "outputId": "c4730877-f356-4c61-b685-5a7f2b5f0ba6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-13 12:52:42,472 Hello! This is Joey-NMT.\n",
            "2019-10-13 12:52:44,000 Total params: 46427648\n",
            "2019-10-13 12:52:44,001 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']\n",
            "2019-10-13 12:52:49,313 cfg.name                           : enti_transformer\n",
            "2019-10-13 12:52:49,314 cfg.data.src                       : en\n",
            "2019-10-13 12:52:49,314 cfg.data.trg                       : ti\n",
            "2019-10-13 12:52:49,314 cfg.data.train                     : data/enti/train.bpe\n",
            "2019-10-13 12:52:49,314 cfg.data.dev                       : data/enti/dev.bpe\n",
            "2019-10-13 12:52:49,314 cfg.data.test                      : data/enti/test.bpe\n",
            "2019-10-13 12:52:49,314 cfg.data.level                     : bpe\n",
            "2019-10-13 12:52:49,315 cfg.data.lowercase                 : False\n",
            "2019-10-13 12:52:49,315 cfg.data.max_sent_length           : 100\n",
            "2019-10-13 12:52:49,315 cfg.data.src_vocab                 : data/enti/vocab.txt\n",
            "2019-10-13 12:52:49,315 cfg.data.trg_vocab                 : data/enti/vocab.txt\n",
            "2019-10-13 12:52:49,315 cfg.testing.beam_size              : 5\n",
            "2019-10-13 12:52:49,316 cfg.testing.alpha                  : 1.0\n",
            "2019-10-13 12:52:49,316 cfg.training.random_seed           : 42\n",
            "2019-10-13 12:52:49,316 cfg.training.optimizer             : adam\n",
            "2019-10-13 12:52:49,316 cfg.training.normalization         : tokens\n",
            "2019-10-13 12:52:49,316 cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2019-10-13 12:52:49,316 cfg.training.scheduling            : noam\n",
            "2019-10-13 12:52:49,316 cfg.training.learning_rate_factor  : 0.5\n",
            "2019-10-13 12:52:49,317 cfg.training.learning_rate_warmup  : 1000\n",
            "2019-10-13 12:52:49,317 cfg.training.patience              : 8\n",
            "2019-10-13 12:52:49,317 cfg.training.decrease_factor       : 0.7\n",
            "2019-10-13 12:52:49,317 cfg.training.loss                  : crossentropy\n",
            "2019-10-13 12:52:49,317 cfg.training.learning_rate         : 0.0002\n",
            "2019-10-13 12:52:49,317 cfg.training.learning_rate_min     : 1e-08\n",
            "2019-10-13 12:52:49,317 cfg.training.weight_decay          : 0.0\n",
            "2019-10-13 12:52:49,318 cfg.training.label_smoothing       : 0.1\n",
            "2019-10-13 12:52:49,318 cfg.training.batch_size            : 4096\n",
            "2019-10-13 12:52:49,318 cfg.training.batch_type            : token\n",
            "2019-10-13 12:52:49,318 cfg.training.eval_batch_size       : 3600\n",
            "2019-10-13 12:52:49,318 cfg.training.eval_batch_type       : token\n",
            "2019-10-13 12:52:49,318 cfg.training.batch_multiplier      : 1\n",
            "2019-10-13 12:52:49,318 cfg.training.early_stopping_metric : ppl\n",
            "2019-10-13 12:52:49,319 cfg.training.epochs                : 30\n",
            "2019-10-13 12:52:49,319 cfg.training.validation_freq       : 400\n",
            "2019-10-13 12:52:49,319 cfg.training.logging_freq          : 100\n",
            "2019-10-13 12:52:49,319 cfg.training.eval_metric           : bleu\n",
            "2019-10-13 12:52:49,319 cfg.training.model_dir             : models/enti_transformer\n",
            "2019-10-13 12:52:49,319 cfg.training.overwrite             : True\n",
            "2019-10-13 12:52:49,320 cfg.training.shuffle               : True\n",
            "2019-10-13 12:52:49,320 cfg.training.use_cuda              : True\n",
            "2019-10-13 12:52:49,320 cfg.training.max_output_length     : 100\n",
            "2019-10-13 12:52:49,320 cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2019-10-13 12:52:49,320 cfg.training.keep_last_ckpts       : 3\n",
            "2019-10-13 12:52:49,320 cfg.model.initializer              : xavier\n",
            "2019-10-13 12:52:49,320 cfg.model.bias_initializer         : zeros\n",
            "2019-10-13 12:52:49,320 cfg.model.init_gain                : 1.0\n",
            "2019-10-13 12:52:49,321 cfg.model.embed_initializer        : xavier\n",
            "2019-10-13 12:52:49,321 cfg.model.embed_init_gain          : 1.0\n",
            "2019-10-13 12:52:49,321 cfg.model.tied_embeddings          : True\n",
            "2019-10-13 12:52:49,321 cfg.model.tied_softmax             : True\n",
            "2019-10-13 12:52:49,321 cfg.model.encoder.type             : transformer\n",
            "2019-10-13 12:52:49,321 cfg.model.encoder.num_layers       : 6\n",
            "2019-10-13 12:52:49,321 cfg.model.encoder.num_heads        : 8\n",
            "2019-10-13 12:52:49,321 cfg.model.encoder.embeddings.embedding_dim : 512\n",
            "2019-10-13 12:52:49,322 cfg.model.encoder.embeddings.scale : True\n",
            "2019-10-13 12:52:49,322 cfg.model.encoder.embeddings.dropout : 0.0\n",
            "2019-10-13 12:52:49,322 cfg.model.encoder.hidden_size      : 512\n",
            "2019-10-13 12:52:49,322 cfg.model.encoder.ff_size          : 2048\n",
            "2019-10-13 12:52:49,322 cfg.model.encoder.dropout          : 0.3\n",
            "2019-10-13 12:52:49,322 cfg.model.decoder.type             : transformer\n",
            "2019-10-13 12:52:49,322 cfg.model.decoder.num_layers       : 6\n",
            "2019-10-13 12:52:49,322 cfg.model.decoder.num_heads        : 8\n",
            "2019-10-13 12:52:49,323 cfg.model.decoder.embeddings.embedding_dim : 512\n",
            "2019-10-13 12:52:49,323 cfg.model.decoder.embeddings.scale : True\n",
            "2019-10-13 12:52:49,323 cfg.model.decoder.embeddings.dropout : 0.0\n",
            "2019-10-13 12:52:49,323 cfg.model.decoder.hidden_size      : 512\n",
            "2019-10-13 12:52:49,323 cfg.model.decoder.ff_size          : 2048\n",
            "2019-10-13 12:52:49,323 cfg.model.decoder.dropout          : 0.3\n",
            "2019-10-13 12:52:49,323 Data set sizes: \n",
            "\ttrain 20876,\n",
            "\tvalid 1001,\n",
            "\ttest 1001\n",
            "2019-10-13 12:52:49,323 First training example:\n",
            "\t[SRC] s@@ our@@ ce@@ _@@ sen@@ ten@@ ce\n",
            "\t[TRG] t@@ ar@@ ge@@ t@@ _@@ sen@@ ten@@ ce\n",
            "2019-10-13 12:52:49,324 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) the (6) , (7) and (8) of (9) to\n",
            "2019-10-13 12:52:49,324 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) the (6) , (7) and (8) of (9) to\n",
            "2019-10-13 12:52:49,324 Number of Src words (types): 4467\n",
            "2019-10-13 12:52:49,325 Number of Trg words (types): 4467\n",
            "2019-10-13 12:52:49,325 Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=8),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=8),\n",
            "\tsrc_embed=Embeddings(embedding_dim=512, vocab_size=4467),\n",
            "\ttrg_embed=Embeddings(embedding_dim=512, vocab_size=4467))\n",
            "2019-10-13 12:52:49,330 EPOCH 1\n",
            "2019-10-13 12:53:59,400 Epoch   1 Step:      100 Batch Loss:     5.546418 Tokens per Sec:     3426, Lr: 0.000070\n",
            "2019-10-13 12:54:31,807 Epoch   1: total training loss 906.46\n",
            "2019-10-13 12:54:31,808 EPOCH 2\n",
            "2019-10-13 12:55:08,845 Epoch   2 Step:      200 Batch Loss:     5.165424 Tokens per Sec:     3344, Lr: 0.000140\n",
            "2019-10-13 12:56:15,461 Epoch   2: total training loss 791.14\n",
            "2019-10-13 12:56:15,462 EPOCH 3\n",
            "2019-10-13 12:56:19,135 Epoch   3 Step:      300 Batch Loss:     5.646509 Tokens per Sec:     3397, Lr: 0.000210\n",
            "2019-10-13 12:57:29,342 Epoch   3 Step:      400 Batch Loss:     5.352345 Tokens per Sec:     3536, Lr: 0.000280\n",
            "2019-10-13 13:01:52,390 Hooray! New best validation result [ppl]!\n",
            "2019-10-13 13:01:52,390 Saving new checkpoint.\n",
            "2019-10-13 13:01:54,161 Example #0\n",
            "2019-10-13 13:01:54,162 \tSource:     source_sentence\n",
            "2019-10-13 13:01:54,162 \tReference:  target_sentence\n",
            "2019-10-13 13:01:54,162 \tHypothesis: \"ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ\n",
            "2019-10-13 13:01:54,162 Example #1\n",
            "2019-10-13 13:01:54,162 \tSource:     partiality in judgment is not good .\n",
            "2019-10-13 13:01:54,162 \tReference:  ኣብ ፍርዲ ኣድልዎ ምግባር ጽቡቕ ኣይኰነን .\n",
            "2019-10-13 13:01:54,163 \tHypothesis: ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ\n",
            "2019-10-13 13:01:54,163 Example #2\n",
            "2019-10-13 13:01:54,163 \tSource:     \"24 whoever says to the wicked one , “ you are righteous , ”\"\n",
            "2019-10-13 13:01:54,163 \tReference:  24 እቲ ንእኩይ “ ጻድቕ ኢኻ ” ዚብሎ\n",
            "2019-10-13 13:01:54,163 \tHypothesis: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "2019-10-13 13:01:54,163 Example #3\n",
            "2019-10-13 13:01:54,163 \tSource:     will be cursed by the peoples and denounced by the nations .\n",
            "2019-10-13 13:01:54,163 \tReference:  ብህዝብታት ኪርገም ብኣህዛብ እውን ኪዅነን እዩ .\n",
            "2019-10-13 13:01:54,164 \tHypothesis: ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ ኣብ\n",
            "2019-10-13 13:01:54,164 Validation result at epoch   3, step      400: bleu:   0.00, loss: 183767.5156, ppl: 183.8284, duration: 264.8208s\n",
            "2019-10-13 13:02:24,068 Epoch   3: total training loss 733.81\n",
            "2019-10-13 13:02:24,068 EPOCH 4\n",
            "2019-10-13 13:03:04,312 Epoch   4 Step:      500 Batch Loss:     4.463258 Tokens per Sec:     3364, Lr: 0.000349\n",
            "2019-10-13 13:04:07,626 Epoch   4: total training loss 681.52\n",
            "2019-10-13 13:04:07,626 EPOCH 5\n",
            "2019-10-13 13:04:14,390 Epoch   5 Step:      600 Batch Loss:     4.156932 Tokens per Sec:     3149, Lr: 0.000419\n",
            "2019-10-13 13:05:24,993 Epoch   5 Step:      700 Batch Loss:     4.382993 Tokens per Sec:     3664, Lr: 0.000489\n",
            "2019-10-13 13:05:51,178 Epoch   5: total training loss 647.69\n",
            "2019-10-13 13:05:51,178 EPOCH 6\n",
            "2019-10-13 13:06:35,274 Epoch   6 Step:      800 Batch Loss:     4.169038 Tokens per Sec:     3330, Lr: 0.000559\n",
            "2019-10-13 13:10:55,876 Hooray! New best validation result [ppl]!\n",
            "2019-10-13 13:10:55,877 Saving new checkpoint.\n",
            "2019-10-13 13:10:57,941 Example #0\n",
            "2019-10-13 13:10:57,941 \tSource:     source_sentence\n",
            "2019-10-13 13:10:57,941 \tReference:  target_sentence\n",
            "2019-10-13 13:10:57,941 \tHypothesis: \"ካብ ምድሪ ምድሪ ምድሪ ይውሉ ,\"\n",
            "2019-10-13 13:10:57,941 Example #1\n",
            "2019-10-13 13:10:57,942 \tSource:     partiality in judgment is not good .\n",
            "2019-10-13 13:10:57,942 \tReference:  ኣብ ፍርዲ ኣድልዎ ምግባር ጽቡቕ ኣይኰነን .\n",
            "2019-10-13 13:10:57,942 \tHypothesis: ንጻድቕ ኣይሕግግል .\n",
            "2019-10-13 13:10:57,942 Example #2\n",
            "2019-10-13 13:10:57,942 \tSource:     \"24 whoever says to the wicked one , “ you are righteous , ”\"\n",
            "2019-10-13 13:10:57,942 \tReference:  24 እቲ ንእኩይ “ ጻድቕ ኢኻ ” ዚብሎ\n",
            "2019-10-13 13:10:57,942 \tHypothesis: \"2 “ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘\n",
            "2019-10-13 13:10:57,942 Example #3\n",
            "2019-10-13 13:10:57,943 \tSource:     will be cursed by the peoples and denounced by the nations .\n",
            "2019-10-13 13:10:57,943 \tReference:  ብህዝብታት ኪርገም ብኣህዛብ እውን ኪዅነን እዩ .\n",
            "2019-10-13 13:10:57,943 \tHypothesis: ኣብ መንጎ ምድሪ ድማ ኪርፍፍርን እዩ .\n",
            "2019-10-13 13:10:57,943 Validation result at epoch   6, step      800: bleu:   1.19, loss: 166880.5312, ppl: 113.8488, duration: 262.6687s\n",
            "2019-10-13 13:11:58,014 Epoch   6: total training loss 606.55\n",
            "2019-10-13 13:11:58,015 EPOCH 7\n",
            "2019-10-13 13:12:07,967 Epoch   7 Step:      900 Batch Loss:     4.583918 Tokens per Sec:     3441, Lr: 0.000629\n",
            "2019-10-13 13:13:18,243 Epoch   7 Step:     1000 Batch Loss:     3.919228 Tokens per Sec:     3848, Lr: 0.000699\n",
            "2019-10-13 13:13:41,219 Epoch   7: total training loss 556.23\n",
            "2019-10-13 13:13:41,219 EPOCH 8\n",
            "2019-10-13 13:14:28,433 Epoch   8 Step:     1100 Batch Loss:     2.999708 Tokens per Sec:     3384, Lr: 0.000666\n",
            "2019-10-13 13:15:24,503 Epoch   8: total training loss 510.97\n",
            "2019-10-13 13:15:24,503 EPOCH 9\n",
            "2019-10-13 13:15:38,051 Epoch   9 Step:     1200 Batch Loss:     3.414409 Tokens per Sec:     3302, Lr: 0.000638\n",
            "2019-10-13 13:19:58,656 Hooray! New best validation result [ppl]!\n",
            "2019-10-13 13:19:58,657 Saving new checkpoint.\n",
            "2019-10-13 13:20:00,203 Example #0\n",
            "2019-10-13 13:20:00,203 \tSource:     source_sentence\n",
            "2019-10-13 13:20:00,203 \tReference:  target_sentence\n",
            "2019-10-13 13:20:00,204 \tHypothesis: \"ካብ ኣኽራን ኣኽራን ይመጽእ ,\"\n",
            "2019-10-13 13:20:00,204 Example #1\n",
            "2019-10-13 13:20:00,204 \tSource:     partiality in judgment is not good .\n",
            "2019-10-13 13:20:00,204 \tReference:  ኣብ ፍርዲ ኣድልዎ ምግባር ጽቡቕ ኣይኰነን .\n",
            "2019-10-13 13:20:00,204 \tHypothesis: ብርሃን እኳ ኣይተሓጐስ .\n",
            "2019-10-13 13:20:00,204 Example #2\n",
            "2019-10-13 13:20:00,204 \tSource:     \"24 whoever says to the wicked one , “ you are righteous , ”\"\n",
            "2019-10-13 13:20:00,205 \tReference:  24 እቲ ንእኩይ “ ጻድቕ ኢኻ ” ዚብሎ\n",
            "2019-10-13 13:20:00,205 \tHypothesis: 24 “ እኩይ እኩይ ግና “ ንስኻ ግና ” ይብል የሆዋ .\n",
            "2019-10-13 13:20:00,205 Example #3\n",
            "2019-10-13 13:20:00,205 \tSource:     will be cursed by the peoples and denounced by the nations .\n",
            "2019-10-13 13:20:00,205 \tReference:  ብህዝብታት ኪርገም ብኣህዛብ እውን ኪዅነን እዩ .\n",
            "2019-10-13 13:20:00,205 \tHypothesis: ንህዝብታት ኪወድቁ እዮም .\n",
            "2019-10-13 13:20:00,205 Validation result at epoch   9, step     1200: bleu:   2.58, loss: 149437.2031, ppl:  69.4047, duration: 262.1541s\n",
            "2019-10-13 13:21:10,460 Epoch   9 Step:     1300 Batch Loss:     3.057364 Tokens per Sec:     4000, Lr: 0.000613\n",
            "2019-10-13 13:21:29,983 Epoch   9: total training loss 481.50\n",
            "2019-10-13 13:21:29,984 EPOCH 10\n",
            "2019-10-13 13:22:20,497 Epoch  10 Step:     1400 Batch Loss:     2.765612 Tokens per Sec:     3354, Lr: 0.000591\n",
            "2019-10-13 13:23:13,516 Epoch  10: total training loss 451.33\n",
            "2019-10-13 13:23:13,516 EPOCH 11\n",
            "2019-10-13 13:23:30,071 Epoch  11 Step:     1500 Batch Loss:     2.873803 Tokens per Sec:     3319, Lr: 0.000571\n",
            "2019-10-13 13:24:40,016 Epoch  11 Step:     1600 Batch Loss:     3.851236 Tokens per Sec:     4143, Lr: 0.000552\n",
            "2019-10-13 13:29:00,345 Hooray! New best validation result [ppl]!\n",
            "2019-10-13 13:29:00,345 Saving new checkpoint.\n",
            "2019-10-13 13:29:02,029 Example #0\n",
            "2019-10-13 13:29:02,029 \tSource:     source_sentence\n",
            "2019-10-13 13:29:02,029 \tReference:  target_sentence\n",
            "2019-10-13 13:29:02,029 \tHypothesis: \"ኣንጻር ኤዶም ኤሊባኖስ እዩ ,\"\n",
            "2019-10-13 13:29:02,029 Example #1\n",
            "2019-10-13 13:29:02,030 \tSource:     partiality in judgment is not good .\n",
            "2019-10-13 13:29:02,030 \tReference:  ኣብ ፍርዲ ኣድልዎ ምግባር ጽቡቕ ኣይኰነን .\n",
            "2019-10-13 13:29:02,030 \tHypothesis: ጽቡቕ ዚገብር ዘበለ ዅሉ ይረክብ .\n",
            "2019-10-13 13:29:02,030 Example #2\n",
            "2019-10-13 13:29:02,030 \tSource:     \"24 whoever says to the wicked one , “ you are righteous , ”\"\n",
            "2019-10-13 13:29:02,030 \tReference:  24 እቲ ንእኩይ “ ጻድቕ ኢኻ ” ዚብሎ\n",
            "2019-10-13 13:29:02,030 \tHypothesis: \"24 ንጻድቕ ዚዛረብ ዘበለ ግና “ ንስኻ ግና ንዘለኣለም ክትገብር ኢኻ ” ትብሉ ,\"\n",
            "2019-10-13 13:29:02,030 Example #3\n",
            "2019-10-13 13:29:02,030 \tSource:     will be cursed by the peoples and denounced by the nations .\n",
            "2019-10-13 13:29:02,031 \tReference:  ብህዝብታት ኪርገም ብኣህዛብ እውን ኪዅነን እዩ .\n",
            "2019-10-13 13:29:02,031 \tHypothesis: ንህዝብታት ድማ ብህዝብታት ኪሕፍር እዩ .\n",
            "2019-10-13 13:29:02,031 Validation result at epoch  11, step     1600: bleu:   3.86, loss: 140869.5469, ppl:  54.4273, duration: 262.0141s\n",
            "2019-10-13 13:29:19,029 Epoch  11: total training loss 431.72\n",
            "2019-10-13 13:29:19,029 EPOCH 12\n",
            "2019-10-13 13:30:12,059 Epoch  12 Step:     1700 Batch Loss:     2.416775 Tokens per Sec:     3362, Lr: 0.000536\n",
            "2019-10-13 13:31:02,035 Epoch  12: total training loss 399.00\n",
            "2019-10-13 13:31:02,036 EPOCH 13\n",
            "2019-10-13 13:31:22,738 Epoch  13 Step:     1800 Batch Loss:     2.643572 Tokens per Sec:     3361, Lr: 0.000521\n",
            "2019-10-13 13:32:31,845 Epoch  13 Step:     1900 Batch Loss:     2.346617 Tokens per Sec:     4342, Lr: 0.000507\n",
            "2019-10-13 13:32:45,615 Epoch  13: total training loss 379.29\n",
            "2019-10-13 13:32:45,616 EPOCH 14\n",
            "2019-10-13 13:33:42,174 Epoch  14 Step:     2000 Batch Loss:     2.686607 Tokens per Sec:     3384, Lr: 0.000494\n",
            "2019-10-13 13:38:02,609 Hooray! New best validation result [ppl]!\n",
            "2019-10-13 13:38:02,609 Saving new checkpoint.\n",
            "2019-10-13 13:38:04,299 Example #0\n",
            "2019-10-13 13:38:04,299 \tSource:     source_sentence\n",
            "2019-10-13 13:38:04,299 \tReference:  target_sentence\n",
            "2019-10-13 13:38:04,299 \tHypothesis: ኣንጻር ነገድ ዮሴፍ\n",
            "2019-10-13 13:38:04,299 Example #1\n",
            "2019-10-13 13:38:04,300 \tSource:     partiality in judgment is not good .\n",
            "2019-10-13 13:38:04,300 \tReference:  ኣብ ፍርዲ ኣድልዎ ምግባር ጽቡቕ ኣይኰነን .\n",
            "2019-10-13 13:38:04,300 \tHypothesis: ጽቡቕ ዘይኰነስ ጽቡቕ ኣይትተሓጐስ .\n",
            "2019-10-13 13:38:04,300 Example #2\n",
            "2019-10-13 13:38:04,300 \tSource:     \"24 whoever says to the wicked one , “ you are righteous , ”\"\n",
            "2019-10-13 13:38:04,300 \tReference:  24 እቲ ንእኩይ “ ጻድቕ ኢኻ ” ዚብሎ\n",
            "2019-10-13 13:38:04,300 \tHypothesis: 24 “ እኩይ ዚምህርኻ ጻድቕ ” ይብል\n",
            "2019-10-13 13:38:04,301 Example #3\n",
            "2019-10-13 13:38:04,301 \tSource:     will be cursed by the peoples and denounced by the nations .\n",
            "2019-10-13 13:38:04,301 \tReference:  ብህዝብታት ኪርገም ብኣህዛብ እውን ኪዅነን እዩ .\n",
            "2019-10-13 13:38:04,301 \tHypothesis: ንደቅኺ ድማ ኣብ መንጎ ህዝብታት ኪዕምሽ እዩ .\n",
            "2019-10-13 13:38:04,301 Validation result at epoch  14, step     2000: bleu:   5.12, loss: 136958.4062, ppl:  48.7106, duration: 262.1262s\n",
            "2019-10-13 13:38:50,805 Epoch  14: total training loss 359.51\n",
            "2019-10-13 13:38:50,806 EPOCH 15\n",
            "2019-10-13 13:39:13,929 Epoch  15 Step:     2100 Batch Loss:     2.060032 Tokens per Sec:     3392, Lr: 0.000482\n",
            "2019-10-13 13:40:24,243 Epoch  15 Step:     2200 Batch Loss:     1.972575 Tokens per Sec:     4469, Lr: 0.000471\n",
            "2019-10-13 13:40:34,312 Epoch  15: total training loss 339.93\n",
            "2019-10-13 13:40:34,312 EPOCH 16\n",
            "2019-10-13 13:41:34,420 Epoch  16 Step:     2300 Batch Loss:     2.152646 Tokens per Sec:     3357, Lr: 0.000461\n",
            "2019-10-13 13:42:17,835 Epoch  16: total training loss 317.33\n",
            "2019-10-13 13:42:17,836 EPOCH 17\n",
            "2019-10-13 13:42:44,611 Epoch  17 Step:     2400 Batch Loss:     1.946952 Tokens per Sec:     3377, Lr: 0.000451\n",
            "2019-10-13 13:47:05,146 Hooray! New best validation result [ppl]!\n",
            "2019-10-13 13:47:05,146 Saving new checkpoint.\n",
            "2019-10-13 13:47:06,819 Example #0\n",
            "2019-10-13 13:47:06,819 \tSource:     source_sentence\n",
            "2019-10-13 13:47:06,819 \tReference:  target_sentence\n",
            "2019-10-13 13:47:06,819 \tHypothesis: t@@\n",
            "2019-10-13 13:47:06,819 Example #1\n",
            "2019-10-13 13:47:06,820 \tSource:     partiality in judgment is not good .\n",
            "2019-10-13 13:47:06,820 \tReference:  ኣብ ፍርዲ ኣድልዎ ምግባር ጽቡቕ ኣይኰነን .\n",
            "2019-10-13 13:47:06,820 \tHypothesis: ብጽቡቕ ዘይኰነስ ብጽቡቕ ኣይኰነን .\n",
            "2019-10-13 13:47:06,820 Example #2\n",
            "2019-10-13 13:47:06,820 \tSource:     \"24 whoever says to the wicked one , “ you are righteous , ”\"\n",
            "2019-10-13 13:47:06,820 \tReference:  24 እቲ ንእኩይ “ ጻድቕ ኢኻ ” ዚብሎ\n",
            "2019-10-13 13:47:06,820 \tHypothesis: 24 ንእኩይ “ ንስኻ ጻድቕ ኢኻ ” እትብሉ\n",
            "2019-10-13 13:47:06,820 Example #3\n",
            "2019-10-13 13:47:06,821 \tSource:     will be cursed by the peoples and denounced by the nations .\n",
            "2019-10-13 13:47:06,821 \tReference:  ብህዝብታት ኪርገም ብኣህዛብ እውን ኪዅነን እዩ .\n",
            "2019-10-13 13:47:06,821 \tHypothesis: ንደቆም ድማ ኣብ ህዝብታት ኪዕምሽ እዩ .\n",
            "2019-10-13 13:47:06,821 Validation result at epoch  17, step     2400: bleu:   5.49, loss: 136141.5625, ppl:  47.5946, duration: 262.2092s\n",
            "2019-10-13 13:48:17,646 Epoch  17 Step:     2500 Batch Loss:     1.931521 Tokens per Sec:     4636, Lr: 0.000442\n",
            "2019-10-13 13:48:23,525 Epoch  17: total training loss 298.01\n",
            "2019-10-13 13:48:23,526 EPOCH 18\n",
            "2019-10-13 13:49:27,684 Epoch  18 Step:     2600 Batch Loss:     1.968414 Tokens per Sec:     3344, Lr: 0.000433\n",
            "2019-10-13 13:50:07,006 Epoch  18: total training loss 281.54\n",
            "2019-10-13 13:50:07,007 EPOCH 19\n",
            "2019-10-13 13:50:38,037 Epoch  19 Step:     2700 Batch Loss:     2.532259 Tokens per Sec:     3388, Lr: 0.000425\n",
            "2019-10-13 13:51:47,788 Epoch  19 Step:     2800 Batch Loss:     1.665744 Tokens per Sec:     4860, Lr: 0.000418\n",
            "2019-10-13 13:56:08,167 Example #0\n",
            "2019-10-13 13:56:08,167 \tSource:     source_sentence\n",
            "2019-10-13 13:56:08,167 \tReference:  target_sentence\n",
            "2019-10-13 13:56:08,167 \tHypothesis: ttttt@@\n",
            "2019-10-13 13:56:08,167 Example #1\n",
            "2019-10-13 13:56:08,168 \tSource:     partiality in judgment is not good .\n",
            "2019-10-13 13:56:08,168 \tReference:  ኣብ ፍርዲ ኣድልዎ ምግባር ጽቡቕ ኣይኰነን .\n",
            "2019-10-13 13:56:08,168 \tHypothesis: ቅኑዕ ዘበለ ኣብ ቅድሚኡ ኣይቅኑዕን እዩ .\n",
            "2019-10-13 13:56:08,168 Example #2\n",
            "2019-10-13 13:56:08,168 \tSource:     \"24 whoever says to the wicked one , “ you are righteous , ”\"\n",
            "2019-10-13 13:56:08,168 \tReference:  24 እቲ ንእኩይ “ ጻድቕ ኢኻ ” ዚብሎ\n",
            "2019-10-13 13:56:08,168 \tHypothesis: 24 ንእኩይ “ ዚንዕቆ ” ይብል የሆዋ\n",
            "2019-10-13 13:56:08,168 Example #3\n",
            "2019-10-13 13:56:08,169 \tSource:     will be cursed by the peoples and denounced by the nations .\n",
            "2019-10-13 13:56:08,169 \tReference:  ብህዝብታት ኪርገም ብኣህዛብ እውን ኪዅነን እዩ .\n",
            "2019-10-13 13:56:08,169 \tHypothesis: ንኣህዛብ ከኣ ብደሓን ኪብሮም እዩ .\n",
            "2019-10-13 13:56:08,169 Validation result at epoch  19, step     2800: bleu:   5.70, loss: 136970.3125, ppl:  48.7270, duration: 260.3806s\n",
            "2019-10-13 13:56:10,725 Epoch  19: total training loss 266.71\n",
            "2019-10-13 13:56:10,725 EPOCH 20\n",
            "2019-10-13 13:57:18,459 Epoch  20 Step:     2900 Batch Loss:     1.766104 Tokens per Sec:     3367, Lr: 0.000410\n",
            "2019-10-13 13:57:54,091 Epoch  20: total training loss 254.16\n",
            "2019-10-13 13:57:54,091 EPOCH 21\n",
            "2019-10-13 13:58:27,901 Epoch  21 Step:     3000 Batch Loss:     1.368845 Tokens per Sec:     3362, Lr: 0.000403\n",
            "2019-10-13 13:59:37,259 Epoch  21: total training loss 234.24\n",
            "2019-10-13 13:59:37,259 EPOCH 22\n",
            "2019-10-13 13:59:37,993 Epoch  22 Step:     3100 Batch Loss:     1.262781 Tokens per Sec:     3350, Lr: 0.000397\n",
            "2019-10-13 14:00:48,326 Epoch  22 Step:     3200 Batch Loss:     1.599789 Tokens per Sec:     3400, Lr: 0.000391\n",
            "2019-10-13 14:05:08,549 Example #0\n",
            "2019-10-13 14:05:08,550 \tSource:     source_sentence\n",
            "2019-10-13 14:05:08,550 \tReference:  target_sentence\n",
            "2019-10-13 14:05:08,550 \tHypothesis: tt_sentence\n",
            "2019-10-13 14:05:08,550 Example #1\n",
            "2019-10-13 14:05:08,550 \tSource:     partiality in judgment is not good .\n",
            "2019-10-13 14:05:08,550 \tReference:  ኣብ ፍርዲ ኣድልዎ ምግባር ጽቡቕ ኣይኰነን .\n",
            "2019-10-13 14:05:08,550 \tHypothesis: ብጽቡቕ እተፈርህዎ ኸኣ ኣብ ብርሃን የልቦን .\n",
            "2019-10-13 14:05:08,550 Example #2\n",
            "2019-10-13 14:05:08,551 \tSource:     \"24 whoever says to the wicked one , “ you are righteous , ”\"\n",
            "2019-10-13 14:05:08,551 \tReference:  24 እቲ ንእኩይ “ ጻድቕ ኢኻ ” ዚብሎ\n",
            "2019-10-13 14:05:08,551 \tHypothesis: 24 ንእኩይ “ ንጻድቕ ኢኻ ” ይብል የሆዋ\n",
            "2019-10-13 14:05:08,551 Example #3\n",
            "2019-10-13 14:05:08,551 \tSource:     will be cursed by the peoples and denounced by the nations .\n",
            "2019-10-13 14:05:08,551 \tReference:  ብህዝብታት ኪርገም ብኣህዛብ እውን ኪዅነን እዩ .\n",
            "2019-10-13 14:05:08,551 \tHypothesis: ንደቅኺ ኸኣ ኣብ ኣህዛብ ኪብጀዎም እዩ .\n",
            "2019-10-13 14:05:08,552 Validation result at epoch  22, step     3200: bleu:   5.77, loss: 140427.0625, ppl:  53.7483, duration: 260.2253s\n",
            "2019-10-13 14:05:40,892 Epoch  22: total training loss 218.01\n",
            "2019-10-13 14:05:40,892 EPOCH 23\n",
            "2019-10-13 14:06:18,600 Epoch  23 Step:     3300 Batch Loss:     1.437041 Tokens per Sec:     3352, Lr: 0.000385\n",
            "2019-10-13 14:07:23,778 Epoch  23: total training loss 204.44\n",
            "2019-10-13 14:07:23,779 EPOCH 24\n",
            "2019-10-13 14:07:29,396 Epoch  24 Step:     3400 Batch Loss:     1.164915 Tokens per Sec:     3262, Lr: 0.000379\n",
            "2019-10-13 14:08:39,298 Epoch  24 Step:     3500 Batch Loss:     1.427594 Tokens per Sec:     3645, Lr: 0.000374\n",
            "2019-10-13 14:09:06,858 Epoch  24: total training loss 190.30\n",
            "2019-10-13 14:09:06,858 EPOCH 25\n",
            "2019-10-13 14:09:49,590 Epoch  25 Step:     3600 Batch Loss:     1.846323 Tokens per Sec:     3368, Lr: 0.000368\n",
            "2019-10-13 14:14:09,822 Example #0\n",
            "2019-10-13 14:14:09,823 \tSource:     source_sentence\n",
            "2019-10-13 14:14:09,823 \tReference:  target_sentence\n",
            "2019-10-13 14:14:09,823 \tHypothesis: tarar@@\n",
            "2019-10-13 14:14:09,823 Example #1\n",
            "2019-10-13 14:14:09,823 \tSource:     partiality in judgment is not good .\n",
            "2019-10-13 14:14:09,824 \tReference:  ኣብ ፍርዲ ኣድልዎ ምግባር ጽቡቕ ኣይኰነን .\n",
            "2019-10-13 14:14:09,824 \tHypothesis: ንፍርዲ ኣይትፈርህን .\n",
            "2019-10-13 14:14:09,824 Example #2\n",
            "2019-10-13 14:14:09,824 \tSource:     \"24 whoever says to the wicked one , “ you are righteous , ”\"\n",
            "2019-10-13 14:14:09,824 \tReference:  24 እቲ ንእኩይ “ ጻድቕ ኢኻ ” ዚብሎ\n",
            "2019-10-13 14:14:09,824 \tHypothesis: 24 ንእኩይ “ ንስኻ ጻድቕ ኢኻ ” ይብል .\n",
            "2019-10-13 14:14:09,824 Example #3\n",
            "2019-10-13 14:14:09,825 \tSource:     will be cursed by the peoples and denounced by the nations .\n",
            "2019-10-13 14:14:09,825 \tReference:  ብህዝብታት ኪርገም ብኣህዛብ እውን ኪዅነን እዩ .\n",
            "2019-10-13 14:14:09,825 \tHypothesis: ንኣህዛብ ከኣ ይበኽሩ .\n",
            "2019-10-13 14:14:09,825 Validation result at epoch  25, step     3600: bleu:   6.36, loss: 144299.9062, ppl:  59.9910, duration: 260.2345s\n",
            "2019-10-13 14:15:10,453 Epoch  25: total training loss 180.31\n",
            "2019-10-13 14:15:10,453 EPOCH 26\n",
            "2019-10-13 14:15:19,613 Epoch  26 Step:     3700 Batch Loss:     1.083112 Tokens per Sec:     3196, Lr: 0.000363\n",
            "2019-10-13 14:16:29,682 Epoch  26 Step:     3800 Batch Loss:     1.131960 Tokens per Sec:     3798, Lr: 0.000358\n",
            "2019-10-13 14:16:53,814 Epoch  26: total training loss 167.29\n",
            "2019-10-13 14:16:53,814 EPOCH 27\n",
            "2019-10-13 14:17:39,995 Epoch  27 Step:     3900 Batch Loss:     1.108651 Tokens per Sec:     3353, Lr: 0.000354\n",
            "2019-10-13 14:18:37,240 Epoch  27: total training loss 157.55\n",
            "2019-10-13 14:18:37,240 EPOCH 28\n",
            "2019-10-13 14:18:50,073 Epoch  28 Step:     4000 Batch Loss:     0.728086 Tokens per Sec:     3395, Lr: 0.000349\n",
            "2019-10-13 14:23:10,251 Example #0\n",
            "2019-10-13 14:23:10,251 \tSource:     source_sentence\n",
            "2019-10-13 14:23:10,251 \tReference:  target_sentence\n",
            "2019-10-13 14:23:10,251 \tHypothesis: target_sentence\n",
            "2019-10-13 14:23:10,251 Example #1\n",
            "2019-10-13 14:23:10,252 \tSource:     partiality in judgment is not good .\n",
            "2019-10-13 14:23:10,252 \tReference:  ኣብ ፍርዲ ኣድልዎ ምግባር ጽቡቕ ኣይኰነን .\n",
            "2019-10-13 14:23:10,252 \tHypothesis: ኣብ ፍርዲ ፍርዲ ኣይተቕረብን .\n",
            "2019-10-13 14:23:10,252 Example #2\n",
            "2019-10-13 14:23:10,252 \tSource:     \"24 whoever says to the wicked one , “ you are righteous , ”\"\n",
            "2019-10-13 14:23:10,252 \tReference:  24 እቲ ንእኩይ “ ጻድቕ ኢኻ ” ዚብሎ\n",
            "2019-10-13 14:23:10,252 \tHypothesis: 24 ንእኩይ “ ንስኻ እትህቦም ” እትብሊ\n",
            "2019-10-13 14:23:10,253 Example #3\n",
            "2019-10-13 14:23:10,253 \tSource:     will be cursed by the peoples and denounced by the nations .\n",
            "2019-10-13 14:23:10,253 \tReference:  ብህዝብታት ኪርገም ብኣህዛብ እውን ኪዅነን እዩ .\n",
            "2019-10-13 14:23:10,253 \tHypothesis: ንደቆም ከኣ ብኣታቶም ኪስሕዞም እዩ .\n",
            "2019-10-13 14:23:10,253 Validation result at epoch  28, step     4000: bleu:   5.96, loss: 147836.7812, ppl:  66.3236, duration: 260.1800s\n",
            "2019-10-13 14:24:19,401 Epoch  28 Step:     4100 Batch Loss:     0.872980 Tokens per Sec:     4000, Lr: 0.000345\n",
            "2019-10-13 14:24:40,421 Epoch  28: total training loss 147.90\n",
            "2019-10-13 14:24:40,421 EPOCH 29\n",
            "2019-10-13 14:25:29,303 Epoch  29 Step:     4200 Batch Loss:     0.939031 Tokens per Sec:     3365, Lr: 0.000341\n",
            "2019-10-13 14:26:23,589 Epoch  29: total training loss 138.32\n",
            "2019-10-13 14:26:23,589 EPOCH 30\n",
            "2019-10-13 14:26:39,881 Epoch  30 Step:     4300 Batch Loss:     0.837478 Tokens per Sec:     3374, Lr: 0.000337\n",
            "2019-10-13 14:27:50,073 Epoch  30 Step:     4400 Batch Loss:     0.974485 Tokens per Sec:     4149, Lr: 0.000333\n",
            "2019-10-13 14:32:10,253 Example #0\n",
            "2019-10-13 14:32:10,253 \tSource:     source_sentence\n",
            "2019-10-13 14:32:10,253 \tReference:  target_sentence\n",
            "2019-10-13 14:32:10,253 \tHypothesis: target_sentence\n",
            "2019-10-13 14:32:10,253 Example #1\n",
            "2019-10-13 14:32:10,253 \tSource:     partiality in judgment is not good .\n",
            "2019-10-13 14:32:10,253 \tReference:  ኣብ ፍርዲ ኣድልዎ ምግባር ጽቡቕ ኣይኰነን .\n",
            "2019-10-13 14:32:10,254 \tHypothesis: ጽቡቕ ነገር ከኣ ኣብ ፍርዲ ኣይቅደልን እዩ .\n",
            "2019-10-13 14:32:10,254 Example #2\n",
            "2019-10-13 14:32:10,254 \tSource:     \"24 whoever says to the wicked one , “ you are righteous , ”\"\n",
            "2019-10-13 14:32:10,254 \tReference:  24 እቲ ንእኩይ “ ጻድቕ ኢኻ ” ዚብሎ\n",
            "2019-10-13 14:32:10,254 \tHypothesis: 24 ንእኩይ ዚንዕቆ\n",
            "2019-10-13 14:32:10,254 Example #3\n",
            "2019-10-13 14:32:10,255 \tSource:     will be cursed by the peoples and denounced by the nations .\n",
            "2019-10-13 14:32:10,255 \tReference:  ብህዝብታት ኪርገም ብኣህዛብ እውን ኪዅነን እዩ .\n",
            "2019-10-13 14:32:10,255 \tHypothesis: ንኣህዛብ ከኣ ብብዓወት ኪርድኦም እዩ .\n",
            "2019-10-13 14:32:10,255 Validation result at epoch  30, step     4400: bleu:   6.70, loss: 150693.1094, ppl:  71.9224, duration: 260.1819s\n",
            "2019-10-13 14:32:26,985 Epoch  30: total training loss 129.73\n",
            "2019-10-13 14:32:26,985 Training ended after  30 epochs.\n",
            "2019-10-13 14:32:26,985 Best validation result at step     2400:  47.59 ppl.\n",
            "2019-10-13 14:36:17,555  dev bleu:   5.86 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2019-10-13 14:36:17,556 Translations saved to: models/enti_transformer/00002400.hyps.dev\n",
            "2019-10-13 14:40:24,139 test bleu:   5.31 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2019-10-13 14:40:24,140 Translations saved to: models/enti_transformer/00002400.hyps.test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En4yLGA0uObA",
        "colab_type": "code",
        "outputId": "948740ec-f830-4cae-8414-9d6b7a9208da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "! cat joeynmt/models/enti_transformer/validations.txt"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 400\tLoss: 183767.51562\tPPL: 183.82838\tbleu: 0.00000\tLR: 0.00027951\t*\n",
            "Steps: 800\tLoss: 166880.53125\tPPL: 113.84876\tbleu: 1.18889\tLR: 0.00055902\t*\n",
            "Steps: 1200\tLoss: 149437.20312\tPPL: 69.40469\tbleu: 2.57752\tLR: 0.00063789\t*\n",
            "Steps: 1600\tLoss: 140869.54688\tPPL: 54.42731\tbleu: 3.85995\tLR: 0.00055243\t*\n",
            "Steps: 2000\tLoss: 136958.40625\tPPL: 48.71057\tbleu: 5.12391\tLR: 0.00049411\t*\n",
            "Steps: 2400\tLoss: 136141.56250\tPPL: 47.59462\tbleu: 5.49313\tLR: 0.00045105\t*\n",
            "Steps: 2800\tLoss: 136970.31250\tPPL: 48.72703\tbleu: 5.70431\tLR: 0.00041760\t\n",
            "Steps: 3200\tLoss: 140427.06250\tPPL: 53.74828\tbleu: 5.76537\tLR: 0.00039063\t\n",
            "Steps: 3600\tLoss: 144299.90625\tPPL: 59.99102\tbleu: 6.35670\tLR: 0.00036828\t\n",
            "Steps: 4000\tLoss: 147836.78125\tPPL: 66.32362\tbleu: 5.96325\tLR: 0.00034939\t\n",
            "Steps: 4400\tLoss: 150693.10938\tPPL: 71.92242\tbleu: 6.70454\tLR: 0.00033313\t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBr1wm9Dl2K9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "!mkdir \"$gdrive_path/models/\"\n",
        "!cp -r joeynmt/models/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JJpKgsrJC1o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "86b0ea9b-9de0-48af-badb-39c7da5b29b9"
      },
      "source": [
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 400\tLoss: 183767.51562\tPPL: 183.82838\tbleu: 0.00000\tLR: 0.00027951\t*\n",
            "Steps: 800\tLoss: 166880.53125\tPPL: 113.84876\tbleu: 1.18889\tLR: 0.00055902\t*\n",
            "Steps: 1200\tLoss: 149437.20312\tPPL: 69.40469\tbleu: 2.57752\tLR: 0.00063789\t*\n",
            "Steps: 1600\tLoss: 140869.54688\tPPL: 54.42731\tbleu: 3.85995\tLR: 0.00055243\t*\n",
            "Steps: 2000\tLoss: 136958.40625\tPPL: 48.71057\tbleu: 5.12391\tLR: 0.00049411\t*\n",
            "Steps: 2400\tLoss: 136141.56250\tPPL: 47.59462\tbleu: 5.49313\tLR: 0.00045105\t*\n",
            "Steps: 2800\tLoss: 136970.31250\tPPL: 48.72703\tbleu: 5.70431\tLR: 0.00041760\t\n",
            "Steps: 3200\tLoss: 140427.06250\tPPL: 53.74828\tbleu: 5.76537\tLR: 0.00039063\t\n",
            "Steps: 3600\tLoss: 144299.90625\tPPL: 59.99102\tbleu: 6.35670\tLR: 0.00036828\t\n",
            "Steps: 4000\tLoss: 147836.78125\tPPL: 66.32362\tbleu: 5.96325\tLR: 0.00034939\t\n",
            "Steps: 4400\tLoss: 150693.10938\tPPL: 71.92242\tbleu: 6.70454\tLR: 0.00033313\t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhisG8H_vGyI",
        "colab_type": "code",
        "outputId": "37b5c4e5-75bd-4479-c21d-805ab2c99c84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "! cd joeynmt; python3 -m joeynmt test models/enti_transformer/config.yaml\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-13 15:10:10,118 -  dev bleu:   5.86 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2019-10-13 15:14:18,063 - test bleu:   5.31 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrCceV22I5PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}