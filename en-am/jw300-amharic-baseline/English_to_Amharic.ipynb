{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English_to_Amharic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UsLDHm-Jw5P",
        "colab_type": "code",
        "outputId": "cd774d64-a13e-43b0-dfe7-bd6ddf962acd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT6gOLJ1apgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\n",
        "import os\n",
        "source_language = \"en\"\n",
        "target_language = \"am\"\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = tag\n",
        "\n",
        "# This will save it to a folder in our gdrive instead!\n",
        "!mkdir -p \"/content/drive/My Drive/masakhane/$src-$tgt\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/masakhane/%s-%s\" % (source_language, target_language)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhI5rCxkLOQz",
        "colab_type": "code",
        "outputId": "5433f557-fb17-436e-9965-4cc4feb3a7f1",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-049815ce-963f-4150-9855-a37ca6fde43e\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-049815ce-963f-4150-9855-a37ca6fde43e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data.csv to data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TsgIUEavc9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "data = pd.read_csv(io.BytesIO(uploaded['data.csv']))\n",
        "# Dataset is now stored in a Pandas Dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66qYqydYwaSB",
        "colab_type": "code",
        "outputId": "5d60477e-2d3a-4f23-d77a-c2cdd70f71f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_sentence</th>\n",
              "      <th>target_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Now the famine was severe in the land.</td>\n",
              "      <td>ረሃቡ በምድሪቱ ላይ በርትቶ ነበር።</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>So when they had finished eating the grain the...</td>\n",
              "      <td>ስለሆነም ከግብፅ ያመጡትን እህል በልተው ሲጨርሱ አባታቸው “ተመልሳችሁ ሂ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Then Judah said to him: “The man clearly warne...</td>\n",
              "      <td>በዚህ ጊዜ ይሁዳ እንዲህ አለው፦ “ሰውየው ‘ወንድማችሁን ይዛችሁት ካልመጣ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>If you send our brother with us, we will go do...</td>\n",
              "      <td>ወንድማችንን ከእኛ ጋር የምትልከው ከሆነ ወደዚያ ወርደን እህል እንገዛልሃለን።</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>But if you do not send him, we will not go dow...</td>\n",
              "      <td>እሱን የማትልከው ከሆነ ግን ወደዚያ አንወርድም፤ ምክንያቱም ሰውየው ‘ወን...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     source_sentence                                    target_sentence\n",
              "0             Now the famine was severe in the land.                             ረሃቡ በምድሪቱ ላይ በርትቶ ነበር።\n",
              "1  So when they had finished eating the grain the...  ስለሆነም ከግብፅ ያመጡትን እህል በልተው ሲጨርሱ አባታቸው “ተመልሳችሁ ሂ...\n",
              "2  Then Judah said to him: “The man clearly warne...  በዚህ ጊዜ ይሁዳ እንዲህ አለው፦ “ሰውየው ‘ወንድማችሁን ይዛችሁት ካልመጣ...\n",
              "3  If you send our brother with us, we will go do...  ወንድማችንን ከእኛ ጋር የምትልከው ከሆነ ወደዚያ ወርደን እህል እንገዛልሃለን።\n",
              "4  But if you do not send him, we will not go dow...  እሱን የማትልከው ከሆነ ግን ወደዚያ አንወርድም፤ ምክንያቱም ሰውየው ‘ወን..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHtFaZtlwcYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.rename(columns={\"English\":\"source_sentence\", \"Amharic\":\"target_sentence\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyODzTVfx5GO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Do the split between dev/test/train and create parallel corpora\n",
        "num_dev_patterns = 1000\n",
        "num_test_patterns = 1000\n",
        "df = data\n",
        "# Lower case the corpora\n",
        "df[\"source_sentence\"] = df[\"source_sentence\"].str.lower()\n",
        "df[\"target_sentence\"] = df[\"target_sentence\"].str.lower()\n",
        "\n",
        "\n",
        "devtest = df.tail(num_dev_patterns + num_test_patterns)\n",
        "test = devtest.tail(num_test_patterns)\n",
        "dev = devtest.head(num_dev_patterns)\n",
        "stripped = df.drop(df.tail(num_dev_patterns + num_test_patterns).index)\n",
        "\n",
        "stripped[[\"source_sentence\"]].to_csv(\"train.en\", index=False)\n",
        "stripped[[\"target_sentence\"]].to_csv(\"train.am\", index=False)\n",
        "\n",
        "dev[[\"source_sentence\"]].to_csv(\"dev.en\", index=False)\n",
        "dev[[\"target_sentence\"]].to_csv(\"dev.am\", index=False)\n",
        "\n",
        "test[[\"source_sentence\"]].to_csv(\"test.en\", index=False)\n",
        "test[[\"target_sentence\"]].to_csv(\"test.am\", index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myWQvGt7yXUM",
        "colab_type": "code",
        "outputId": "200df411-cdf4-4344-84c6-588c73610103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'joeynmt'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 2051 (delta 25), reused 22 (delta 10), pack-reused 2006\n",
            "Receiving objects: 100% (2051/2051), 2.39 MiB | 4.43 MiB/s, done.\n",
            "Resolving deltas: 100% (1414/1414), done.\n",
            "Processing /content/joeynmt\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (4.3.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.16.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (41.2.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.2.0)\n",
            "Requirement already satisfied: tensorflow>=1.14 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.15.0rc3)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.3.1)\n",
            "Collecting sacrebleu>=1.3.6 (from joeynmt==0.0.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/0e/e5/93d252182f7cbd4b59bb3ec5797e2ce33cfd6f5aadaf327db170cf4b7887/sacrebleu-1.4.2-py3-none-any.whl\n",
            "Collecting subword-nmt (from joeynmt==0.0.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/26/08/58267cb3ac00f5f895457777ed9e0d106dbb5e6388fa7923d8663b04b849/subword_nmt-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (3.0.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.9.0)\n",
            "Collecting pyyaml>=5.1 (from joeynmt==0.0.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/e8/b3212641ee2718d556df0f23f78de8303f068fe29cdaa7a91018849582fe/PyYAML-5.1.2.tar.gz (265kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 15.9MB/s \n",
            "\u001b[?25hCollecting pylint (from joeynmt==0.0.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/ed/1cb8e7b85a31807aa0bff8b3e60935370bed7e141df8b530aac6352bddff/pylint-2.4.2-py3-none-any.whl (302kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->joeynmt==0.0.1) (0.46)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.33.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.7.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.1.7)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.11.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (2.21.0)\n",
            "Collecting typing (from sacrebleu>=1.3.6->joeynmt==0.0.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/2e/b480ee1b75e6d17d2993738670e75c1feeb9ff7f64452153cf018051cc92/typing-3.7.4.1-py3-none-any.whl\n",
            "Collecting portalocker (from sacrebleu>=1.3.6->joeynmt==0.0.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/60/ec/836a494dbaa72541f691ec4e66f29fdc8db9bcc7f49e1c2d457ba13ced42/portalocker-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.4.2)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt==0.0.1) (1.3.1)\n",
            "Requirement already satisfied: pandas>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt==0.0.1) (0.24.2)\n",
            "Collecting isort<5,>=4.2.5 (from pylint->joeynmt==0.0.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/b0/c121fd1fa3419ea9bfd55c7f9c4fedfec5143208d8c7ad3ce3db6c623c21/isort-4.3.21-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 25.0MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7,>=0.6 (from pylint->joeynmt==0.0.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Collecting astroid<2.4,>=2.3.0 (from pylint->joeynmt==0.0.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/e1/74a63c85c501c29c52da5be604c025e368f4dd77daf1fa13c878a33e5a36/astroid-2.3.1-py3-none-any.whl (205kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 43.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow>=1.14->joeynmt==0.0.1) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14->joeynmt==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14->joeynmt==0.0.1) (3.1.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn->joeynmt==0.0.1) (2018.9)\n",
            "Collecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\" (from astroid<2.4,>=2.3.0->pylint->joeynmt==0.0.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/d3/9d1802c161626d0278bafb1ffb32f76b9d01e123881bbf9d91e8ccf28e18/typed_ast-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (736kB)\n",
            "\u001b[K     |████████████████████████████████| 737kB 42.2MB/s \n",
            "\u001b[?25hCollecting lazy-object-proxy==1.4.* (from astroid<2.4,>=2.3.0->pylint->joeynmt==0.0.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/26/534a6d32572a9dbca11619321535c0a7ab34688545d9d67c2c204b9e3a3d/lazy_object_proxy-1.4.2-cp36-cp36m-manylinux1_x86_64.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 21.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: joeynmt, pyyaml\n",
            "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for joeynmt: filename=joeynmt-0.0.1-cp36-none-any.whl size=69430 sha256=e9896e30e7cae1bf92df275966028a5faaa6afb53f46963e83bd4908a9a7eb16\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fd6vvoos/wheels/db/01/db/751cc9f3e7f6faec127c43644ba250a3ea7ad200594aeda70a\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1.2-cp36-cp36m-linux_x86_64.whl size=44104 sha256=f4437ec31c94aab6d464da80f9f5de694bedfda70dc17248e75c9be884b8082e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/45/dd/65f0b38450c47cf7e5312883deb97d065e030c5cca0a365030\n",
            "Successfully built joeynmt pyyaml\n",
            "Installing collected packages: typing, portalocker, sacrebleu, subword-nmt, pyyaml, isort, mccabe, typed-ast, lazy-object-proxy, astroid, pylint, joeynmt\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed astroid-2.3.1 isort-4.3.21 joeynmt-0.0.1 lazy-object-proxy-1.4.2 mccabe-0.6.1 portalocker-1.5.1 pylint-2.4.2 pyyaml-5.1.2 sacrebleu-1.4.2 subword-nmt-0.3.6 typed-ast-1.4.0 typing-3.7.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoLltS6xybX3",
        "colab_type": "code",
        "outputId": "63446944-fe4d-45ae-adeb-296509e047ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
        "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
        "\n",
        "# Do subword NMT\n",
        "! mkdir joeynmt/data/\n",
        "! mkdir joeynmt/data/enam/\n",
        "! export data_path=joeynmt/data/$src$tgt/\n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
        "\n",
        "# Create directory, move everyone we care about to the correct location\n",
        "#! mkdir -p $data_path\n",
        "! cp train.* joeynmt/data/enam/\n",
        "! cp test.* joeynmt/data/enam/\n",
        "! cp dev.* joeynmt/data/enam/\n",
        "! cp bpe.codes.4000 $data_path\n",
        "! ls $data_path\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# Some output\n",
        "! echo \"BPE amharic Sentences\"\n",
        "! tail -n 5 test.bpe.$tgt\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! tail -n 10 joeynmt/data/enam/vocab.txt\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: missing destination file operand after 'bpe.codes.4000'\n",
            "Try 'cp --help' for more information.\n",
            "bpe.codes.4000\tdev.bpe.en  sample_data  test.en       train.en\n",
            "data.csv\tdev.en\t    test.am\t train.am      vocab.am\n",
            "dev.am\t\tdrive\t    test.bpe.am  train.bpe.am  vocab.en\n",
            "dev.bpe.am\tjoeynmt     test.bpe.en  train.bpe.en\n",
            "BPE amharic Sentences\n",
            "በ@@ ባሕር ውስጥ ካ@@ ሉ ሕ@@ ያ@@ ዋን ፍጥ@@ ረ@@ ታ@@ ትም አንድ ሦስተ@@ ኛው ሞተ@@ ፤ ከመ@@ ርከ@@ ቦ@@ ችም አንድ ሦስተ@@ ኛው ወደ@@ መ@@ ።\n",
            "ሦስተ@@ ኛው መልአክ መለከ@@ ቱን ነፋ@@ ። እንደ መብ@@ ራት ቦ@@ ግ ያለ አንድ ት@@ ልቅ ኮ@@ ከብ@@ ም ከ@@ ሰማይ ወደ@@ ቀ@@ ፤ በ@@ ወን@@ ዞ@@ ች አንድ ሦስተ@@ ኛ@@ ና በ@@ ውኃ ምን@@ ጮ@@ ች ላይ ወደ@@ ቀ@@ ።\n",
            "ኮ@@ ከ@@ ቡ ጭ@@ ቁ@@ ኝ ይ@@ ባላ@@ ል። የ@@ ውኃ@@ ውም አንድ ሦስተ@@ ኛ እንደ ጭ@@ ቁ@@ ኝ መራ@@ ራ ሆነ@@ ፤ ውኃ@@ ውም መራ@@ ራ ከመ@@ ሆኑ የተነሳ ብዙ ሰዎች በ@@ ውኃ@@ ው ጠ@@ ን@@ ቅ ሞ@@ ቱ@@ ።\n",
            "አራ@@ ተኛው መልአክ መለከ@@ ቱን ነፋ@@ ። የ@@ ፀሐይ አንድ ሦስተ@@ ኛ@@ ፣ የ@@ ጨረ@@ ቃ አንድ ሦስተ@@ ኛ@@ ና የ@@ ከ@@ ዋ@@ ክብ@@ ት አንድ ሦስተ@@ ኛ ተ@@ መታ@@ ፤ ይህም የሆነው የ@@ እነዚህ አካ@@ ላት አንድ ሦስተ@@ ኛው እንዲ@@ ጨ@@ ልም እንዲሁም የቀ@@ ኑ አንድ ሦስተ@@ ኛ@@ ና የ@@ ሌሊ@@ ቱ አንድ ሦስተ@@ ኛ ብርሃን እንዳ@@ ያ@@ ገኝ ነው።\n",
            "እኔም አየ@@ ሁ@@ ፤ አንድ ን@@ ስ@@ ር በ@@ ሰማይ መካከል እየ@@ በረ@@ ረ በታላቅ ድምፅ እንዲህ ሲል ሰማ@@ ሁ@@ ፦ “@@ መለከ@@ ቶቻ@@ ቸውን ሊ@@ ነ@@ ፉ የተ@@ ዘጋ@@ ጁ@@ ት ሦ@@ ስቱ መላ@@ እክ@@ ት በሚ@@ ያ@@ ሰ@@ ሟ@@ ቸው በቀ@@ ሩት ኃይ@@ ለኛ የ@@ መለከት ድም@@ ፆ@@ ች የተነሳ በምድር ላይ ለሚ@@ ኖ@@ ሩት ወዮ@@ ላ@@ ቸው@@ ! ወዮ@@ ላ@@ ቸው@@ ! ወዮ@@ ላ@@ ቸው@@ !”\n",
            "Combined BPE Vocab\n",
            "taber@@\n",
            "ጲ\n",
            "ቭ\n",
            "sovere@@\n",
            "gan\n",
            "stron@@\n",
            "spiri@@\n",
            "festi@@\n",
            "pharʹa@@\n",
            "ኦ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9xZAtq7KBy1",
        "colab_type": "code",
        "outputId": "2b692c06-dd50-4bef-b4f9-cba4fa3001b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.bpe.en  test.bpe.am  train.am      train.en\n",
            "dev.am\t\tdev.en\t    test.bpe.en  train.bpe.am\n",
            "dev.bpe.am\ttest.am     test.en\t train.bpe.en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cokx_Tjmy80O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/train.bpe\"\n",
        "    dev:   \"data/{name}/dev.bpe\"\n",
        "    test:  \"data/{name}/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"models/{name}_transformer/12000.ckpt\" # if given, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"noam\"            # Try switching from plateau to Noam scheduling\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    patience: 8\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0002\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 14 # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 400 # Decrease this for testing\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: True\n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 8\n",
        "        embeddings:\n",
        "            embedding_dim: 512\n",
        "            scale: True\n",
        "            dropout: 0.\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 512\n",
        "        ff_size: 2048\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 8\n",
        "        embeddings:\n",
        "            embedding_dim: 512\n",
        "            scale: True\n",
        "            dropout: 0.\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 512\n",
        "        ff_size: 2048\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkxjU17f0MGf",
        "colab_type": "code",
        "outputId": "b9460938-5e42-43cc-de03-c187300cb394",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-13 13:00:12,181 Hello! This is Joey-NMT.\n",
            "2019-10-13 13:00:13,769 Total params: 46480384\n",
            "2019-10-13 13:00:13,770 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']\n",
            "2019-10-13 13:00:19,456 cfg.name                           : enam_transformer\n",
            "2019-10-13 13:00:19,456 cfg.data.src                       : en\n",
            "2019-10-13 13:00:19,456 cfg.data.trg                       : am\n",
            "2019-10-13 13:00:19,456 cfg.data.train                     : data/enam/train.bpe\n",
            "2019-10-13 13:00:19,456 cfg.data.dev                       : data/enam/dev.bpe\n",
            "2019-10-13 13:00:19,456 cfg.data.test                      : data/enam/test.bpe\n",
            "2019-10-13 13:00:19,457 cfg.data.level                     : bpe\n",
            "2019-10-13 13:00:19,457 cfg.data.lowercase                 : False\n",
            "2019-10-13 13:00:19,457 cfg.data.max_sent_length           : 100\n",
            "2019-10-13 13:00:19,457 cfg.data.src_vocab                 : data/enam/vocab.txt\n",
            "2019-10-13 13:00:19,457 cfg.data.trg_vocab                 : data/enam/vocab.txt\n",
            "2019-10-13 13:00:19,457 cfg.testing.beam_size              : 5\n",
            "2019-10-13 13:00:19,457 cfg.testing.alpha                  : 1.0\n",
            "2019-10-13 13:00:19,457 cfg.training.random_seed           : 42\n",
            "2019-10-13 13:00:19,457 cfg.training.optimizer             : adam\n",
            "2019-10-13 13:00:19,457 cfg.training.normalization         : tokens\n",
            "2019-10-13 13:00:19,457 cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2019-10-13 13:00:19,458 cfg.training.scheduling            : noam\n",
            "2019-10-13 13:00:19,458 cfg.training.learning_rate_factor  : 0.5\n",
            "2019-10-13 13:00:19,458 cfg.training.learning_rate_warmup  : 1000\n",
            "2019-10-13 13:00:19,458 cfg.training.patience              : 8\n",
            "2019-10-13 13:00:19,458 cfg.training.decrease_factor       : 0.7\n",
            "2019-10-13 13:00:19,458 cfg.training.loss                  : crossentropy\n",
            "2019-10-13 13:00:19,458 cfg.training.learning_rate         : 0.0002\n",
            "2019-10-13 13:00:19,458 cfg.training.learning_rate_min     : 1e-08\n",
            "2019-10-13 13:00:19,458 cfg.training.weight_decay          : 0.0\n",
            "2019-10-13 13:00:19,459 cfg.training.label_smoothing       : 0.1\n",
            "2019-10-13 13:00:19,459 cfg.training.batch_size            : 4096\n",
            "2019-10-13 13:00:19,459 cfg.training.batch_type            : token\n",
            "2019-10-13 13:00:19,459 cfg.training.eval_batch_size       : 3600\n",
            "2019-10-13 13:00:19,459 cfg.training.eval_batch_type       : token\n",
            "2019-10-13 13:00:19,459 cfg.training.batch_multiplier      : 1\n",
            "2019-10-13 13:00:19,459 cfg.training.early_stopping_metric : ppl\n",
            "2019-10-13 13:00:19,459 cfg.training.epochs                : 14\n",
            "2019-10-13 13:00:19,459 cfg.training.validation_freq       : 400\n",
            "2019-10-13 13:00:19,459 cfg.training.logging_freq          : 100\n",
            "2019-10-13 13:00:19,459 cfg.training.eval_metric           : bleu\n",
            "2019-10-13 13:00:19,460 cfg.training.model_dir             : models/enam_transformer\n",
            "2019-10-13 13:00:19,460 cfg.training.overwrite             : True\n",
            "2019-10-13 13:00:19,460 cfg.training.shuffle               : True\n",
            "2019-10-13 13:00:19,460 cfg.training.use_cuda              : True\n",
            "2019-10-13 13:00:19,460 cfg.training.max_output_length     : 100\n",
            "2019-10-13 13:00:19,460 cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2019-10-13 13:00:19,460 cfg.training.keep_last_ckpts       : 3\n",
            "2019-10-13 13:00:19,460 cfg.model.initializer              : xavier\n",
            "2019-10-13 13:00:19,460 cfg.model.bias_initializer         : zeros\n",
            "2019-10-13 13:00:19,460 cfg.model.init_gain                : 1.0\n",
            "2019-10-13 13:00:19,461 cfg.model.embed_initializer        : xavier\n",
            "2019-10-13 13:00:19,461 cfg.model.embed_init_gain          : 1.0\n",
            "2019-10-13 13:00:19,461 cfg.model.tied_embeddings          : True\n",
            "2019-10-13 13:00:19,461 cfg.model.tied_softmax             : True\n",
            "2019-10-13 13:00:19,461 cfg.model.encoder.type             : transformer\n",
            "2019-10-13 13:00:19,461 cfg.model.encoder.num_layers       : 6\n",
            "2019-10-13 13:00:19,461 cfg.model.encoder.num_heads        : 8\n",
            "2019-10-13 13:00:19,461 cfg.model.encoder.embeddings.embedding_dim : 512\n",
            "2019-10-13 13:00:19,461 cfg.model.encoder.embeddings.scale : True\n",
            "2019-10-13 13:00:19,462 cfg.model.encoder.embeddings.dropout : 0.0\n",
            "2019-10-13 13:00:19,462 cfg.model.encoder.hidden_size      : 512\n",
            "2019-10-13 13:00:19,462 cfg.model.encoder.ff_size          : 2048\n",
            "2019-10-13 13:00:19,462 cfg.model.encoder.dropout          : 0.3\n",
            "2019-10-13 13:00:19,462 cfg.model.decoder.type             : transformer\n",
            "2019-10-13 13:00:19,462 cfg.model.decoder.num_layers       : 6\n",
            "2019-10-13 13:00:19,462 cfg.model.decoder.num_heads        : 8\n",
            "2019-10-13 13:00:19,462 cfg.model.decoder.embeddings.embedding_dim : 512\n",
            "2019-10-13 13:00:19,462 cfg.model.decoder.embeddings.scale : True\n",
            "2019-10-13 13:00:19,463 cfg.model.decoder.embeddings.dropout : 0.0\n",
            "2019-10-13 13:00:19,463 cfg.model.decoder.hidden_size      : 512\n",
            "2019-10-13 13:00:19,463 cfg.model.decoder.ff_size          : 2048\n",
            "2019-10-13 13:00:19,463 cfg.model.decoder.dropout          : 0.3\n",
            "2019-10-13 13:00:19,463 Data set sizes: \n",
            "\ttrain 29063,\n",
            "\tvalid 1001,\n",
            "\ttest 1001\n",
            "2019-10-13 13:00:19,463 First training example:\n",
            "\t[SRC] s@@ our@@ ce@@ _@@ sen@@ ten@@ ce\n",
            "\t[TRG] tar@@ ge@@ t@@ _@@ sen@@ ten@@ ce\n",
            "2019-10-13 13:00:19,463 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) of (7) to (8) በ@@ (9) የ@@\n",
            "2019-10-13 13:00:19,464 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) of (7) to (8) በ@@ (9) የ@@\n",
            "2019-10-13 13:00:19,464 Number of Src words (types): 4570\n",
            "2019-10-13 13:00:19,464 Number of Trg words (types): 4570\n",
            "2019-10-13 13:00:19,465 Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=8),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=8),\n",
            "\tsrc_embed=Embeddings(embedding_dim=512, vocab_size=4570),\n",
            "\ttrg_embed=Embeddings(embedding_dim=512, vocab_size=4570))\n",
            "2019-10-13 13:00:19,469 EPOCH 1\n",
            "2019-10-13 13:01:34,968 Epoch   1 Step:      100 Batch Loss:     6.194415 Tokens per Sec:     3657, Lr: 0.000070\n",
            "2019-10-13 13:02:50,304 Epoch   1 Step:      200 Batch Loss:     5.984047 Tokens per Sec:     7328, Lr: 0.000140\n",
            "2019-10-13 13:04:06,773 Epoch   1 Step:      300 Batch Loss:     5.749145 Tokens per Sec:    10908, Lr: 0.000210\n",
            "2019-10-13 13:04:52,047 Epoch   1: total training loss 2225.86\n",
            "2019-10-13 13:04:52,048 EPOCH 2\n",
            "2019-10-13 13:05:21,788 Epoch   2 Step:      400 Batch Loss:     5.439086 Tokens per Sec:     3666, Lr: 0.000280\n",
            "2019-10-13 13:09:27,816 Hooray! New best validation result [ppl]!\n",
            "2019-10-13 13:09:27,816 Saving new checkpoint.\n",
            "2019-10-13 13:09:29,562 Example #0\n",
            "2019-10-13 13:09:29,562 \tSource:     source_sentence\n",
            "2019-10-13 13:09:29,562 \tReference:  target_sentence\n",
            "2019-10-13 13:09:29,562 \tHypothesis: ከዚያም የየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየከከከከከከከከከከያ@@\n",
            "2019-10-13 13:09:29,563 Example #1\n",
            "2019-10-13 13:09:29,563 \tSource:     \"is the law, therefore, against the promises of god? certainly not! for if a law had been given that could give life, righteousness would actually have been by means of law.\"\n",
            "2019-10-13 13:09:29,563 \tReference:  ታዲያ ሕጉ የአምላክን የተስፋ ቃል ይጻረራል ማለት ነው? በጭራሽ! ሕይወት ሊያስገኝ የሚችል ሕግ ተሰጥቶ ቢሆን ኖሮ ጽድቅ የሚገኘው በሕግ አማካኝነት በሆነ ነበር።\n",
            "2019-10-13 13:09:29,563 \tHypothesis: ““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““““@@\n",
            "2019-10-13 13:09:29,563 Example #2\n",
            "2019-10-13 13:09:29,564 \tSource:     \"but the scripture handed all things over to the custody of sin, so that the promise resulting from faith in jesus christ might be given to those exercising faith.\"\n",
            "2019-10-13 13:09:29,564 \tReference:  ሆኖም ቅዱስ መጽሐፉ ሁሉም ነገሮች የኃጢአት እስረኛ እንዲሆኑ አሳልፎ ሰጥቷቸዋል፤ ይህም የሆነው በኢየሱስ ክርስቶስ በማመን የሚገኘው የተስፋ ቃል ለሚያምኑ ሁሉ ይሰጥ ዘንድ ነው።\n",
            "2019-10-13 13:09:29,564 \tHypothesis: ከዚያም የየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበ@@\n",
            "2019-10-13 13:09:29,564 Example #3\n",
            "2019-10-13 13:09:29,564 \tSource:     \"however, before the faith arrived, we were being guarded under law, being handed over into custody, looking to the faith that was about to be revealed.\"\n",
            "2019-10-13 13:09:29,565 \tReference:  ይሁን እንጂ እምነት ከመምጣቱ በፊት፣ ሊገለጥ ያለውን እምነት እየተጠባበቅን በሕግ ጥበቃ ሥር እስረኞች እንድንሆን አልፈን ተሰጥተናል።\n",
            "2019-10-13 13:09:29,565 \tHypothesis: ከዚያም የየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየበበበበበበበበበበበበበበበበበበበበበበበበበበበበበ@@\n",
            "2019-10-13 13:09:29,565 Validation result at epoch   2, step      400: bleu:   0.00, loss: 188883.8906, ppl: 250.5675, duration: 247.7768s\n",
            "2019-10-13 13:10:46,033 Epoch   2 Step:      500 Batch Loss:     5.285314 Tokens per Sec:     5071, Lr: 0.000349\n",
            "2019-10-13 13:12:02,345 Epoch   2 Step:      600 Batch Loss:     4.753535 Tokens per Sec:     8702, Lr: 0.000419\n",
            "2019-10-13 13:13:18,194 Epoch   2 Step:      700 Batch Loss:     4.618568 Tokens per Sec:    12417, Lr: 0.000489\n",
            "2019-10-13 13:13:33,809 Epoch   2: total training loss 1900.14\n",
            "2019-10-13 13:13:33,809 EPOCH 3\n",
            "2019-10-13 13:14:33,597 Epoch   3 Step:      800 Batch Loss:     4.761633 Tokens per Sec:     3649, Lr: 0.000559\n",
            "2019-10-13 13:18:38,934 Hooray! New best validation result [ppl]!\n",
            "2019-10-13 13:18:38,934 Saving new checkpoint.\n",
            "2019-10-13 13:18:40,446 Example #0\n",
            "2019-10-13 13:18:40,447 \tSource:     source_sentence\n",
            "2019-10-13 13:18:40,447 \tReference:  target_sentence\n",
            "2019-10-13 13:18:40,447 \tHypothesis: በሕሳን፣ የቤታታን፣ የቤታታታን፣ የቤታታታቸውን በወራሉ።\n",
            "2019-10-13 13:18:40,447 Example #1\n",
            "2019-10-13 13:18:40,447 \tSource:     \"is the law, therefore, against the promises of god? certainly not! for if a law had been given that could give life, righteousness would actually have been by means of law.\"\n",
            "2019-10-13 13:18:40,447 \tReference:  ታዲያ ሕጉ የአምላክን የተስፋ ቃል ይጻረራል ማለት ነው? በጭራሽ! ሕይወት ሊያስገኝ የሚችል ሕግ ተሰጥቶ ቢሆን ኖሮ ጽድቅ የሚገኘው በሕግ አማካኝነት በሆነ ነበር።\n",
            "2019-10-13 13:18:40,447 \tHypothesis: የአምላክ ሰው ሰው ሰው ሰው ሰው ሰው ሰው ሰው ሰው ሰው ሰው የሰው ሰው ሰው ሰው ሰው ሰው ሰው ሰው የሰው ሰው ሰው ሰው ሰው የሰው ሰው ሰው ሰው ሰው ሰው የሰው ሰው ሰው ሰው ሰው ሰው ሰው ሰው ነው።\n",
            "2019-10-13 13:18:40,447 Example #2\n",
            "2019-10-13 13:18:40,448 \tSource:     \"but the scripture handed all things over to the custody of sin, so that the promise resulting from faith in jesus christ might be given to those exercising faith.\"\n",
            "2019-10-13 13:18:40,448 \tReference:  ሆኖም ቅዱስ መጽሐፉ ሁሉም ነገሮች የኃጢአት እስረኛ እንዲሆኑ አሳልፎ ሰጥቷቸዋል፤ ይህም የሆነው በኢየሱስ ክርስቶስ በማመን የሚገኘው የተስፋ ቃል ለሚያምኑ ሁሉ ይሰጥ ዘንድ ነው።\n",
            "2019-10-13 13:18:40,448 \tHypothesis: ሆኖም አምላክ ከአምላክ የአምላክ አምላክ አምላክ አምላክ የአምላክ አምላክ አምላክ አምላክ አምላክ የአምላክ አምላክ አምላክ አምላክ አምላክ የአምላክ አምላክ አምላክ አምላክ የአምላክ አምላክ አምላክ አምላክ ነው።\n",
            "2019-10-13 13:18:40,448 Example #3\n",
            "2019-10-13 13:18:40,448 \tSource:     \"however, before the faith arrived, we were being guarded under law, being handed over into custody, looking to the faith that was about to be revealed.\"\n",
            "2019-10-13 13:18:40,448 \tReference:  ይሁን እንጂ እምነት ከመምጣቱ በፊት፣ ሊገለጥ ያለውን እምነት እየተጠባበቅን በሕግ ጥበቃ ሥር እስረኞች እንድንሆን አልፈን ተሰጥተናል።\n",
            "2019-10-13 13:18:40,448 \tHypothesis: ሆኖም ከኢየሱስ ጋር ከኢየሱስ ጋር ከሕዝቡ ጋር ከሕዝቡ ጋር ከሕዝቡ ጋር ከሕዝቡ ጋር ከሕዝቡ ጋር ከሕዝቡ ጋር ከሕዝቡ ጋር ተፈቶ ነበር።\n",
            "2019-10-13 13:18:40,449 Validation result at epoch   3, step      800: bleu:   0.00, loss: 170874.8281, ppl: 147.9791, duration: 246.8516s\n",
            "2019-10-13 13:19:56,251 Epoch   3 Step:      900 Batch Loss:     4.612202 Tokens per Sec:     6534, Lr: 0.000629\n",
            "2019-10-13 13:21:11,697 Epoch   3 Step:     1000 Batch Loss:     4.382854 Tokens per Sec:    10241, Lr: 0.000699\n",
            "2019-10-13 13:22:13,422 Epoch   3: total training loss 1636.44\n",
            "2019-10-13 13:22:13,422 EPOCH 4\n",
            "2019-10-13 13:22:27,073 Epoch   4 Step:     1100 Batch Loss:     4.009602 Tokens per Sec:     3633, Lr: 0.000666\n",
            "2019-10-13 13:23:43,005 Epoch   4 Step:     1200 Batch Loss:     3.859449 Tokens per Sec:     4338, Lr: 0.000638\n",
            "2019-10-13 13:27:48,016 Hooray! New best validation result [ppl]!\n",
            "2019-10-13 13:27:48,016 Saving new checkpoint.\n",
            "2019-10-13 13:27:49,512 Example #0\n",
            "2019-10-13 13:27:49,513 \tSource:     source_sentence\n",
            "2019-10-13 13:27:49,513 \tReference:  target_sentence\n",
            "2019-10-13 13:27:49,513 \tHypothesis: በተጨማሪም በወርቅ የተሠሩ ሰዎች ላይ የተረፈውን አፍፍታል፤\n",
            "2019-10-13 13:27:49,513 Example #1\n",
            "2019-10-13 13:27:49,513 \tSource:     \"is the law, therefore, against the promises of god? certainly not! for if a law had been given that could give life, righteousness would actually have been by means of law.\"\n",
            "2019-10-13 13:27:49,513 \tReference:  ታዲያ ሕጉ የአምላክን የተስፋ ቃል ይጻረራል ማለት ነው? በጭራሽ! ሕይወት ሊያስገኝ የሚችል ሕግ ተሰጥቶ ቢሆን ኖሮ ጽድቅ የሚገኘው በሕግ አማካኝነት በሆነ ነበር።\n",
            "2019-10-13 13:27:49,513 \tHypothesis: ስለዚህ አምላክ ሆይ፣ የአምላክ አምላክ አምላክ አምላክ አምላክ ነህ! ምክንያቱም አምላክ አምላክ የጽድቅ ልጅ መኖር ይችላል? የጽድቅ ልጅ መኖር ይችላል?\n",
            "2019-10-13 13:27:49,513 Example #2\n",
            "2019-10-13 13:27:49,514 \tSource:     \"but the scripture handed all things over to the custody of sin, so that the promise resulting from faith in jesus christ might be given to those exercising faith.\"\n",
            "2019-10-13 13:27:49,514 \tReference:  ሆኖም ቅዱስ መጽሐፉ ሁሉም ነገሮች የኃጢአት እስረኛ እንዲሆኑ አሳልፎ ሰጥቷቸዋል፤ ይህም የሆነው በኢየሱስ ክርስቶስ በማመን የሚገኘው የተስፋ ቃል ለሚያምኑ ሁሉ ይሰጥ ዘንድ ነው።\n",
            "2019-10-13 13:27:49,514 \tHypothesis: ሆኖም ከሕዝቡ መካከል የተረገመ ሁሉ ሁሉ ሁሉ ከክርስቶስ ጋር የተረገመ ከሆነ ከክርስቶስ ጋር የተረገመ ከሆነ ከክርስቶስ ጋር የተረገመ ነው።\n",
            "2019-10-13 13:27:49,514 Example #3\n",
            "2019-10-13 13:27:49,514 \tSource:     \"however, before the faith arrived, we were being guarded under law, being handed over into custody, looking to the faith that was about to be revealed.\"\n",
            "2019-10-13 13:27:49,514 \tReference:  ይሁን እንጂ እምነት ከመምጣቱ በፊት፣ ሊገለጥ ያለውን እምነት እየተጠባበቅን በሕግ ጥበቃ ሥር እስረኞች እንድንሆን አልፈን ተሰጥተናል።\n",
            "2019-10-13 13:27:49,514 \tHypothesis: ሆኖም ጳውሎስ ከእኛ ጋር ለመኖር ከሞት ከሞት ከሞት ከሞት ከሞት ከሞት ጋር ጋር ተለይቶ ነበር።\n",
            "2019-10-13 13:27:49,514 Validation result at epoch   4, step     1200: bleu:   0.00, loss: 145509.5312, ppl:  70.4770, duration: 246.5093s\n",
            "2019-10-13 13:29:04,644 Epoch   4 Step:     1300 Batch Loss:     3.726732 Tokens per Sec:     8045, Lr: 0.000613\n",
            "2019-10-13 13:30:19,880 Epoch   4 Step:     1400 Batch Loss:     3.575398 Tokens per Sec:    11679, Lr: 0.000591\n",
            "2019-10-13 13:30:52,646 Epoch   4: total training loss 1382.56\n",
            "2019-10-13 13:30:52,646 EPOCH 5\n",
            "2019-10-13 13:31:34,716 Epoch   5 Step:     1500 Batch Loss:     3.466145 Tokens per Sec:     3668, Lr: 0.000571\n",
            "2019-10-13 13:32:50,713 Epoch   5 Step:     1600 Batch Loss:     3.382964 Tokens per Sec:     5687, Lr: 0.000552\n",
            "2019-10-13 13:36:55,323 Hooray! New best validation result [ppl]!\n",
            "2019-10-13 13:36:55,324 Saving new checkpoint.\n",
            "2019-10-13 13:36:56,997 Example #0\n",
            "2019-10-13 13:36:56,998 \tSource:     source_sentence\n",
            "2019-10-13 13:36:56,998 \tReference:  target_sentence\n",
            "2019-10-13 13:36:56,998 \tHypothesis: ከዚህም የተነሳ ከምዕራብ ወሰን <unk> ወሰን <unk> ተጽፏል።\n",
            "2019-10-13 13:36:56,998 Example #1\n",
            "2019-10-13 13:36:56,998 \tSource:     \"is the law, therefore, against the promises of god? certainly not! for if a law had been given that could give life, righteousness would actually have been by means of law.\"\n",
            "2019-10-13 13:36:56,998 \tReference:  ታዲያ ሕጉ የአምላክን የተስፋ ቃል ይጻረራል ማለት ነው? በጭራሽ! ሕይወት ሊያስገኝ የሚችል ሕግ ተሰጥቶ ቢሆን ኖሮ ጽድቅ የሚገኘው በሕግ አማካኝነት በሆነ ነበር።\n",
            "2019-10-13 13:36:56,999 \tHypothesis: ስለዚህ አምላክ በጽድቅ ላይ የሚችል ማን ነው? አምላክ የሚፈልግ ከሆነ አምላክ ከአምላክ ጋር የሚገባው ነገር የለም? በሕጉ ላይ የሚናገረውን ነገር ሁሉ በሆነ መንገድ ነው።\n",
            "2019-10-13 13:36:56,999 Example #2\n",
            "2019-10-13 13:36:56,999 \tSource:     \"but the scripture handed all things over to the custody of sin, so that the promise resulting from faith in jesus christ might be given to those exercising faith.\"\n",
            "2019-10-13 13:36:56,999 \tReference:  ሆኖም ቅዱስ መጽሐፉ ሁሉም ነገሮች የኃጢአት እስረኛ እንዲሆኑ አሳልፎ ሰጥቷቸዋል፤ ይህም የሆነው በኢየሱስ ክርስቶስ በማመን የሚገኘው የተስፋ ቃል ለሚያምኑ ሁሉ ይሰጥ ዘንድ ነው።\n",
            "2019-10-13 13:36:56,999 \tHypothesis: ሆኖም ከኢየሱስ ጋር የክርስቶስ ሕግ ሕግ ሁሉ የክርስቶስ ሕግ ሕግ ሕግ ሁሉ የክርስቶስ ሕግ ሕግ ሕግ ሁሉ የክርስቶስ ሕግ ሕግ ሕግ ሕግ ሁሉ የክርስቶስ ሕግ ሕግ ሕግ ሕግ ለማወቅ ነው።\n",
            "2019-10-13 13:36:56,999 Example #3\n",
            "2019-10-13 13:36:57,000 \tSource:     \"however, before the faith arrived, we were being guarded under law, being handed over into custody, looking to the faith that was about to be revealed.\"\n",
            "2019-10-13 13:36:57,000 \tReference:  ይሁን እንጂ እምነት ከመምጣቱ በፊት፣ ሊገለጥ ያለውን እምነት እየተጠባበቅን በሕግ ጥበቃ ሥር እስረኞች እንድንሆን አልፈን ተሰጥተናል።\n",
            "2019-10-13 13:36:57,000 \tHypothesis: ይሁን እንጂ ኢየሱስ ከእኛ ጋር ለመሄድ ከእኛ ጋር ለመሄድ እንድንችል ከእኛ ጋር በመሆን ወደ እሱ ለመሄድ ተከተለን።\n",
            "2019-10-13 13:36:57,000 Validation result at epoch   5, step     1600: bleu:   0.17, loss: 132005.6562, ppl:  47.4833, duration: 246.2865s\n",
            "2019-10-13 13:38:12,960 Epoch   5 Step:     1700 Batch Loss:     2.904638 Tokens per Sec:     9398, Lr: 0.000536\n",
            "2019-10-13 13:39:28,277 Epoch   5 Step:     1800 Batch Loss:     3.199094 Tokens per Sec:    13153, Lr: 0.000521\n",
            "2019-10-13 13:39:30,550 Epoch   5: total training loss 1208.48\n",
            "2019-10-13 13:39:30,550 EPOCH 6\n",
            "2019-10-13 13:40:43,971 Epoch   6 Step:     1900 Batch Loss:     2.892642 Tokens per Sec:     3675, Lr: 0.000507\n",
            "2019-10-13 13:41:59,131 Epoch   6 Step:     2000 Batch Loss:     3.037948 Tokens per Sec:     7253, Lr: 0.000494\n",
            "2019-10-13 13:46:03,964 Hooray! New best validation result [ppl]!\n",
            "2019-10-13 13:46:03,964 Saving new checkpoint.\n",
            "2019-10-13 13:46:05,662 Example #0\n",
            "2019-10-13 13:46:05,663 \tSource:     source_sentence\n",
            "2019-10-13 13:46:05,663 \tReference:  target_sentence\n",
            "2019-10-13 13:46:05,663 \tHypothesis: \n",
            "2019-10-13 13:46:05,663 Example #1\n",
            "2019-10-13 13:46:05,663 \tSource:     \"is the law, therefore, against the promises of god? certainly not! for if a law had been given that could give life, righteousness would actually have been by means of law.\"\n",
            "2019-10-13 13:46:05,663 \tReference:  ታዲያ ሕጉ የአምላክን የተስፋ ቃል ይጻረራል ማለት ነው? በጭራሽ! ሕይወት ሊያስገኝ የሚችል ሕግ ተሰጥቶ ቢሆን ኖሮ ጽድቅ የሚገኘው በሕግ አማካኝነት በሆነ ነበር።\n",
            "2019-10-13 13:46:05,663 \tHypothesis: ስለዚህ አምላክ የጽድቅ የሆነውን ነገር የሚያውቀው ማን ነው? አምላክ ከሞት ይልቅ በጽድቅ የሚፈልግ ከሆነ ሕጉ በጽድቅ መሆናቸው በጽድቅ ተሞልቶ ሊሆን ይችላል።\n",
            "2019-10-13 13:46:05,663 Example #2\n",
            "2019-10-13 13:46:05,664 \tSource:     \"but the scripture handed all things over to the custody of sin, so that the promise resulting from faith in jesus christ might be given to those exercising faith.\"\n",
            "2019-10-13 13:46:05,664 \tReference:  ሆኖም ቅዱስ መጽሐፉ ሁሉም ነገሮች የኃጢአት እስረኛ እንዲሆኑ አሳልፎ ሰጥቷቸዋል፤ ይህም የሆነው በኢየሱስ ክርስቶስ በማመን የሚገኘው የተስፋ ቃል ለሚያምኑ ሁሉ ይሰጥ ዘንድ ነው።\n",
            "2019-10-13 13:46:05,664 \tHypothesis: ይሁንና በክርስቶስ አማካኝነት በክርስቶስ አማካኝነት በክርስቶስ አማካኝነት በክርስቶስ አማካኝነት በክርስቶስ አማካኝነት ለክርስቶስ ሕግ የሚሰጠው ነገር ሁሉ እንዲናገሩ ነው።\n",
            "2019-10-13 13:46:05,664 Example #3\n",
            "2019-10-13 13:46:05,664 \tSource:     \"however, before the faith arrived, we were being guarded under law, being handed over into custody, looking to the faith that was about to be revealed.\"\n",
            "2019-10-13 13:46:05,664 \tReference:  ይሁን እንጂ እምነት ከመምጣቱ በፊት፣ ሊገለጥ ያለውን እምነት እየተጠባበቅን በሕግ ጥበቃ ሥር እስረኞች እንድንሆን አልፈን ተሰጥተናል።\n",
            "2019-10-13 13:46:05,664 \tHypothesis: ይሁን እንጂ ወደ እሱ ሲሄድ በጣም ተደፍተናለን። ይህን ያደረገው ነገር ለንጉሡ ለንጉሡ የተናገረውን ነገር ለማድረግ ነው።\n",
            "2019-10-13 13:46:05,664 Validation result at epoch   6, step     2000: bleu:   0.22, loss: 124469.2500, ppl:  38.0912, duration: 246.5331s\n",
            "2019-10-13 13:47:21,244 Epoch   6 Step:     2100 Batch Loss:     3.148763 Tokens per Sec:    10901, Lr: 0.000482\n",
            "2019-10-13 13:48:08,938 Epoch   6: total training loss 1100.16\n",
            "2019-10-13 13:48:08,938 EPOCH 7\n",
            "2019-10-13 13:48:37,269 Epoch   7 Step:     2200 Batch Loss:     2.947865 Tokens per Sec:     3712, Lr: 0.000471\n",
            "2019-10-13 13:49:52,116 Epoch   7 Step:     2300 Batch Loss:     2.866735 Tokens per Sec:     5053, Lr: 0.000461\n",
            "2019-10-13 13:51:07,156 Epoch   7 Step:     2400 Batch Loss:     2.772743 Tokens per Sec:     8702, Lr: 0.000451\n",
            "2019-10-13 13:55:12,280 Hooray! New best validation result [ppl]!\n",
            "2019-10-13 13:55:12,280 Saving new checkpoint.\n",
            "2019-10-13 13:55:13,846 Example #0\n",
            "2019-10-13 13:55:13,847 \tSource:     source_sentence\n",
            "2019-10-13 13:55:13,847 \tReference:  target_sentence\n",
            "2019-10-13 13:55:13,847 \tHypothesis: \n",
            "2019-10-13 13:55:13,847 Example #1\n",
            "2019-10-13 13:55:13,847 \tSource:     \"is the law, therefore, against the promises of god? certainly not! for if a law had been given that could give life, righteousness would actually have been by means of law.\"\n",
            "2019-10-13 13:55:13,847 \tReference:  ታዲያ ሕጉ የአምላክን የተስፋ ቃል ይጻረራል ማለት ነው? በጭራሽ! ሕይወት ሊያስገኝ የሚችል ሕግ ተሰጥቶ ቢሆን ኖሮ ጽድቅ የሚገኘው በሕግ አማካኝነት በሆነ ነበር።\n",
            "2019-10-13 13:55:13,847 \tHypothesis: ስለዚህ አምላክ በሕጉ ላይ የሚኖር ማን ነው? አምላክ ከሞት የሚገባው በጽድቅ አማካኝነት ለፈጸመ ነገር ብቻ ነው፤ ምክንያቱም ሕጉ በሕጉ ላይ የሚኖር ሰው ነው።\n",
            "2019-10-13 13:55:13,847 Example #2\n",
            "2019-10-13 13:55:13,848 \tSource:     \"but the scripture handed all things over to the custody of sin, so that the promise resulting from faith in jesus christ might be given to those exercising faith.\"\n",
            "2019-10-13 13:55:13,848 \tReference:  ሆኖም ቅዱስ መጽሐፉ ሁሉም ነገሮች የኃጢአት እስረኛ እንዲሆኑ አሳልፎ ሰጥቷቸዋል፤ ይህም የሆነው በኢየሱስ ክርስቶስ በማመን የሚገኘው የተስፋ ቃል ለሚያምኑ ሁሉ ይሰጥ ዘንድ ነው።\n",
            "2019-10-13 13:55:13,848 \tHypothesis: ሆኖም ኢየሱስ ከአምላክ ጋር በተያያዘ የተቀመጠውን ነገር ሁሉ ለኢየሱስ ክርስቶስ እንዲሆኑ ለኢየሱስ ክርስቶስ እንዲያስፈልጉ ነው።\n",
            "2019-10-13 13:55:13,848 Example #3\n",
            "2019-10-13 13:55:13,852 \tSource:     \"however, before the faith arrived, we were being guarded under law, being handed over into custody, looking to the faith that was about to be revealed.\"\n",
            "2019-10-13 13:55:13,852 \tReference:  ይሁን እንጂ እምነት ከመምጣቱ በፊት፣ ሊገለጥ ያለውን እምነት እየተጠባበቅን በሕግ ጥበቃ ሥር እስረኞች እንድንሆን አልፈን ተሰጥተናል።\n",
            "2019-10-13 13:55:13,852 \tHypothesis: ሆኖም እኛ ከእኛ ጋር በተያያዘ ሳለ ንጉሡ ከንጉሡ ጋር በተያያዘ የተጣለ አንድ ሰው እንደሆነና ስለ እሱ ስለ እሱ ስለ እሱ ስለ እሱ የሚናገረውን ነገር ለማየት ነው።\n",
            "2019-10-13 13:55:13,852 Validation result at epoch   7, step     2400: bleu:   0.29, loss: 119074.0703, ppl:  32.5314, duration: 246.6957s\n",
            "2019-10-13 13:56:29,199 Epoch   7 Step:     2500 Batch Loss:     2.827759 Tokens per Sec:    12370, Lr: 0.000442\n",
            "2019-10-13 13:56:47,650 Epoch   7: total training loss 1025.24\n",
            "2019-10-13 13:56:47,650 EPOCH 8\n",
            "2019-10-13 13:57:44,935 Epoch   8 Step:     2600 Batch Loss:     2.582517 Tokens per Sec:     3705, Lr: 0.000433\n",
            "2019-10-13 13:59:00,355 Epoch   8 Step:     2700 Batch Loss:     2.767641 Tokens per Sec:     6474, Lr: 0.000425\n",
            "2019-10-13 14:00:16,649 Epoch   8 Step:     2800 Batch Loss:     2.604730 Tokens per Sec:    10094, Lr: 0.000418\n",
            "2019-10-13 14:04:21,372 Hooray! New best validation result [ppl]!\n",
            "2019-10-13 14:04:21,372 Saving new checkpoint.\n",
            "2019-10-13 14:04:22,977 Example #0\n",
            "2019-10-13 14:04:22,977 \tSource:     source_sentence\n",
            "2019-10-13 14:04:22,977 \tReference:  target_sentence\n",
            "2019-10-13 14:04:22,977 \tHypothesis: \n",
            "2019-10-13 14:04:22,978 Example #1\n",
            "2019-10-13 14:04:22,978 \tSource:     \"is the law, therefore, against the promises of god? certainly not! for if a law had been given that could give life, righteousness would actually have been by means of law.\"\n",
            "2019-10-13 14:04:22,978 \tReference:  ታዲያ ሕጉ የአምላክን የተስፋ ቃል ይጻረራል ማለት ነው? በጭራሽ! ሕይወት ሊያስገኝ የሚችል ሕግ ተሰጥቶ ቢሆን ኖሮ ጽድቅ የሚገኘው በሕግ አማካኝነት በሆነ ነበር።\n",
            "2019-10-13 14:04:22,978 \tHypothesis: ስለዚህ በሕጉ ላይ የተላለፈ ማን ነው? ሕጉ በሕጉ ላይ ኃጢአት ቢሠራ ኖሮ በሕጉ ላይ ኃጢአት መፈጸም አለበት!\n",
            "2019-10-13 14:04:22,978 Example #2\n",
            "2019-10-13 14:04:22,978 \tSource:     \"but the scripture handed all things over to the custody of sin, so that the promise resulting from faith in jesus christ might be given to those exercising faith.\"\n",
            "2019-10-13 14:04:22,978 \tReference:  ሆኖም ቅዱስ መጽሐፉ ሁሉም ነገሮች የኃጢአት እስረኛ እንዲሆኑ አሳልፎ ሰጥቷቸዋል፤ ይህም የሆነው በኢየሱስ ክርስቶስ በማመን የሚገኘው የተስፋ ቃል ለሚያምኑ ሁሉ ይሰጥ ዘንድ ነው።\n",
            "2019-10-13 14:04:22,979 \tHypothesis: ሆኖም የኃጢአት መባዎች ሁሉ የክርስቶስ ኢየሱስ ክርስቶስን ሥራ እንዲሠሩ በኢየሱስ አማካኝነት በኢየሱስ አማካኝነት በተመሳሳይ ሁኔታ እንዲሠሩ ነው።\n",
            "2019-10-13 14:04:22,979 Example #3\n",
            "2019-10-13 14:04:22,979 \tSource:     \"however, before the faith arrived, we were being guarded under law, being handed over into custody, looking to the faith that was about to be revealed.\"\n",
            "2019-10-13 14:04:22,979 \tReference:  ይሁን እንጂ እምነት ከመምጣቱ በፊት፣ ሊገለጥ ያለውን እምነት እየተጠባበቅን በሕግ ጥበቃ ሥር እስረኞች እንድንሆን አልፈን ተሰጥተናል።\n",
            "2019-10-13 14:04:22,979 \tHypothesis: ይሁን እንጂ በገና እየተጣፋ ሳለ ከንጉሡ ጋር በስተቀር በእውነተኛው አምላክ ፊት በኃይል እየነደረ እንዳለ አወቀ።\n",
            "2019-10-13 14:04:22,979 Validation result at epoch   8, step     2800: bleu:   0.00, loss: 114965.3750, ppl:  28.8483, duration: 246.3298s\n",
            "2019-10-13 14:05:25,569 Epoch   8: total training loss 934.02\n",
            "2019-10-13 14:05:25,570 EPOCH 9\n",
            "2019-10-13 14:05:38,511 Epoch   9 Step:     2900 Batch Loss:     2.289430 Tokens per Sec:     3661, Lr: 0.000410\n",
            "2019-10-13 14:06:54,109 Epoch   9 Step:     3000 Batch Loss:     2.336237 Tokens per Sec:     4294, Lr: 0.000403\n",
            "2019-10-13 14:08:10,017 Epoch   9 Step:     3100 Batch Loss:     2.710559 Tokens per Sec:     7964, Lr: 0.000397\n",
            "2019-10-13 14:09:24,918 Epoch   9 Step:     3200 Batch Loss:     2.438411 Tokens per Sec:    11729, Lr: 0.000391\n",
            "2019-10-13 14:13:29,638 Hooray! New best validation result [ppl]!\n",
            "2019-10-13 14:13:29,638 Saving new checkpoint.\n",
            "2019-10-13 14:13:31,771 Example #0\n",
            "2019-10-13 14:13:31,771 \tSource:     source_sentence\n",
            "2019-10-13 14:13:31,771 \tReference:  target_sentence\n",
            "2019-10-13 14:13:31,771 \tHypothesis: በር ጠባቂውም እንዲህ ሲል ተናገረ፦\n",
            "2019-10-13 14:13:31,771 Example #1\n",
            "2019-10-13 14:13:31,771 \tSource:     \"is the law, therefore, against the promises of god? certainly not! for if a law had been given that could give life, righteousness would actually have been by means of law.\"\n",
            "2019-10-13 14:13:31,771 \tReference:  ታዲያ ሕጉ የአምላክን የተስፋ ቃል ይጻረራል ማለት ነው? በጭራሽ! ሕይወት ሊያስገኝ የሚችል ሕግ ተሰጥቶ ቢሆን ኖሮ ጽድቅ የሚገኘው በሕግ አማካኝነት በሆነ ነበር።\n",
            "2019-10-13 14:13:31,771 \tHypothesis: ታዲያ አምላክ በሕጉ ላይ የሚመሠክርበት ማን ነው? አምላክ መቼም ቢሆን ኖሮ ሕጉ ውስጥ የሚኖር ከሆነ ሕጉ ከሕጉ ጋር ተግባር ሊሆን ይችላል።\n",
            "2019-10-13 14:13:31,772 Example #2\n",
            "2019-10-13 14:13:31,772 \tSource:     \"but the scripture handed all things over to the custody of sin, so that the promise resulting from faith in jesus christ might be given to those exercising faith.\"\n",
            "2019-10-13 14:13:31,772 \tReference:  ሆኖም ቅዱስ መጽሐፉ ሁሉም ነገሮች የኃጢአት እስረኛ እንዲሆኑ አሳልፎ ሰጥቷቸዋል፤ ይህም የሆነው በኢየሱስ ክርስቶስ በማመን የሚገኘው የተስፋ ቃል ለሚያምኑ ሁሉ ይሰጥ ዘንድ ነው።\n",
            "2019-10-13 14:13:31,772 \tHypothesis: ሆኖም ኢየሱስ ከኃጢአተኞች ሁሉ የኃጢአት መባ እንዲሆኑ ለማድረግ ከኢየሱስ ክርስቶስ ጋር በተያያዘ የተያያዘ ኃላፊነት እንዲኖር ለማድረግ ነው።\n",
            "2019-10-13 14:13:31,772 Example #3\n",
            "2019-10-13 14:13:31,772 \tSource:     \"however, before the faith arrived, we were being guarded under law, being handed over into custody, looking to the faith that was about to be revealed.\"\n",
            "2019-10-13 14:13:31,772 \tReference:  ይሁን እንጂ እምነት ከመምጣቱ በፊት፣ ሊገለጥ ያለውን እምነት እየተጠባበቅን በሕግ ጥበቃ ሥር እስረኞች እንድንሆን አልፈን ተሰጥተናል።\n",
            "2019-10-13 14:13:31,772 \tHypothesis: ይሁን እንጂ እኛ በእኛ ላይ የሚገኘውን የሥጋ ደዌ እንዳስነሳው በንግግር ውስጥ እንዳለን ለማየት ወደ እሱ በመሄድ እሱን ለመደሰት ፈቃደኛ አልሆነም።\n",
            "2019-10-13 14:13:31,772 Validation result at epoch   9, step     3200: bleu:   0.66, loss: 112569.3750, ppl:  26.8962, duration: 246.8538s\n",
            "2019-10-13 14:14:04,156 Epoch   9: total training loss 875.30\n",
            "2019-10-13 14:14:04,156 EPOCH 10\n",
            "2019-10-13 14:14:47,651 Epoch  10 Step:     3300 Batch Loss:     2.255931 Tokens per Sec:     3675, Lr: 0.000385\n",
            "2019-10-13 14:16:02,576 Epoch  10 Step:     3400 Batch Loss:     2.217765 Tokens per Sec:     5796, Lr: 0.000379\n",
            "2019-10-13 14:17:18,372 Epoch  10 Step:     3500 Batch Loss:     2.337091 Tokens per Sec:     9416, Lr: 0.000374\n",
            "2019-10-13 14:18:33,746 Epoch  10 Step:     3600 Batch Loss:     2.246951 Tokens per Sec:    13148, Lr: 0.000368\n",
            "2019-10-13 14:22:38,308 Hooray! New best validation result [ppl]!\n",
            "2019-10-13 14:22:38,309 Saving new checkpoint.\n",
            "2019-10-13 14:22:39,992 Example #0\n",
            "2019-10-13 14:22:39,992 \tSource:     source_sentence\n",
            "2019-10-13 14:22:39,992 \tReference:  target_sentence\n",
            "2019-10-13 14:22:39,992 \tHypothesis: \n",
            "2019-10-13 14:22:39,992 Example #1\n",
            "2019-10-13 14:22:39,993 \tSource:     \"is the law, therefore, against the promises of god? certainly not! for if a law had been given that could give life, righteousness would actually have been by means of law.\"\n",
            "2019-10-13 14:22:39,993 \tReference:  ታዲያ ሕጉ የአምላክን የተስፋ ቃል ይጻረራል ማለት ነው? በጭራሽ! ሕይወት ሊያስገኝ የሚችል ሕግ ተሰጥቶ ቢሆን ኖሮ ጽድቅ የሚገኘው በሕግ አማካኝነት በሆነ ነበር።\n",
            "2019-10-13 14:22:39,993 \tHypothesis: ስለዚህ አምላክ በሕጉ ላይ የሚመሠክር ማን ነው? ሕጉ የሕጉ መቼም ቢሆን ኖሮ ኃጢአት መሥራቱ እንደሆነች ሁሉ የጽድቅ ሥራ መሥራት አለበት።\n",
            "2019-10-13 14:22:39,993 Example #2\n",
            "2019-10-13 14:22:39,993 \tSource:     \"but the scripture handed all things over to the custody of sin, so that the promise resulting from faith in jesus christ might be given to those exercising faith.\"\n",
            "2019-10-13 14:22:39,993 \tReference:  ሆኖም ቅዱስ መጽሐፉ ሁሉም ነገሮች የኃጢአት እስረኛ እንዲሆኑ አሳልፎ ሰጥቷቸዋል፤ ይህም የሆነው በኢየሱስ ክርስቶስ በማመን የሚገኘው የተስፋ ቃል ለሚያምኑ ሁሉ ይሰጥ ዘንድ ነው።\n",
            "2019-10-13 14:22:39,993 \tHypothesis: ሆኖም ኢየሱስ ከኃጢአተኞች ሁሉ ጋር በተያያዘ የተጻፉትን ሥራ ሁሉ ለአምላክ ታማኝ ፍቅር ያከብሩ ዘንድ ለአምላክ ሕግ አስቀድሞ እንዲሰጡት ነው።\n",
            "2019-10-13 14:22:39,993 Example #3\n",
            "2019-10-13 14:22:39,993 \tSource:     \"however, before the faith arrived, we were being guarded under law, being handed over into custody, looking to the faith that was about to be revealed.\"\n",
            "2019-10-13 14:22:39,994 \tReference:  ይሁን እንጂ እምነት ከመምጣቱ በፊት፣ ሊገለጥ ያለውን እምነት እየተጠባበቅን በሕግ ጥበቃ ሥር እስረኞች እንድንሆን አልፈን ተሰጥተናል።\n",
            "2019-10-13 14:22:39,994 \tHypothesis: ይሁን እንጂ ኢየሱስ በእምነት እየጠነቀ ሳለ ንጉሡ በእኛ ላይ እየተስፋፋ ነው፤ ምክንያቱም ንጉሡ ስለ እሱ ስለ እሱ ስለ እሱ ስለ እሱ ስለ እኛ ይገለጣል።\n",
            "2019-10-13 14:22:39,994 Validation result at epoch  10, step     3600: bleu:   0.70, loss: 110032.3359, ppl:  24.9729, duration: 246.2470s\n",
            "2019-10-13 14:22:42,199 Epoch  10: total training loss 829.62\n",
            "2019-10-13 14:22:42,199 EPOCH 11\n",
            "2019-10-13 14:23:55,995 Epoch  11 Step:     3700 Batch Loss:     2.090297 Tokens per Sec:     3681, Lr: 0.000363\n",
            "2019-10-13 14:25:10,694 Epoch  11 Step:     3800 Batch Loss:     1.995729 Tokens per Sec:     7308, Lr: 0.000358\n",
            "2019-10-13 14:26:26,072 Epoch  11 Step:     3900 Batch Loss:     2.147690 Tokens per Sec:    10913, Lr: 0.000354\n",
            "2019-10-13 14:27:13,587 Epoch  11: total training loss 783.87\n",
            "2019-10-13 14:27:13,587 EPOCH 12\n",
            "2019-10-13 14:27:41,118 Epoch  12 Step:     4000 Batch Loss:     2.164329 Tokens per Sec:     3668, Lr: 0.000349\n",
            "2019-10-13 14:31:45,557 Hooray! New best validation result [ppl]!\n",
            "2019-10-13 14:31:45,557 Saving new checkpoint.\n",
            "2019-10-13 14:31:47,098 Example #0\n",
            "2019-10-13 14:31:47,099 \tSource:     source_sentence\n",
            "2019-10-13 14:31:47,099 \tReference:  target_sentence\n",
            "2019-10-13 14:31:47,099 \tHypothesis: \n",
            "2019-10-13 14:31:47,099 Example #1\n",
            "2019-10-13 14:31:47,100 \tSource:     \"is the law, therefore, against the promises of god? certainly not! for if a law had been given that could give life, righteousness would actually have been by means of law.\"\n",
            "2019-10-13 14:31:47,100 \tReference:  ታዲያ ሕጉ የአምላክን የተስፋ ቃል ይጻረራል ማለት ነው? በጭራሽ! ሕይወት ሊያስገኝ የሚችል ሕግ ተሰጥቶ ቢሆን ኖሮ ጽድቅ የሚገኘው በሕግ አማካኝነት በሆነ ነበር።\n",
            "2019-10-13 14:31:47,100 \tHypothesis: ታዲያ በአምላክ ሕግ ላይ የተላለፈው ማን ነው? ሕጉ አብ መሆኑ ቢሆንም እንኳ ሕጉ በሕይወት መኖር ይችላል።\n",
            "2019-10-13 14:31:47,101 Example #2\n",
            "2019-10-13 14:31:47,101 \tSource:     \"but the scripture handed all things over to the custody of sin, so that the promise resulting from faith in jesus christ might be given to those exercising faith.\"\n",
            "2019-10-13 14:31:47,101 \tReference:  ሆኖም ቅዱስ መጽሐፉ ሁሉም ነገሮች የኃጢአት እስረኛ እንዲሆኑ አሳልፎ ሰጥቷቸዋል፤ ይህም የሆነው በኢየሱስ ክርስቶስ በማመን የሚገኘው የተስፋ ቃል ለሚያምኑ ሁሉ ይሰጥ ዘንድ ነው።\n",
            "2019-10-13 14:31:47,101 \tHypothesis: ሆኖም ኢየሱስ ክርስቶስ የሚመስለው ነገር ሁሉ የኃጢአት መሥራቱን ይቀበላል፤ ይህም የሆነው ኢየሱስ ክርስቶስ የሚመላለሱትን እምነት የሚጣልበት እምነት እንዲያስከትል ነው።\n",
            "2019-10-13 14:31:47,101 Example #3\n",
            "2019-10-13 14:31:47,101 \tSource:     \"however, before the faith arrived, we were being guarded under law, being handed over into custody, looking to the faith that was about to be revealed.\"\n",
            "2019-10-13 14:31:47,101 \tReference:  ይሁን እንጂ እምነት ከመምጣቱ በፊት፣ ሊገለጥ ያለውን እምነት እየተጠባበቅን በሕግ ጥበቃ ሥር እስረኞች እንድንሆን አልፈን ተሰጥተናል።\n",
            "2019-10-13 14:31:47,101 \tHypothesis: ይሁን እንጂ እኛ በእኛ ላይ ስለተነሳ ስለ እኛ የሚገልጸውን ነገር በተመለከተ ልዩ ልዩ ልዩ ልዩ ልዩ ልዩ ልዩ ልዩ ልዩ ልዩ ልዩ ልዩ ልዩ ልዩ ልዩ ልዩ ልዩ ነው።\n",
            "2019-10-13 14:31:47,101 Validation result at epoch  12, step     4000: bleu:   1.14, loss: 109588.0234, ppl:  24.6505, duration: 245.9835s\n",
            "2019-10-13 14:33:02,325 Epoch  12 Step:     4100 Batch Loss:     2.059304 Tokens per Sec:     5008, Lr: 0.000345\n",
            "2019-10-13 14:34:17,746 Epoch  12 Step:     4200 Batch Loss:     2.045001 Tokens per Sec:     8683, Lr: 0.000341\n",
            "2019-10-13 14:35:33,118 Epoch  12 Step:     4300 Batch Loss:     2.011956 Tokens per Sec:    12369, Lr: 0.000337\n",
            "2019-10-13 14:35:51,381 Epoch  12: total training loss 742.97\n",
            "2019-10-13 14:35:51,382 EPOCH 13\n",
            "2019-10-13 14:36:49,503 Epoch  13 Step:     4400 Batch Loss:     2.172949 Tokens per Sec:     3680, Lr: 0.000333\n",
            "2019-10-13 14:40:54,030 Hooray! New best validation result [ppl]!\n",
            "2019-10-13 14:40:54,030 Saving new checkpoint.\n",
            "2019-10-13 14:40:55,700 Example #0\n",
            "2019-10-13 14:40:55,700 \tSource:     source_sentence\n",
            "2019-10-13 14:40:55,700 \tReference:  target_sentence\n",
            "2019-10-13 14:40:55,700 \tHypothesis: በር ጠባቂውም 12ቱን\n",
            "2019-10-13 14:40:55,701 Example #1\n",
            "2019-10-13 14:40:55,701 \tSource:     \"is the law, therefore, against the promises of god? certainly not! for if a law had been given that could give life, righteousness would actually have been by means of law.\"\n",
            "2019-10-13 14:40:55,701 \tReference:  ታዲያ ሕጉ የአምላክን የተስፋ ቃል ይጻረራል ማለት ነው? በጭራሽ! ሕይወት ሊያስገኝ የሚችል ሕግ ተሰጥቶ ቢሆን ኖሮ ጽድቅ የሚገኘው በሕግ አማካኝነት በሆነ ነበር።\n",
            "2019-10-13 14:40:55,701 \tHypothesis: እንግዲህ አምላክ በሕግ ላይ የተመሠረተ ነገር አለ? ሕግ ባይሆን ኖሮ ኃጢአት መሥራቱን መፈጸም የሚችል ከሆነ ሕግን ለሕግ ተሰጠ።\n",
            "2019-10-13 14:40:55,701 Example #2\n",
            "2019-10-13 14:40:55,701 \tSource:     \"but the scripture handed all things over to the custody of sin, so that the promise resulting from faith in jesus christ might be given to those exercising faith.\"\n",
            "2019-10-13 14:40:55,701 \tReference:  ሆኖም ቅዱስ መጽሐፉ ሁሉም ነገሮች የኃጢአት እስረኛ እንዲሆኑ አሳልፎ ሰጥቷቸዋል፤ ይህም የሆነው በኢየሱስ ክርስቶስ በማመን የሚገኘው የተስፋ ቃል ለሚያምኑ ሁሉ ይሰጥ ዘንድ ነው።\n",
            "2019-10-13 14:40:55,702 \tHypothesis: ይሁንና ኢየሱስ እምነት የሚጣልበት ነገር ሁሉ የክርስቶስ ጸጋና የጽድቅ ቃል እንዲገለጥ ነው።\n",
            "2019-10-13 14:40:55,702 Example #3\n",
            "2019-10-13 14:40:55,702 \tSource:     \"however, before the faith arrived, we were being guarded under law, being handed over into custody, looking to the faith that was about to be revealed.\"\n",
            "2019-10-13 14:40:55,702 \tReference:  ይሁን እንጂ እምነት ከመምጣቱ በፊት፣ ሊገለጥ ያለውን እምነት እየተጠባበቅን በሕግ ጥበቃ ሥር እስረኞች እንድንሆን አልፈን ተሰጥተናል።\n",
            "2019-10-13 14:40:55,702 \tHypothesis: ይሁን እንጂ ኢየሱስ በእኛ ላይ እየተስፋፋ ነው፤ ንጉሡ ሊፈታው የሚችል አቅም በኃይል እንዲገለጥ በእሱ ዘንድ ተቀባይነት ተፈቅዶ ነበር።\n",
            "2019-10-13 14:40:55,702 Validation result at epoch  13, step     4400: bleu:   1.11, loss: 109201.2969, ppl:  24.3733, duration: 246.1993s\n",
            "2019-10-13 14:42:10,630 Epoch  13 Step:     4500 Batch Loss:     2.205527 Tokens per Sec:     6521, Lr: 0.000329\n",
            "2019-10-13 14:43:26,575 Epoch  13 Step:     4600 Batch Loss:     1.907866 Tokens per Sec:    10120, Lr: 0.000326\n",
            "2019-10-13 14:44:29,283 Epoch  13: total training loss 699.80\n",
            "2019-10-13 14:44:29,283 EPOCH 14\n",
            "2019-10-13 14:44:41,841 Epoch  14 Step:     4700 Batch Loss:     1.896798 Tokens per Sec:     3677, Lr: 0.000322\n",
            "2019-10-13 14:45:57,598 Epoch  14 Step:     4800 Batch Loss:     1.813917 Tokens per Sec:     4305, Lr: 0.000319\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wSyOBpksEOv",
        "colab_type": "code",
        "outputId": "9eac8632-7693-46cd-e31b-b83eece0b3fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "! cat joeynmt/models/enam_transformer/validations.txt"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 400\tLoss: 188883.89062\tPPL: 250.56749\tbleu: 0.00000\tLR: 0.00027951\t*\n",
            "Steps: 800\tLoss: 170874.82812\tPPL: 147.97906\tbleu: 0.00000\tLR: 0.00055902\t*\n",
            "Steps: 1200\tLoss: 145509.53125\tPPL: 70.47702\tbleu: 0.00000\tLR: 0.00063789\t*\n",
            "Steps: 1600\tLoss: 132005.65625\tPPL: 47.48334\tbleu: 0.16893\tLR: 0.00055243\t*\n",
            "Steps: 2000\tLoss: 124469.25000\tPPL: 38.09122\tbleu: 0.22400\tLR: 0.00049411\t*\n",
            "Steps: 2400\tLoss: 119074.07031\tPPL: 32.53144\tbleu: 0.28985\tLR: 0.00045105\t*\n",
            "Steps: 2800\tLoss: 114965.37500\tPPL: 28.84833\tbleu: 0.00000\tLR: 0.00041760\t*\n",
            "Steps: 3200\tLoss: 112569.37500\tPPL: 26.89616\tbleu: 0.65852\tLR: 0.00039063\t*\n",
            "Steps: 3600\tLoss: 110032.33594\tPPL: 24.97287\tbleu: 0.70298\tLR: 0.00036828\t*\n",
            "Steps: 4000\tLoss: 109588.02344\tPPL: 24.65049\tbleu: 1.14420\tLR: 0.00034939\t*\n",
            "Steps: 4400\tLoss: 109201.29688\tPPL: 24.37328\tbleu: 1.11332\tLR: 0.00033313\t*\n",
            "Steps: 4800\tLoss: 108250.94531\tPPL: 23.70522\tbleu: 1.69124\tLR: 0.00031894\t*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZJGEqJdnPsh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8bbd92e2-e440-4f2f-b3d0-73195a2ef915"
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "!mkdir \"$gdrive_path/models/\"\n",
        "!cp -r joeynmt/models/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot create symbolic link '/content/drive/My Drive/masakhane/en-am/models/enam_transformer/best.ckpt': Function not implemented\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDaExTv7KhWD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "d2541746-dc5f-4384-ed31-37a53fa4df58"
      },
      "source": [
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 400\tLoss: 188883.89062\tPPL: 250.56749\tbleu: 0.00000\tLR: 0.00027951\t*\n",
            "Steps: 800\tLoss: 170874.82812\tPPL: 147.97906\tbleu: 0.00000\tLR: 0.00055902\t*\n",
            "Steps: 1200\tLoss: 145509.53125\tPPL: 70.47702\tbleu: 0.00000\tLR: 0.00063789\t*\n",
            "Steps: 1600\tLoss: 132005.65625\tPPL: 47.48334\tbleu: 0.16893\tLR: 0.00055243\t*\n",
            "Steps: 2000\tLoss: 124469.25000\tPPL: 38.09122\tbleu: 0.22400\tLR: 0.00049411\t*\n",
            "Steps: 2400\tLoss: 119074.07031\tPPL: 32.53144\tbleu: 0.28985\tLR: 0.00045105\t*\n",
            "Steps: 2800\tLoss: 114965.37500\tPPL: 28.84833\tbleu: 0.00000\tLR: 0.00041760\t*\n",
            "Steps: 3200\tLoss: 112569.37500\tPPL: 26.89616\tbleu: 0.65852\tLR: 0.00039063\t*\n",
            "Steps: 3600\tLoss: 110032.33594\tPPL: 24.97287\tbleu: 0.70298\tLR: 0.00036828\t*\n",
            "Steps: 4000\tLoss: 109588.02344\tPPL: 24.65049\tbleu: 1.14420\tLR: 0.00034939\t*\n",
            "Steps: 4400\tLoss: 109201.29688\tPPL: 24.37328\tbleu: 1.11332\tLR: 0.00033313\t*\n",
            "Steps: 4800\tLoss: 108250.94531\tPPL: 23.70522\tbleu: 1.69124\tLR: 0.00031894\t*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrehNFeVwNzZ",
        "colab_type": "code",
        "outputId": "8a3dd3b7-4559-435b-d334-122a96c52ca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "! cd joeynmt; python3 -m joeynmt test models/enam_transformer/config.yaml\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-07 21:27:56,066 -  dev bleu:   1.89 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2019-10-07 21:31:13,308 - test bleu:   1.40 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zma-QOZiKmp0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e7d30548-1f68-4057-a334-d1b1bd85dd23"
      },
      "source": [
        "! cd joeynmt; python3 -m joeynmt test models/enam_transformer/config.yaml\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-13 15:11:17,887 -  dev bleu:   1.89 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2019-10-13 15:14:35,678 - test bleu:   1.40 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}