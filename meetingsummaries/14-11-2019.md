# Masakhane Non-Meeting Notes: 14/11/2019

There was no meeting so these were some highlights from the slack conversations.

## Updates from Ari:

@Alp. @Daniel Whitenack have you noticed any English sentences in your TI text ? I found quite a few in the Zulu JW300 corpus. I ran some LID on the zulu text and there are some contiguous chunks of text that have been identified as english. Will investigate further but just thought I should mention it here.

## Updates from Julia

Raw JW300 from Julia: 
Just got it! Here's the raw data: http://statmt.org/heafield/jw/ . The files are meant to be the same. content.json.zip is the original but dealing with zip files > 4 GB is often broken. He has attempted to extract it then gzip which is content.json.gz


## Updates from Daniel
(1) I was able to get in contact with SIL Senegal (http://silsenegal.org/en). They are currently spinning up some Wolof projects and are interested in helping on this front by evaluating some translations and/or providing resources. In particular, they have accessed to some transcribed radio programs and are working to clear any copyright concerns with those. @Elias W. BA @Thierno Ibrahima Diop @Daoudatandiang Djiba what is the status on your wolof experiments? Maybe I can help by integrating this radio transcript data? Or maybe there are other things I can help with in terms of connecting the work with SIL Senegal?
(2) To follow up on this issue: https://masakhane-nlp.slack.com/archives/CNWEXUB17/p1572298438106200, I'm working on integrating fuzzy wuzzy into my notebooks to remove "almost overlapping" sentences from the the training set. Once I test this to ensure it is working, I will PR a change to the starter notebook.
(3) @Alp regarding this: https://masakhane-nlp.slack.com/archives/CNWEXUB17/p1573644378000900, I think that dropping conflicting translations isn't always necessary. However, if that change makes a big difference for your training set size, you might run the risk of overfitting in general given the size of the data and the similarity in samples. But if that's all the data available, it might be unavoidable.
(4) @Julia I tested JoeyNMT on CoLab with the reverse example and a separate experiment I'm running for English-Indonesian. All seems to be going as expected. As such, I think the issue (2) mentioned above might be the source of the weird test/dev issues we have been discussing.
(5) @Jade Abbott One of my friends David Ojika (https://software.intel.com/en-us/blogs/2018/11/15/intel-software-innovator-david-ojika-bringing-ai-to-more-people) is working with several groups in Nigeria on AI-related things and hackathons. He is also working on a platform for AI inference. He has offered to help us get demo inference servers up and running for all the models that we end up getting into Masakhane. If that sounds interesting for you maybe we could work on a Streamlit app for the EMNLP submission/demo and/or for anyone to try out the models?
(6) I found this data for Bambara if anyone is interested: http://cormand.huma-num.fr/. I already went through the pain of extracting the data from their very weird UI. Let me know if this could be useful to anyone. If not, I might circle back around to that around finishing my other submissions
