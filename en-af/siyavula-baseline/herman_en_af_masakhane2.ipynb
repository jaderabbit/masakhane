{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "herman_en-af_masakhane2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Igc5itf-xMGj"
      },
      "source": [
        "# Masakhane - Machine Translation for African Languages (Using JoeyNMT)\n",
        "\n",
        "Languages: English-Afrikaans\n",
        "\n",
        "Author: Herman Kamper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l929HimrxS0a"
      },
      "source": [
        "## Retrieve data and make a parallel corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oGRmDELn7Az0",
        "outputId": "131f9768-2ed8-4653-c121-b81270a6ad4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cn3tgQLzUxwn",
        "colab": {}
      },
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\n",
        "import os\n",
        "source_language = \"en\"\n",
        "target_language = \"af\"\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = tag\n",
        "\n",
        "# This will save it to a folder in our gdrive instead!\n",
        "!mkdir -p \"/content/drive/My Drive/colab/masakhane/$src-$tgt-$tag\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/colab/masakhane/%s-%s-%s\" % (source_language, target_language, tag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kBSgJHEw7Nvx",
        "outputId": "308420d6-9e7b-42c4-8567-0c3c3037053c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!echo $gdrive_path"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/colab/masakhane/en-af-baseline\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xq-tDZVks7ZD",
        "outputId": "921fe965-5508-4873-ac20-6db0e5297413",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        }
      },
      "source": [
        "# Download the corpus\n",
        "! wget \"https://www.kamperh.com/data/siyavula_en_af.noweb.3.zip\"\n",
        "! unzip siyavula_en_af.noweb.3.zip\n",
        "! ls -lah\n",
        "! head -3 train.en\n",
        "! head -3 train.af\n",
        "! cat train.en | wc -l\n",
        "! cat train.af | wc -l"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-12 16:18:51--  https://www.kamperh.com/data/siyavula_en_af.noweb.3.zip\n",
            "Resolving www.kamperh.com (www.kamperh.com)... 185.199.110.153, 185.199.108.153, 185.199.109.153, ...\n",
            "Connecting to www.kamperh.com (www.kamperh.com)|185.199.110.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 303093 (296K) [application/zip]\n",
            "Saving to: ‘siyavula_en_af.noweb.3.zip’\n",
            "\n",
            "\r          siyavula_   0%[                    ]       0  --.-KB/s               \rsiyavula_en_af.nowe 100%[===================>] 295.99K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2019-10-12 16:18:51 (10.4 MB/s) - ‘siyavula_en_af.noweb.3.zip’ saved [303093/303093]\n",
            "\n",
            "Archive:  siyavula_en_af.noweb.3.zip\n",
            "  inflating: dev.af                  \n",
            "  inflating: dev.en                  \n",
            "  inflating: readme.md               \n",
            "  inflating: test.af                 \n",
            "  inflating: test.en                 \n",
            "  inflating: train.af                \n",
            "  inflating: train.en                \n",
            "total 1.4M\n",
            "drwxr-xr-x 1 root root 4.0K Oct 12 16:18 .\n",
            "drwxr-xr-x 1 root root 4.0K Oct 12 16:12 ..\n",
            "drwxr-xr-x 1 root root 4.0K Oct  8 20:06 .config\n",
            "-rw-rw-r-- 1 root root  29K Oct 11 11:50 dev.af\n",
            "-rw-rw-r-- 1 root root  28K Oct 11 11:50 dev.en\n",
            "drwx------ 3 root root 4.0K Oct 12 16:18 drive\n",
            "-rw-rw-r-- 1 root root  310 Oct 11 11:46 readme.md\n",
            "drwxr-xr-x 1 root root 4.0K Aug 27 16:17 sample_data\n",
            "-rw-r--r-- 1 root root 296K Oct 11 09:52 siyavula_en_af.noweb.3.zip\n",
            "-rw-rw-r-- 1 root root  33K Oct 11 11:50 test.af\n",
            "-rw-rw-r-- 1 root root  32K Oct 11 11:50 test.en\n",
            "-rw-rw-r-- 1 root root 461K Oct 11 11:50 train.af\n",
            "-rw-rw-r-- 1 root root 446K Oct 11 11:50 train.en\n",
            "how to introduce this topic\n",
            "remind them of the lessons in the last term of gr. 4 when they learnt about the earth sun moon and planets .\n",
            "use figure 1 to start them thinking about what is on the surface of the earth and under the surface of the earth .\n",
            "hoe om hierdie onderwerp bekend te stel\n",
            "herinner die leerders aan die lesse in die laaste kwartaal van graad 4 toe hulle van die aarde son maan en planete geleer het .\n",
            "gebruik figuur 1 om hulle aan die dink te kry oor wat op die oppervlakte van die aarde en onder die oppervlakte van die aarde is .\n",
            "6585\n",
            "6585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epeCydmCyS8X"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Installation of JoeyNMT\n",
        "\n",
        "JoeyNMT is a simple, minimalist NMT package which is useful for learning and teaching. Check out the documentation for JoeyNMT [here](https://joeynmt.readthedocs.io)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iBRMm4kMxZ8L",
        "outputId": "c7c4e308-02cc-4442-8979-5d0f44f98b02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'joeynmt'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/45)\u001b[K\rremote: Counting objects:   4% (2/45)\u001b[K\rremote: Counting objects:   6% (3/45)\u001b[K\rremote: Counting objects:   8% (4/45)\u001b[K\rremote: Counting objects:  11% (5/45)\u001b[K\rremote: Counting objects:  13% (6/45)\u001b[K\rremote: Counting objects:  15% (7/45)\u001b[K\rremote: Counting objects:  17% (8/45)\u001b[K\rremote: Counting objects:  20% (9/45)\u001b[K\rremote: Counting objects:  22% (10/45)\u001b[K\rremote: Counting objects:  24% (11/45)\u001b[K\rremote: Counting objects:  26% (12/45)\u001b[K\rremote: Counting objects:  28% (13/45)\u001b[K\rremote: Counting objects:  31% (14/45)\u001b[K\rremote: Counting objects:  33% (15/45)\u001b[K\rremote: Counting objects:  35% (16/45)\u001b[K\rremote: Counting objects:  37% (17/45)\u001b[K\rremote: Counting objects:  40% (18/45)\u001b[K\rremote: Counting objects:  42% (19/45)\u001b[K\rremote: Counting objects:  44% (20/45)\u001b[K\rremote: Counting objects:  46% (21/45)\u001b[K\rremote: Counting objects:  48% (22/45)\u001b[K\rremote: Counting objects:  51% (23/45)\u001b[K\rremote: Counting objects:  53% (24/45)\u001b[K\rremote: Counting objects:  55% (25/45)\u001b[K\rremote: Counting objects:  57% (26/45)\u001b[K\rremote: Counting objects:  60% (27/45)\u001b[K\rremote: Counting objects:  62% (28/45)\u001b[K\rremote: Counting objects:  64% (29/45)\u001b[K\rremote: Counting objects:  66% (30/45)\u001b[K\rremote: Counting objects:  68% (31/45)\u001b[K\rremote: Counting objects:  71% (32/45)\u001b[K\rremote: Counting objects:  73% (33/45)\u001b[K\rremote: Counting objects:  75% (34/45)\u001b[K\rremote: Counting objects:  77% (35/45)\u001b[K\rremote: Counting objects:  80% (36/45)\u001b[K\rremote: Counting objects:  82% (37/45)\u001b[K\rremote: Counting objects:  84% (38/45)\u001b[K\rremote: Counting objects:  86% (39/45)\u001b[K\rremote: Counting objects:  88% (40/45)\u001b[K\rremote: Counting objects:  91% (41/45)\u001b[K\rremote: Counting objects:  93% (42/45)\u001b[K\rremote: Counting objects:  95% (43/45)\u001b[K\rremote: Counting objects:  97% (44/45)\u001b[K\rremote: Counting objects: 100% (45/45)\u001b[K\rremote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 2051 (delta 25), reused 22 (delta 10), pack-reused 2006\u001b[K\n",
            "Receiving objects: 100% (2051/2051), 2.39 MiB | 7.13 MiB/s, done.\n",
            "Resolving deltas: 100% (1414/1414), done.\n",
            "Processing /content/joeynmt\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (4.3.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.16.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (41.2.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.2.0)\n",
            "Requirement already satisfied: tensorflow>=1.14 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.15.0rc3)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.3.1)\n",
            "Collecting sacrebleu>=1.3.6 (from joeynmt==0.0.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/0e/e5/93d252182f7cbd4b59bb3ec5797e2ce33cfd6f5aadaf327db170cf4b7887/sacrebleu-1.4.2-py3-none-any.whl\n",
            "Collecting subword-nmt (from joeynmt==0.0.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/26/08/58267cb3ac00f5f895457777ed9e0d106dbb5e6388fa7923d8663b04b849/subword_nmt-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (3.0.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.9.0)\n",
            "Collecting pyyaml>=5.1 (from joeynmt==0.0.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/e8/b3212641ee2718d556df0f23f78de8303f068fe29cdaa7a91018849582fe/PyYAML-5.1.2.tar.gz (265kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 13.1MB/s \n",
            "\u001b[?25hCollecting pylint (from joeynmt==0.0.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/ed/1cb8e7b85a31807aa0bff8b3e60935370bed7e141df8b530aac6352bddff/pylint-2.4.2-py3-none-any.whl (302kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 48.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->joeynmt==0.0.1) (0.46)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.33.6)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.7.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.1.7)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.2.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (2.21.0)\n",
            "Collecting portalocker (from sacrebleu>=1.3.6->joeynmt==0.0.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/60/ec/836a494dbaa72541f691ec4e66f29fdc8db9bcc7f49e1c2d457ba13ced42/portalocker-1.5.1-py2.py3-none-any.whl\n",
            "Collecting typing (from sacrebleu>=1.3.6->joeynmt==0.0.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/2e/b480ee1b75e6d17d2993738670e75c1feeb9ff7f64452153cf018051cc92/typing-3.7.4.1-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.4.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: pandas>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt==0.0.1) (0.24.2)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt==0.0.1) (1.3.1)\n",
            "Collecting mccabe<0.7,>=0.6 (from pylint->joeynmt==0.0.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Collecting astroid<2.4,>=2.3.0 (from pylint->joeynmt==0.0.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/e1/74a63c85c501c29c52da5be604c025e368f4dd77daf1fa13c878a33e5a36/astroid-2.3.1-py3-none-any.whl (205kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 44.8MB/s \n",
            "\u001b[?25hCollecting isort<5,>=4.2.5 (from pylint->joeynmt==0.0.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/b0/c121fd1fa3419ea9bfd55c7f9c4fedfec5143208d8c7ad3ce3db6c623c21/isort-4.3.21-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 24.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow>=1.14->joeynmt==0.0.1) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14->joeynmt==0.0.1) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14->joeynmt==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2019.9.11)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn->joeynmt==0.0.1) (2018.9)\n",
            "Collecting lazy-object-proxy==1.4.* (from astroid<2.4,>=2.3.0->pylint->joeynmt==0.0.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/26/534a6d32572a9dbca11619321535c0a7ab34688545d9d67c2c204b9e3a3d/lazy_object_proxy-1.4.2-cp36-cp36m-manylinux1_x86_64.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 22.6MB/s \n",
            "\u001b[?25hCollecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\" (from astroid<2.4,>=2.3.0->pylint->joeynmt==0.0.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/d3/9d1802c161626d0278bafb1ffb32f76b9d01e123881bbf9d91e8ccf28e18/typed_ast-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (736kB)\n",
            "\u001b[K     |████████████████████████████████| 737kB 38.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: joeynmt, pyyaml\n",
            "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for joeynmt: filename=joeynmt-0.0.1-cp36-none-any.whl size=69430 sha256=e80fdd8d60859a568c4e29185b2dffbab8de618ba277519ea4e9ee6cf9ad82f8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-66sof5ap/wheels/db/01/db/751cc9f3e7f6faec127c43644ba250a3ea7ad200594aeda70a\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1.2-cp36-cp36m-linux_x86_64.whl size=44104 sha256=91873ab41a8b5d7b3425c653472403fe743eb691da320eb2934983501932f764\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/45/dd/65f0b38450c47cf7e5312883deb97d065e030c5cca0a365030\n",
            "Successfully built joeynmt pyyaml\n",
            "Installing collected packages: portalocker, typing, sacrebleu, subword-nmt, pyyaml, mccabe, lazy-object-proxy, typed-ast, astroid, isort, pylint, joeynmt\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed astroid-2.3.1 isort-4.3.21 joeynmt-0.0.1 lazy-object-proxy-1.4.2 mccabe-0.6.1 portalocker-1.5.1 pylint-2.4.2 pyyaml-5.1.2 sacrebleu-1.4.2 subword-nmt-0.3.6 typed-ast-1.4.0 typing-3.7.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AaE77Tcppex9"
      },
      "source": [
        "# Preprocessing the Data into Subword BPE Tokens\n",
        "\n",
        "- One of the most powerful improvements for agglutinative languages (a feature of most Bantu languages) is using BPE tokenization [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n",
        "\n",
        "- It was also shown that by optimizing the umber of BPE codes we significantly improve results for low-resourced languages [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021) [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)\n",
        "\n",
        "- Below we have the scripts for doing BPE tokenization of our data. We use 4000 tokens as recommended by [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021). You do not need to change anything. Simply running the below will be suitable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H-TyjtmXB1mL",
        "outputId": "7aa360b6-344f-4467-b966-c34f0fe5fd51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
        "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
        "\n",
        "# Do subword NMT\n",
        "from os import path\n",
        "\n",
        "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
        "\n",
        "# Create directory, move everyone we care about to the correct location\n",
        "! mkdir -p $data_path\n",
        "! cp train.* $data_path\n",
        "! cp test.* $data_path\n",
        "! cp dev.* $data_path\n",
        "! cp bpe.codes.4000 $data_path\n",
        "! ls $data_path\n",
        "\n",
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\"\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# Some output\n",
        "! echo \"BPE Afrikaans Sentences\"\n",
        "! tail -n 5 test.bpe.$tgt\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! tail -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.bpe.en  test.bpe.af  train.af      train.en\n",
            "dev.af\t\tdev.en\t    test.bpe.en  train.bpe.af\n",
            "dev.bpe.af\ttest.af     test.en\t train.bpe.en\n",
            "bpe.codes.4000\tdev.bpe.en  test.af\t test.en       train.bpe.en\n",
            "dev.af\t\tdev.en\t    test.bpe.af  train.af      train.en\n",
            "dev.bpe.af\tmodels\t    test.bpe.en  train.bpe.af\n",
            "BPE Afrikaans Sentences\n",
            "wat is 'n on@@ we@@ t@@ tige elektriese skak@@ el@@ ings ?\n",
            "hoe dink jy kan die plaas@@ like re@@ ger@@ ing dit keer of die hoeveelheid on@@ we@@ t@@ tige skak@@ el@@ ings ver@@ minder .\n",
            "'n on@@ we@@ t@@ tige skak@@ eling is wanneer ie@@ mand toe@@ gan@@ g kry tot elektrisiteit deur 'n kra@@ gl@@ yn te sny en 'n ander l@@ yn daaraan te verbind sonder om daar@@ voor te be@@ taal .\n",
            "die plaas@@ like re@@ ger@@ ing kan dit probeer stop deur eer@@ st@@ ens te probeer om die ar@@ mer geb@@ ie@@ de met genoeg elektriese toe@@ g@@ ang@@ sp@@ unte te voorsien rond te gaan en te kyk of daar ge@@ vaar@@ like skak@@ el@@ ings is be@@ w@@ us@@ theid oor die gev@@ are van on@@ we@@ t@@ tige skak@@ el@@ ings te verbeter deur ad@@ ver@@ ten@@ sie@@ bor@@ de radi@@ o die ko@@ er@@ ant ens .\n",
            "asses@@ seer enige ander rele@@ van@@ te antwoorde wat die leerder mag h&#234; .\n",
            "Combined BPE Vocab\n",
            "sour@@\n",
            "oorspron@@\n",
            "desc@@\n",
            "unti@@\n",
            "lay@@\n",
            "ft\n",
            "prob@@\n",
            "ingsge@@\n",
            "werp@@\n",
            "youn@@\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IlMitUHR8Qy-",
        "outputId": "1b9fe5f0-17e1-4364-b595-a250f30e5c31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.bpe.en  test.af\t test.en       train.bpe.en\n",
            "dev.af\t\tdev.en\t    test.bpe.af  train.af      train.en\n",
            "dev.bpe.af\tmodels\t    test.bpe.en  train.bpe.af\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ixmzi60WsUZ8"
      },
      "source": [
        "# Creating the JoeyNMT Config\n",
        "\n",
        "JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n",
        "\n",
        "- We used Transformer architecture \n",
        "- We set our dropout to reasonably high: 0.3 (recommended in  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
        "\n",
        "Things worth playing with:\n",
        "- The batch size (also recommended to change for low-resourced languages)\n",
        "- The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n",
        "- The decoder options (beam_size, alpha)\n",
        "- Evaluation metrics (BLEU versus Crhf4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PIs1lY2hxMsl",
        "colab": {}
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/train.bpe\"\n",
        "    dev:   \"data/{name}/dev.bpe\"\n",
        "    test:  \"data/{name}/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"noam\"            # Try switching from plateau to Noam scheduling\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    patience: 8\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0002\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 8192 # 4096  # Herman\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 1000 # 3600  # Herman\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"eval_metric\"  # \"ppl\"  # Herman\n",
        "    epochs: 200 # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 500 # 4000 # Decrease this for testing  # Herman\n",
        "    logging_freq: 50 # 100  # Herman\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: True\n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 8\n",
        "        embeddings:\n",
        "            embedding_dim: 512\n",
        "            scale: True\n",
        "            dropout: 0.\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 512\n",
        "        ff_size: 2048\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 8\n",
        "        embeddings:\n",
        "            embedding_dim: 512\n",
        "            scale: True\n",
        "            dropout: 0.\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 512\n",
        "        ff_size: 2048\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pIifxE3Qzuvs"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "This single line of joeynmt runs the training using the config we made above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6ZBPFwT94WpI",
        "outputId": "47c41c30-ad35-4836-be7c-d4b9e6c92102",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the model\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-12 16:20:48,522 Hello! This is Joey-NMT.\n",
            "2019-10-12 16:20:50,197 Total params: 46140416\n",
            "2019-10-12 16:20:50,198 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']\n",
            "2019-10-12 16:20:55,621 cfg.name                           : enaf_transformer\n",
            "2019-10-12 16:20:55,622 cfg.data.src                       : en\n",
            "2019-10-12 16:20:55,622 cfg.data.trg                       : af\n",
            "2019-10-12 16:20:55,622 cfg.data.train                     : data/enaf/train.bpe\n",
            "2019-10-12 16:20:55,622 cfg.data.dev                       : data/enaf/dev.bpe\n",
            "2019-10-12 16:20:55,622 cfg.data.test                      : data/enaf/test.bpe\n",
            "2019-10-12 16:20:55,622 cfg.data.level                     : bpe\n",
            "2019-10-12 16:20:55,622 cfg.data.lowercase                 : False\n",
            "2019-10-12 16:20:55,622 cfg.data.max_sent_length           : 100\n",
            "2019-10-12 16:20:55,622 cfg.data.src_vocab                 : data/enaf/vocab.txt\n",
            "2019-10-12 16:20:55,622 cfg.data.trg_vocab                 : data/enaf/vocab.txt\n",
            "2019-10-12 16:20:55,622 cfg.testing.beam_size              : 5\n",
            "2019-10-12 16:20:55,622 cfg.testing.alpha                  : 1.0\n",
            "2019-10-12 16:20:55,623 cfg.training.random_seed           : 42\n",
            "2019-10-12 16:20:55,623 cfg.training.optimizer             : adam\n",
            "2019-10-12 16:20:55,623 cfg.training.normalization         : tokens\n",
            "2019-10-12 16:20:55,623 cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2019-10-12 16:20:55,623 cfg.training.scheduling            : noam\n",
            "2019-10-12 16:20:55,623 cfg.training.learning_rate_factor  : 0.5\n",
            "2019-10-12 16:20:55,623 cfg.training.learning_rate_warmup  : 1000\n",
            "2019-10-12 16:20:55,623 cfg.training.patience              : 8\n",
            "2019-10-12 16:20:55,623 cfg.training.decrease_factor       : 0.7\n",
            "2019-10-12 16:20:55,623 cfg.training.loss                  : crossentropy\n",
            "2019-10-12 16:20:55,623 cfg.training.learning_rate         : 0.0002\n",
            "2019-10-12 16:20:55,623 cfg.training.learning_rate_min     : 1e-08\n",
            "2019-10-12 16:20:55,623 cfg.training.weight_decay          : 0.0\n",
            "2019-10-12 16:20:55,623 cfg.training.label_smoothing       : 0.1\n",
            "2019-10-12 16:20:55,624 cfg.training.batch_size            : 8192\n",
            "2019-10-12 16:20:55,624 cfg.training.batch_type            : token\n",
            "2019-10-12 16:20:55,624 cfg.training.eval_batch_size       : 1000\n",
            "2019-10-12 16:20:55,624 cfg.training.eval_batch_type       : token\n",
            "2019-10-12 16:20:55,624 cfg.training.batch_multiplier      : 1\n",
            "2019-10-12 16:20:55,624 cfg.training.early_stopping_metric : eval_metric\n",
            "2019-10-12 16:20:55,624 cfg.training.epochs                : 400\n",
            "2019-10-12 16:20:55,624 cfg.training.validation_freq       : 500\n",
            "2019-10-12 16:20:55,624 cfg.training.logging_freq          : 50\n",
            "2019-10-12 16:20:55,624 cfg.training.eval_metric           : bleu\n",
            "2019-10-12 16:20:55,624 cfg.training.model_dir             : models/enaf_transformer\n",
            "2019-10-12 16:20:55,624 cfg.training.overwrite             : True\n",
            "2019-10-12 16:20:55,624 cfg.training.shuffle               : True\n",
            "2019-10-12 16:20:55,624 cfg.training.use_cuda              : True\n",
            "2019-10-12 16:20:55,624 cfg.training.max_output_length     : 100\n",
            "2019-10-12 16:20:55,624 cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2019-10-12 16:20:55,624 cfg.training.keep_last_ckpts       : 3\n",
            "2019-10-12 16:20:55,625 cfg.model.initializer              : xavier\n",
            "2019-10-12 16:20:55,625 cfg.model.bias_initializer         : zeros\n",
            "2019-10-12 16:20:55,625 cfg.model.init_gain                : 1.0\n",
            "2019-10-12 16:20:55,625 cfg.model.embed_initializer        : xavier\n",
            "2019-10-12 16:20:55,625 cfg.model.embed_init_gain          : 1.0\n",
            "2019-10-12 16:20:55,625 cfg.model.tied_embeddings          : True\n",
            "2019-10-12 16:20:55,625 cfg.model.tied_softmax             : True\n",
            "2019-10-12 16:20:55,625 cfg.model.encoder.type             : transformer\n",
            "2019-10-12 16:20:55,625 cfg.model.encoder.num_layers       : 6\n",
            "2019-10-12 16:20:55,625 cfg.model.encoder.num_heads        : 8\n",
            "2019-10-12 16:20:55,625 cfg.model.encoder.embeddings.embedding_dim : 512\n",
            "2019-10-12 16:20:55,625 cfg.model.encoder.embeddings.scale : True\n",
            "2019-10-12 16:20:55,625 cfg.model.encoder.embeddings.dropout : 0.0\n",
            "2019-10-12 16:20:55,625 cfg.model.encoder.hidden_size      : 512\n",
            "2019-10-12 16:20:55,625 cfg.model.encoder.ff_size          : 2048\n",
            "2019-10-12 16:20:55,625 cfg.model.encoder.dropout          : 0.3\n",
            "2019-10-12 16:20:55,626 cfg.model.decoder.type             : transformer\n",
            "2019-10-12 16:20:55,626 cfg.model.decoder.num_layers       : 6\n",
            "2019-10-12 16:20:55,626 cfg.model.decoder.num_heads        : 8\n",
            "2019-10-12 16:20:55,626 cfg.model.decoder.embeddings.embedding_dim : 512\n",
            "2019-10-12 16:20:55,626 cfg.model.decoder.embeddings.scale : True\n",
            "2019-10-12 16:20:55,626 cfg.model.decoder.embeddings.dropout : 0.0\n",
            "2019-10-12 16:20:55,626 cfg.model.decoder.hidden_size      : 512\n",
            "2019-10-12 16:20:55,626 cfg.model.decoder.ff_size          : 2048\n",
            "2019-10-12 16:20:55,626 cfg.model.decoder.dropout          : 0.3\n",
            "2019-10-12 16:20:55,626 Data set sizes: \n",
            "\ttrain 6582,\n",
            "\tvalid 400,\n",
            "\ttest 400\n",
            "2019-10-12 16:20:55,626 First training example:\n",
            "\t[SRC] how to introduce this topic\n",
            "\t[TRG] hoe om hierdie onderwerp bekend te stel\n",
            "2019-10-12 16:20:55,626 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) die (6) the (7) is (8) in (9) of\n",
            "2019-10-12 16:20:55,627 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) die (6) the (7) is (8) in (9) of\n",
            "2019-10-12 16:20:55,627 Number of Src words (types): 3906\n",
            "2019-10-12 16:20:55,627 Number of Trg words (types): 3906\n",
            "2019-10-12 16:20:55,628 Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=8),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=8),\n",
            "\tsrc_embed=Embeddings(embedding_dim=512, vocab_size=3906),\n",
            "\ttrg_embed=Embeddings(embedding_dim=512, vocab_size=3906))\n",
            "2019-10-12 16:20:55,631 EPOCH 1\n",
            "2019-10-12 16:21:39,850 Epoch   1: total training loss 243.81\n",
            "2019-10-12 16:21:39,850 EPOCH 2\n",
            "2019-10-12 16:21:53,452 Epoch   2 Step:       50 Batch Loss:     6.284245 Tokens per Sec:     2779, Lr: 0.000035\n",
            "2019-10-12 16:22:24,156 Epoch   2: total training loss 224.23\n",
            "2019-10-12 16:22:24,157 EPOCH 3\n",
            "2019-10-12 16:22:50,943 Epoch   3 Step:      100 Batch Loss:     5.503494 Tokens per Sec:     2412, Lr: 0.000070\n",
            "2019-10-12 16:23:08,807 Epoch   3: total training loss 211.34\n",
            "2019-10-12 16:23:08,807 EPOCH 4\n",
            "2019-10-12 16:23:50,057 Epoch   4 Step:      150 Batch Loss:     5.099205 Tokens per Sec:     2574, Lr: 0.000105\n",
            "2019-10-12 16:23:52,491 Epoch   4: total training loss 190.40\n",
            "2019-10-12 16:23:52,492 EPOCH 5\n",
            "2019-10-12 16:24:36,788 Epoch   5: total training loss 192.08\n",
            "2019-10-12 16:24:36,788 EPOCH 6\n",
            "2019-10-12 16:24:48,192 Epoch   6 Step:      200 Batch Loss:     4.973370 Tokens per Sec:     2476, Lr: 0.000140\n",
            "2019-10-12 16:25:20,339 Epoch   6: total training loss 181.75\n",
            "2019-10-12 16:25:20,339 EPOCH 7\n",
            "2019-10-12 16:25:46,001 Epoch   7 Step:      250 Batch Loss:     4.832087 Tokens per Sec:     2339, Lr: 0.000175\n",
            "2019-10-12 16:26:04,520 Epoch   7: total training loss 183.76\n",
            "2019-10-12 16:26:04,521 EPOCH 8\n",
            "2019-10-12 16:26:45,388 Epoch   8 Step:      300 Batch Loss:     4.528731 Tokens per Sec:     2553, Lr: 0.000210\n",
            "2019-10-12 16:26:48,898 Epoch   8: total training loss 178.99\n",
            "2019-10-12 16:26:48,898 EPOCH 9\n",
            "2019-10-12 16:27:33,226 Epoch   9: total training loss 171.56\n",
            "2019-10-12 16:27:33,226 EPOCH 10\n",
            "2019-10-12 16:27:43,622 Epoch  10 Step:      350 Batch Loss:     3.306069 Tokens per Sec:     2455, Lr: 0.000245\n",
            "2019-10-12 16:28:17,078 Epoch  10: total training loss 165.06\n",
            "2019-10-12 16:28:17,079 EPOCH 11\n",
            "2019-10-12 16:28:40,452 Epoch  11 Step:      400 Batch Loss:     4.233754 Tokens per Sec:     2433, Lr: 0.000280\n",
            "2019-10-12 16:29:01,942 Epoch  11: total training loss 163.78\n",
            "2019-10-12 16:29:01,942 EPOCH 12\n",
            "2019-10-12 16:29:38,872 Epoch  12 Step:      450 Batch Loss:     3.916130 Tokens per Sec:     2539, Lr: 0.000314\n",
            "2019-10-12 16:29:46,164 Epoch  12: total training loss 153.07\n",
            "2019-10-12 16:29:46,164 EPOCH 13\n",
            "2019-10-12 16:30:30,041 Epoch  13: total training loss 147.70\n",
            "2019-10-12 16:30:30,042 EPOCH 14\n",
            "2019-10-12 16:30:37,160 Epoch  14 Step:      500 Batch Loss:     4.652871 Tokens per Sec:     2145, Lr: 0.000349\n",
            "2019-10-12 16:32:20,413 Hooray! New best validation result [eval_metric]!\n",
            "2019-10-12 16:32:20,413 Saving new checkpoint.\n",
            "2019-10-12 16:32:21,993 Example #0\n",
            "2019-10-12 16:32:21,993 \tSource:     resources about water\n",
            "2019-10-12 16:32:21,993 \tReference:  hulpbronne oor water\n",
            "2019-10-12 16:32:21,993 \tHypothesis: materiale\n",
            "2019-10-12 16:32:21,993 Example #1\n",
            "2019-10-12 16:32:21,993 \tSource:     what does it mean to purify water ?\n",
            "2019-10-12 16:32:21,994 \tReference:  wat beteken dit om water te suiwer ?\n",
            "2019-10-12 16:32:21,994 \tHypothesis: wat het dit ?\n",
            "2019-10-12 16:32:21,994 Example #2\n",
            "2019-10-12 16:32:21,994 \tSource:     it means to clean water; to remove pollutants from the water .\n",
            "2019-10-12 16:32:21,994 \tReference:  om water te suiwer beteken om besoedeling uit water te verwyder .\n",
            "2019-10-12 16:32:21,994 \tHypothesis: dit is baie baie baie baie baie baie baie baie energie .\n",
            "2019-10-12 16:32:21,994 Example #3\n",
            "2019-10-12 16:32:21,995 \tSource:     what is clean water ?\n",
            "2019-10-12 16:32:21,995 \tReference:  wat is skoon water ?\n",
            "2019-10-12 16:32:21,995 \tHypothesis: wat is 'n mengsel ?\n",
            "2019-10-12 16:32:21,995 Validation result at epoch  14, step      500: bleu:   0.74, loss: 28996.0254, ppl:  65.3205, duration: 104.8343s\n",
            "2019-10-12 16:32:58,850 Epoch  14: total training loss 143.06\n",
            "2019-10-12 16:32:58,851 EPOCH 15\n",
            "2019-10-12 16:33:18,962 Epoch  15 Step:      550 Batch Loss:     3.723561 Tokens per Sec:     2332, Lr: 0.000384\n",
            "2019-10-12 16:33:42,964 Epoch  15: total training loss 139.90\n",
            "2019-10-12 16:33:42,964 EPOCH 16\n",
            "2019-10-12 16:34:18,630 Epoch  16 Step:      600 Batch Loss:     2.977489 Tokens per Sec:     2532, Lr: 0.000419\n",
            "2019-10-12 16:34:27,485 Epoch  16: total training loss 130.22\n",
            "2019-10-12 16:34:27,485 EPOCH 17\n",
            "2019-10-12 16:35:11,491 Epoch  17: total training loss 123.59\n",
            "2019-10-12 16:35:11,491 EPOCH 18\n",
            "2019-10-12 16:35:16,453 Epoch  18 Step:      650 Batch Loss:     2.841545 Tokens per Sec:     2592, Lr: 0.000454\n",
            "2019-10-12 16:35:55,895 Epoch  18: total training loss 119.31\n",
            "2019-10-12 16:35:55,895 EPOCH 19\n",
            "2019-10-12 16:36:15,184 Epoch  19 Step:      700 Batch Loss:     3.136394 Tokens per Sec:     2709, Lr: 0.000489\n",
            "2019-10-12 16:36:39,547 Epoch  19: total training loss 110.25\n",
            "2019-10-12 16:36:39,548 EPOCH 20\n",
            "2019-10-12 16:37:14,046 Epoch  20 Step:      750 Batch Loss:     2.843098 Tokens per Sec:     2545, Lr: 0.000524\n",
            "2019-10-12 16:37:23,538 Epoch  20: total training loss 105.24\n",
            "2019-10-12 16:37:23,538 EPOCH 21\n",
            "2019-10-12 16:38:07,420 Epoch  21: total training loss 99.54\n",
            "2019-10-12 16:38:07,420 EPOCH 22\n",
            "2019-10-12 16:38:10,644 Epoch  22 Step:      800 Batch Loss:     2.183887 Tokens per Sec:     2725, Lr: 0.000559\n",
            "2019-10-12 16:38:50,893 Epoch  22: total training loss 92.74\n",
            "2019-10-12 16:38:50,894 EPOCH 23\n",
            "2019-10-12 16:39:08,508 Epoch  23 Step:      850 Batch Loss:     2.824461 Tokens per Sec:     2708, Lr: 0.000594\n",
            "2019-10-12 16:39:34,834 Epoch  23: total training loss 86.66\n",
            "2019-10-12 16:39:34,834 EPOCH 24\n",
            "2019-10-12 16:40:05,835 Epoch  24 Step:      900 Batch Loss:     2.684372 Tokens per Sec:     2658, Lr: 0.000629\n",
            "2019-10-12 16:40:18,477 Epoch  24: total training loss 83.95\n",
            "2019-10-12 16:40:18,477 EPOCH 25\n",
            "2019-10-12 16:41:03,082 Epoch  25 Step:      950 Batch Loss:     2.335960 Tokens per Sec:     2538, Lr: 0.000664\n",
            "2019-10-12 16:41:03,083 Epoch  25: total training loss 80.38\n",
            "2019-10-12 16:41:03,083 EPOCH 26\n",
            "2019-10-12 16:41:47,388 Epoch  26: total training loss 82.31\n",
            "2019-10-12 16:41:47,388 EPOCH 27\n",
            "2019-10-12 16:42:00,122 Epoch  27 Step:     1000 Batch Loss:     2.434019 Tokens per Sec:     2387, Lr: 0.000699\n",
            "2019-10-12 16:43:43,052 Hooray! New best validation result [eval_metric]!\n",
            "2019-10-12 16:43:43,052 Saving new checkpoint.\n",
            "2019-10-12 16:43:44,565 Example #0\n",
            "2019-10-12 16:43:44,565 \tSource:     resources about water\n",
            "2019-10-12 16:43:44,565 \tReference:  hulpbronne oor water\n",
            "2019-10-12 16:43:44,565 \tHypothesis: bronne van water\n",
            "2019-10-12 16:43:44,565 Example #1\n",
            "2019-10-12 16:43:44,566 \tSource:     what does it mean to purify water ?\n",
            "2019-10-12 16:43:44,566 \tReference:  wat beteken dit om water te suiwer ?\n",
            "2019-10-12 16:43:44,566 \tHypothesis: wat beteken dit om water te suiwer ?\n",
            "2019-10-12 16:43:44,566 Example #2\n",
            "2019-10-12 16:43:44,566 \tSource:     it means to clean water; to remove pollutants from the water .\n",
            "2019-10-12 16:43:44,566 \tReference:  om water te suiwer beteken om besoedeling uit water te verwyder .\n",
            "2019-10-12 16:43:44,566 \tHypothesis: dit beteken om skoon skoon skoon water te suiwer .\n",
            "2019-10-12 16:43:44,566 Example #3\n",
            "2019-10-12 16:43:44,566 \tSource:     what is clean water ?\n",
            "2019-10-12 16:43:44,567 \tReference:  wat is skoon water ?\n",
            "2019-10-12 16:43:44,567 \tHypothesis: wat is skoon skoon skoon ?\n",
            "2019-10-12 16:43:44,567 Validation result at epoch  27, step     1000: bleu:  12.16, loss: 22725.3184, ppl:  26.4561, duration: 104.4439s\n",
            "2019-10-12 16:44:16,307 Epoch  27: total training loss 73.07\n",
            "2019-10-12 16:44:16,307 EPOCH 28\n",
            "2019-10-12 16:44:42,337 Epoch  28 Step:     1050 Batch Loss:     1.411839 Tokens per Sec:     2385, Lr: 0.000682\n",
            "2019-10-12 16:45:00,784 Epoch  28: total training loss 66.75\n",
            "2019-10-12 16:45:00,784 EPOCH 29\n",
            "2019-10-12 16:45:41,459 Epoch  29 Step:     1100 Batch Loss:     0.512128 Tokens per Sec:     2535, Lr: 0.000666\n",
            "2019-10-12 16:45:44,723 Epoch  29: total training loss 60.52\n",
            "2019-10-12 16:45:44,723 EPOCH 30\n",
            "2019-10-12 16:46:28,485 Epoch  30: total training loss 56.05\n",
            "2019-10-12 16:46:28,485 EPOCH 31\n",
            "2019-10-12 16:46:38,791 Epoch  31 Step:     1150 Batch Loss:     1.653119 Tokens per Sec:     2441, Lr: 0.000652\n",
            "2019-10-12 16:47:12,254 Epoch  31: total training loss 52.82\n",
            "2019-10-12 16:47:12,254 EPOCH 32\n",
            "2019-10-12 16:47:36,239 Epoch  32 Step:     1200 Batch Loss:     2.135983 Tokens per Sec:     2574, Lr: 0.000638\n",
            "2019-10-12 16:47:56,289 Epoch  32: total training loss 50.90\n",
            "2019-10-12 16:47:56,289 EPOCH 33\n",
            "2019-10-12 16:48:34,983 Epoch  33 Step:     1250 Batch Loss:     1.599838 Tokens per Sec:     2591, Lr: 0.000625\n",
            "2019-10-12 16:48:40,612 Epoch  33: total training loss 46.43\n",
            "2019-10-12 16:48:40,612 EPOCH 34\n",
            "2019-10-12 16:49:25,061 Epoch  34: total training loss 43.44\n",
            "2019-10-12 16:49:25,062 EPOCH 35\n",
            "2019-10-12 16:49:34,099 Epoch  35 Step:     1300 Batch Loss:     1.549213 Tokens per Sec:     3061, Lr: 0.000613\n",
            "2019-10-12 16:50:08,647 Epoch  35: total training loss 40.26\n",
            "2019-10-12 16:50:08,647 EPOCH 36\n",
            "2019-10-12 16:50:32,479 Epoch  36 Step:     1350 Batch Loss:     0.857685 Tokens per Sec:     2704, Lr: 0.000601\n",
            "2019-10-12 16:50:53,141 Epoch  36: total training loss 39.70\n",
            "2019-10-12 16:50:53,141 EPOCH 37\n",
            "2019-10-12 16:51:29,822 Epoch  37 Step:     1400 Batch Loss:     0.373730 Tokens per Sec:     2615, Lr: 0.000591\n",
            "2019-10-12 16:51:37,708 Epoch  37: total training loss 37.27\n",
            "2019-10-12 16:51:37,709 EPOCH 38\n",
            "2019-10-12 16:52:22,170 Epoch  38: total training loss 33.74\n",
            "2019-10-12 16:52:22,171 EPOCH 39\n",
            "2019-10-12 16:52:27,700 Epoch  39 Step:     1450 Batch Loss:     0.598258 Tokens per Sec:     2526, Lr: 0.000580\n",
            "2019-10-12 16:53:06,218 Epoch  39: total training loss 31.06\n",
            "2019-10-12 16:53:06,219 EPOCH 40\n",
            "2019-10-12 16:53:26,503 Epoch  40 Step:     1500 Batch Loss:     1.455330 Tokens per Sec:     2768, Lr: 0.000571\n",
            "2019-10-12 16:55:09,366 Hooray! New best validation result [eval_metric]!\n",
            "2019-10-12 16:55:09,367 Saving new checkpoint.\n",
            "2019-10-12 16:55:10,947 Example #0\n",
            "2019-10-12 16:55:10,948 \tSource:     resources about water\n",
            "2019-10-12 16:55:10,948 \tReference:  hulpbronne oor water\n",
            "2019-10-12 16:55:10,948 \tHypothesis: bronne van water\n",
            "2019-10-12 16:55:10,948 Example #1\n",
            "2019-10-12 16:55:10,948 \tSource:     what does it mean to purify water ?\n",
            "2019-10-12 16:55:10,948 \tReference:  wat beteken dit om water te suiwer ?\n",
            "2019-10-12 16:55:10,948 \tHypothesis: wat beteken dit om water te suiwer ?\n",
            "2019-10-12 16:55:10,949 Example #2\n",
            "2019-10-12 16:55:10,949 \tSource:     it means to clean water; to remove pollutants from the water .\n",
            "2019-10-12 16:55:10,949 \tReference:  om water te suiwer beteken om besoedeling uit water te verwyder .\n",
            "2019-10-12 16:55:10,949 \tHypothesis: dit beteken om skoon skoon te suiwer .\n",
            "2019-10-12 16:55:10,949 Example #3\n",
            "2019-10-12 16:55:10,949 \tSource:     what is clean water ?\n",
            "2019-10-12 16:55:10,949 \tReference:  wat is skoon water ?\n",
            "2019-10-12 16:55:10,949 \tHypothesis: wat is skoon ?\n",
            "2019-10-12 16:55:10,949 Validation result at epoch  40, step     1500: bleu:  17.04, loss: 22900.8672, ppl:  27.1340, duration: 104.4460s\n",
            "2019-10-12 16:55:33,926 Epoch  40: total training loss 29.62\n",
            "2019-10-12 16:55:33,926 EPOCH 41\n",
            "2019-10-12 16:56:08,653 Epoch  41 Step:     1550 Batch Loss:     0.592781 Tokens per Sec:     2555, Lr: 0.000561\n",
            "2019-10-12 16:56:18,258 Epoch  41: total training loss 28.76\n",
            "2019-10-12 16:56:18,258 EPOCH 42\n",
            "2019-10-12 16:57:02,873 Epoch  42: total training loss 26.91\n",
            "2019-10-12 16:57:02,873 EPOCH 43\n",
            "2019-10-12 16:57:07,239 Epoch  43 Step:     1600 Batch Loss:     0.668454 Tokens per Sec:     1856, Lr: 0.000552\n",
            "2019-10-12 16:57:46,843 Epoch  43: total training loss 24.07\n",
            "2019-10-12 16:57:46,843 EPOCH 44\n",
            "2019-10-12 16:58:05,951 Epoch  44 Step:     1650 Batch Loss:     0.893992 Tokens per Sec:     2417, Lr: 0.000544\n",
            "2019-10-12 16:58:30,904 Epoch  44: total training loss 24.59\n",
            "2019-10-12 16:58:30,905 EPOCH 45\n",
            "2019-10-12 16:59:04,750 Epoch  45 Step:     1700 Batch Loss:     0.445778 Tokens per Sec:     2422, Lr: 0.000536\n",
            "2019-10-12 16:59:15,565 Epoch  45: total training loss 23.14\n",
            "2019-10-12 16:59:15,565 EPOCH 46\n",
            "2019-10-12 16:59:59,706 Epoch  46: total training loss 21.27\n",
            "2019-10-12 16:59:59,706 EPOCH 47\n",
            "2019-10-12 17:00:03,280 Epoch  47 Step:     1750 Batch Loss:     0.667498 Tokens per Sec:     2566, Lr: 0.000528\n",
            "2019-10-12 17:00:43,817 Epoch  47: total training loss 20.37\n",
            "2019-10-12 17:00:43,818 EPOCH 48\n",
            "2019-10-12 17:01:00,876 Epoch  48 Step:     1800 Batch Loss:     0.309361 Tokens per Sec:     2703, Lr: 0.000521\n",
            "2019-10-12 17:01:27,732 Epoch  48: total training loss 18.82\n",
            "2019-10-12 17:01:27,732 EPOCH 49\n",
            "2019-10-12 17:01:59,793 Epoch  49 Step:     1850 Batch Loss:     0.322012 Tokens per Sec:     2442, Lr: 0.000514\n",
            "2019-10-12 17:02:11,940 Epoch  49: total training loss 17.56\n",
            "2019-10-12 17:02:11,940 EPOCH 50\n",
            "2019-10-12 17:02:56,280 Epoch  50: total training loss 17.69\n",
            "2019-10-12 17:02:56,281 EPOCH 51\n",
            "2019-10-12 17:02:57,663 Epoch  51 Step:     1900 Batch Loss:     0.616119 Tokens per Sec:     3558, Lr: 0.000507\n",
            "2019-10-12 17:03:41,424 Epoch  51: total training loss 17.46\n",
            "2019-10-12 17:03:41,424 EPOCH 52\n",
            "2019-10-12 17:03:55,034 Epoch  52 Step:     1950 Batch Loss:     0.675091 Tokens per Sec:     2258, Lr: 0.000500\n",
            "2019-10-12 17:04:26,419 Epoch  52: total training loss 16.69\n",
            "2019-10-12 17:04:26,420 EPOCH 53\n",
            "2019-10-12 17:04:53,052 Epoch  53 Step:     2000 Batch Loss:     0.352030 Tokens per Sec:     2446, Lr: 0.000494\n",
            "2019-10-12 17:06:35,907 Hooray! New best validation result [eval_metric]!\n",
            "2019-10-12 17:06:35,907 Saving new checkpoint.\n",
            "2019-10-12 17:06:37,479 Example #0\n",
            "2019-10-12 17:06:37,479 \tSource:     resources about water\n",
            "2019-10-12 17:06:37,479 \tReference:  hulpbronne oor water\n",
            "2019-10-12 17:06:37,479 \tHypothesis: bronne oor water\n",
            "2019-10-12 17:06:37,479 Example #1\n",
            "2019-10-12 17:06:37,479 \tSource:     what does it mean to purify water ?\n",
            "2019-10-12 17:06:37,480 \tReference:  wat beteken dit om water te suiwer ?\n",
            "2019-10-12 17:06:37,480 \tHypothesis: wat beteken dit om water te suiwer ?\n",
            "2019-10-12 17:06:37,480 Example #2\n",
            "2019-10-12 17:06:37,480 \tSource:     it means to clean water; to remove pollutants from the water .\n",
            "2019-10-12 17:06:37,480 \tReference:  om water te suiwer beteken om besoedeling uit water te verwyder .\n",
            "2019-10-12 17:06:37,480 \tHypothesis: dit beteken om skoon water te skoon water om die besoedelende stowwe te verwyder .\n",
            "2019-10-12 17:06:37,480 Example #3\n",
            "2019-10-12 17:06:37,481 \tSource:     what is clean water ?\n",
            "2019-10-12 17:06:37,481 \tReference:  wat is skoon water ?\n",
            "2019-10-12 17:06:37,481 \tHypothesis: wat is water skoon ?\n",
            "2019-10-12 17:06:37,481 Validation result at epoch  53, step     2000: bleu:  17.21, loss: 24123.1777, ppl:  32.3613, duration: 104.4287s\n",
            "2019-10-12 17:06:54,897 Epoch  53: total training loss 18.56\n",
            "2019-10-12 17:06:54,897 EPOCH 54\n",
            "2019-10-12 17:07:35,539 Epoch  54 Step:     2050 Batch Loss:     0.561110 Tokens per Sec:     2523, Lr: 0.000488\n",
            "2019-10-12 17:07:39,245 Epoch  54: total training loss 17.73\n",
            "2019-10-12 17:07:39,245 EPOCH 55\n",
            "2019-10-12 17:08:23,089 Epoch  55: total training loss 15.37\n",
            "2019-10-12 17:08:23,089 EPOCH 56\n",
            "2019-10-12 17:08:33,535 Epoch  56 Step:     2100 Batch Loss:     0.561460 Tokens per Sec:     2443, Lr: 0.000482\n",
            "2019-10-12 17:09:07,135 Epoch  56: total training loss 14.34\n",
            "2019-10-12 17:09:07,135 EPOCH 57\n",
            "2019-10-12 17:09:32,621 Epoch  57 Step:     2150 Batch Loss:     0.454353 Tokens per Sec:     2658, Lr: 0.000477\n",
            "2019-10-12 17:09:51,086 Epoch  57: total training loss 12.75\n",
            "2019-10-12 17:09:51,087 EPOCH 58\n",
            "2019-10-12 17:10:30,114 Epoch  58 Step:     2200 Batch Loss:     0.293787 Tokens per Sec:     2577, Lr: 0.000471\n",
            "2019-10-12 17:10:35,049 Epoch  58: total training loss 12.47\n",
            "2019-10-12 17:10:35,049 EPOCH 59\n",
            "2019-10-12 17:11:19,296 Epoch  59: total training loss 12.31\n",
            "2019-10-12 17:11:19,296 EPOCH 60\n",
            "2019-10-12 17:11:27,311 Epoch  60 Step:     2250 Batch Loss:     0.331783 Tokens per Sec:     2508, Lr: 0.000466\n",
            "2019-10-12 17:12:03,002 Epoch  60: total training loss 11.84\n",
            "2019-10-12 17:12:03,003 EPOCH 61\n",
            "2019-10-12 17:12:27,500 Epoch  61 Step:     2300 Batch Loss:     0.355591 Tokens per Sec:     2759, Lr: 0.000461\n",
            "2019-10-12 17:12:47,571 Epoch  61: total training loss 11.31\n",
            "2019-10-12 17:12:47,571 EPOCH 62\n",
            "2019-10-12 17:13:24,697 Epoch  62 Step:     2350 Batch Loss:     0.338809 Tokens per Sec:     2497, Lr: 0.000456\n",
            "2019-10-12 17:13:31,951 Epoch  62: total training loss 11.09\n",
            "2019-10-12 17:13:31,951 EPOCH 63\n",
            "2019-10-12 17:14:16,169 Epoch  63: total training loss 10.54\n",
            "2019-10-12 17:14:16,170 EPOCH 64\n",
            "2019-10-12 17:14:23,962 Epoch  64 Step:     2400 Batch Loss:     0.281328 Tokens per Sec:     3343, Lr: 0.000451\n",
            "2019-10-12 17:15:00,673 Epoch  64: total training loss 11.28\n",
            "2019-10-12 17:15:00,673 EPOCH 65\n",
            "2019-10-12 17:15:21,252 Epoch  65 Step:     2450 Batch Loss:     0.240633 Tokens per Sec:     2432, Lr: 0.000446\n",
            "2019-10-12 17:15:45,053 Epoch  65: total training loss 10.81\n",
            "2019-10-12 17:15:45,053 EPOCH 66\n",
            "2019-10-12 17:16:19,425 Epoch  66 Step:     2500 Batch Loss:     0.209775 Tokens per Sec:     2534, Lr: 0.000442\n",
            "2019-10-12 17:18:02,303 Hooray! New best validation result [eval_metric]!\n",
            "2019-10-12 17:18:02,303 Saving new checkpoint.\n",
            "2019-10-12 17:18:03,977 Example #0\n",
            "2019-10-12 17:18:03,978 \tSource:     resources about water\n",
            "2019-10-12 17:18:03,978 \tReference:  hulpbronne oor water\n",
            "2019-10-12 17:18:03,978 \tHypothesis: bronne van water\n",
            "2019-10-12 17:18:03,978 Example #1\n",
            "2019-10-12 17:18:03,978 \tSource:     what does it mean to purify water ?\n",
            "2019-10-12 17:18:03,978 \tReference:  wat beteken dit om water te suiwer ?\n",
            "2019-10-12 17:18:03,978 \tHypothesis: wat beteken dit om water te suiwer ?\n",
            "2019-10-12 17:18:03,979 Example #2\n",
            "2019-10-12 17:18:03,979 \tSource:     it means to clean water; to remove pollutants from the water .\n",
            "2019-10-12 17:18:03,979 \tReference:  om water te suiwer beteken om besoedeling uit water te verwyder .\n",
            "2019-10-12 17:18:03,979 \tHypothesis: dit beteken om skoon te skoon water verwyder om van die besoedelende stowwe te verwyder .\n",
            "2019-10-12 17:18:03,979 Example #3\n",
            "2019-10-12 17:18:03,979 \tSource:     what is clean water ?\n",
            "2019-10-12 17:18:03,979 \tReference:  wat is skoon water ?\n",
            "2019-10-12 17:18:03,979 \tHypothesis: wat is skoon ?\n",
            "2019-10-12 17:18:03,980 Validation result at epoch  66, step     2500: bleu:  18.17, loss: 23582.6387, ppl:  29.9358, duration: 104.5547s\n",
            "2019-10-12 17:18:13,655 Epoch  66: total training loss 10.34\n",
            "2019-10-12 17:18:13,656 EPOCH 67\n",
            "2019-10-12 17:18:57,851 Epoch  67: total training loss 9.82\n",
            "2019-10-12 17:18:57,851 EPOCH 68\n",
            "2019-10-12 17:19:02,143 Epoch  68 Step:     2550 Batch Loss:     0.154689 Tokens per Sec:     1705, Lr: 0.000438\n",
            "2019-10-12 17:19:41,514 Epoch  68: total training loss 9.49\n",
            "2019-10-12 17:19:41,514 EPOCH 69\n",
            "2019-10-12 17:20:00,741 Epoch  69 Step:     2600 Batch Loss:     0.333186 Tokens per Sec:     2608, Lr: 0.000433\n",
            "2019-10-12 17:20:26,481 Epoch  69: total training loss 10.82\n",
            "2019-10-12 17:20:26,481 EPOCH 70\n",
            "2019-10-12 17:20:59,148 Epoch  70 Step:     2650 Batch Loss:     0.212004 Tokens per Sec:     2507, Lr: 0.000429\n",
            "2019-10-12 17:21:10,280 Epoch  70: total training loss 9.24\n",
            "2019-10-12 17:21:10,280 EPOCH 71\n",
            "2019-10-12 17:21:54,400 Epoch  71: total training loss 9.03\n",
            "2019-10-12 17:21:54,400 EPOCH 72\n",
            "2019-10-12 17:21:57,853 Epoch  72 Step:     2700 Batch Loss:     0.170961 Tokens per Sec:     2512, Lr: 0.000425\n",
            "2019-10-12 17:22:38,126 Epoch  72: total training loss 8.93\n",
            "2019-10-12 17:22:38,126 EPOCH 73\n",
            "2019-10-12 17:22:56,305 Epoch  73 Step:     2750 Batch Loss:     0.158216 Tokens per Sec:     2565, Lr: 0.000421\n",
            "2019-10-12 17:23:21,576 Epoch  73: total training loss 8.52\n",
            "2019-10-12 17:23:21,576 EPOCH 74\n",
            "2019-10-12 17:23:54,763 Epoch  74 Step:     2800 Batch Loss:     0.256563 Tokens per Sec:     2516, Lr: 0.000418\n",
            "2019-10-12 17:24:06,449 Epoch  74: total training loss 8.64\n",
            "2019-10-12 17:24:06,449 EPOCH 75\n",
            "2019-10-12 17:24:50,263 Epoch  75: total training loss 8.26\n",
            "2019-10-12 17:24:50,263 EPOCH 76\n",
            "2019-10-12 17:24:52,434 Epoch  76 Step:     2850 Batch Loss:     0.157216 Tokens per Sec:     1741, Lr: 0.000414\n",
            "2019-10-12 17:25:35,069 Epoch  76: total training loss 8.09\n",
            "2019-10-12 17:25:35,069 EPOCH 77\n",
            "2019-10-12 17:25:51,837 Epoch  77 Step:     2900 Batch Loss:     0.272253 Tokens per Sec:     2569, Lr: 0.000410\n",
            "2019-10-12 17:26:19,298 Epoch  77: total training loss 7.99\n",
            "2019-10-12 17:26:19,298 EPOCH 78\n",
            "2019-10-12 17:26:49,454 Epoch  78 Step:     2950 Batch Loss:     0.166586 Tokens per Sec:     2468, Lr: 0.000407\n",
            "2019-10-12 17:27:03,235 Epoch  78: total training loss 7.65\n",
            "2019-10-12 17:27:03,235 EPOCH 79\n",
            "2019-10-12 17:27:46,793 Epoch  79: total training loss 7.88\n",
            "2019-10-12 17:27:46,793 EPOCH 80\n",
            "2019-10-12 17:27:48,063 Epoch  80 Step:     3000 Batch Loss:     0.237017 Tokens per Sec:     2918, Lr: 0.000403\n",
            "2019-10-12 17:29:30,981 Hooray! New best validation result [eval_metric]!\n",
            "2019-10-12 17:29:30,981 Saving new checkpoint.\n",
            "2019-10-12 17:29:32,581 Example #0\n",
            "2019-10-12 17:29:32,582 \tSource:     resources about water\n",
            "2019-10-12 17:29:32,582 \tReference:  hulpbronne oor water\n",
            "2019-10-12 17:29:32,582 \tHypothesis: bronne van water\n",
            "2019-10-12 17:29:32,582 Example #1\n",
            "2019-10-12 17:29:32,582 \tSource:     what does it mean to purify water ?\n",
            "2019-10-12 17:29:32,582 \tReference:  wat beteken dit om water te suiwer ?\n",
            "2019-10-12 17:29:32,583 \tHypothesis: wat beteken dit om water te suiwer ?\n",
            "2019-10-12 17:29:32,583 Example #2\n",
            "2019-10-12 17:29:32,583 \tSource:     it means to clean water; to remove pollutants from the water .\n",
            "2019-10-12 17:29:32,583 \tReference:  om water te suiwer beteken om besoedeling uit water te verwyder .\n",
            "2019-10-12 17:29:32,583 \tHypothesis: dit beteken om skoon water te verwyder om die besoedelende stowwe te verwyder .\n",
            "2019-10-12 17:29:32,583 Example #3\n",
            "2019-10-12 17:29:32,583 \tSource:     what is clean water ?\n",
            "2019-10-12 17:29:32,584 \tReference:  wat is skoon water ?\n",
            "2019-10-12 17:29:32,584 \tHypothesis: wat is skoon water ?\n",
            "2019-10-12 17:29:32,584 Validation result at epoch  80, step     3000: bleu:  19.40, loss: 23164.7363, ppl:  28.1859, duration: 104.5209s\n",
            "2019-10-12 17:30:14,863 Epoch  80: total training loss 7.62\n",
            "2019-10-12 17:30:14,864 EPOCH 81\n",
            "2019-10-12 17:30:31,126 Epoch  81 Step:     3050 Batch Loss:     0.174892 Tokens per Sec:     2383, Lr: 0.000400\n",
            "2019-10-12 17:30:59,328 Epoch  81: total training loss 8.45\n",
            "2019-10-12 17:30:59,328 EPOCH 82\n",
            "2019-10-12 17:31:30,402 Epoch  82 Step:     3100 Batch Loss:     0.218997 Tokens per Sec:     2847, Lr: 0.000397\n",
            "2019-10-12 17:31:43,453 Epoch  82: total training loss 8.23\n",
            "2019-10-12 17:31:43,453 EPOCH 83\n",
            "2019-10-12 17:32:27,726 Epoch  83 Step:     3150 Batch Loss:     0.164823 Tokens per Sec:     2557, Lr: 0.000394\n",
            "2019-10-12 17:32:27,726 Epoch  83: total training loss 7.74\n",
            "2019-10-12 17:32:27,726 EPOCH 84\n",
            "2019-10-12 17:33:11,502 Epoch  84: total training loss 7.45\n",
            "2019-10-12 17:33:11,502 EPOCH 85\n",
            "2019-10-12 17:33:26,344 Epoch  85 Step:     3200 Batch Loss:     0.252240 Tokens per Sec:     2957, Lr: 0.000391\n",
            "2019-10-12 17:33:55,577 Epoch  85: total training loss 7.32\n",
            "2019-10-12 17:33:55,577 EPOCH 86\n",
            "2019-10-12 17:34:23,168 Epoch  86 Step:     3250 Batch Loss:     0.212392 Tokens per Sec:     2511, Lr: 0.000388\n",
            "2019-10-12 17:34:39,954 Epoch  86: total training loss 7.32\n",
            "2019-10-12 17:34:39,954 EPOCH 87\n",
            "2019-10-12 17:35:21,842 Epoch  87 Step:     3300 Batch Loss:     0.170707 Tokens per Sec:     2618, Lr: 0.000385\n",
            "2019-10-12 17:35:23,956 Epoch  87: total training loss 7.03\n",
            "2019-10-12 17:35:23,956 EPOCH 88\n",
            "2019-10-12 17:36:07,495 Epoch  88: total training loss 6.63\n",
            "2019-10-12 17:36:07,495 EPOCH 89\n",
            "2019-10-12 17:36:20,624 Epoch  89 Step:     3350 Batch Loss:     0.137976 Tokens per Sec:     2574, Lr: 0.000382\n",
            "2019-10-12 17:36:51,514 Epoch  89: total training loss 6.78\n",
            "2019-10-12 17:36:51,514 EPOCH 90\n",
            "2019-10-12 17:37:19,037 Epoch  90 Step:     3400 Batch Loss:     0.149517 Tokens per Sec:     2628, Lr: 0.000379\n",
            "2019-10-12 17:37:35,880 Epoch  90: total training loss 6.61\n",
            "2019-10-12 17:37:35,880 EPOCH 91\n",
            "2019-10-12 17:38:16,494 Epoch  91 Step:     3450 Batch Loss:     0.152670 Tokens per Sec:     2490, Lr: 0.000376\n",
            "2019-10-12 17:38:20,358 Epoch  91: total training loss 6.58\n",
            "2019-10-12 17:38:20,358 EPOCH 92\n",
            "2019-10-12 17:39:04,045 Epoch  92: total training loss 6.68\n",
            "2019-10-12 17:39:04,045 EPOCH 93\n",
            "2019-10-12 17:39:14,108 Epoch  93 Step:     3500 Batch Loss:     0.228386 Tokens per Sec:     2999, Lr: 0.000374\n",
            "2019-10-12 17:40:57,050 Hooray! New best validation result [eval_metric]!\n",
            "2019-10-12 17:40:57,050 Saving new checkpoint.\n",
            "2019-10-12 17:40:58,491 Example #0\n",
            "2019-10-12 17:40:58,492 \tSource:     resources about water\n",
            "2019-10-12 17:40:58,492 \tReference:  hulpbronne oor water\n",
            "2019-10-12 17:40:58,492 \tHypothesis: bronne van water\n",
            "2019-10-12 17:40:58,492 Example #1\n",
            "2019-10-12 17:40:58,493 \tSource:     what does it mean to purify water ?\n",
            "2019-10-12 17:40:58,493 \tReference:  wat beteken dit om water te suiwer ?\n",
            "2019-10-12 17:40:58,493 \tHypothesis: wat beteken dit om water te suiwer ?\n",
            "2019-10-12 17:40:58,493 Example #2\n",
            "2019-10-12 17:40:58,493 \tSource:     it means to clean water; to remove pollutants from the water .\n",
            "2019-10-12 17:40:58,494 \tReference:  om water te suiwer beteken om besoedeling uit water te verwyder .\n",
            "2019-10-12 17:40:58,494 \tHypothesis: dit beteken om skoon water te verwyder met die besoedelende stowwe van water verwyder .\n",
            "2019-10-12 17:40:58,494 Example #3\n",
            "2019-10-12 17:40:58,494 \tSource:     what is clean water ?\n",
            "2019-10-12 17:40:58,494 \tReference:  wat is skoon water ?\n",
            "2019-10-12 17:40:58,494 \tHypothesis: wat is skoon water ?\n",
            "2019-10-12 17:40:58,494 Validation result at epoch  93, step     3500: bleu:  19.46, loss: 23084.5352, ppl:  27.8619, duration: 104.3857s\n",
            "2019-10-12 17:41:32,241 Epoch  93: total training loss 6.75\n",
            "2019-10-12 17:41:32,241 EPOCH 94\n",
            "2019-10-12 17:41:55,057 Epoch  94 Step:     3550 Batch Loss:     0.158869 Tokens per Sec:     2354, Lr: 0.000371\n",
            "2019-10-12 17:42:16,325 Epoch  94: total training loss 6.86\n",
            "2019-10-12 17:42:16,326 EPOCH 95\n",
            "2019-10-12 17:42:53,591 Epoch  95 Step:     3600 Batch Loss:     0.143565 Tokens per Sec:     2431, Lr: 0.000368\n",
            "2019-10-12 17:43:01,201 Epoch  95: total training loss 6.29\n",
            "2019-10-12 17:43:01,201 EPOCH 96\n",
            "2019-10-12 17:43:45,247 Epoch  96: total training loss 6.22\n",
            "2019-10-12 17:43:45,248 EPOCH 97\n",
            "2019-10-12 17:43:52,289 Epoch  97 Step:     3650 Batch Loss:     0.185701 Tokens per Sec:     2382, Lr: 0.000366\n",
            "2019-10-12 17:44:28,504 Epoch  97: total training loss 6.30\n",
            "2019-10-12 17:44:28,504 EPOCH 98\n",
            "2019-10-12 17:44:49,340 Epoch  98 Step:     3700 Batch Loss:     0.153850 Tokens per Sec:     2422, Lr: 0.000363\n",
            "2019-10-12 17:45:12,731 Epoch  98: total training loss 5.96\n",
            "2019-10-12 17:45:12,731 EPOCH 99\n",
            "2019-10-12 17:45:49,090 Epoch  99 Step:     3750 Batch Loss:     0.175414 Tokens per Sec:     2568, Lr: 0.000361\n",
            "2019-10-12 17:45:56,723 Epoch  99: total training loss 6.01\n",
            "2019-10-12 17:45:56,724 EPOCH 100\n",
            "2019-10-12 17:46:40,488 Epoch 100: total training loss 5.91\n",
            "2019-10-12 17:46:40,488 EPOCH 101\n",
            "2019-10-12 17:46:46,061 Epoch 101 Step:     3800 Batch Loss:     0.200783 Tokens per Sec:     2031, Lr: 0.000358\n",
            "2019-10-12 17:47:24,639 Epoch 101: total training loss 5.88\n",
            "2019-10-12 17:47:24,640 EPOCH 102\n",
            "2019-10-12 17:47:44,503 Epoch 102 Step:     3850 Batch Loss:     0.125010 Tokens per Sec:     2477, Lr: 0.000356\n",
            "2019-10-12 17:48:09,207 Epoch 102: total training loss 5.88\n",
            "2019-10-12 17:48:09,207 EPOCH 103\n",
            "2019-10-12 17:48:43,049 Epoch 103 Step:     3900 Batch Loss:     0.118563 Tokens per Sec:     2516, Lr: 0.000354\n",
            "2019-10-12 17:48:53,683 Epoch 103: total training loss 5.97\n",
            "2019-10-12 17:48:53,683 EPOCH 104\n",
            "2019-10-12 17:49:37,690 Epoch 104: total training loss 6.07\n",
            "2019-10-12 17:49:37,690 EPOCH 105\n",
            "2019-10-12 17:49:40,083 Epoch 105 Step:     3950 Batch Loss:     0.115281 Tokens per Sec:     2545, Lr: 0.000352\n",
            "2019-10-12 17:50:22,265 Epoch 105: total training loss 6.08\n",
            "2019-10-12 17:50:22,265 EPOCH 106\n",
            "2019-10-12 17:50:36,639 Epoch 106 Step:     4000 Batch Loss:     0.119315 Tokens per Sec:     2307, Lr: 0.000349\n",
            "2019-10-12 17:52:19,522 Example #0\n",
            "2019-10-12 17:52:19,523 \tSource:     resources about water\n",
            "2019-10-12 17:52:19,523 \tReference:  hulpbronne oor water\n",
            "2019-10-12 17:52:19,523 \tHypothesis: bronne van water\n",
            "2019-10-12 17:52:19,523 Example #1\n",
            "2019-10-12 17:52:19,523 \tSource:     what does it mean to purify water ?\n",
            "2019-10-12 17:52:19,523 \tReference:  wat beteken dit om water te suiwer ?\n",
            "2019-10-12 17:52:19,523 \tHypothesis: wat beteken dit om water te suiwer ?\n",
            "2019-10-12 17:52:19,523 Example #2\n",
            "2019-10-12 17:52:19,524 \tSource:     it means to clean water; to remove pollutants from the water .\n",
            "2019-10-12 17:52:19,524 \tReference:  om water te suiwer beteken om besoedeling uit water te verwyder .\n",
            "2019-10-12 17:52:19,524 \tHypothesis: dit beteken om skoon water te verwyder om van die besoedelende stowwe uit te verwyder .\n",
            "2019-10-12 17:52:19,524 Example #3\n",
            "2019-10-12 17:52:19,524 \tSource:     what is clean water ?\n",
            "2019-10-12 17:52:19,524 \tReference:  wat is skoon water ?\n",
            "2019-10-12 17:52:19,524 \tHypothesis: wat is skoon water ?\n",
            "2019-10-12 17:52:19,524 Validation result at epoch 106, step     4000: bleu:  19.10, loss: 23180.0195, ppl:  28.2480, duration: 102.8844s\n",
            "2019-10-12 17:52:49,638 Epoch 106: total training loss 6.27\n",
            "2019-10-12 17:52:49,638 EPOCH 107\n",
            "2019-10-12 17:53:17,852 Epoch 107 Step:     4050 Batch Loss:     0.133647 Tokens per Sec:     2490, Lr: 0.000347\n",
            "2019-10-12 17:53:33,749 Epoch 107: total training loss 5.95\n",
            "2019-10-12 17:53:33,749 EPOCH 108\n",
            "2019-10-12 17:54:15,568 Epoch 108 Step:     4100 Batch Loss:     0.121054 Tokens per Sec:     2582, Lr: 0.000345\n",
            "2019-10-12 17:54:17,884 Epoch 108: total training loss 5.57\n",
            "2019-10-12 17:54:17,884 EPOCH 109\n",
            "2019-10-12 17:55:01,962 Epoch 109: total training loss 5.54\n",
            "2019-10-12 17:55:01,962 EPOCH 110\n",
            "2019-10-12 17:55:14,500 Epoch 110 Step:     4150 Batch Loss:     0.147833 Tokens per Sec:     2726, Lr: 0.000343\n",
            "2019-10-12 17:55:45,977 Epoch 110: total training loss 6.10\n",
            "2019-10-12 17:55:45,977 EPOCH 111\n",
            "2019-10-12 17:56:12,557 Epoch 111 Step:     4200 Batch Loss:     0.191419 Tokens per Sec:     2734, Lr: 0.000341\n",
            "2019-10-12 17:56:30,582 Epoch 111: total training loss 6.05\n",
            "2019-10-12 17:56:30,582 EPOCH 112\n",
            "2019-10-12 17:57:08,465 Epoch 112 Step:     4250 Batch Loss:     0.196317 Tokens per Sec:     2554, Lr: 0.000339\n",
            "2019-10-12 17:57:14,454 Epoch 112: total training loss 5.70\n",
            "2019-10-12 17:57:14,454 EPOCH 113\n",
            "2019-10-12 17:57:58,569 Epoch 113: total training loss 5.56\n",
            "2019-10-12 17:57:58,569 EPOCH 114\n",
            "2019-10-12 17:58:06,313 Epoch 114 Step:     4300 Batch Loss:     0.164063 Tokens per Sec:     2227, Lr: 0.000337\n",
            "2019-10-12 17:58:42,599 Epoch 114: total training loss 5.26\n",
            "2019-10-12 17:58:42,600 EPOCH 115\n",
            "2019-10-12 17:59:05,001 Epoch 115 Step:     4350 Batch Loss:     0.157448 Tokens per Sec:     2368, Lr: 0.000335\n",
            "2019-10-12 17:59:27,233 Epoch 115: total training loss 5.38\n",
            "2019-10-12 17:59:27,233 EPOCH 116\n",
            "2019-10-12 18:00:04,956 Epoch 116 Step:     4400 Batch Loss:     0.115247 Tokens per Sec:     2706, Lr: 0.000333\n",
            "2019-10-12 18:00:10,535 Epoch 116: total training loss 4.90\n",
            "2019-10-12 18:00:10,535 EPOCH 117\n",
            "2019-10-12 18:00:55,166 Epoch 117: total training loss 5.22\n",
            "2019-10-12 18:00:55,166 EPOCH 118\n",
            "2019-10-12 18:01:02,340 Epoch 118 Step:     4450 Batch Loss:     0.132237 Tokens per Sec:     2814, Lr: 0.000331\n",
            "2019-10-12 18:01:38,855 Epoch 118: total training loss 5.24\n",
            "2019-10-12 18:01:38,855 EPOCH 119\n",
            "2019-10-12 18:01:59,341 Epoch 119 Step:     4500 Batch Loss:     0.134408 Tokens per Sec:     2469, Lr: 0.000329\n",
            "2019-10-12 18:03:42,164 Hooray! New best validation result [eval_metric]!\n",
            "2019-10-12 18:03:42,164 Saving new checkpoint.\n",
            "2019-10-12 18:03:43,736 Example #0\n",
            "2019-10-12 18:03:43,736 \tSource:     resources about water\n",
            "2019-10-12 18:03:43,736 \tReference:  hulpbronne oor water\n",
            "2019-10-12 18:03:43,736 \tHypothesis: bronne van water oor water\n",
            "2019-10-12 18:03:43,736 Example #1\n",
            "2019-10-12 18:03:43,736 \tSource:     what does it mean to purify water ?\n",
            "2019-10-12 18:03:43,736 \tReference:  wat beteken dit om water te suiwer ?\n",
            "2019-10-12 18:03:43,736 \tHypothesis: wat beteken dit om water te suiwer ?\n",
            "2019-10-12 18:03:43,736 Example #2\n",
            "2019-10-12 18:03:43,737 \tSource:     it means to clean water; to remove pollutants from the water .\n",
            "2019-10-12 18:03:43,737 \tReference:  om water te suiwer beteken om besoedeling uit water te verwyder .\n",
            "2019-10-12 18:03:43,737 \tHypothesis: dit beteken om skoon te skoon water as die besoedelende stowwe uit die water te verwyder .\n",
            "2019-10-12 18:03:43,737 Example #3\n",
            "2019-10-12 18:03:43,737 \tSource:     what is clean water ?\n",
            "2019-10-12 18:03:43,737 \tReference:  wat is skoon water ?\n",
            "2019-10-12 18:03:43,737 \tHypothesis: wat is skoon water ?\n",
            "2019-10-12 18:03:43,737 Validation result at epoch 119, step     4500: bleu:  20.05, loss: 22994.5508, ppl:  27.5029, duration: 104.3956s\n",
            "2019-10-12 18:04:07,297 Epoch 119: total training loss 5.40\n",
            "2019-10-12 18:04:07,298 EPOCH 120\n",
            "2019-10-12 18:04:42,212 Epoch 120 Step:     4550 Batch Loss:     0.159049 Tokens per Sec:     2420, Lr: 0.000328\n",
            "2019-10-12 18:04:51,058 Epoch 120: total training loss 4.95\n",
            "2019-10-12 18:04:51,059 EPOCH 121\n",
            "2019-10-12 18:05:34,338 Epoch 121: total training loss 4.95\n",
            "2019-10-12 18:05:34,339 EPOCH 122\n",
            "2019-10-12 18:05:41,503 Epoch 122 Step:     4600 Batch Loss:     0.114603 Tokens per Sec:     2773, Lr: 0.000326\n",
            "2019-10-12 18:06:18,883 Epoch 122: total training loss 4.89\n",
            "2019-10-12 18:06:18,883 EPOCH 123\n",
            "2019-10-12 18:06:40,691 Epoch 123 Step:     4650 Batch Loss:     0.131104 Tokens per Sec:     2876, Lr: 0.000324\n",
            "2019-10-12 18:07:03,550 Epoch 123: total training loss 4.92\n",
            "2019-10-12 18:07:03,551 EPOCH 124\n",
            "2019-10-12 18:07:38,582 Epoch 124 Step:     4700 Batch Loss:     0.138700 Tokens per Sec:     2713, Lr: 0.000322\n",
            "2019-10-12 18:07:47,226 Epoch 124: total training loss 5.00\n",
            "2019-10-12 18:07:47,226 EPOCH 125\n",
            "2019-10-12 18:08:30,960 Epoch 125: total training loss 4.77\n",
            "2019-10-12 18:08:30,960 EPOCH 126\n",
            "2019-10-12 18:08:36,904 Epoch 126 Step:     4750 Batch Loss:     0.123805 Tokens per Sec:     2870, Lr: 0.000321\n",
            "2019-10-12 18:09:15,697 Epoch 126: total training loss 5.07\n",
            "2019-10-12 18:09:15,697 EPOCH 127\n",
            "2019-10-12 18:09:33,001 Epoch 127 Step:     4800 Batch Loss:     0.147086 Tokens per Sec:     2774, Lr: 0.000319\n",
            "2019-10-12 18:09:59,534 Epoch 127: total training loss 4.90\n",
            "2019-10-12 18:09:59,534 EPOCH 128\n",
            "2019-10-12 18:10:30,191 Epoch 128 Step:     4850 Batch Loss:     0.136196 Tokens per Sec:     2537, Lr: 0.000317\n",
            "2019-10-12 18:10:43,239 Epoch 128: total training loss 4.81\n",
            "2019-10-12 18:10:43,239 EPOCH 129\n",
            "2019-10-12 18:11:27,175 Epoch 129: total training loss 4.81\n",
            "2019-10-12 18:11:27,175 EPOCH 130\n",
            "2019-10-12 18:11:28,248 Epoch 130 Step:     4900 Batch Loss:     0.099777 Tokens per Sec:     1552, Lr: 0.000316\n",
            "2019-10-12 18:12:10,673 Epoch 130: total training loss 4.67\n",
            "2019-10-12 18:12:10,674 EPOCH 131\n",
            "2019-10-12 18:12:26,346 Epoch 131 Step:     4950 Batch Loss:     0.112898 Tokens per Sec:     2680, Lr: 0.000314\n",
            "2019-10-12 18:12:54,735 Epoch 131: total training loss 4.60\n",
            "2019-10-12 18:12:54,735 EPOCH 132\n",
            "2019-10-12 18:13:23,594 Epoch 132 Step:     5000 Batch Loss:     0.106110 Tokens per Sec:     2487, Lr: 0.000313\n",
            "2019-10-12 18:15:06,433 Example #0\n",
            "2019-10-12 18:15:06,433 \tSource:     resources about water\n",
            "2019-10-12 18:15:06,433 \tReference:  hulpbronne oor water\n",
            "2019-10-12 18:15:06,433 \tHypothesis: bronne van water\n",
            "2019-10-12 18:15:06,434 Example #1\n",
            "2019-10-12 18:15:06,434 \tSource:     what does it mean to purify water ?\n",
            "2019-10-12 18:15:06,434 \tReference:  wat beteken dit om water te suiwer ?\n",
            "2019-10-12 18:15:06,434 \tHypothesis: wat beteken dit om water te suiwer ?\n",
            "2019-10-12 18:15:06,434 Example #2\n",
            "2019-10-12 18:15:06,434 \tSource:     it means to clean water; to remove pollutants from the water .\n",
            "2019-10-12 18:15:06,434 \tReference:  om water te suiwer beteken om besoedeling uit water te verwyder .\n",
            "2019-10-12 18:15:06,435 \tHypothesis: dit beteken om skoon te skoon waterbesoedeling uit te verwyder .\n",
            "2019-10-12 18:15:06,435 Example #3\n",
            "2019-10-12 18:15:06,435 \tSource:     what is clean water ?\n",
            "2019-10-12 18:15:06,435 \tReference:  wat is skoon water ?\n",
            "2019-10-12 18:15:06,435 \tHypothesis: wat is skoon water ?\n",
            "2019-10-12 18:15:06,435 Validation result at epoch 132, step     5000: bleu:  19.67, loss: 22928.5996, ppl:  27.2427, duration: 102.8408s\n",
            "2019-10-12 18:15:22,504 Epoch 132: total training loss 4.63\n",
            "2019-10-12 18:15:22,504 EPOCH 133\n",
            "2019-10-12 18:16:04,090 Epoch 133 Step:     5050 Batch Loss:     0.131274 Tokens per Sec:     2558, Lr: 0.000311\n",
            "2019-10-12 18:16:06,493 Epoch 133: total training loss 4.53\n",
            "2019-10-12 18:16:06,493 EPOCH 134\n",
            "2019-10-12 18:16:50,555 Epoch 134: total training loss 4.46\n",
            "2019-10-12 18:16:50,555 EPOCH 135\n",
            "2019-10-12 18:17:02,661 Epoch 135 Step:     5100 Batch Loss:     0.126018 Tokens per Sec:     2607, Lr: 0.000309\n",
            "2019-10-12 18:17:34,464 Epoch 135: total training loss 4.38\n",
            "2019-10-12 18:17:34,465 EPOCH 136\n",
            "2019-10-12 18:18:00,162 Epoch 136 Step:     5150 Batch Loss:     0.164608 Tokens per Sec:     2538, Lr: 0.000308\n",
            "2019-10-12 18:18:18,294 Epoch 136: total training loss 4.49\n",
            "2019-10-12 18:18:18,294 EPOCH 137\n",
            "2019-10-12 18:18:58,688 Epoch 137 Step:     5200 Batch Loss:     0.108804 Tokens per Sec:     2534, Lr: 0.000306\n",
            "2019-10-12 18:19:02,403 Epoch 137: total training loss 4.41\n",
            "2019-10-12 18:19:02,403 EPOCH 138\n",
            "2019-10-12 18:19:46,698 Epoch 138: total training loss 4.36\n",
            "2019-10-12 18:19:46,698 EPOCH 139\n",
            "2019-10-12 18:19:57,207 Epoch 139 Step:     5250 Batch Loss:     0.104978 Tokens per Sec:     2468, Lr: 0.000305\n",
            "2019-10-12 18:20:30,766 Epoch 139: total training loss 4.53\n",
            "2019-10-12 18:20:30,767 EPOCH 140\n",
            "2019-10-12 18:20:55,426 Epoch 140 Step:     5300 Batch Loss:     0.123745 Tokens per Sec:     2642, Lr: 0.000304\n",
            "2019-10-12 18:21:14,348 Epoch 140: total training loss 4.45\n",
            "2019-10-12 18:21:14,348 EPOCH 141\n",
            "2019-10-12 18:21:53,397 Epoch 141 Step:     5350 Batch Loss:     0.097444 Tokens per Sec:     2637, Lr: 0.000302\n",
            "2019-10-12 18:21:58,011 Epoch 141: total training loss 4.23\n",
            "2019-10-12 18:21:58,012 EPOCH 142\n",
            "2019-10-12 18:22:42,745 Epoch 142: total training loss 4.25\n",
            "2019-10-12 18:22:42,745 EPOCH 143\n",
            "2019-10-12 18:22:52,343 Epoch 143 Step:     5400 Batch Loss:     0.119980 Tokens per Sec:     2617, Lr: 0.000301\n",
            "2019-10-12 18:23:26,405 Epoch 143: total training loss 4.24\n",
            "2019-10-12 18:23:26,405 EPOCH 144\n",
            "2019-10-12 18:23:49,645 Epoch 144 Step:     5450 Batch Loss:     0.101792 Tokens per Sec:     2613, Lr: 0.000299\n",
            "2019-10-12 18:24:10,189 Epoch 144: total training loss 4.37\n",
            "2019-10-12 18:24:10,189 EPOCH 145\n",
            "2019-10-12 18:24:47,628 Epoch 145 Step:     5500 Batch Loss:     0.105144 Tokens per Sec:     2524, Lr: 0.000298\n",
            "2019-10-12 18:26:30,435 Example #0\n",
            "2019-10-12 18:26:30,435 \tSource:     resources about water\n",
            "2019-10-12 18:26:30,435 \tReference:  hulpbronne oor water\n",
            "2019-10-12 18:26:30,436 \tHypothesis: bronne van water\n",
            "2019-10-12 18:26:30,436 Example #1\n",
            "2019-10-12 18:26:30,436 \tSource:     what does it mean to purify water ?\n",
            "2019-10-12 18:26:30,436 \tReference:  wat beteken dit om water te suiwer ?\n",
            "2019-10-12 18:26:30,436 \tHypothesis: wat beteken dit om water te suiwer ?\n",
            "2019-10-12 18:26:30,436 Example #2\n",
            "2019-10-12 18:26:30,436 \tSource:     it means to clean water; to remove pollutants from the water .\n",
            "2019-10-12 18:26:30,437 \tReference:  om water te suiwer beteken om besoedeling uit water te verwyder .\n",
            "2019-10-12 18:26:30,437 \tHypothesis: dit beteken om skoon te maak omdat die besoedelende stowwe uit die water verwyder .\n",
            "2019-10-12 18:26:30,437 Example #3\n",
            "2019-10-12 18:26:30,437 \tSource:     what is clean water ?\n",
            "2019-10-12 18:26:30,437 \tReference:  wat is skoon water ?\n",
            "2019-10-12 18:26:30,437 \tHypothesis: wat is skoon water ?\n",
            "2019-10-12 18:26:30,437 Validation result at epoch 145, step     5500: bleu:  18.71, loss: 22814.3848, ppl:  26.7979, duration: 102.8086s\n",
            "2019-10-12 18:26:37,114 Epoch 145: total training loss 4.29\n",
            "2019-10-12 18:26:37,114 EPOCH 146\n",
            "2019-10-12 18:27:21,401 Epoch 146: total training loss 4.21\n",
            "2019-10-12 18:27:21,401 EPOCH 147\n",
            "2019-10-12 18:27:29,212 Epoch 147 Step:     5550 Batch Loss:     0.108643 Tokens per Sec:     2772, Lr: 0.000297\n",
            "2019-10-12 18:28:05,291 Epoch 147: total training loss 4.23\n",
            "2019-10-12 18:28:05,291 EPOCH 148\n",
            "2019-10-12 18:28:26,338 Epoch 148 Step:     5600 Batch Loss:     0.086152 Tokens per Sec:     2484, Lr: 0.000295\n",
            "2019-10-12 18:28:48,840 Epoch 148: total training loss 4.11\n",
            "2019-10-12 18:28:48,841 EPOCH 149\n",
            "2019-10-12 18:29:25,003 Epoch 149 Step:     5650 Batch Loss:     0.085340 Tokens per Sec:     2705, Lr: 0.000294\n",
            "2019-10-12 18:29:31,800 Epoch 149: total training loss 4.02\n",
            "2019-10-12 18:29:31,800 EPOCH 150\n",
            "2019-10-12 18:30:16,167 Epoch 150: total training loss 4.07\n",
            "2019-10-12 18:30:16,167 EPOCH 151\n",
            "2019-10-12 18:30:23,798 Epoch 151 Step:     5700 Batch Loss:     0.091254 Tokens per Sec:     2521, Lr: 0.000293\n",
            "2019-10-12 18:30:59,963 Epoch 151: total training loss 3.99\n",
            "2019-10-12 18:30:59,963 EPOCH 152\n",
            "2019-10-12 18:31:23,538 Epoch 152 Step:     5750 Batch Loss:     0.106065 Tokens per Sec:     2813, Lr: 0.000291\n",
            "2019-10-12 18:31:44,555 Epoch 152: total training loss 3.99\n",
            "2019-10-12 18:31:44,556 EPOCH 153\n",
            "2019-10-12 18:32:21,317 Epoch 153 Step:     5800 Batch Loss:     0.094301 Tokens per Sec:     2612, Lr: 0.000290\n",
            "2019-10-12 18:32:28,122 Epoch 153: total training loss 3.94\n",
            "2019-10-12 18:32:28,122 EPOCH 154\n",
            "2019-10-12 18:33:11,601 Epoch 154: total training loss 3.92\n",
            "2019-10-12 18:33:11,602 EPOCH 155\n",
            "2019-10-12 18:33:20,246 Epoch 155 Step:     5850 Batch Loss:     0.089254 Tokens per Sec:     2872, Lr: 0.000289\n",
            "2019-10-12 18:33:55,796 Epoch 155: total training loss 4.01\n",
            "2019-10-12 18:33:55,796 EPOCH 156\n",
            "2019-10-12 18:34:19,151 Epoch 156 Step:     5900 Batch Loss:     0.083274 Tokens per Sec:     2791, Lr: 0.000288\n",
            "2019-10-12 18:34:40,440 Epoch 156: total training loss 3.97\n",
            "2019-10-12 18:34:40,440 EPOCH 157\n",
            "2019-10-12 18:35:16,741 Epoch 157 Step:     5950 Batch Loss:     0.085921 Tokens per Sec:     2600, Lr: 0.000286\n",
            "2019-10-12 18:35:24,029 Epoch 157: total training loss 3.82\n",
            "2019-10-12 18:35:24,029 EPOCH 158\n",
            "2019-10-12 18:36:07,953 Epoch 158: total training loss 3.89\n",
            "2019-10-12 18:36:07,953 EPOCH 159\n",
            "2019-10-12 18:36:15,124 Epoch 159 Step:     6000 Batch Loss:     0.089015 Tokens per Sec:     2839, Lr: 0.000285\n",
            "2019-10-12 18:37:57,971 Example #0\n",
            "2019-10-12 18:37:57,972 \tSource:     resources about water\n",
            "2019-10-12 18:37:57,972 \tReference:  hulpbronne oor water\n",
            "2019-10-12 18:37:57,972 \tHypothesis: bronne van water oor water\n",
            "2019-10-12 18:37:57,972 Example #1\n",
            "2019-10-12 18:37:57,972 \tSource:     what does it mean to purify water ?\n",
            "2019-10-12 18:37:57,972 \tReference:  wat beteken dit om water te suiwer ?\n",
            "2019-10-12 18:37:57,972 \tHypothesis: wat beteken dit om water te suiwer ?\n",
            "2019-10-12 18:37:57,973 Example #2\n",
            "2019-10-12 18:37:57,973 \tSource:     it means to clean water; to remove pollutants from the water .\n",
            "2019-10-12 18:37:57,973 \tReference:  om water te suiwer beteken om besoedeling uit water te verwyder .\n",
            "2019-10-12 18:37:57,973 \tHypothesis: dit beteken om skoon te skoon waterbesoedeling uit te verwyder .\n",
            "2019-10-12 18:37:57,973 Example #3\n",
            "2019-10-12 18:37:57,973 \tSource:     what is clean water ?\n",
            "2019-10-12 18:37:57,973 \tReference:  wat is skoon water ?\n",
            "2019-10-12 18:37:57,973 \tHypothesis: wat is skoon ?\n",
            "2019-10-12 18:37:57,974 Validation result at epoch 159, step     6000: bleu:  19.54, loss: 22747.0566, ppl:  26.5391, duration: 102.8493s\n",
            "2019-10-12 18:38:34,446 Epoch 159: total training loss 3.82\n",
            "2019-10-12 18:38:34,447 EPOCH 160\n",
            "2019-10-12 18:38:56,769 Epoch 160 Step:     6050 Batch Loss:     0.093865 Tokens per Sec:     2669, Lr: 0.000284\n",
            "2019-10-12 18:39:18,967 Epoch 160: total training loss 3.91\n",
            "2019-10-12 18:39:18,967 EPOCH 161\n",
            "2019-10-12 18:39:55,444 Epoch 161 Step:     6100 Batch Loss:     0.100295 Tokens per Sec:     2570, Lr: 0.000283\n",
            "2019-10-12 18:40:03,452 Epoch 161: total training loss 3.87\n",
            "2019-10-12 18:40:03,452 EPOCH 162\n",
            "2019-10-12 18:40:46,994 Epoch 162: total training loss 3.83\n",
            "2019-10-12 18:40:46,994 EPOCH 163\n",
            "2019-10-12 18:40:54,301 Epoch 163 Step:     6150 Batch Loss:     0.098249 Tokens per Sec:     2764, Lr: 0.000282\n",
            "2019-10-12 18:41:30,569 Epoch 163: total training loss 3.94\n",
            "2019-10-12 18:41:30,569 EPOCH 164\n",
            "2019-10-12 18:41:54,066 Epoch 164 Step:     6200 Batch Loss:     0.084627 Tokens per Sec:     2806, Lr: 0.000281\n",
            "2019-10-12 18:42:14,606 Epoch 164: total training loss 3.91\n",
            "2019-10-12 18:42:14,606 EPOCH 165\n",
            "2019-10-12 18:42:49,123 Epoch 165 Step:     6250 Batch Loss:     0.123084 Tokens per Sec:     2461, Lr: 0.000280\n",
            "2019-10-12 18:42:58,934 Epoch 165: total training loss 3.97\n",
            "2019-10-12 18:42:58,935 EPOCH 166\n",
            "2019-10-12 18:43:42,096 Epoch 166: total training loss 3.88\n",
            "2019-10-12 18:43:42,096 EPOCH 167\n",
            "2019-10-12 18:43:48,047 Epoch 167 Step:     6300 Batch Loss:     0.080319 Tokens per Sec:     2499, Lr: 0.000278\n",
            "2019-10-12 18:44:26,803 Epoch 167: total training loss 3.75\n",
            "2019-10-12 18:44:26,803 EPOCH 168\n",
            "2019-10-12 18:44:47,585 Epoch 168 Step:     6350 Batch Loss:     0.100967 Tokens per Sec:     2764, Lr: 0.000277\n",
            "2019-10-12 18:45:11,388 Epoch 168: total training loss 3.67\n",
            "2019-10-12 18:45:11,389 EPOCH 169\n",
            "2019-10-12 18:45:44,906 Epoch 169 Step:     6400 Batch Loss:     0.096259 Tokens per Sec:     2521, Lr: 0.000276\n",
            "2019-10-12 18:45:55,788 Epoch 169: total training loss 3.66\n",
            "2019-10-12 18:45:55,788 EPOCH 170\n",
            "2019-10-12 18:46:39,866 Epoch 170: total training loss 3.74\n",
            "2019-10-12 18:46:39,866 EPOCH 171\n",
            "2019-10-12 18:46:43,607 Epoch 171 Step:     6450 Batch Loss:     0.091458 Tokens per Sec:     2681, Lr: 0.000275\n",
            "2019-10-12 18:47:23,773 Epoch 171: total training loss 3.71\n",
            "2019-10-12 18:47:23,773 EPOCH 172\n",
            "2019-10-12 18:47:41,676 Epoch 172 Step:     6500 Batch Loss:     0.089749 Tokens per Sec:     2628, Lr: 0.000274\n",
            "2019-10-12 18:49:24,444 Example #0\n",
            "2019-10-12 18:49:24,444 \tSource:     resources about water\n",
            "2019-10-12 18:49:24,445 \tReference:  hulpbronne oor water\n",
            "2019-10-12 18:49:24,445 \tHypothesis: bronne oor water\n",
            "2019-10-12 18:49:24,445 Example #1\n",
            "2019-10-12 18:49:24,445 \tSource:     what does it mean to purify water ?\n",
            "2019-10-12 18:49:24,445 \tReference:  wat beteken dit om water te suiwer ?\n",
            "2019-10-12 18:49:24,445 \tHypothesis: wat beteken dit om water te suiwer ?\n",
            "2019-10-12 18:49:24,445 Example #2\n",
            "2019-10-12 18:49:24,446 \tSource:     it means to clean water; to remove pollutants from the water .\n",
            "2019-10-12 18:49:24,446 \tReference:  om water te suiwer beteken om besoedeling uit water te verwyder .\n",
            "2019-10-12 18:49:24,446 \tHypothesis: dit beteken om skoon te skoon water om uit die water te verwyder .\n",
            "2019-10-12 18:49:24,446 Example #3\n",
            "2019-10-12 18:49:24,446 \tSource:     what is clean water ?\n",
            "2019-10-12 18:49:24,446 \tReference:  wat is skoon water ?\n",
            "2019-10-12 18:49:24,446 \tHypothesis: wat is skoon water ?\n",
            "2019-10-12 18:49:24,446 Validation result at epoch 172, step     6500: bleu:  19.13, loss: 22670.4238, ppl:  26.2476, duration: 102.7704s\n",
            "2019-10-12 18:49:50,293 Epoch 172: total training loss 3.57\n",
            "2019-10-12 18:49:50,293 EPOCH 173\n",
            "2019-10-12 18:50:21,781 Epoch 173 Step:     6550 Batch Loss:     0.079830 Tokens per Sec:     2439, Lr: 0.000273\n",
            "2019-10-12 18:50:34,267 Epoch 173: total training loss 3.59\n",
            "2019-10-12 18:50:34,267 EPOCH 174\n",
            "2019-10-12 18:51:18,657 Epoch 174: total training loss 3.58\n",
            "2019-10-12 18:51:18,658 EPOCH 175\n",
            "2019-10-12 18:51:21,008 Epoch 175 Step:     6600 Batch Loss:     0.118011 Tokens per Sec:     2138, Lr: 0.000272\n",
            "2019-10-12 18:52:02,802 Epoch 175: total training loss 3.56\n",
            "2019-10-12 18:52:02,802 EPOCH 176\n",
            "2019-10-12 18:52:19,163 Epoch 176 Step:     6650 Batch Loss:     0.083052 Tokens per Sec:     2457, Lr: 0.000271\n",
            "2019-10-12 18:52:47,825 Epoch 176: total training loss 3.63\n",
            "2019-10-12 18:52:47,825 EPOCH 177\n",
            "2019-10-12 18:53:16,742 Epoch 177 Step:     6700 Batch Loss:     0.107319 Tokens per Sec:     2408, Lr: 0.000270\n",
            "2019-10-12 18:53:32,439 Epoch 177: total training loss 3.56\n",
            "2019-10-12 18:53:32,439 EPOCH 178\n",
            "2019-10-12 18:54:15,142 Epoch 178 Step:     6750 Batch Loss:     0.090097 Tokens per Sec:     2603, Lr: 0.000269\n",
            "2019-10-12 18:54:16,210 Epoch 178: total training loss 3.44\n",
            "2019-10-12 18:54:16,210 EPOCH 179\n",
            "2019-10-12 18:55:00,115 Epoch 179: total training loss 3.48\n",
            "2019-10-12 18:55:00,115 EPOCH 180\n",
            "2019-10-12 18:55:10,747 Epoch 180 Step:     6800 Batch Loss:     0.067829 Tokens per Sec:     2303, Lr: 0.000268\n",
            "2019-10-12 18:55:44,203 Epoch 180: total training loss 3.62\n",
            "2019-10-12 18:55:44,203 EPOCH 181\n",
            "2019-10-12 18:56:10,370 Epoch 181 Step:     6850 Batch Loss:     0.071262 Tokens per Sec:     2717, Lr: 0.000267\n",
            "2019-10-12 18:56:28,675 Epoch 181: total training loss 3.52\n",
            "2019-10-12 18:56:28,676 EPOCH 182\n",
            "2019-10-12 18:57:07,518 Epoch 182 Step:     6900 Batch Loss:     0.092488 Tokens per Sec:     2542, Lr: 0.000266\n",
            "2019-10-12 18:57:13,362 Epoch 182: total training loss 3.63\n",
            "2019-10-12 18:57:13,363 EPOCH 183\n",
            "2019-10-12 18:57:57,568 Epoch 183: total training loss 3.38\n",
            "2019-10-12 18:57:57,568 EPOCH 184\n",
            "2019-10-12 18:58:07,264 Epoch 184 Step:     6950 Batch Loss:     0.089769 Tokens per Sec:     2543, Lr: 0.000265\n",
            "2019-10-12 18:58:41,307 Epoch 184: total training loss 3.33\n",
            "2019-10-12 18:58:41,307 EPOCH 185\n",
            "2019-10-12 18:59:06,900 Epoch 185 Step:     7000 Batch Loss:     0.090459 Tokens per Sec:     2700, Lr: 0.000264\n",
            "2019-10-12 19:00:49,775 Example #0\n",
            "2019-10-12 19:00:49,775 \tSource:     resources about water\n",
            "2019-10-12 19:00:49,775 \tReference:  hulpbronne oor water\n",
            "2019-10-12 19:00:49,775 \tHypothesis: bronne oor water\n",
            "2019-10-12 19:00:49,775 Example #1\n",
            "2019-10-12 19:00:49,776 \tSource:     what does it mean to purify water ?\n",
            "2019-10-12 19:00:49,776 \tReference:  wat beteken dit om water te suiwer ?\n",
            "2019-10-12 19:00:49,776 \tHypothesis: wat beteken dit om water te suiwer ?\n",
            "2019-10-12 19:00:49,776 Example #2\n",
            "2019-10-12 19:00:49,776 \tSource:     it means to clean water; to remove pollutants from the water .\n",
            "2019-10-12 19:00:49,776 \tReference:  om water te suiwer beteken om besoedeling uit water te verwyder .\n",
            "2019-10-12 19:00:49,776 \tHypothesis: dit beteken om skoon water te verwyder om die besoedelende stowwe uit water te verwyder .\n",
            "2019-10-12 19:00:49,776 Example #3\n",
            "2019-10-12 19:00:49,777 \tSource:     what is clean water ?\n",
            "2019-10-12 19:00:49,777 \tReference:  wat is skoon water ?\n",
            "2019-10-12 19:00:49,777 \tHypothesis: wat is water skoon ?\n",
            "2019-10-12 19:00:49,777 Validation result at epoch 185, step     7000: bleu:  19.77, loss: 22537.8945, ppl:  25.7509, duration: 102.8766s\n",
            "2019-10-12 19:01:08,201 Epoch 185: total training loss 3.30\n",
            "2019-10-12 19:01:08,201 EPOCH 186\n",
            "2019-10-12 19:01:48,089 Epoch 186 Step:     7050 Batch Loss:     0.095937 Tokens per Sec:     2516, Lr: 0.000263\n",
            "2019-10-12 19:01:52,961 Epoch 186: total training loss 3.36\n",
            "2019-10-12 19:01:52,961 EPOCH 187\n",
            "2019-10-12 19:02:36,490 Epoch 187: total training loss 3.51\n",
            "2019-10-12 19:02:36,490 EPOCH 188\n",
            "2019-10-12 19:02:46,127 Epoch 188 Step:     7100 Batch Loss:     0.073996 Tokens per Sec:     2779, Lr: 0.000262\n",
            "2019-10-12 19:03:20,283 Epoch 188: total training loss 3.47\n",
            "2019-10-12 19:03:20,283 EPOCH 189\n",
            "2019-10-12 19:03:42,065 Epoch 189 Step:     7150 Batch Loss:     0.084476 Tokens per Sec:     2634, Lr: 0.000261\n",
            "2019-10-12 19:04:03,694 Epoch 189: total training loss 3.44\n",
            "2019-10-12 19:04:03,694 EPOCH 190\n",
            "2019-10-12 19:04:41,273 Epoch 190 Step:     7200 Batch Loss:     0.064450 Tokens per Sec:     2576, Lr: 0.000260\n",
            "2019-10-12 19:04:47,254 Epoch 190: total training loss 3.27\n",
            "2019-10-12 19:04:47,254 EPOCH 191\n",
            "2019-10-12 19:05:30,909 Epoch 191: total training loss 3.32\n",
            "2019-10-12 19:05:30,909 EPOCH 192\n",
            "2019-10-12 19:05:39,145 Epoch 192 Step:     7250 Batch Loss:     0.105951 Tokens per Sec:     2615, Lr: 0.000260\n",
            "2019-10-12 19:06:14,300 Epoch 192: total training loss 3.26\n",
            "2019-10-12 19:06:14,300 EPOCH 193\n",
            "2019-10-12 19:06:38,127 Epoch 193 Step:     7300 Batch Loss:     0.102511 Tokens per Sec:     2594, Lr: 0.000259\n",
            "2019-10-12 19:06:58,195 Epoch 193: total training loss 3.32\n",
            "2019-10-12 19:06:58,196 EPOCH 194\n",
            "2019-10-12 19:07:36,019 Epoch 194 Step:     7350 Batch Loss:     0.070585 Tokens per Sec:     2533, Lr: 0.000258\n",
            "2019-10-12 19:07:42,226 Epoch 194: total training loss 3.49\n",
            "2019-10-12 19:07:42,227 EPOCH 195\n",
            "2019-10-12 19:08:25,736 Epoch 195: total training loss 3.32\n",
            "2019-10-12 19:08:25,736 EPOCH 196\n",
            "2019-10-12 19:08:34,366 Epoch 196 Step:     7400 Batch Loss:     0.075225 Tokens per Sec:     2740, Lr: 0.000257\n",
            "2019-10-12 19:09:09,992 Epoch 196: total training loss 3.36\n",
            "2019-10-12 19:09:09,992 EPOCH 197\n",
            "2019-10-12 19:09:31,782 Epoch 197 Step:     7450 Batch Loss:     0.108361 Tokens per Sec:     2444, Lr: 0.000256\n",
            "2019-10-12 19:09:54,459 Epoch 197: total training loss 3.30\n",
            "2019-10-12 19:09:54,460 EPOCH 198\n",
            "2019-10-12 19:10:29,900 Epoch 198 Step:     7500 Batch Loss:     0.096420 Tokens per Sec:     2580, Lr: 0.000255\n",
            "2019-10-12 19:12:12,762 Example #0\n",
            "2019-10-12 19:12:12,763 \tSource:     resources about water\n",
            "2019-10-12 19:12:12,763 \tReference:  hulpbronne oor water\n",
            "2019-10-12 19:12:12,763 \tHypothesis: bronne van water\n",
            "2019-10-12 19:12:12,763 Example #1\n",
            "2019-10-12 19:12:12,763 \tSource:     what does it mean to purify water ?\n",
            "2019-10-12 19:12:12,763 \tReference:  wat beteken dit om water te suiwer ?\n",
            "2019-10-12 19:12:12,764 \tHypothesis: wat beteken dit om water te suiwer ?\n",
            "2019-10-12 19:12:12,764 Example #2\n",
            "2019-10-12 19:12:12,764 \tSource:     it means to clean water; to remove pollutants from the water .\n",
            "2019-10-12 19:12:12,764 \tReference:  om water te suiwer beteken om besoedeling uit water te verwyder .\n",
            "2019-10-12 19:12:12,764 \tHypothesis: dit beteken om skoon te skoon waterbesoedeling uit die water te verwyder .\n",
            "2019-10-12 19:12:12,764 Example #3\n",
            "2019-10-12 19:12:12,764 \tSource:     what is clean water ?\n",
            "2019-10-12 19:12:12,764 \tReference:  wat is skoon water ?\n",
            "2019-10-12 19:12:12,765 \tHypothesis: wat is skoon ?\n",
            "2019-10-12 19:12:12,765 Validation result at epoch 198, step     7500: bleu:  20.05, loss: 22478.7402, ppl:  25.5323, duration: 102.8642s\n",
            "2019-10-12 19:12:21,030 Epoch 198: total training loss 3.23\n",
            "2019-10-12 19:12:21,030 EPOCH 199\n",
            "2019-10-12 19:13:05,219 Epoch 199: total training loss 3.35\n",
            "2019-10-12 19:13:05,219 EPOCH 200\n",
            "2019-10-12 19:13:10,133 Epoch 200 Step:     7550 Batch Loss:     0.081487 Tokens per Sec:     2832, Lr: 0.000254\n",
            "2019-10-12 19:13:48,419 Epoch 200: total training loss 3.27\n",
            "2019-10-12 19:13:48,420 EPOCH 201\n",
            "2019-10-12 19:14:07,453 Epoch 201 Step:     7600 Batch Loss:     0.071485 Tokens per Sec:     2751, Lr: 0.000253\n",
            "2019-10-12 19:14:32,554 Epoch 201: total training loss 3.27\n",
            "2019-10-12 19:14:32,554 EPOCH 202\n",
            "2019-10-12 19:15:05,052 Epoch 202 Step:     7650 Batch Loss:     0.084953 Tokens per Sec:     2614, Lr: 0.000253\n",
            "2019-10-12 19:15:16,573 Epoch 202: total training loss 3.25\n",
            "2019-10-12 19:15:16,573 EPOCH 203\n",
            "2019-10-12 19:16:01,304 Epoch 203: total training loss 3.27\n",
            "2019-10-12 19:16:01,305 EPOCH 204\n",
            "2019-10-12 19:16:02,427 Epoch 204 Step:     7700 Batch Loss:     0.072010 Tokens per Sec:     2024, Lr: 0.000252\n",
            "2019-10-12 19:16:45,317 Epoch 204: total training loss 3.27\n",
            "2019-10-12 19:16:45,317 EPOCH 205\n",
            "2019-10-12 19:17:00,274 Epoch 205 Step:     7750 Batch Loss:     0.061731 Tokens per Sec:     2311, Lr: 0.000251\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 41, in <module>\n",
            "    main()\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 29, in main\n",
            "    train(cfg_file=args.config_path)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 560, in train\n",
            "    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 264, in train_and_validate\n",
            "    batch_loss = self._train_batch(batch, update=update)\n",
            "  File \"/content/joeynmt/joeynmt/training.py\", line 415, in _train_batch\n",
            "    norm_batch_multiply.backward()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 118, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\", line 93, in backward\n",
            "    allow_unreachable=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MBoDS09JM807",
        "colab": {}
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "!mkdir -p \"$gdrive_path/models/${src}${tgt}_transformer/\" # Herman\n",
        "!cp -r joeynmt/models/${src}${tgt}_transformer/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n94wlrCjVc17",
        "outputId": "27dcb26a-4a26-456d-afed-caf05bb6f2f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 500\tLoss: 28996.02539\tPPL: 65.32051\tbleu: 0.74017\tLR: 0.00034939\t*\n",
            "Steps: 1000\tLoss: 22725.31836\tPPL: 26.45606\tbleu: 12.15630\tLR: 0.00069877\t*\n",
            "Steps: 1500\tLoss: 22900.86719\tPPL: 27.13401\tbleu: 17.04406\tLR: 0.00057054\t*\n",
            "Steps: 2000\tLoss: 24123.17773\tPPL: 32.36132\tbleu: 17.20765\tLR: 0.00049411\t*\n",
            "Steps: 2500\tLoss: 23582.63867\tPPL: 29.93578\tbleu: 18.16604\tLR: 0.00044194\t*\n",
            "Steps: 3000\tLoss: 23164.73633\tPPL: 28.18586\tbleu: 19.39783\tLR: 0.00040344\t*\n",
            "Steps: 3500\tLoss: 23084.53516\tPPL: 27.86192\tbleu: 19.46346\tLR: 0.00037351\t*\n",
            "Steps: 4000\tLoss: 23180.01953\tPPL: 28.24801\tbleu: 19.10164\tLR: 0.00034939\t\n",
            "Steps: 4500\tLoss: 22994.55078\tPPL: 27.50288\tbleu: 20.05288\tLR: 0.00032940\t*\n",
            "Steps: 5000\tLoss: 22928.59961\tPPL: 27.24268\tbleu: 19.66884\tLR: 0.00031250\t\n",
            "Steps: 5500\tLoss: 22814.38477\tPPL: 26.79788\tbleu: 18.71092\tLR: 0.00029796\t\n",
            "Steps: 6000\tLoss: 22747.05664\tPPL: 26.53909\tbleu: 19.54311\tLR: 0.00028527\t\n",
            "Steps: 6500\tLoss: 22670.42383\tPPL: 26.24757\tbleu: 19.12990\tLR: 0.00027408\t\n",
            "Steps: 7000\tLoss: 22537.89453\tPPL: 25.75094\tbleu: 19.76692\tLR: 0.00026411\t\n",
            "Steps: 7500\tLoss: 22478.74023\tPPL: 25.53232\tbleu: 20.04524\tLR: 0.00025516\t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "66WhRE9lIhoD",
        "outputId": "e7c7180c-0986-4e87-a8a1-717b4c457c55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Test our model\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-12 19:18:46,910 -  dev bleu:  20.47 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2019-10-12 19:19:20,381 - test bleu:  15.26 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPqSUwVzLLMM",
        "colab_type": "text"
      },
      "source": [
        "## Record\n",
        "\n",
        "After 200 epochs:\n",
        "\n",
        "    Steps: 500\tLoss: 28996.02539\tPPL: 65.32051\tbleu: 0.74017\tLR: 0.00034939\t*\n",
        "    Steps: 1000\tLoss: 22725.31836\tPPL: 26.45606\tbleu: 12.15630\tLR: 0.00069877\t*\n",
        "    Steps: 1500\tLoss: 22900.86719\tPPL: 27.13401\tbleu: 17.04406\tLR: 0.00057054\t*\n",
        "    Steps: 2000\tLoss: 24123.17773\tPPL: 32.36132\tbleu: 17.20765\tLR: 0.00049411\t*\n",
        "    Steps: 2500\tLoss: 23582.63867\tPPL: 29.93578\tbleu: 18.16604\tLR: 0.00044194\t*\n",
        "    Steps: 3000\tLoss: 23164.73633\tPPL: 28.18586\tbleu: 19.39783\tLR: 0.00040344\t*\n",
        "    Steps: 3500\tLoss: 23084.53516\tPPL: 27.86192\tbleu: 19.46346\tLR: 0.00037351\t*\n",
        "    Steps: 4000\tLoss: 23180.01953\tPPL: 28.24801\tbleu: 19.10164\tLR: 0.00034939\t\n",
        "    Steps: 4500\tLoss: 22994.55078\tPPL: 27.50288\tbleu: 20.05288\tLR: 0.00032940\t*\n",
        "    Steps: 5000\tLoss: 22928.59961\tPPL: 27.24268\tbleu: 19.66884\tLR: 0.00031250\t\n",
        "    Steps: 5500\tLoss: 22814.38477\tPPL: 26.79788\tbleu: 18.71092\tLR: 0.00029796\t\n",
        "    Steps: 6000\tLoss: 22747.05664\tPPL: 26.53909\tbleu: 19.54311\tLR: 0.00028527\t\n",
        "    Steps: 6500\tLoss: 22670.42383\tPPL: 26.24757\tbleu: 19.12990\tLR: 0.00027408\t\n",
        "    Steps: 7000\tLoss: 22537.89453\tPPL: 25.75094\tbleu: 19.76692\tLR: 0.00026411\t\n",
        "    Steps: 7500\tLoss: 22478.74023\tPPL: 25.53232\tbleu: 20.04524\tLR: 0.00025516\t"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvFGcTI4aXMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}